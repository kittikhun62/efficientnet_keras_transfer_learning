{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPanKVSMq0h2xbG/hCjlTVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/UNfreeze2Class_h5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "bB2gfuYWczY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpXyZ9k5YIJu",
        "outputId": "701d9416-58bb-40ec-964c-509ab404b456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7mqo2OLAc2pG",
        "outputId": "796ab769-4fe6-4c3c-9b8c-ef566f7019be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fcc26b14-edca-4f38-8346-f80f93344361\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcc26b14-edca-4f38-8346-f80f93344361')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fcc26b14-edca-4f38-8346-f80f93344361 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fcc26b14-edca-4f38-8346-f80f93344361');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "2b4575ad-fdbc-4a8f-c140-86593289c025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Total 837 (delta 0), reused 0 (delta 0), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.85 MiB | 12.97 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3dcc64-740e-48d0-cbb5-e83710082047"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "f3da5829-173c-4b93-b0e1-4fbe861f15b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/new/2Class_3000.h5')\n",
        "\n",
        "from efficientnet.layers import Swish, DropConnect\n",
        "from efficientnet.model import ConvKernalInitializer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'ConvKernalInitializer': ConvKernalInitializer,\n",
        "    'Swish': Swish,\n",
        "    'DropConnect':DropConnect\n",
        "})"
      ],
      "metadata": {
        "id": "MwNk3sNbc2Qx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model \n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/new/2Class_3000.h5')\n",
        "height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "4v4nzKQtJZ4z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_OUk52Yz94T",
        "outputId": "63f667eb-7ca3-45b5-f0c2-7b1042a79a34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UDxvF-b8Jcq1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "     \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znE38DtIJeN-",
        "outputId": "7f8d1733-e0dc-4a7c-e4de-307cf7107b06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply_16\n",
        "# set 'multiply_16' and following layers trainable\n",
        "model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'multiply_16':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False  \n",
        "     "
      ],
      "metadata": {
        "id": "tEaPoHJCJq13"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "xkN8n36elECN",
        "outputId": "2990ba02-12c8-463e-f3de-55b79a809d3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7ERVUfUJsQq",
        "outputId": "e5c2988c-9a45-453f-f30b-71105bfd2fc3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-caa7b37242a8>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 87s 2s/step - loss: 0.5086 - acc: 0.7732 - val_loss: 0.6730 - val_acc: 0.6771\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5381 - acc: 0.7354 - val_loss: 0.6673 - val_acc: 0.6458\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4958 - acc: 0.7629 - val_loss: 0.6547 - val_acc: 0.6667\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5027 - acc: 0.7595 - val_loss: 0.6420 - val_acc: 0.6875\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5227 - acc: 0.7595 - val_loss: 0.6746 - val_acc: 0.6458\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5130 - acc: 0.7405 - val_loss: 0.6661 - val_acc: 0.6458\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5129 - acc: 0.7509 - val_loss: 0.6651 - val_acc: 0.6562\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5100 - acc: 0.7388 - val_loss: 0.6610 - val_acc: 0.6458\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.4870 - acc: 0.7440 - val_loss: 0.6561 - val_acc: 0.6667\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.5216 - acc: 0.7268 - val_loss: 0.6741 - val_acc: 0.6458\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5127 - acc: 0.7302 - val_loss: 0.6816 - val_acc: 0.6250\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4958 - acc: 0.7423 - val_loss: 0.6770 - val_acc: 0.6458\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5214 - acc: 0.7474 - val_loss: 0.6705 - val_acc: 0.6458\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5249 - acc: 0.7543 - val_loss: 0.6739 - val_acc: 0.6562\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4966 - acc: 0.7509 - val_loss: 0.6753 - val_acc: 0.6458\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5098 - acc: 0.7595 - val_loss: 0.6746 - val_acc: 0.6458\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.5062 - acc: 0.7491 - val_loss: 0.6681 - val_acc: 0.6562\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5157 - acc: 0.7388 - val_loss: 0.6550 - val_acc: 0.6667\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5363 - acc: 0.7320 - val_loss: 0.6847 - val_acc: 0.6354\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.4786 - acc: 0.7491 - val_loss: 0.6713 - val_acc: 0.6458\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4785 - acc: 0.7698 - val_loss: 0.6770 - val_acc: 0.6458\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5119 - acc: 0.7320 - val_loss: 0.6732 - val_acc: 0.6771\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4966 - acc: 0.7423 - val_loss: 0.6750 - val_acc: 0.6354\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5320 - acc: 0.7371 - val_loss: 0.6308 - val_acc: 0.6875\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.5138 - acc: 0.7509 - val_loss: 0.6654 - val_acc: 0.6667\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.5045 - acc: 0.7354 - val_loss: 0.6761 - val_acc: 0.6458\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5075 - acc: 0.7560 - val_loss: 0.6815 - val_acc: 0.6458\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.5255 - acc: 0.7440 - val_loss: 0.6714 - val_acc: 0.6458\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5105 - acc: 0.7474 - val_loss: 0.6652 - val_acc: 0.6667\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5450 - acc: 0.7234 - val_loss: 0.6770 - val_acc: 0.6458\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4630 - acc: 0.7612 - val_loss: 0.6813 - val_acc: 0.6562\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.4934 - acc: 0.7543 - val_loss: 0.6638 - val_acc: 0.6562\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4769 - acc: 0.7784 - val_loss: 0.6523 - val_acc: 0.6458\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5023 - acc: 0.7457 - val_loss: 0.6555 - val_acc: 0.6562\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5117 - acc: 0.7646 - val_loss: 0.6606 - val_acc: 0.6667\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.4920 - acc: 0.7457 - val_loss: 0.6453 - val_acc: 0.6667\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5002 - acc: 0.7543 - val_loss: 0.6617 - val_acc: 0.6354\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4824 - acc: 0.7457 - val_loss: 0.6457 - val_acc: 0.6458\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5143 - acc: 0.7474 - val_loss: 0.6450 - val_acc: 0.6562\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5438 - acc: 0.7216 - val_loss: 0.6524 - val_acc: 0.6562\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4703 - acc: 0.7801 - val_loss: 0.6398 - val_acc: 0.6667\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5286 - acc: 0.7474 - val_loss: 0.6523 - val_acc: 0.6667\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5095 - acc: 0.7423 - val_loss: 0.6624 - val_acc: 0.6458\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5019 - acc: 0.7440 - val_loss: 0.6494 - val_acc: 0.6562\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5113 - acc: 0.7388 - val_loss: 0.6676 - val_acc: 0.6562\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4635 - acc: 0.7766 - val_loss: 0.6487 - val_acc: 0.6667\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5138 - acc: 0.7285 - val_loss: 0.6506 - val_acc: 0.6354\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4609 - acc: 0.7749 - val_loss: 0.6650 - val_acc: 0.6562\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5045 - acc: 0.7646 - val_loss: 0.6494 - val_acc: 0.6562\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5052 - acc: 0.7612 - val_loss: 0.6489 - val_acc: 0.6875\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5282 - acc: 0.7216 - val_loss: 0.6480 - val_acc: 0.6667\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5572 - acc: 0.7165 - val_loss: 0.6506 - val_acc: 0.6354\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.5489 - acc: 0.7182 - val_loss: 0.6487 - val_acc: 0.6562\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5057 - acc: 0.7474 - val_loss: 0.6603 - val_acc: 0.6458\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.5186 - acc: 0.7457 - val_loss: 0.6660 - val_acc: 0.6458\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.5195 - acc: 0.7388 - val_loss: 0.6647 - val_acc: 0.6458\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5018 - acc: 0.7423 - val_loss: 0.6677 - val_acc: 0.6562\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5043 - acc: 0.7509 - val_loss: 0.6591 - val_acc: 0.6562\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4581 - acc: 0.7921 - val_loss: 0.6421 - val_acc: 0.6667\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5005 - acc: 0.7354 - val_loss: 0.6682 - val_acc: 0.6562\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5276 - acc: 0.7423 - val_loss: 0.6634 - val_acc: 0.6458\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5090 - acc: 0.7491 - val_loss: 0.6664 - val_acc: 0.6562\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5485 - acc: 0.7148 - val_loss: 0.6667 - val_acc: 0.6458\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4969 - acc: 0.7595 - val_loss: 0.6717 - val_acc: 0.6458\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4884 - acc: 0.7595 - val_loss: 0.6707 - val_acc: 0.6458\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5153 - acc: 0.7199 - val_loss: 0.6667 - val_acc: 0.6458\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 0.5060 - acc: 0.7320 - val_loss: 0.6746 - val_acc: 0.6354\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 0.4648 - acc: 0.7869 - val_loss: 0.6745 - val_acc: 0.6146\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5027 - acc: 0.7595 - val_loss: 0.6798 - val_acc: 0.6562\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.4953 - acc: 0.7698 - val_loss: 0.6795 - val_acc: 0.6458\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5103 - acc: 0.7457 - val_loss: 0.6691 - val_acc: 0.6562\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5262 - acc: 0.7457 - val_loss: 0.6676 - val_acc: 0.6562\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4854 - acc: 0.7526 - val_loss: 0.6620 - val_acc: 0.6562\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5350 - acc: 0.7371 - val_loss: 0.6766 - val_acc: 0.6458\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5262 - acc: 0.7509 - val_loss: 0.6541 - val_acc: 0.6562\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5174 - acc: 0.7388 - val_loss: 0.6703 - val_acc: 0.6458\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5001 - acc: 0.7440 - val_loss: 0.6670 - val_acc: 0.6667\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5691 - acc: 0.6976 - val_loss: 0.6749 - val_acc: 0.6354\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4851 - acc: 0.7646 - val_loss: 0.6629 - val_acc: 0.6458\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5294 - acc: 0.7423 - val_loss: 0.6496 - val_acc: 0.6354\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5082 - acc: 0.7612 - val_loss: 0.6669 - val_acc: 0.6458\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5122 - acc: 0.7526 - val_loss: 0.6682 - val_acc: 0.6562\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5276 - acc: 0.7148 - val_loss: 0.6509 - val_acc: 0.6562\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5180 - acc: 0.7509 - val_loss: 0.6389 - val_acc: 0.6667\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4943 - acc: 0.7399 - val_loss: 0.6545 - val_acc: 0.6562\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5261 - acc: 0.7509 - val_loss: 0.6592 - val_acc: 0.6562\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.5198 - acc: 0.7337 - val_loss: 0.6780 - val_acc: 0.6354\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4737 - acc: 0.7629 - val_loss: 0.6645 - val_acc: 0.6458\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5309 - acc: 0.7354 - val_loss: 0.6629 - val_acc: 0.6562\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4637 - acc: 0.7869 - val_loss: 0.6709 - val_acc: 0.6354\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4986 - acc: 0.7526 - val_loss: 0.6611 - val_acc: 0.6562\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4959 - acc: 0.7474 - val_loss: 0.6404 - val_acc: 0.6667\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5121 - acc: 0.7595 - val_loss: 0.6432 - val_acc: 0.6667\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4765 - acc: 0.7680 - val_loss: 0.6630 - val_acc: 0.6354\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4631 - acc: 0.7835 - val_loss: 0.6481 - val_acc: 0.6667\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4785 - acc: 0.7818 - val_loss: 0.6662 - val_acc: 0.6354\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5142 - acc: 0.7595 - val_loss: 0.6678 - val_acc: 0.6458\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5069 - acc: 0.7680 - val_loss: 0.6565 - val_acc: 0.6562\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5092 - acc: 0.7457 - val_loss: 0.6467 - val_acc: 0.6667\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4808 - acc: 0.7835 - val_loss: 0.6573 - val_acc: 0.6562\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5404 - acc: 0.7543 - val_loss: 0.6733 - val_acc: 0.6354\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4920 - acc: 0.7584 - val_loss: 0.6429 - val_acc: 0.6667\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.4777 - acc: 0.7715 - val_loss: 0.6519 - val_acc: 0.6458\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5473 - acc: 0.7148 - val_loss: 0.6526 - val_acc: 0.6562\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5436 - acc: 0.7405 - val_loss: 0.6594 - val_acc: 0.6562\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5107 - acc: 0.7320 - val_loss: 0.6786 - val_acc: 0.6458\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5266 - acc: 0.7474 - val_loss: 0.6654 - val_acc: 0.6458\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5104 - acc: 0.7423 - val_loss: 0.6711 - val_acc: 0.6458\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4861 - acc: 0.7663 - val_loss: 0.6513 - val_acc: 0.6458\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4837 - acc: 0.7749 - val_loss: 0.6501 - val_acc: 0.6562\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5074 - acc: 0.7405 - val_loss: 0.6535 - val_acc: 0.6458\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4847 - acc: 0.7560 - val_loss: 0.6565 - val_acc: 0.6562\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5269 - acc: 0.7440 - val_loss: 0.6493 - val_acc: 0.6667\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4893 - acc: 0.7749 - val_loss: 0.6538 - val_acc: 0.6562\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5034 - acc: 0.7526 - val_loss: 0.6554 - val_acc: 0.6562\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.4913 - acc: 0.7612 - val_loss: 0.6545 - val_acc: 0.6458\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4702 - acc: 0.7766 - val_loss: 0.6686 - val_acc: 0.6354\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5102 - acc: 0.7526 - val_loss: 0.6664 - val_acc: 0.6354\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5044 - acc: 0.7320 - val_loss: 0.6624 - val_acc: 0.6354\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5262 - acc: 0.7337 - val_loss: 0.6704 - val_acc: 0.6250\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5278 - acc: 0.7457 - val_loss: 0.6684 - val_acc: 0.6458\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5177 - acc: 0.7423 - val_loss: 0.6428 - val_acc: 0.6562\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5007 - acc: 0.7663 - val_loss: 0.6674 - val_acc: 0.6458\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5064 - acc: 0.7526 - val_loss: 0.6543 - val_acc: 0.6562\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5187 - acc: 0.7577 - val_loss: 0.6523 - val_acc: 0.6562\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4986 - acc: 0.7595 - val_loss: 0.6688 - val_acc: 0.6562\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5086 - acc: 0.7474 - val_loss: 0.6728 - val_acc: 0.6458\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5220 - acc: 0.7302 - val_loss: 0.6619 - val_acc: 0.6667\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4981 - acc: 0.7612 - val_loss: 0.6417 - val_acc: 0.6771\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4845 - acc: 0.7646 - val_loss: 0.6602 - val_acc: 0.6562\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5220 - acc: 0.7440 - val_loss: 0.6415 - val_acc: 0.6667\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5184 - acc: 0.7491 - val_loss: 0.6742 - val_acc: 0.6458\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4820 - acc: 0.7560 - val_loss: 0.6614 - val_acc: 0.6562\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5036 - acc: 0.7526 - val_loss: 0.6492 - val_acc: 0.6562\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5099 - acc: 0.7680 - val_loss: 0.6549 - val_acc: 0.6562\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4796 - acc: 0.7835 - val_loss: 0.6728 - val_acc: 0.6250\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4799 - acc: 0.7595 - val_loss: 0.6819 - val_acc: 0.6354\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5354 - acc: 0.7354 - val_loss: 0.6715 - val_acc: 0.6458\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4927 - acc: 0.7629 - val_loss: 0.6699 - val_acc: 0.6250\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5194 - acc: 0.7337 - val_loss: 0.6743 - val_acc: 0.6354\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4958 - acc: 0.7491 - val_loss: 0.6552 - val_acc: 0.6562\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5054 - acc: 0.7457 - val_loss: 0.6770 - val_acc: 0.6354\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5195 - acc: 0.7595 - val_loss: 0.6787 - val_acc: 0.6354\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4779 - acc: 0.7560 - val_loss: 0.6602 - val_acc: 0.6562\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4933 - acc: 0.7715 - val_loss: 0.6391 - val_acc: 0.6146\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5220 - acc: 0.7302 - val_loss: 0.6681 - val_acc: 0.6354\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.4674 - acc: 0.7646 - val_loss: 0.6762 - val_acc: 0.6458\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5042 - acc: 0.7543 - val_loss: 0.6516 - val_acc: 0.6562\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4902 - acc: 0.7509 - val_loss: 0.6727 - val_acc: 0.6458\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 0.5025 - acc: 0.7491 - val_loss: 0.6774 - val_acc: 0.6250\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5080 - acc: 0.7680 - val_loss: 0.6621 - val_acc: 0.6458\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5202 - acc: 0.7182 - val_loss: 0.6521 - val_acc: 0.6771\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5162 - acc: 0.7440 - val_loss: 0.6675 - val_acc: 0.6562\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4907 - acc: 0.7698 - val_loss: 0.6846 - val_acc: 0.6354\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5254 - acc: 0.7165 - val_loss: 0.6557 - val_acc: 0.6667\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.4890 - acc: 0.7509 - val_loss: 0.6701 - val_acc: 0.6562\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5366 - acc: 0.7302 - val_loss: 0.6534 - val_acc: 0.6458\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5262 - acc: 0.7423 - val_loss: 0.6662 - val_acc: 0.6458\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5066 - acc: 0.7526 - val_loss: 0.6767 - val_acc: 0.6354\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.4800 - acc: 0.7732 - val_loss: 0.6661 - val_acc: 0.6458\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5033 - acc: 0.7595 - val_loss: 0.6512 - val_acc: 0.6667\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5143 - acc: 0.7509 - val_loss: 0.6586 - val_acc: 0.6562\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5123 - acc: 0.7423 - val_loss: 0.6603 - val_acc: 0.6562\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5015 - acc: 0.7405 - val_loss: 0.6747 - val_acc: 0.6458\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4826 - acc: 0.7526 - val_loss: 0.6581 - val_acc: 0.6562\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4854 - acc: 0.7491 - val_loss: 0.6754 - val_acc: 0.6458\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5226 - acc: 0.7354 - val_loss: 0.6310 - val_acc: 0.6771\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4937 - acc: 0.7612 - val_loss: 0.6828 - val_acc: 0.6354\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5251 - acc: 0.7560 - val_loss: 0.6607 - val_acc: 0.6562\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5076 - acc: 0.7457 - val_loss: 0.6622 - val_acc: 0.6458\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4985 - acc: 0.7543 - val_loss: 0.6492 - val_acc: 0.6667\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5221 - acc: 0.7354 - val_loss: 0.6659 - val_acc: 0.6250\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.4809 - acc: 0.7491 - val_loss: 0.6564 - val_acc: 0.6562\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4811 - acc: 0.7595 - val_loss: 0.6730 - val_acc: 0.6354\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5214 - acc: 0.7457 - val_loss: 0.6656 - val_acc: 0.6458\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4992 - acc: 0.7560 - val_loss: 0.6664 - val_acc: 0.6562\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5001 - acc: 0.7440 - val_loss: 0.6584 - val_acc: 0.6667\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5204 - acc: 0.7491 - val_loss: 0.6540 - val_acc: 0.6771\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5141 - acc: 0.7440 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.5290 - acc: 0.7179 - val_loss: 0.6672 - val_acc: 0.6562\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.5339 - acc: 0.7337 - val_loss: 0.6747 - val_acc: 0.6354\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.4943 - acc: 0.7698 - val_loss: 0.6598 - val_acc: 0.6458\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5259 - acc: 0.7234 - val_loss: 0.6666 - val_acc: 0.6458\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5082 - acc: 0.7474 - val_loss: 0.6717 - val_acc: 0.6458\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.5056 - acc: 0.7440 - val_loss: 0.6622 - val_acc: 0.6458\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5232 - acc: 0.7440 - val_loss: 0.6613 - val_acc: 0.6458\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5136 - acc: 0.7354 - val_loss: 0.6677 - val_acc: 0.6458\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4946 - acc: 0.7612 - val_loss: 0.6790 - val_acc: 0.6354\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5167 - acc: 0.7337 - val_loss: 0.6409 - val_acc: 0.6562\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5195 - acc: 0.7457 - val_loss: 0.6536 - val_acc: 0.6667\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5146 - acc: 0.7612 - val_loss: 0.6402 - val_acc: 0.6771\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5307 - acc: 0.7526 - val_loss: 0.6670 - val_acc: 0.6562\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5118 - acc: 0.7337 - val_loss: 0.6568 - val_acc: 0.6667\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4785 - acc: 0.7801 - val_loss: 0.6750 - val_acc: 0.6562\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5438 - acc: 0.7234 - val_loss: 0.6549 - val_acc: 0.6562\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5060 - acc: 0.7509 - val_loss: 0.6423 - val_acc: 0.6771\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4928 - acc: 0.7612 - val_loss: 0.6735 - val_acc: 0.6562\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5171 - acc: 0.7526 - val_loss: 0.6409 - val_acc: 0.6667\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5252 - acc: 0.7457 - val_loss: 0.6664 - val_acc: 0.6354\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5024 - acc: 0.7543 - val_loss: 0.6598 - val_acc: 0.6562\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4978 - acc: 0.7509 - val_loss: 0.6610 - val_acc: 0.6667\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4562 - acc: 0.7818 - val_loss: 0.6581 - val_acc: 0.6562\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5020 - acc: 0.7474 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5289 - acc: 0.7388 - val_loss: 0.6684 - val_acc: 0.6354\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5209 - acc: 0.7474 - val_loss: 0.6726 - val_acc: 0.6562\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4878 - acc: 0.7784 - val_loss: 0.6774 - val_acc: 0.6458\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5142 - acc: 0.7577 - val_loss: 0.6651 - val_acc: 0.6458\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4806 - acc: 0.7715 - val_loss: 0.6476 - val_acc: 0.6562\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5229 - acc: 0.7354 - val_loss: 0.6596 - val_acc: 0.6667\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5334 - acc: 0.7165 - val_loss: 0.6692 - val_acc: 0.6458\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4939 - acc: 0.7577 - val_loss: 0.6681 - val_acc: 0.6562\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5052 - acc: 0.7629 - val_loss: 0.6773 - val_acc: 0.6354\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5054 - acc: 0.7509 - val_loss: 0.6592 - val_acc: 0.6562\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5012 - acc: 0.7852 - val_loss: 0.6707 - val_acc: 0.6458\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4586 - acc: 0.7784 - val_loss: 0.6734 - val_acc: 0.6458\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5415 - acc: 0.7165 - val_loss: 0.6425 - val_acc: 0.6667\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5395 - acc: 0.7165 - val_loss: 0.6776 - val_acc: 0.6354\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4999 - acc: 0.7577 - val_loss: 0.6736 - val_acc: 0.6458\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4827 - acc: 0.7601 - val_loss: 0.6812 - val_acc: 0.6458\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5109 - acc: 0.7560 - val_loss: 0.6691 - val_acc: 0.6458\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 0.5434 - acc: 0.7337 - val_loss: 0.6741 - val_acc: 0.6354\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5505 - acc: 0.7354 - val_loss: 0.6602 - val_acc: 0.6562\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5033 - acc: 0.7457 - val_loss: 0.6592 - val_acc: 0.6458\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5073 - acc: 0.7698 - val_loss: 0.6483 - val_acc: 0.6667\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5444 - acc: 0.7320 - val_loss: 0.6508 - val_acc: 0.6667\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5011 - acc: 0.7698 - val_loss: 0.6446 - val_acc: 0.6771\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4911 - acc: 0.7526 - val_loss: 0.6823 - val_acc: 0.6354\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4830 - acc: 0.7560 - val_loss: 0.6664 - val_acc: 0.6458\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5030 - acc: 0.7509 - val_loss: 0.6608 - val_acc: 0.6562\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.4903 - acc: 0.7577 - val_loss: 0.6528 - val_acc: 0.6667\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5356 - acc: 0.7216 - val_loss: 0.6560 - val_acc: 0.6667\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4940 - acc: 0.7835 - val_loss: 0.6663 - val_acc: 0.6562\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5386 - acc: 0.7251 - val_loss: 0.6684 - val_acc: 0.6458\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5184 - acc: 0.7423 - val_loss: 0.6734 - val_acc: 0.6354\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5426 - acc: 0.7216 - val_loss: 0.6399 - val_acc: 0.6562\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5338 - acc: 0.7440 - val_loss: 0.6558 - val_acc: 0.6458\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4918 - acc: 0.7612 - val_loss: 0.6694 - val_acc: 0.6458\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4931 - acc: 0.7629 - val_loss: 0.6736 - val_acc: 0.6354\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4868 - acc: 0.7663 - val_loss: 0.6522 - val_acc: 0.6458\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5040 - acc: 0.7629 - val_loss: 0.6528 - val_acc: 0.6458\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5031 - acc: 0.7635 - val_loss: 0.6334 - val_acc: 0.6667\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.4807 - acc: 0.7669 - val_loss: 0.6783 - val_acc: 0.6354\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5439 - acc: 0.7320 - val_loss: 0.6589 - val_acc: 0.6354\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.5326 - acc: 0.7526 - val_loss: 0.6508 - val_acc: 0.6562\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5307 - acc: 0.7302 - val_loss: 0.6500 - val_acc: 0.6562\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5434 - acc: 0.7371 - val_loss: 0.6318 - val_acc: 0.6667\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4606 - acc: 0.7766 - val_loss: 0.6566 - val_acc: 0.6562\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5154 - acc: 0.7491 - val_loss: 0.6655 - val_acc: 0.6458\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4952 - acc: 0.7629 - val_loss: 0.6638 - val_acc: 0.6458\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5366 - acc: 0.7234 - val_loss: 0.6420 - val_acc: 0.6667\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5190 - acc: 0.7509 - val_loss: 0.6419 - val_acc: 0.6354\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5124 - acc: 0.7234 - val_loss: 0.6289 - val_acc: 0.6458\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5449 - acc: 0.7113 - val_loss: 0.6559 - val_acc: 0.6458\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5114 - acc: 0.7474 - val_loss: 0.6500 - val_acc: 0.6562\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5098 - acc: 0.7371 - val_loss: 0.6520 - val_acc: 0.6562\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4858 - acc: 0.7560 - val_loss: 0.6650 - val_acc: 0.6458\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5322 - acc: 0.7388 - val_loss: 0.6497 - val_acc: 0.6562\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5173 - acc: 0.7595 - val_loss: 0.6422 - val_acc: 0.6562\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5074 - acc: 0.7491 - val_loss: 0.6543 - val_acc: 0.6354\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5339 - acc: 0.7423 - val_loss: 0.6458 - val_acc: 0.6458\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5110 - acc: 0.7526 - val_loss: 0.6529 - val_acc: 0.6458\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4933 - acc: 0.7646 - val_loss: 0.6532 - val_acc: 0.6458\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5065 - acc: 0.7474 - val_loss: 0.6478 - val_acc: 0.6458\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5061 - acc: 0.7251 - val_loss: 0.6362 - val_acc: 0.6667\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.4918 - acc: 0.7457 - val_loss: 0.6451 - val_acc: 0.6562\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5306 - acc: 0.7234 - val_loss: 0.6429 - val_acc: 0.6458\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.4880 - acc: 0.7663 - val_loss: 0.6348 - val_acc: 0.6771\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5456 - acc: 0.7302 - val_loss: 0.6499 - val_acc: 0.6458\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5168 - acc: 0.7560 - val_loss: 0.6435 - val_acc: 0.6667\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5061 - acc: 0.7663 - val_loss: 0.6363 - val_acc: 0.6562\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5118 - acc: 0.7388 - val_loss: 0.6369 - val_acc: 0.6562\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4689 - acc: 0.7715 - val_loss: 0.6480 - val_acc: 0.6458\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5438 - acc: 0.7457 - val_loss: 0.6524 - val_acc: 0.6458\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5569 - acc: 0.7320 - val_loss: 0.6529 - val_acc: 0.6458\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4955 - acc: 0.7560 - val_loss: 0.6510 - val_acc: 0.6562\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5053 - acc: 0.7440 - val_loss: 0.6480 - val_acc: 0.6562\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5389 - acc: 0.7405 - val_loss: 0.6230 - val_acc: 0.6667\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4991 - acc: 0.7629 - val_loss: 0.6554 - val_acc: 0.6458\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5359 - acc: 0.7234 - val_loss: 0.6620 - val_acc: 0.6354\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5204 - acc: 0.7371 - val_loss: 0.6580 - val_acc: 0.6458\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4892 - acc: 0.7388 - val_loss: 0.6537 - val_acc: 0.6458\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5182 - acc: 0.7268 - val_loss: 0.6335 - val_acc: 0.6771\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4914 - acc: 0.7652 - val_loss: 0.6480 - val_acc: 0.6458\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5049 - acc: 0.7371 - val_loss: 0.6383 - val_acc: 0.6667\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5084 - acc: 0.7526 - val_loss: 0.6438 - val_acc: 0.6667\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5454 - acc: 0.7399 - val_loss: 0.6388 - val_acc: 0.6354\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4919 - acc: 0.7595 - val_loss: 0.6464 - val_acc: 0.6562\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5004 - acc: 0.7388 - val_loss: 0.6623 - val_acc: 0.6458\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5229 - acc: 0.7096 - val_loss: 0.6462 - val_acc: 0.6562\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5143 - acc: 0.7337 - val_loss: 0.6473 - val_acc: 0.6667\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5066 - acc: 0.7612 - val_loss: 0.6633 - val_acc: 0.6458\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4951 - acc: 0.7440 - val_loss: 0.6454 - val_acc: 0.6667\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4983 - acc: 0.7577 - val_loss: 0.6605 - val_acc: 0.6458\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.4876 - acc: 0.7732 - val_loss: 0.6486 - val_acc: 0.6667\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5297 - acc: 0.7285 - val_loss: 0.6311 - val_acc: 0.6667\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5148 - acc: 0.7440 - val_loss: 0.6564 - val_acc: 0.6562\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4968 - acc: 0.7646 - val_loss: 0.6547 - val_acc: 0.6562\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4774 - acc: 0.7766 - val_loss: 0.6716 - val_acc: 0.6354\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5221 - acc: 0.7302 - val_loss: 0.6596 - val_acc: 0.6250\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5189 - acc: 0.7509 - val_loss: 0.6598 - val_acc: 0.6458\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5074 - acc: 0.7285 - val_loss: 0.6566 - val_acc: 0.6562\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5100 - acc: 0.7423 - val_loss: 0.6672 - val_acc: 0.6458\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.5130 - acc: 0.7526 - val_loss: 0.6607 - val_acc: 0.6458\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5005 - acc: 0.7388 - val_loss: 0.6746 - val_acc: 0.6354\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4934 - acc: 0.7680 - val_loss: 0.6734 - val_acc: 0.6458\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5083 - acc: 0.7457 - val_loss: 0.6581 - val_acc: 0.6667\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.4936 - acc: 0.7440 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.4751 - acc: 0.7801 - val_loss: 0.6582 - val_acc: 0.6562\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.4825 - acc: 0.7629 - val_loss: 0.6706 - val_acc: 0.6458\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4976 - acc: 0.7509 - val_loss: 0.6706 - val_acc: 0.6458\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5082 - acc: 0.7560 - val_loss: 0.6610 - val_acc: 0.6250\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5203 - acc: 0.7715 - val_loss: 0.6684 - val_acc: 0.6354\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5034 - acc: 0.7577 - val_loss: 0.6501 - val_acc: 0.6667\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5455 - acc: 0.7251 - val_loss: 0.6663 - val_acc: 0.6354\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5014 - acc: 0.7543 - val_loss: 0.6377 - val_acc: 0.6562\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.4887 - acc: 0.7784 - val_loss: 0.6646 - val_acc: 0.6458\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5025 - acc: 0.7405 - val_loss: 0.6607 - val_acc: 0.6562\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4915 - acc: 0.7543 - val_loss: 0.6642 - val_acc: 0.6458\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.5251 - acc: 0.7371 - val_loss: 0.6438 - val_acc: 0.6562\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4815 - acc: 0.7629 - val_loss: 0.6338 - val_acc: 0.6458\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5219 - acc: 0.7302 - val_loss: 0.6513 - val_acc: 0.6667\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4688 - acc: 0.7801 - val_loss: 0.6484 - val_acc: 0.6562\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5164 - acc: 0.7354 - val_loss: 0.6463 - val_acc: 0.6562\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5067 - acc: 0.7474 - val_loss: 0.6446 - val_acc: 0.6667\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4938 - acc: 0.7612 - val_loss: 0.6477 - val_acc: 0.6562\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5260 - acc: 0.7457 - val_loss: 0.6407 - val_acc: 0.6667\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4901 - acc: 0.7474 - val_loss: 0.6592 - val_acc: 0.6562\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5127 - acc: 0.7440 - val_loss: 0.6504 - val_acc: 0.6667\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5286 - acc: 0.7474 - val_loss: 0.6624 - val_acc: 0.6562\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4907 - acc: 0.7509 - val_loss: 0.6654 - val_acc: 0.6458\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.5460 - acc: 0.7371 - val_loss: 0.6462 - val_acc: 0.6667\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5373 - acc: 0.7302 - val_loss: 0.6730 - val_acc: 0.6354\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5278 - acc: 0.7388 - val_loss: 0.6423 - val_acc: 0.6667\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5533 - acc: 0.7302 - val_loss: 0.6602 - val_acc: 0.6146\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5189 - acc: 0.7234 - val_loss: 0.6441 - val_acc: 0.6354\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5178 - acc: 0.7526 - val_loss: 0.6612 - val_acc: 0.6458\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5190 - acc: 0.7285 - val_loss: 0.6486 - val_acc: 0.6562\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4970 - acc: 0.7371 - val_loss: 0.6474 - val_acc: 0.6562\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4993 - acc: 0.7543 - val_loss: 0.6615 - val_acc: 0.6458\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4877 - acc: 0.7680 - val_loss: 0.6429 - val_acc: 0.6458\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4994 - acc: 0.7509 - val_loss: 0.6451 - val_acc: 0.6562\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5056 - acc: 0.7423 - val_loss: 0.6438 - val_acc: 0.6354\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5185 - acc: 0.7354 - val_loss: 0.6416 - val_acc: 0.6250\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4767 - acc: 0.7663 - val_loss: 0.6422 - val_acc: 0.6562\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5476 - acc: 0.7165 - val_loss: 0.6551 - val_acc: 0.6250\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 0.5106 - acc: 0.7560 - val_loss: 0.6422 - val_acc: 0.6562\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5184 - acc: 0.7423 - val_loss: 0.6529 - val_acc: 0.6354\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5127 - acc: 0.7457 - val_loss: 0.6308 - val_acc: 0.6771\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5066 - acc: 0.7629 - val_loss: 0.6444 - val_acc: 0.6562\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4995 - acc: 0.7577 - val_loss: 0.6669 - val_acc: 0.6354\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5384 - acc: 0.7320 - val_loss: 0.6514 - val_acc: 0.6458\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5032 - acc: 0.7577 - val_loss: 0.6364 - val_acc: 0.6667\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5317 - acc: 0.7045 - val_loss: 0.6375 - val_acc: 0.6667\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5019 - acc: 0.7457 - val_loss: 0.6578 - val_acc: 0.6354\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4936 - acc: 0.7543 - val_loss: 0.6389 - val_acc: 0.6667\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5222 - acc: 0.7423 - val_loss: 0.6654 - val_acc: 0.6458\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4712 - acc: 0.7577 - val_loss: 0.6299 - val_acc: 0.6771\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.5172 - acc: 0.7388 - val_loss: 0.6610 - val_acc: 0.6354\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 0.4948 - acc: 0.7526 - val_loss: 0.6548 - val_acc: 0.6458\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4975 - acc: 0.7646 - val_loss: 0.6536 - val_acc: 0.6562\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5338 - acc: 0.7337 - val_loss: 0.6488 - val_acc: 0.6562\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4836 - acc: 0.7703 - val_loss: 0.6310 - val_acc: 0.6771\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5090 - acc: 0.7440 - val_loss: 0.6361 - val_acc: 0.6667\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5098 - acc: 0.7440 - val_loss: 0.6462 - val_acc: 0.6667\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5234 - acc: 0.7474 - val_loss: 0.6636 - val_acc: 0.6458\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5195 - acc: 0.7568 - val_loss: 0.6389 - val_acc: 0.6667\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4964 - acc: 0.7663 - val_loss: 0.6695 - val_acc: 0.6250\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.4964 - acc: 0.7371 - val_loss: 0.6315 - val_acc: 0.6875\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5116 - acc: 0.7612 - val_loss: 0.6507 - val_acc: 0.6667\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.4919 - acc: 0.7491 - val_loss: 0.6613 - val_acc: 0.6354\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5201 - acc: 0.7457 - val_loss: 0.6408 - val_acc: 0.6562\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4667 - acc: 0.7818 - val_loss: 0.6537 - val_acc: 0.6458\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5117 - acc: 0.7457 - val_loss: 0.6636 - val_acc: 0.6562\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5143 - acc: 0.7595 - val_loss: 0.6544 - val_acc: 0.6562\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5200 - acc: 0.7251 - val_loss: 0.6512 - val_acc: 0.6667\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5189 - acc: 0.7491 - val_loss: 0.6733 - val_acc: 0.6562\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5260 - acc: 0.7543 - val_loss: 0.6737 - val_acc: 0.6458\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.5091 - acc: 0.7629 - val_loss: 0.6407 - val_acc: 0.6771\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5059 - acc: 0.7285 - val_loss: 0.6510 - val_acc: 0.6458\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5180 - acc: 0.7663 - val_loss: 0.6485 - val_acc: 0.6667\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.5157 - acc: 0.7440 - val_loss: 0.6713 - val_acc: 0.6458\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5052 - acc: 0.7612 - val_loss: 0.6636 - val_acc: 0.6458\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5181 - acc: 0.7423 - val_loss: 0.6545 - val_acc: 0.6667\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.4926 - acc: 0.7663 - val_loss: 0.6362 - val_acc: 0.6562\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5242 - acc: 0.7371 - val_loss: 0.6532 - val_acc: 0.6667\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5183 - acc: 0.7440 - val_loss: 0.6454 - val_acc: 0.6667\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4948 - acc: 0.7388 - val_loss: 0.6838 - val_acc: 0.6562\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4796 - acc: 0.7749 - val_loss: 0.6683 - val_acc: 0.6354\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5119 - acc: 0.7423 - val_loss: 0.6753 - val_acc: 0.6354\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4797 - acc: 0.7698 - val_loss: 0.6677 - val_acc: 0.6562\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.5405 - acc: 0.7148 - val_loss: 0.6882 - val_acc: 0.6354\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 0.5509 - acc: 0.7302 - val_loss: 0.6586 - val_acc: 0.6771\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.4696 - acc: 0.7818 - val_loss: 0.6631 - val_acc: 0.6562\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4796 - acc: 0.7595 - val_loss: 0.6830 - val_acc: 0.6458\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5104 - acc: 0.7302 - val_loss: 0.6811 - val_acc: 0.6458\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4999 - acc: 0.7388 - val_loss: 0.6764 - val_acc: 0.6458\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5415 - acc: 0.7354 - val_loss: 0.6771 - val_acc: 0.6458\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4923 - acc: 0.7526 - val_loss: 0.6767 - val_acc: 0.6458\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.4912 - acc: 0.7560 - val_loss: 0.6788 - val_acc: 0.6458\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5047 - acc: 0.7646 - val_loss: 0.6762 - val_acc: 0.6458\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4951 - acc: 0.7337 - val_loss: 0.6580 - val_acc: 0.6667\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4948 - acc: 0.7595 - val_loss: 0.6677 - val_acc: 0.6667\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.5536 - acc: 0.7320 - val_loss: 0.6701 - val_acc: 0.6562\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5547 - acc: 0.7199 - val_loss: 0.6851 - val_acc: 0.6354\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5002 - acc: 0.7543 - val_loss: 0.6587 - val_acc: 0.6458\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5063 - acc: 0.7698 - val_loss: 0.6671 - val_acc: 0.6562\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5065 - acc: 0.7491 - val_loss: 0.6690 - val_acc: 0.6562\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5284 - acc: 0.7457 - val_loss: 0.6863 - val_acc: 0.6458\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5385 - acc: 0.7474 - val_loss: 0.6855 - val_acc: 0.6458\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5273 - acc: 0.7302 - val_loss: 0.6638 - val_acc: 0.6562\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5430 - acc: 0.7474 - val_loss: 0.6614 - val_acc: 0.6458\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.4869 - acc: 0.7509 - val_loss: 0.6551 - val_acc: 0.6667\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5085 - acc: 0.7595 - val_loss: 0.6421 - val_acc: 0.6458\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5051 - acc: 0.7526 - val_loss: 0.6456 - val_acc: 0.6667\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5315 - acc: 0.7302 - val_loss: 0.6699 - val_acc: 0.6354\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4955 - acc: 0.7474 - val_loss: 0.6528 - val_acc: 0.6667\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5179 - acc: 0.7440 - val_loss: 0.6806 - val_acc: 0.6354\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4792 - acc: 0.7595 - val_loss: 0.6693 - val_acc: 0.6458\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5099 - acc: 0.7268 - val_loss: 0.6685 - val_acc: 0.6562\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5376 - acc: 0.7302 - val_loss: 0.6570 - val_acc: 0.6667\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4930 - acc: 0.7766 - val_loss: 0.6453 - val_acc: 0.6458\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5104 - acc: 0.7680 - val_loss: 0.6693 - val_acc: 0.6458\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 0.5134 - acc: 0.7474 - val_loss: 0.6541 - val_acc: 0.6562\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5012 - acc: 0.7354 - val_loss: 0.6793 - val_acc: 0.6354\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4916 - acc: 0.7577 - val_loss: 0.6500 - val_acc: 0.6354\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4578 - acc: 0.7801 - val_loss: 0.6512 - val_acc: 0.6771\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5233 - acc: 0.7577 - val_loss: 0.6600 - val_acc: 0.6667\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5296 - acc: 0.7405 - val_loss: 0.6831 - val_acc: 0.6354\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5001 - acc: 0.7491 - val_loss: 0.6658 - val_acc: 0.6562\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4846 - acc: 0.7560 - val_loss: 0.6844 - val_acc: 0.6562\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5288 - acc: 0.7131 - val_loss: 0.6800 - val_acc: 0.6354\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 6s 129ms/step - loss: 0.5041 - acc: 0.7715 - val_loss: 0.6732 - val_acc: 0.6458\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5250 - acc: 0.7285 - val_loss: 0.6682 - val_acc: 0.6458\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5070 - acc: 0.7491 - val_loss: 0.6654 - val_acc: 0.6250\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 0.4933 - acc: 0.7560 - val_loss: 0.6633 - val_acc: 0.6458\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5222 - acc: 0.7577 - val_loss: 0.6564 - val_acc: 0.6667\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5131 - acc: 0.7354 - val_loss: 0.6666 - val_acc: 0.6458\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5496 - acc: 0.7165 - val_loss: 0.6323 - val_acc: 0.6458\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4845 - acc: 0.7629 - val_loss: 0.6444 - val_acc: 0.6562\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4869 - acc: 0.7491 - val_loss: 0.6408 - val_acc: 0.6562\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5222 - acc: 0.7285 - val_loss: 0.6578 - val_acc: 0.6562\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5191 - acc: 0.7646 - val_loss: 0.6435 - val_acc: 0.6667\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5292 - acc: 0.7354 - val_loss: 0.6733 - val_acc: 0.6354\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4655 - acc: 0.7698 - val_loss: 0.6739 - val_acc: 0.6354\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4982 - acc: 0.7526 - val_loss: 0.6429 - val_acc: 0.6458\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5042 - acc: 0.7491 - val_loss: 0.6612 - val_acc: 0.6458\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5276 - acc: 0.7234 - val_loss: 0.6585 - val_acc: 0.6250\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5345 - acc: 0.7302 - val_loss: 0.6452 - val_acc: 0.6458\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5401 - acc: 0.7216 - val_loss: 0.6571 - val_acc: 0.6562\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4914 - acc: 0.7612 - val_loss: 0.6613 - val_acc: 0.6458\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5302 - acc: 0.7612 - val_loss: 0.6577 - val_acc: 0.6667\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5318 - acc: 0.7371 - val_loss: 0.6631 - val_acc: 0.6562\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5056 - acc: 0.7715 - val_loss: 0.6522 - val_acc: 0.6667\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4725 - acc: 0.7629 - val_loss: 0.6724 - val_acc: 0.6354\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5140 - acc: 0.7405 - val_loss: 0.6564 - val_acc: 0.6562\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4972 - acc: 0.7440 - val_loss: 0.6483 - val_acc: 0.6562\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4618 - acc: 0.7784 - val_loss: 0.6467 - val_acc: 0.6562\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4967 - acc: 0.7526 - val_loss: 0.6633 - val_acc: 0.6458\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.4947 - acc: 0.7577 - val_loss: 0.6640 - val_acc: 0.6458\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.4997 - acc: 0.7474 - val_loss: 0.6663 - val_acc: 0.6458\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5637 - acc: 0.7045 - val_loss: 0.6554 - val_acc: 0.6562\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.5219 - acc: 0.7560 - val_loss: 0.6558 - val_acc: 0.6562\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.4995 - acc: 0.7595 - val_loss: 0.6526 - val_acc: 0.6562\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5303 - acc: 0.7371 - val_loss: 0.6737 - val_acc: 0.6354\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5108 - acc: 0.7526 - val_loss: 0.6578 - val_acc: 0.6458\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4826 - acc: 0.7732 - val_loss: 0.6527 - val_acc: 0.6562\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5196 - acc: 0.7440 - val_loss: 0.6507 - val_acc: 0.6562\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4722 - acc: 0.7715 - val_loss: 0.6574 - val_acc: 0.6562\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5466 - acc: 0.7131 - val_loss: 0.6704 - val_acc: 0.6354\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5170 - acc: 0.7440 - val_loss: 0.6683 - val_acc: 0.6562\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5171 - acc: 0.7560 - val_loss: 0.6599 - val_acc: 0.6562\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5026 - acc: 0.7818 - val_loss: 0.6633 - val_acc: 0.6354\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5103 - acc: 0.7646 - val_loss: 0.6742 - val_acc: 0.6458\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4822 - acc: 0.7698 - val_loss: 0.6449 - val_acc: 0.6667\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5098 - acc: 0.7405 - val_loss: 0.6512 - val_acc: 0.6667\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5181 - acc: 0.7371 - val_loss: 0.6454 - val_acc: 0.6667\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 0.5133 - acc: 0.7457 - val_loss: 0.6722 - val_acc: 0.6458\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4983 - acc: 0.7595 - val_loss: 0.6609 - val_acc: 0.6562\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4993 - acc: 0.7405 - val_loss: 0.6378 - val_acc: 0.6562\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4993 - acc: 0.7474 - val_loss: 0.6549 - val_acc: 0.6667\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5289 - acc: 0.7474 - val_loss: 0.6750 - val_acc: 0.6458\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5373 - acc: 0.7440 - val_loss: 0.6647 - val_acc: 0.6458\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5255 - acc: 0.7509 - val_loss: 0.6661 - val_acc: 0.6458\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5151 - acc: 0.7526 - val_loss: 0.6567 - val_acc: 0.6458\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4912 - acc: 0.7560 - val_loss: 0.6694 - val_acc: 0.6458\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5122 - acc: 0.7491 - val_loss: 0.6516 - val_acc: 0.6667\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 0.5553 - acc: 0.7096 - val_loss: 0.6595 - val_acc: 0.6562\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5066 - acc: 0.7698 - val_loss: 0.6593 - val_acc: 0.6562\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4936 - acc: 0.7818 - val_loss: 0.6482 - val_acc: 0.6667\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5012 - acc: 0.7388 - val_loss: 0.6816 - val_acc: 0.6354\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5221 - acc: 0.7302 - val_loss: 0.6589 - val_acc: 0.6667\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5151 - acc: 0.7388 - val_loss: 0.6723 - val_acc: 0.6458\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5371 - acc: 0.7113 - val_loss: 0.6550 - val_acc: 0.6458\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4656 - acc: 0.7818 - val_loss: 0.6573 - val_acc: 0.6562\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.4993 - acc: 0.7371 - val_loss: 0.6523 - val_acc: 0.6458\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4980 - acc: 0.7629 - val_loss: 0.6565 - val_acc: 0.6354\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5233 - acc: 0.7652 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5023 - acc: 0.7365 - val_loss: 0.6505 - val_acc: 0.6458\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5005 - acc: 0.7509 - val_loss: 0.6511 - val_acc: 0.6667\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4820 - acc: 0.7749 - val_loss: 0.6499 - val_acc: 0.6667\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.5067 - acc: 0.7618 - val_loss: 0.6441 - val_acc: 0.6562\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4988 - acc: 0.7612 - val_loss: 0.6582 - val_acc: 0.6667\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5158 - acc: 0.7371 - val_loss: 0.6714 - val_acc: 0.6458\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5294 - acc: 0.7148 - val_loss: 0.6738 - val_acc: 0.6458\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5182 - acc: 0.7491 - val_loss: 0.6666 - val_acc: 0.6562\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 0.5016 - acc: 0.7577 - val_loss: 0.6589 - val_acc: 0.6458\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4903 - acc: 0.7560 - val_loss: 0.6339 - val_acc: 0.6771\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5272 - acc: 0.7423 - val_loss: 0.6530 - val_acc: 0.6562\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5081 - acc: 0.7320 - val_loss: 0.6519 - val_acc: 0.6667\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5063 - acc: 0.7568 - val_loss: 0.6540 - val_acc: 0.6562\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5125 - acc: 0.7371 - val_loss: 0.6528 - val_acc: 0.6562\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4935 - acc: 0.7423 - val_loss: 0.6689 - val_acc: 0.6354\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5533 - acc: 0.7320 - val_loss: 0.6582 - val_acc: 0.6562\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 0.5196 - acc: 0.7216 - val_loss: 0.6342 - val_acc: 0.6771\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5067 - acc: 0.7595 - val_loss: 0.6547 - val_acc: 0.6458\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5560 - acc: 0.6976 - val_loss: 0.6679 - val_acc: 0.6458\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5268 - acc: 0.7612 - val_loss: 0.6580 - val_acc: 0.6354\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4841 - acc: 0.7715 - val_loss: 0.6484 - val_acc: 0.6667\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5109 - acc: 0.7526 - val_loss: 0.6594 - val_acc: 0.6458\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4931 - acc: 0.7509 - val_loss: 0.6598 - val_acc: 0.6458\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4797 - acc: 0.7715 - val_loss: 0.6528 - val_acc: 0.6562\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5106 - acc: 0.7423 - val_loss: 0.6608 - val_acc: 0.6458\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 0.4682 - acc: 0.7663 - val_loss: 0.6765 - val_acc: 0.6354\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.5314 - acc: 0.7285 - val_loss: 0.6618 - val_acc: 0.6458\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5032 - acc: 0.7595 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5062 - acc: 0.7560 - val_loss: 0.6604 - val_acc: 0.6458\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.4843 - acc: 0.7629 - val_loss: 0.6617 - val_acc: 0.6354\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4965 - acc: 0.7715 - val_loss: 0.6560 - val_acc: 0.6667\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5257 - acc: 0.7474 - val_loss: 0.6591 - val_acc: 0.6354\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5030 - acc: 0.7612 - val_loss: 0.6665 - val_acc: 0.6250\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.4908 - acc: 0.7491 - val_loss: 0.6513 - val_acc: 0.6250\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4883 - acc: 0.7560 - val_loss: 0.6550 - val_acc: 0.6562\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5037 - acc: 0.7491 - val_loss: 0.6674 - val_acc: 0.6354\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5046 - acc: 0.7595 - val_loss: 0.6628 - val_acc: 0.6354\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5372 - acc: 0.7457 - val_loss: 0.6606 - val_acc: 0.6458\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5401 - acc: 0.7320 - val_loss: 0.6602 - val_acc: 0.6354\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5105 - acc: 0.7766 - val_loss: 0.6665 - val_acc: 0.6562\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5169 - acc: 0.7474 - val_loss: 0.6702 - val_acc: 0.6250\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5325 - acc: 0.7440 - val_loss: 0.6700 - val_acc: 0.6562\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5161 - acc: 0.7388 - val_loss: 0.6544 - val_acc: 0.6562\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 8s 226ms/step - loss: 0.4769 - acc: 0.7629 - val_loss: 0.6523 - val_acc: 0.6562\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4904 - acc: 0.7801 - val_loss: 0.6762 - val_acc: 0.6250\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5086 - acc: 0.7440 - val_loss: 0.6736 - val_acc: 0.6250\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4811 - acc: 0.7753 - val_loss: 0.6669 - val_acc: 0.6354\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5067 - acc: 0.7491 - val_loss: 0.6540 - val_acc: 0.6667\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5290 - acc: 0.7595 - val_loss: 0.6492 - val_acc: 0.6562\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5145 - acc: 0.7543 - val_loss: 0.6693 - val_acc: 0.6458\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4922 - acc: 0.7629 - val_loss: 0.6532 - val_acc: 0.6458\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5070 - acc: 0.7474 - val_loss: 0.6751 - val_acc: 0.6458\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4722 - acc: 0.7749 - val_loss: 0.6662 - val_acc: 0.6458\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.5008 - acc: 0.7526 - val_loss: 0.6691 - val_acc: 0.6354\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4997 - acc: 0.7560 - val_loss: 0.6410 - val_acc: 0.6667\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5077 - acc: 0.7423 - val_loss: 0.6467 - val_acc: 0.6771\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5144 - acc: 0.7388 - val_loss: 0.6858 - val_acc: 0.6354\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 0.5220 - acc: 0.7440 - val_loss: 0.6663 - val_acc: 0.6562\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5005 - acc: 0.7423 - val_loss: 0.6792 - val_acc: 0.6458\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5110 - acc: 0.7577 - val_loss: 0.6728 - val_acc: 0.6771\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4844 - acc: 0.7818 - val_loss: 0.6742 - val_acc: 0.6458\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5066 - acc: 0.7595 - val_loss: 0.6687 - val_acc: 0.6458\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5266 - acc: 0.7337 - val_loss: 0.6621 - val_acc: 0.6667\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5100 - acc: 0.7509 - val_loss: 0.6803 - val_acc: 0.6458\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5120 - acc: 0.7474 - val_loss: 0.6673 - val_acc: 0.6667\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4903 - acc: 0.7577 - val_loss: 0.6656 - val_acc: 0.6562\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4996 - acc: 0.7457 - val_loss: 0.6589 - val_acc: 0.6667\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5326 - acc: 0.7457 - val_loss: 0.6753 - val_acc: 0.6562\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5095 - acc: 0.7577 - val_loss: 0.6693 - val_acc: 0.6458\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.5090 - acc: 0.7698 - val_loss: 0.6604 - val_acc: 0.6250\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5180 - acc: 0.7371 - val_loss: 0.6521 - val_acc: 0.6667\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.5209 - acc: 0.7354 - val_loss: 0.6739 - val_acc: 0.6458\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5037 - acc: 0.7526 - val_loss: 0.6713 - val_acc: 0.6458\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.4929 - acc: 0.7732 - val_loss: 0.6835 - val_acc: 0.6354\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5526 - acc: 0.7216 - val_loss: 0.6610 - val_acc: 0.6562\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5077 - acc: 0.7526 - val_loss: 0.6694 - val_acc: 0.6458\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4968 - acc: 0.7560 - val_loss: 0.6684 - val_acc: 0.6667\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4685 - acc: 0.7732 - val_loss: 0.6770 - val_acc: 0.6354\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5042 - acc: 0.7612 - val_loss: 0.6728 - val_acc: 0.6458\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5032 - acc: 0.7595 - val_loss: 0.6465 - val_acc: 0.6562\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5081 - acc: 0.7405 - val_loss: 0.6792 - val_acc: 0.6250\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5148 - acc: 0.7440 - val_loss: 0.6788 - val_acc: 0.6354\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4688 - acc: 0.7560 - val_loss: 0.6737 - val_acc: 0.6562\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5352 - acc: 0.7337 - val_loss: 0.6536 - val_acc: 0.6667\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 0.5111 - acc: 0.7749 - val_loss: 0.6590 - val_acc: 0.6562\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5058 - acc: 0.7354 - val_loss: 0.6556 - val_acc: 0.6562\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5368 - acc: 0.7131 - val_loss: 0.6773 - val_acc: 0.6458\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4934 - acc: 0.7560 - val_loss: 0.6724 - val_acc: 0.6458\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5237 - acc: 0.7423 - val_loss: 0.6484 - val_acc: 0.6667\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5321 - acc: 0.7285 - val_loss: 0.6477 - val_acc: 0.6458\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4841 - acc: 0.7804 - val_loss: 0.6587 - val_acc: 0.6354\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5259 - acc: 0.7474 - val_loss: 0.6701 - val_acc: 0.6562\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5108 - acc: 0.7577 - val_loss: 0.6524 - val_acc: 0.6250\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5321 - acc: 0.7440 - val_loss: 0.6452 - val_acc: 0.6562\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4935 - acc: 0.7732 - val_loss: 0.6696 - val_acc: 0.6458\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5032 - acc: 0.7423 - val_loss: 0.6559 - val_acc: 0.6667\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5133 - acc: 0.7440 - val_loss: 0.6576 - val_acc: 0.6562\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 0.5300 - acc: 0.7320 - val_loss: 0.6594 - val_acc: 0.6562\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5453 - acc: 0.7148 - val_loss: 0.6561 - val_acc: 0.6667\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4883 - acc: 0.7577 - val_loss: 0.6607 - val_acc: 0.6562\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5225 - acc: 0.7302 - val_loss: 0.6498 - val_acc: 0.6667\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5168 - acc: 0.7560 - val_loss: 0.6602 - val_acc: 0.6562\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4993 - acc: 0.7440 - val_loss: 0.6620 - val_acc: 0.6562\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4981 - acc: 0.7612 - val_loss: 0.6663 - val_acc: 0.6562\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.4970 - acc: 0.7337 - val_loss: 0.6618 - val_acc: 0.6771\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5716 - acc: 0.7268 - val_loss: 0.6551 - val_acc: 0.6562\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5078 - acc: 0.7526 - val_loss: 0.6611 - val_acc: 0.6562\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4710 - acc: 0.7766 - val_loss: 0.6790 - val_acc: 0.6562\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4953 - acc: 0.7612 - val_loss: 0.6696 - val_acc: 0.6458\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4865 - acc: 0.7629 - val_loss: 0.6498 - val_acc: 0.6458\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4952 - acc: 0.7560 - val_loss: 0.6525 - val_acc: 0.6562\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5042 - acc: 0.7577 - val_loss: 0.6685 - val_acc: 0.6354\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5327 - acc: 0.7148 - val_loss: 0.6604 - val_acc: 0.6562\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5312 - acc: 0.7405 - val_loss: 0.6274 - val_acc: 0.6667\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5182 - acc: 0.7491 - val_loss: 0.6484 - val_acc: 0.6458\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 0.5176 - acc: 0.7440 - val_loss: 0.6502 - val_acc: 0.6458\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5352 - acc: 0.7337 - val_loss: 0.6656 - val_acc: 0.6562\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4858 - acc: 0.7646 - val_loss: 0.6434 - val_acc: 0.6354\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5059 - acc: 0.7388 - val_loss: 0.6746 - val_acc: 0.6354\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5196 - acc: 0.7251 - val_loss: 0.6630 - val_acc: 0.6354\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5002 - acc: 0.7784 - val_loss: 0.6618 - val_acc: 0.6458\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.5066 - acc: 0.7543 - val_loss: 0.6630 - val_acc: 0.6458\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 0.5173 - acc: 0.7354 - val_loss: 0.6468 - val_acc: 0.6458\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.4997 - acc: 0.7560 - val_loss: 0.6491 - val_acc: 0.6562\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5102 - acc: 0.7251 - val_loss: 0.6499 - val_acc: 0.6562\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5100 - acc: 0.7388 - val_loss: 0.6734 - val_acc: 0.6354\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.5416 - acc: 0.7388 - val_loss: 0.6598 - val_acc: 0.6354\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4731 - acc: 0.7770 - val_loss: 0.6482 - val_acc: 0.6354\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5308 - acc: 0.7285 - val_loss: 0.6519 - val_acc: 0.6562\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5700 - acc: 0.7045 - val_loss: 0.6498 - val_acc: 0.6562\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5175 - acc: 0.7629 - val_loss: 0.6562 - val_acc: 0.6562\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5002 - acc: 0.7526 - val_loss: 0.6651 - val_acc: 0.6458\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5084 - acc: 0.7474 - val_loss: 0.6620 - val_acc: 0.6458\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5109 - acc: 0.7474 - val_loss: 0.6538 - val_acc: 0.6562\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5174 - acc: 0.7371 - val_loss: 0.6632 - val_acc: 0.6458\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5201 - acc: 0.7405 - val_loss: 0.6536 - val_acc: 0.6562\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4969 - acc: 0.7612 - val_loss: 0.6838 - val_acc: 0.6354\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4940 - acc: 0.7577 - val_loss: 0.6584 - val_acc: 0.6354\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5350 - acc: 0.7320 - val_loss: 0.6452 - val_acc: 0.6562\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5187 - acc: 0.7423 - val_loss: 0.6578 - val_acc: 0.6562\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5161 - acc: 0.7354 - val_loss: 0.6793 - val_acc: 0.6562\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5054 - acc: 0.7251 - val_loss: 0.6545 - val_acc: 0.6667\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5026 - acc: 0.7698 - val_loss: 0.6679 - val_acc: 0.6354\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4865 - acc: 0.7784 - val_loss: 0.6793 - val_acc: 0.6354\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5121 - acc: 0.7440 - val_loss: 0.6487 - val_acc: 0.6667\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5374 - acc: 0.7405 - val_loss: 0.6641 - val_acc: 0.6667\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5193 - acc: 0.7474 - val_loss: 0.6798 - val_acc: 0.6458\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5295 - acc: 0.7199 - val_loss: 0.6668 - val_acc: 0.6458\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5025 - acc: 0.7534 - val_loss: 0.6621 - val_acc: 0.6354\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5206 - acc: 0.7285 - val_loss: 0.6663 - val_acc: 0.6458\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5279 - acc: 0.7320 - val_loss: 0.6654 - val_acc: 0.6354\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5203 - acc: 0.7601 - val_loss: 0.6705 - val_acc: 0.6458\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5146 - acc: 0.7560 - val_loss: 0.6589 - val_acc: 0.6458\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4877 - acc: 0.7629 - val_loss: 0.6648 - val_acc: 0.6458\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4916 - acc: 0.7686 - val_loss: 0.6636 - val_acc: 0.6562\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5050 - acc: 0.7698 - val_loss: 0.6601 - val_acc: 0.6562\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5091 - acc: 0.7509 - val_loss: 0.6695 - val_acc: 0.6458\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4935 - acc: 0.7663 - val_loss: 0.6426 - val_acc: 0.6667\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5057 - acc: 0.7474 - val_loss: 0.6739 - val_acc: 0.6458\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4862 - acc: 0.7715 - val_loss: 0.6587 - val_acc: 0.6562\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5098 - acc: 0.7629 - val_loss: 0.6616 - val_acc: 0.6667\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 9s 249ms/step - loss: 0.5055 - acc: 0.7663 - val_loss: 0.6552 - val_acc: 0.6458\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5045 - acc: 0.7388 - val_loss: 0.6684 - val_acc: 0.6458\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4665 - acc: 0.7818 - val_loss: 0.6735 - val_acc: 0.6458\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 0.5221 - acc: 0.7302 - val_loss: 0.6675 - val_acc: 0.6458\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5054 - acc: 0.7491 - val_loss: 0.6487 - val_acc: 0.6458\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4930 - acc: 0.7595 - val_loss: 0.6486 - val_acc: 0.6458\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4741 - acc: 0.7715 - val_loss: 0.6637 - val_acc: 0.6458\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4821 - acc: 0.7629 - val_loss: 0.6554 - val_acc: 0.6562\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4840 - acc: 0.7612 - val_loss: 0.6719 - val_acc: 0.6562\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5185 - acc: 0.7354 - val_loss: 0.6693 - val_acc: 0.6667\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5243 - acc: 0.7280 - val_loss: 0.6777 - val_acc: 0.6354\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5615 - acc: 0.7165 - val_loss: 0.6528 - val_acc: 0.6458\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 0.4879 - acc: 0.7612 - val_loss: 0.6512 - val_acc: 0.6979\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5236 - acc: 0.7337 - val_loss: 0.6633 - val_acc: 0.6354\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5583 - acc: 0.7113 - val_loss: 0.6462 - val_acc: 0.6458\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4776 - acc: 0.7784 - val_loss: 0.6424 - val_acc: 0.6458\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 0.5345 - acc: 0.7234 - val_loss: 0.6390 - val_acc: 0.6458\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5080 - acc: 0.7405 - val_loss: 0.6446 - val_acc: 0.6562\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.4590 - acc: 0.7749 - val_loss: 0.6571 - val_acc: 0.6562\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.4980 - acc: 0.7509 - val_loss: 0.6544 - val_acc: 0.6458\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5008 - acc: 0.7509 - val_loss: 0.6676 - val_acc: 0.6354\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5085 - acc: 0.7423 - val_loss: 0.6642 - val_acc: 0.6458\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5191 - acc: 0.7388 - val_loss: 0.6558 - val_acc: 0.6354\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.5042 - acc: 0.7337 - val_loss: 0.6694 - val_acc: 0.6354\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5240 - acc: 0.7457 - val_loss: 0.6378 - val_acc: 0.6667\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.5014 - acc: 0.7474 - val_loss: 0.6598 - val_acc: 0.6250\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.4923 - acc: 0.7543 - val_loss: 0.6419 - val_acc: 0.6562\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5068 - acc: 0.7543 - val_loss: 0.6483 - val_acc: 0.6667\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4757 - acc: 0.7715 - val_loss: 0.6663 - val_acc: 0.6354\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.4905 - acc: 0.7646 - val_loss: 0.6513 - val_acc: 0.6562\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4709 - acc: 0.7801 - val_loss: 0.6806 - val_acc: 0.6354\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5225 - acc: 0.7371 - val_loss: 0.6558 - val_acc: 0.6562\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5069 - acc: 0.7509 - val_loss: 0.6588 - val_acc: 0.6458\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4690 - acc: 0.7801 - val_loss: 0.6589 - val_acc: 0.6562\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4782 - acc: 0.7663 - val_loss: 0.6539 - val_acc: 0.6562\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 0.5253 - acc: 0.7457 - val_loss: 0.6542 - val_acc: 0.6562\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5142 - acc: 0.7560 - val_loss: 0.6543 - val_acc: 0.6562\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5113 - acc: 0.7749 - val_loss: 0.6582 - val_acc: 0.6458\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4814 - acc: 0.7869 - val_loss: 0.6666 - val_acc: 0.6354\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4994 - acc: 0.7526 - val_loss: 0.6603 - val_acc: 0.6354\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4869 - acc: 0.7680 - val_loss: 0.6392 - val_acc: 0.6771\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4998 - acc: 0.7423 - val_loss: 0.6683 - val_acc: 0.6562\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.5052 - acc: 0.7526 - val_loss: 0.6772 - val_acc: 0.6458\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5144 - acc: 0.7388 - val_loss: 0.6621 - val_acc: 0.6354\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5381 - acc: 0.7337 - val_loss: 0.6567 - val_acc: 0.6458\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 0.5278 - acc: 0.7423 - val_loss: 0.6599 - val_acc: 0.6354\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5007 - acc: 0.7629 - val_loss: 0.6606 - val_acc: 0.6562\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 0.4976 - acc: 0.7474 - val_loss: 0.6397 - val_acc: 0.6771\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5187 - acc: 0.7491 - val_loss: 0.6457 - val_acc: 0.6562\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5135 - acc: 0.7440 - val_loss: 0.6627 - val_acc: 0.6562\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5179 - acc: 0.7423 - val_loss: 0.6518 - val_acc: 0.6562\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5224 - acc: 0.7371 - val_loss: 0.6492 - val_acc: 0.6562\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5375 - acc: 0.7131 - val_loss: 0.6561 - val_acc: 0.6562\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5519 - acc: 0.7096 - val_loss: 0.6481 - val_acc: 0.6458\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 0.4899 - acc: 0.7388 - val_loss: 0.6471 - val_acc: 0.6458\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.4978 - acc: 0.7646 - val_loss: 0.6568 - val_acc: 0.6562\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5039 - acc: 0.7612 - val_loss: 0.6620 - val_acc: 0.6458\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5142 - acc: 0.7457 - val_loss: 0.6551 - val_acc: 0.6667\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.4819 - acc: 0.7680 - val_loss: 0.6603 - val_acc: 0.6562\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4999 - acc: 0.7405 - val_loss: 0.6657 - val_acc: 0.6354\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4987 - acc: 0.7543 - val_loss: 0.6550 - val_acc: 0.5833\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.4990 - acc: 0.7629 - val_loss: 0.6602 - val_acc: 0.6354\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5489 - acc: 0.7199 - val_loss: 0.6670 - val_acc: 0.6354\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.5565 - acc: 0.7216 - val_loss: 0.6761 - val_acc: 0.6354\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5075 - acc: 0.7285 - val_loss: 0.6768 - val_acc: 0.6458\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.4861 - acc: 0.7526 - val_loss: 0.6686 - val_acc: 0.6458\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.4873 - acc: 0.7457 - val_loss: 0.6500 - val_acc: 0.6667\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4903 - acc: 0.7766 - val_loss: 0.6584 - val_acc: 0.6562\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5514 - acc: 0.7268 - val_loss: 0.6608 - val_acc: 0.6667\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5195 - acc: 0.7199 - val_loss: 0.6748 - val_acc: 0.6458\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 0.4801 - acc: 0.7595 - val_loss: 0.6329 - val_acc: 0.6771\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.5216 - acc: 0.7457 - val_loss: 0.6656 - val_acc: 0.6458\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5025 - acc: 0.7337 - val_loss: 0.6702 - val_acc: 0.6354\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5148 - acc: 0.7337 - val_loss: 0.6519 - val_acc: 0.6667\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4983 - acc: 0.7629 - val_loss: 0.6490 - val_acc: 0.6667\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4937 - acc: 0.7577 - val_loss: 0.6357 - val_acc: 0.6771\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5323 - acc: 0.7302 - val_loss: 0.6631 - val_acc: 0.6562\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4998 - acc: 0.7543 - val_loss: 0.6787 - val_acc: 0.6354\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.5195 - acc: 0.7234 - val_loss: 0.6504 - val_acc: 0.6667\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5278 - acc: 0.7354 - val_loss: 0.6581 - val_acc: 0.6667\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5548 - acc: 0.6942 - val_loss: 0.6616 - val_acc: 0.6458\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 0.5340 - acc: 0.7337 - val_loss: 0.6601 - val_acc: 0.6458\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5213 - acc: 0.7457 - val_loss: 0.6680 - val_acc: 0.6458\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5078 - acc: 0.7491 - val_loss: 0.6431 - val_acc: 0.6667\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4902 - acc: 0.7646 - val_loss: 0.6569 - val_acc: 0.6458\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 0.5082 - acc: 0.7474 - val_loss: 0.6574 - val_acc: 0.6458\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.5046 - acc: 0.7612 - val_loss: 0.6732 - val_acc: 0.6354\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5502 - acc: 0.7131 - val_loss: 0.6592 - val_acc: 0.6458\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5193 - acc: 0.7337 - val_loss: 0.6663 - val_acc: 0.6354\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4978 - acc: 0.7698 - val_loss: 0.6647 - val_acc: 0.6458\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4995 - acc: 0.7663 - val_loss: 0.6552 - val_acc: 0.6562\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5170 - acc: 0.7491 - val_loss: 0.6530 - val_acc: 0.6458\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5246 - acc: 0.7371 - val_loss: 0.6486 - val_acc: 0.6458\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 0.4955 - acc: 0.7749 - val_loss: 0.6378 - val_acc: 0.6562\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4935 - acc: 0.7646 - val_loss: 0.6597 - val_acc: 0.6562\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5095 - acc: 0.7423 - val_loss: 0.6717 - val_acc: 0.6458\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5381 - acc: 0.7216 - val_loss: 0.6594 - val_acc: 0.6562\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4993 - acc: 0.7405 - val_loss: 0.6546 - val_acc: 0.6667\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5143 - acc: 0.7320 - val_loss: 0.6594 - val_acc: 0.6562\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 0.4866 - acc: 0.7680 - val_loss: 0.6538 - val_acc: 0.6562\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5373 - acc: 0.7388 - val_loss: 0.6358 - val_acc: 0.6667\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5259 - acc: 0.7577 - val_loss: 0.6540 - val_acc: 0.6562\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 0.5297 - acc: 0.7423 - val_loss: 0.6594 - val_acc: 0.6458\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4987 - acc: 0.7818 - val_loss: 0.6537 - val_acc: 0.6562\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5210 - acc: 0.7526 - val_loss: 0.6441 - val_acc: 0.6667\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5122 - acc: 0.7491 - val_loss: 0.6500 - val_acc: 0.6667\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 0.4741 - acc: 0.7577 - val_loss: 0.6598 - val_acc: 0.6458\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4683 - acc: 0.7852 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4784 - acc: 0.7646 - val_loss: 0.6527 - val_acc: 0.6562\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5198 - acc: 0.7646 - val_loss: 0.6676 - val_acc: 0.6458\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 0.5065 - acc: 0.7405 - val_loss: 0.6539 - val_acc: 0.6562\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.5242 - acc: 0.7320 - val_loss: 0.6506 - val_acc: 0.6667\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.5270 - acc: 0.7474 - val_loss: 0.6644 - val_acc: 0.6562\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 0.5160 - acc: 0.7560 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.4910 - acc: 0.7517 - val_loss: 0.6685 - val_acc: 0.6562\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.4985 - acc: 0.7663 - val_loss: 0.6590 - val_acc: 0.6562\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.5457 - acc: 0.7199 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5121 - acc: 0.7423 - val_loss: 0.6603 - val_acc: 0.6458\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4984 - acc: 0.7509 - val_loss: 0.6528 - val_acc: 0.6562\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4771 - acc: 0.7749 - val_loss: 0.6587 - val_acc: 0.6771\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5137 - acc: 0.7320 - val_loss: 0.6877 - val_acc: 0.6458\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4853 - acc: 0.7749 - val_loss: 0.6704 - val_acc: 0.6562\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5307 - acc: 0.7251 - val_loss: 0.6611 - val_acc: 0.6562\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.4688 - acc: 0.7955 - val_loss: 0.6802 - val_acc: 0.6354\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5062 - acc: 0.7388 - val_loss: 0.6663 - val_acc: 0.6667\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 0.5099 - acc: 0.7595 - val_loss: 0.6617 - val_acc: 0.6562\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5121 - acc: 0.7268 - val_loss: 0.6842 - val_acc: 0.6354\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5199 - acc: 0.7440 - val_loss: 0.6534 - val_acc: 0.6667\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5145 - acc: 0.7440 - val_loss: 0.6676 - val_acc: 0.6562\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4840 - acc: 0.7560 - val_loss: 0.6742 - val_acc: 0.6667\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.4834 - acc: 0.7663 - val_loss: 0.6689 - val_acc: 0.6458\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4999 - acc: 0.7526 - val_loss: 0.6652 - val_acc: 0.6354\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 0.4887 - acc: 0.7423 - val_loss: 0.6436 - val_acc: 0.6771\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5170 - acc: 0.7560 - val_loss: 0.6523 - val_acc: 0.6667\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4863 - acc: 0.7629 - val_loss: 0.6512 - val_acc: 0.6667\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5245 - acc: 0.7388 - val_loss: 0.6722 - val_acc: 0.6562\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5203 - acc: 0.7440 - val_loss: 0.6708 - val_acc: 0.6458\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5114 - acc: 0.7577 - val_loss: 0.6722 - val_acc: 0.6458\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.5033 - acc: 0.7577 - val_loss: 0.6612 - val_acc: 0.6562\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5053 - acc: 0.7491 - val_loss: 0.6350 - val_acc: 0.6667\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4646 - acc: 0.7835 - val_loss: 0.6669 - val_acc: 0.6458\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5165 - acc: 0.7388 - val_loss: 0.6658 - val_acc: 0.6458\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5278 - acc: 0.7354 - val_loss: 0.6769 - val_acc: 0.6354\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4922 - acc: 0.7543 - val_loss: 0.6422 - val_acc: 0.6667\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4879 - acc: 0.7766 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 0.5172 - acc: 0.7474 - val_loss: 0.6813 - val_acc: 0.6458\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5421 - acc: 0.7251 - val_loss: 0.6497 - val_acc: 0.6667\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5248 - acc: 0.7423 - val_loss: 0.6773 - val_acc: 0.6458\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5002 - acc: 0.7371 - val_loss: 0.6557 - val_acc: 0.6562\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 0.5059 - acc: 0.7560 - val_loss: 0.6267 - val_acc: 0.6771\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 0.5028 - acc: 0.7474 - val_loss: 0.6457 - val_acc: 0.6667\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.5219 - acc: 0.7474 - val_loss: 0.6587 - val_acc: 0.6562\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5052 - acc: 0.7534 - val_loss: 0.6561 - val_acc: 0.6667\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5201 - acc: 0.7337 - val_loss: 0.6524 - val_acc: 0.6458\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5069 - acc: 0.7491 - val_loss: 0.6389 - val_acc: 0.6667\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.4990 - acc: 0.7491 - val_loss: 0.6441 - val_acc: 0.6562\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 0.5053 - acc: 0.7543 - val_loss: 0.6567 - val_acc: 0.6458\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.5155 - acc: 0.7629 - val_loss: 0.6564 - val_acc: 0.6458\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4975 - acc: 0.7543 - val_loss: 0.6705 - val_acc: 0.6458\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.4807 - acc: 0.7612 - val_loss: 0.6566 - val_acc: 0.6458\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5159 - acc: 0.7543 - val_loss: 0.6495 - val_acc: 0.6458\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5224 - acc: 0.7302 - val_loss: 0.6468 - val_acc: 0.6458\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 0.5044 - acc: 0.7560 - val_loss: 0.6392 - val_acc: 0.6667\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5093 - acc: 0.7405 - val_loss: 0.6324 - val_acc: 0.6562\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4970 - acc: 0.7371 - val_loss: 0.6587 - val_acc: 0.6354\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4875 - acc: 0.7595 - val_loss: 0.6548 - val_acc: 0.6562\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 0.5116 - acc: 0.7543 - val_loss: 0.6699 - val_acc: 0.6458\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4751 - acc: 0.7629 - val_loss: 0.6664 - val_acc: 0.6458\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 0.4897 - acc: 0.7663 - val_loss: 0.6765 - val_acc: 0.6354\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4784 - acc: 0.7568 - val_loss: 0.6501 - val_acc: 0.6562\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5137 - acc: 0.7423 - val_loss: 0.6592 - val_acc: 0.6354\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5045 - acc: 0.7491 - val_loss: 0.6498 - val_acc: 0.6458\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5118 - acc: 0.7388 - val_loss: 0.6590 - val_acc: 0.6562\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5052 - acc: 0.7440 - val_loss: 0.6748 - val_acc: 0.6562\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5160 - acc: 0.7509 - val_loss: 0.6390 - val_acc: 0.6667\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5099 - acc: 0.7474 - val_loss: 0.6428 - val_acc: 0.6562\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5413 - acc: 0.7302 - val_loss: 0.6756 - val_acc: 0.6354\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.5064 - acc: 0.7715 - val_loss: 0.6553 - val_acc: 0.6562\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5333 - acc: 0.7165 - val_loss: 0.6703 - val_acc: 0.6354\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4960 - acc: 0.7715 - val_loss: 0.6518 - val_acc: 0.6667\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5192 - acc: 0.7388 - val_loss: 0.6650 - val_acc: 0.6458\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.4841 - acc: 0.7509 - val_loss: 0.6549 - val_acc: 0.6562\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5047 - acc: 0.7440 - val_loss: 0.6648 - val_acc: 0.6458\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.4711 - acc: 0.7612 - val_loss: 0.6606 - val_acc: 0.6458\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5071 - acc: 0.7526 - val_loss: 0.6469 - val_acc: 0.6458\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4861 - acc: 0.7560 - val_loss: 0.6597 - val_acc: 0.6354\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5062 - acc: 0.7474 - val_loss: 0.6364 - val_acc: 0.6562\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 0.4907 - acc: 0.7629 - val_loss: 0.6371 - val_acc: 0.6667\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4809 - acc: 0.7801 - val_loss: 0.6550 - val_acc: 0.6354\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5330 - acc: 0.7251 - val_loss: 0.6426 - val_acc: 0.6250\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5244 - acc: 0.7354 - val_loss: 0.6493 - val_acc: 0.6458\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.4832 - acc: 0.7732 - val_loss: 0.6572 - val_acc: 0.6458\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4963 - acc: 0.7663 - val_loss: 0.6427 - val_acc: 0.6667\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5177 - acc: 0.7440 - val_loss: 0.6426 - val_acc: 0.6667\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5294 - acc: 0.7320 - val_loss: 0.6397 - val_acc: 0.6458\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5196 - acc: 0.7474 - val_loss: 0.6473 - val_acc: 0.6667\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4876 - acc: 0.7749 - val_loss: 0.6614 - val_acc: 0.6562\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.5382 - acc: 0.7348 - val_loss: 0.6534 - val_acc: 0.6354\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5000 - acc: 0.7509 - val_loss: 0.6393 - val_acc: 0.6458\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 0.5245 - acc: 0.7526 - val_loss: 0.6605 - val_acc: 0.6354\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4994 - acc: 0.7663 - val_loss: 0.6440 - val_acc: 0.6458\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.4849 - acc: 0.7577 - val_loss: 0.6509 - val_acc: 0.6667\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5049 - acc: 0.7526 - val_loss: 0.6716 - val_acc: 0.6354\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 0.5068 - acc: 0.7405 - val_loss: 0.6420 - val_acc: 0.6667\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 0.5339 - acc: 0.7234 - val_loss: 0.6510 - val_acc: 0.6458\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5146 - acc: 0.7320 - val_loss: 0.6657 - val_acc: 0.6562\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5110 - acc: 0.7629 - val_loss: 0.6464 - val_acc: 0.6667\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5117 - acc: 0.7440 - val_loss: 0.6574 - val_acc: 0.6562\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 0.5125 - acc: 0.7371 - val_loss: 0.6327 - val_acc: 0.6771\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5126 - acc: 0.7646 - val_loss: 0.6684 - val_acc: 0.6458\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4859 - acc: 0.7440 - val_loss: 0.6434 - val_acc: 0.6667\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 0.5059 - acc: 0.7474 - val_loss: 0.6664 - val_acc: 0.6458\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5360 - acc: 0.7388 - val_loss: 0.6574 - val_acc: 0.6667\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5108 - acc: 0.7491 - val_loss: 0.6518 - val_acc: 0.6667\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5184 - acc: 0.7302 - val_loss: 0.6654 - val_acc: 0.6354\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.4861 - acc: 0.7680 - val_loss: 0.6771 - val_acc: 0.6354\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4998 - acc: 0.7612 - val_loss: 0.6608 - val_acc: 0.6667\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4957 - acc: 0.7663 - val_loss: 0.6506 - val_acc: 0.6667\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5157 - acc: 0.7457 - val_loss: 0.6339 - val_acc: 0.6771\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5051 - acc: 0.7526 - val_loss: 0.6631 - val_acc: 0.6562\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5372 - acc: 0.7079 - val_loss: 0.6520 - val_acc: 0.6562\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4852 - acc: 0.7663 - val_loss: 0.6753 - val_acc: 0.6458\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5164 - acc: 0.7560 - val_loss: 0.6684 - val_acc: 0.6354\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5449 - acc: 0.7113 - val_loss: 0.6506 - val_acc: 0.6458\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5010 - acc: 0.7595 - val_loss: 0.6704 - val_acc: 0.6458\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4994 - acc: 0.7491 - val_loss: 0.6653 - val_acc: 0.6354\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5484 - acc: 0.7234 - val_loss: 0.6664 - val_acc: 0.6458\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5013 - acc: 0.7663 - val_loss: 0.6612 - val_acc: 0.6562\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 0.4871 - acc: 0.7491 - val_loss: 0.6684 - val_acc: 0.6458\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5022 - acc: 0.7354 - val_loss: 0.6425 - val_acc: 0.6667\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 0.4926 - acc: 0.7560 - val_loss: 0.6712 - val_acc: 0.6458\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5516 - acc: 0.7216 - val_loss: 0.6551 - val_acc: 0.6667\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.5680 - acc: 0.7027 - val_loss: 0.6507 - val_acc: 0.6562\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5091 - acc: 0.7577 - val_loss: 0.6689 - val_acc: 0.6354\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5033 - acc: 0.7526 - val_loss: 0.6532 - val_acc: 0.6562\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 0.4911 - acc: 0.7680 - val_loss: 0.6406 - val_acc: 0.6354\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5228 - acc: 0.7302 - val_loss: 0.6593 - val_acc: 0.6458\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 0.5275 - acc: 0.7457 - val_loss: 0.6590 - val_acc: 0.6458\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4955 - acc: 0.7663 - val_loss: 0.6723 - val_acc: 0.6354\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4886 - acc: 0.7663 - val_loss: 0.6415 - val_acc: 0.6562\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4905 - acc: 0.7543 - val_loss: 0.6585 - val_acc: 0.6458\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5121 - acc: 0.7491 - val_loss: 0.6453 - val_acc: 0.6667\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.4897 - acc: 0.7526 - val_loss: 0.6501 - val_acc: 0.6667\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5160 - acc: 0.7423 - val_loss: 0.6514 - val_acc: 0.6458\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4734 - acc: 0.7715 - val_loss: 0.6541 - val_acc: 0.6458\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.4711 - acc: 0.7560 - val_loss: 0.6460 - val_acc: 0.6562\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4913 - acc: 0.7595 - val_loss: 0.6724 - val_acc: 0.6354\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5088 - acc: 0.7491 - val_loss: 0.6272 - val_acc: 0.6250\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4971 - acc: 0.7517 - val_loss: 0.6757 - val_acc: 0.6354\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 0.5630 - acc: 0.7062 - val_loss: 0.6548 - val_acc: 0.6458\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4996 - acc: 0.7595 - val_loss: 0.6553 - val_acc: 0.6562\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 0.5092 - acc: 0.7526 - val_loss: 0.6529 - val_acc: 0.6458\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5304 - acc: 0.7320 - val_loss: 0.6556 - val_acc: 0.6562\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5046 - acc: 0.7440 - val_loss: 0.6438 - val_acc: 0.6562\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5159 - acc: 0.7354 - val_loss: 0.6429 - val_acc: 0.6562\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5390 - acc: 0.7268 - val_loss: 0.6520 - val_acc: 0.6458\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5019 - acc: 0.7595 - val_loss: 0.6721 - val_acc: 0.6354\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5204 - acc: 0.7423 - val_loss: 0.6379 - val_acc: 0.6667\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5222 - acc: 0.7320 - val_loss: 0.6537 - val_acc: 0.6562\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 0.5114 - acc: 0.7285 - val_loss: 0.6512 - val_acc: 0.6562\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4913 - acc: 0.7474 - val_loss: 0.6485 - val_acc: 0.6562\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 0.4943 - acc: 0.7595 - val_loss: 0.6414 - val_acc: 0.6562\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.5125 - acc: 0.7440 - val_loss: 0.6704 - val_acc: 0.6354\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5355 - acc: 0.7354 - val_loss: 0.6665 - val_acc: 0.6354\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.5257 - acc: 0.7302 - val_loss: 0.6671 - val_acc: 0.6354\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 0.5108 - acc: 0.7560 - val_loss: 0.6575 - val_acc: 0.6562\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5048 - acc: 0.7509 - val_loss: 0.6561 - val_acc: 0.6562\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4918 - acc: 0.7405 - val_loss: 0.6657 - val_acc: 0.6458\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 0.5196 - acc: 0.7388 - val_loss: 0.6608 - val_acc: 0.6458\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 0.4716 - acc: 0.7818 - val_loss: 0.6598 - val_acc: 0.6562\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5288 - acc: 0.7234 - val_loss: 0.6754 - val_acc: 0.6458\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4982 - acc: 0.7595 - val_loss: 0.6600 - val_acc: 0.6562\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.5070 - acc: 0.7680 - val_loss: 0.6643 - val_acc: 0.6458\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5090 - acc: 0.7543 - val_loss: 0.6541 - val_acc: 0.6562\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5044 - acc: 0.7285 - val_loss: 0.6569 - val_acc: 0.6667\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.4773 - acc: 0.7818 - val_loss: 0.6711 - val_acc: 0.6354\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5359 - acc: 0.7285 - val_loss: 0.6643 - val_acc: 0.6562\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5336 - acc: 0.7268 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5329 - acc: 0.7320 - val_loss: 0.6603 - val_acc: 0.6354\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5012 - acc: 0.7320 - val_loss: 0.6436 - val_acc: 0.6667\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5073 - acc: 0.7612 - val_loss: 0.6296 - val_acc: 0.6875\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.4987 - acc: 0.7509 - val_loss: 0.6567 - val_acc: 0.6458\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4911 - acc: 0.7663 - val_loss: 0.6322 - val_acc: 0.6771\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5016 - acc: 0.7595 - val_loss: 0.6650 - val_acc: 0.6562\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5106 - acc: 0.7457 - val_loss: 0.6538 - val_acc: 0.6667\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.4842 - acc: 0.7766 - val_loss: 0.6600 - val_acc: 0.6562\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.4921 - acc: 0.7577 - val_loss: 0.6616 - val_acc: 0.6354\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4997 - acc: 0.7440 - val_loss: 0.6612 - val_acc: 0.6458\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 0.4840 - acc: 0.7698 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5229 - acc: 0.7449 - val_loss: 0.6717 - val_acc: 0.6562\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 11s 264ms/step - loss: 0.5314 - acc: 0.7474 - val_loss: 0.6505 - val_acc: 0.6562\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.5299 - acc: 0.7337 - val_loss: 0.6460 - val_acc: 0.6771\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.4983 - acc: 0.7577 - val_loss: 0.6789 - val_acc: 0.6458\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.5300 - acc: 0.7423 - val_loss: 0.6592 - val_acc: 0.6562\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5138 - acc: 0.7698 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.4958 - acc: 0.7526 - val_loss: 0.6568 - val_acc: 0.6458\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4939 - acc: 0.7423 - val_loss: 0.6682 - val_acc: 0.6354\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4641 - acc: 0.7852 - val_loss: 0.6454 - val_acc: 0.6562\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5213 - acc: 0.7405 - val_loss: 0.6490 - val_acc: 0.6458\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5028 - acc: 0.7698 - val_loss: 0.6457 - val_acc: 0.6562\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4939 - acc: 0.7543 - val_loss: 0.6459 - val_acc: 0.6562\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5234 - acc: 0.7251 - val_loss: 0.6455 - val_acc: 0.6667\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5098 - acc: 0.7577 - val_loss: 0.6406 - val_acc: 0.6771\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5129 - acc: 0.7526 - val_loss: 0.6666 - val_acc: 0.6562\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 0.5112 - acc: 0.7440 - val_loss: 0.6605 - val_acc: 0.6458\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4907 - acc: 0.7577 - val_loss: 0.6653 - val_acc: 0.6562\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4979 - acc: 0.7646 - val_loss: 0.6575 - val_acc: 0.6667\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5064 - acc: 0.7612 - val_loss: 0.6626 - val_acc: 0.6562\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.4950 - acc: 0.7646 - val_loss: 0.6507 - val_acc: 0.6562\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 0.5352 - acc: 0.7268 - val_loss: 0.6309 - val_acc: 0.6667\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4880 - acc: 0.7698 - val_loss: 0.6398 - val_acc: 0.6562\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4851 - acc: 0.7474 - val_loss: 0.6613 - val_acc: 0.6458\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 0.4991 - acc: 0.7320 - val_loss: 0.6643 - val_acc: 0.6562\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5110 - acc: 0.7646 - val_loss: 0.6536 - val_acc: 0.6667\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4980 - acc: 0.7526 - val_loss: 0.6571 - val_acc: 0.6458\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5637 - acc: 0.7079 - val_loss: 0.6577 - val_acc: 0.6458\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 0.5110 - acc: 0.7354 - val_loss: 0.6414 - val_acc: 0.6562\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.5400 - acc: 0.7382 - val_loss: 0.6305 - val_acc: 0.6667\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5327 - acc: 0.7268 - val_loss: 0.6498 - val_acc: 0.6667\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4974 - acc: 0.7577 - val_loss: 0.6700 - val_acc: 0.6458\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 0.5353 - acc: 0.7199 - val_loss: 0.6672 - val_acc: 0.6458\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5188 - acc: 0.7079 - val_loss: 0.6540 - val_acc: 0.6458\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5012 - acc: 0.7491 - val_loss: 0.6546 - val_acc: 0.6667\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4751 - acc: 0.7560 - val_loss: 0.6526 - val_acc: 0.6562\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 0.5011 - acc: 0.7388 - val_loss: 0.6615 - val_acc: 0.6458\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4910 - acc: 0.7680 - val_loss: 0.6693 - val_acc: 0.6354\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4760 - acc: 0.7577 - val_loss: 0.6468 - val_acc: 0.6667\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5093 - acc: 0.7405 - val_loss: 0.6524 - val_acc: 0.6458\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 0.5178 - acc: 0.7405 - val_loss: 0.6628 - val_acc: 0.6562\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5019 - acc: 0.7509 - val_loss: 0.6492 - val_acc: 0.6354\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.5051 - acc: 0.7354 - val_loss: 0.6528 - val_acc: 0.6562\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.4926 - acc: 0.7612 - val_loss: 0.6423 - val_acc: 0.6562\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.4985 - acc: 0.7560 - val_loss: 0.6555 - val_acc: 0.6771\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5073 - acc: 0.7285 - val_loss: 0.6496 - val_acc: 0.6562\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.5020 - acc: 0.7320 - val_loss: 0.6561 - val_acc: 0.6562\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 0.4935 - acc: 0.7560 - val_loss: 0.6539 - val_acc: 0.6458\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 0.4863 - acc: 0.7698 - val_loss: 0.6222 - val_acc: 0.6771\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4728 - acc: 0.7457 - val_loss: 0.6675 - val_acc: 0.6354\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4801 - acc: 0.7595 - val_loss: 0.6562 - val_acc: 0.6458\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5260 - acc: 0.7320 - val_loss: 0.6673 - val_acc: 0.6354\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5001 - acc: 0.7388 - val_loss: 0.6474 - val_acc: 0.6562\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5056 - acc: 0.7509 - val_loss: 0.6404 - val_acc: 0.6458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training performance"
      ],
      "metadata": {
        "id": "YFxMJycOpEew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "l9PYKbRNpEK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "99434104-657b-41fb-baed-4846464ee8d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAVE model"
      ],
      "metadata": {
        "id": "kNtlq_LZpKNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_UNfreeze_3000.h5')"
      ],
      "metadata": {
        "id": "TegSVqblpLEZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "KdsdaIgMpMeG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_UNfreeze_3000.h5\")"
      ],
      "metadata": {
        "id": "sZFZ1c_kpMcN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1af7dc1c-c652-45d1-e313-da10b55f61b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_46d3be1c-ef12-4a34-808f-4b331923c865\", \"2Class_UNfreeze_3000.h5\", 16604952)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEfCB1RfpMZ6"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}