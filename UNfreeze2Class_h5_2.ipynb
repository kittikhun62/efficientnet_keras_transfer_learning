{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMAf6NYWESfUPihhYRuWCMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/UNfreeze2Class_h5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "bB2gfuYWczY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpXyZ9k5YIJu",
        "outputId": "c4f197ea-35fb-4808-a71e-456e3a353233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7mqo2OLAc2pG",
        "outputId": "5809bf0c-ed73-4a91-ffb9-4a772de2027a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8a6a918-d87d-4506-b42c-b2f82883183e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8a6a918-d87d-4506-b42c-b2f82883183e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8a6a918-d87d-4506-b42c-b2f82883183e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8a6a918-d87d-4506-b42c-b2f82883183e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "6d31e002-6983-4709-dc8d-ea81c99d77b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 19.17 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df21cdb-3908-4c6d-bf69-8f41ceb4d68d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "564e627c-c365-46da-af53-b6686fa915ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/new/2Class_2000.h5')\n",
        "\n",
        "from efficientnet.layers import Swish, DropConnect\n",
        "from efficientnet.model import ConvKernalInitializer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'ConvKernalInitializer': ConvKernalInitializer,\n",
        "    'Swish': Swish,\n",
        "    'DropConnect':DropConnect\n",
        "})"
      ],
      "metadata": {
        "id": "MwNk3sNbc2Qx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model \n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/new/2Class_2000.h5')\n",
        "height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "4v4nzKQtJZ4z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UDxvF-b8Jcq1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "     \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znE38DtIJeN-",
        "outputId": "b5c192a2-53bb-4d5f-d659-17683f0fd147"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply_16\n",
        "# set 'multiply_16' and following layers trainable\n",
        "model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'multiply_16':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False  \n",
        "     "
      ],
      "metadata": {
        "id": "tEaPoHJCJq13"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "xkN8n36elECN",
        "outputId": "3bf93bf4-5418-4709-df0a-b3920c0747ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7ERVUfUJsQq",
        "outputId": "4fdc22b2-1f64-48d9-9a7f-9fdab6fa9a3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-caa7b37242a8>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 86s 2s/step - loss: 0.4807 - acc: 0.7440 - val_loss: 0.6758 - val_acc: 0.6354\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5301 - acc: 0.7216 - val_loss: 0.6626 - val_acc: 0.6458\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5554 - acc: 0.7199 - val_loss: 0.6648 - val_acc: 0.6354\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 8s 224ms/step - loss: 0.5156 - acc: 0.7440 - val_loss: 0.6723 - val_acc: 0.6354\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 3s 86ms/step - loss: 0.5016 - acc: 0.7405 - val_loss: 0.6537 - val_acc: 0.6354\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4632 - acc: 0.7904 - val_loss: 0.6599 - val_acc: 0.6354\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.4931 - acc: 0.7526 - val_loss: 0.6593 - val_acc: 0.6458\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5286 - acc: 0.7440 - val_loss: 0.6671 - val_acc: 0.6354\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.5071 - acc: 0.7320 - val_loss: 0.6730 - val_acc: 0.6354\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5075 - acc: 0.7423 - val_loss: 0.6599 - val_acc: 0.6458\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.5234 - acc: 0.7440 - val_loss: 0.6631 - val_acc: 0.6458\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5043 - acc: 0.7715 - val_loss: 0.6927 - val_acc: 0.6354\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5878 - acc: 0.6942 - val_loss: 0.6625 - val_acc: 0.6667\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5182 - acc: 0.7491 - val_loss: 0.6755 - val_acc: 0.6250\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4805 - acc: 0.7612 - val_loss: 0.6710 - val_acc: 0.6250\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.4945 - acc: 0.7698 - val_loss: 0.6883 - val_acc: 0.6146\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 8s 204ms/step - loss: 0.5141 - acc: 0.7199 - val_loss: 0.6608 - val_acc: 0.6250\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5089 - acc: 0.7526 - val_loss: 0.6667 - val_acc: 0.6354\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5299 - acc: 0.7509 - val_loss: 0.6587 - val_acc: 0.6458\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4901 - acc: 0.7509 - val_loss: 0.6774 - val_acc: 0.6250\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5089 - acc: 0.7560 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4971 - acc: 0.7732 - val_loss: 0.6357 - val_acc: 0.6562\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4749 - acc: 0.7595 - val_loss: 0.6604 - val_acc: 0.6354\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5271 - acc: 0.7509 - val_loss: 0.6482 - val_acc: 0.6458\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5030 - acc: 0.7302 - val_loss: 0.6603 - val_acc: 0.6354\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5320 - acc: 0.7045 - val_loss: 0.6673 - val_acc: 0.6562\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4740 - acc: 0.7732 - val_loss: 0.6509 - val_acc: 0.6667\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 0.5057 - acc: 0.7320 - val_loss: 0.6622 - val_acc: 0.6250\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5218 - acc: 0.7440 - val_loss: 0.6305 - val_acc: 0.6562\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.5236 - acc: 0.7354 - val_loss: 0.6396 - val_acc: 0.6771\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5155 - acc: 0.7526 - val_loss: 0.6559 - val_acc: 0.6458\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5201 - acc: 0.7457 - val_loss: 0.6544 - val_acc: 0.6354\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.4757 - acc: 0.7629 - val_loss: 0.6685 - val_acc: 0.6562\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5248 - acc: 0.7230 - val_loss: 0.6492 - val_acc: 0.6771\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.5221 - acc: 0.7405 - val_loss: 0.6665 - val_acc: 0.6354\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5108 - acc: 0.7371 - val_loss: 0.6582 - val_acc: 0.6562\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4898 - acc: 0.7663 - val_loss: 0.6563 - val_acc: 0.6667\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.4911 - acc: 0.7371 - val_loss: 0.6686 - val_acc: 0.6146\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5137 - acc: 0.7646 - val_loss: 0.6721 - val_acc: 0.6458\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5453 - acc: 0.7234 - val_loss: 0.6885 - val_acc: 0.6354\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.5060 - acc: 0.7715 - val_loss: 0.6479 - val_acc: 0.6458\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5148 - acc: 0.7457 - val_loss: 0.6706 - val_acc: 0.6458\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5099 - acc: 0.7423 - val_loss: 0.6492 - val_acc: 0.6771\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.4992 - acc: 0.7474 - val_loss: 0.6581 - val_acc: 0.6354\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5113 - acc: 0.7440 - val_loss: 0.6560 - val_acc: 0.6354\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5162 - acc: 0.7526 - val_loss: 0.6668 - val_acc: 0.6250\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5120 - acc: 0.7405 - val_loss: 0.6549 - val_acc: 0.6458\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4812 - acc: 0.7663 - val_loss: 0.6692 - val_acc: 0.6458\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5345 - acc: 0.7234 - val_loss: 0.6606 - val_acc: 0.6562\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5348 - acc: 0.7423 - val_loss: 0.6569 - val_acc: 0.6354\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5222 - acc: 0.7371 - val_loss: 0.6437 - val_acc: 0.6458\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5165 - acc: 0.7457 - val_loss: 0.6752 - val_acc: 0.6562\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4802 - acc: 0.7749 - val_loss: 0.6455 - val_acc: 0.6562\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.4863 - acc: 0.7543 - val_loss: 0.6633 - val_acc: 0.6250\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4903 - acc: 0.7646 - val_loss: 0.6653 - val_acc: 0.6562\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.5149 - acc: 0.7577 - val_loss: 0.6609 - val_acc: 0.6562\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4851 - acc: 0.7818 - val_loss: 0.6809 - val_acc: 0.6771\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5230 - acc: 0.7285 - val_loss: 0.6681 - val_acc: 0.6354\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5423 - acc: 0.7234 - val_loss: 0.6648 - val_acc: 0.6250\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5204 - acc: 0.7388 - val_loss: 0.6793 - val_acc: 0.6458\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5216 - acc: 0.7348 - val_loss: 0.6812 - val_acc: 0.6250\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.5314 - acc: 0.7423 - val_loss: 0.6758 - val_acc: 0.6250\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5105 - acc: 0.7405 - val_loss: 0.6632 - val_acc: 0.6667\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.4919 - acc: 0.7595 - val_loss: 0.6658 - val_acc: 0.6562\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5729 - acc: 0.7234 - val_loss: 0.6523 - val_acc: 0.6667\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4870 - acc: 0.7543 - val_loss: 0.6699 - val_acc: 0.6458\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5043 - acc: 0.7715 - val_loss: 0.6708 - val_acc: 0.6667\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5210 - acc: 0.7423 - val_loss: 0.6783 - val_acc: 0.6458\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5050 - acc: 0.7423 - val_loss: 0.6729 - val_acc: 0.6354\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5116 - acc: 0.7354 - val_loss: 0.6834 - val_acc: 0.6250\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5040 - acc: 0.7440 - val_loss: 0.6825 - val_acc: 0.6458\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5125 - acc: 0.7491 - val_loss: 0.6810 - val_acc: 0.6146\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5074 - acc: 0.7543 - val_loss: 0.6559 - val_acc: 0.6354\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5235 - acc: 0.7509 - val_loss: 0.6517 - val_acc: 0.6354\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5536 - acc: 0.7285 - val_loss: 0.6708 - val_acc: 0.6354\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5145 - acc: 0.7371 - val_loss: 0.6841 - val_acc: 0.6562\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5075 - acc: 0.7526 - val_loss: 0.6838 - val_acc: 0.6250\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5066 - acc: 0.7612 - val_loss: 0.6541 - val_acc: 0.6458\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5003 - acc: 0.7629 - val_loss: 0.6721 - val_acc: 0.6250\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4811 - acc: 0.7818 - val_loss: 0.6832 - val_acc: 0.6146\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4790 - acc: 0.7663 - val_loss: 0.6550 - val_acc: 0.6354\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4973 - acc: 0.7646 - val_loss: 0.6745 - val_acc: 0.6250\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5024 - acc: 0.7440 - val_loss: 0.6657 - val_acc: 0.6354\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5329 - acc: 0.7371 - val_loss: 0.6644 - val_acc: 0.6562\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5196 - acc: 0.7629 - val_loss: 0.6905 - val_acc: 0.6458\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4966 - acc: 0.7474 - val_loss: 0.6743 - val_acc: 0.6458\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4942 - acc: 0.7577 - val_loss: 0.6788 - val_acc: 0.6146\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 9s 213ms/step - loss: 0.5090 - acc: 0.7388 - val_loss: 0.6643 - val_acc: 0.6562\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5028 - acc: 0.7509 - val_loss: 0.6518 - val_acc: 0.6458\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4829 - acc: 0.7749 - val_loss: 0.6591 - val_acc: 0.6250\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5334 - acc: 0.7337 - val_loss: 0.6367 - val_acc: 0.6458\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5027 - acc: 0.7491 - val_loss: 0.6805 - val_acc: 0.6250\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5450 - acc: 0.7234 - val_loss: 0.6610 - val_acc: 0.6354\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5514 - acc: 0.7388 - val_loss: 0.6415 - val_acc: 0.6458\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5081 - acc: 0.7371 - val_loss: 0.6641 - val_acc: 0.6354\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5043 - acc: 0.7560 - val_loss: 0.6764 - val_acc: 0.6146\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5250 - acc: 0.7405 - val_loss: 0.6644 - val_acc: 0.6458\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5258 - acc: 0.7457 - val_loss: 0.6571 - val_acc: 0.6354\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5073 - acc: 0.7474 - val_loss: 0.6725 - val_acc: 0.6562\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4914 - acc: 0.7440 - val_loss: 0.6787 - val_acc: 0.6458\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4798 - acc: 0.7938 - val_loss: 0.6810 - val_acc: 0.6354\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5106 - acc: 0.7474 - val_loss: 0.6719 - val_acc: 0.6354\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5128 - acc: 0.7285 - val_loss: 0.6602 - val_acc: 0.6354\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5149 - acc: 0.7320 - val_loss: 0.6438 - val_acc: 0.6458\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.4900 - acc: 0.7577 - val_loss: 0.6729 - val_acc: 0.6562\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5132 - acc: 0.7474 - val_loss: 0.6698 - val_acc: 0.6250\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5345 - acc: 0.7302 - val_loss: 0.6652 - val_acc: 0.6354\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4794 - acc: 0.7491 - val_loss: 0.6448 - val_acc: 0.6667\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4896 - acc: 0.7818 - val_loss: 0.6711 - val_acc: 0.6250\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5271 - acc: 0.7354 - val_loss: 0.6576 - val_acc: 0.6354\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4841 - acc: 0.7715 - val_loss: 0.6652 - val_acc: 0.6354\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 8s 203ms/step - loss: 0.5315 - acc: 0.7148 - val_loss: 0.6877 - val_acc: 0.6354\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5276 - acc: 0.7474 - val_loss: 0.6539 - val_acc: 0.6354\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5530 - acc: 0.7440 - val_loss: 0.6642 - val_acc: 0.6354\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.5177 - acc: 0.7526 - val_loss: 0.6777 - val_acc: 0.6250\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.4942 - acc: 0.7543 - val_loss: 0.6615 - val_acc: 0.6458\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5283 - acc: 0.7354 - val_loss: 0.6485 - val_acc: 0.6562\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5029 - acc: 0.7646 - val_loss: 0.6648 - val_acc: 0.6562\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4821 - acc: 0.7543 - val_loss: 0.6788 - val_acc: 0.6354\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.5343 - acc: 0.7199 - val_loss: 0.6502 - val_acc: 0.6771\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5344 - acc: 0.7405 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5435 - acc: 0.7371 - val_loss: 0.6565 - val_acc: 0.6667\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5123 - acc: 0.7354 - val_loss: 0.6692 - val_acc: 0.6458\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5326 - acc: 0.7268 - val_loss: 0.6716 - val_acc: 0.6250\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5362 - acc: 0.7440 - val_loss: 0.6497 - val_acc: 0.6354\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5256 - acc: 0.7320 - val_loss: 0.6371 - val_acc: 0.6667\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4964 - acc: 0.7577 - val_loss: 0.6624 - val_acc: 0.6354\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5427 - acc: 0.7199 - val_loss: 0.6740 - val_acc: 0.6354\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5136 - acc: 0.7405 - val_loss: 0.6655 - val_acc: 0.6354\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5163 - acc: 0.7474 - val_loss: 0.6620 - val_acc: 0.6354\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5060 - acc: 0.7560 - val_loss: 0.6717 - val_acc: 0.6250\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4916 - acc: 0.7526 - val_loss: 0.6688 - val_acc: 0.6354\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5056 - acc: 0.7509 - val_loss: 0.6692 - val_acc: 0.6354\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5060 - acc: 0.7371 - val_loss: 0.6738 - val_acc: 0.6250\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5486 - acc: 0.7148 - val_loss: 0.6612 - val_acc: 0.6354\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4896 - acc: 0.7749 - val_loss: 0.6734 - val_acc: 0.6250\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5097 - acc: 0.7440 - val_loss: 0.6639 - val_acc: 0.6354\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4948 - acc: 0.7534 - val_loss: 0.6699 - val_acc: 0.6250\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5245 - acc: 0.7491 - val_loss: 0.6633 - val_acc: 0.6458\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4868 - acc: 0.7663 - val_loss: 0.6744 - val_acc: 0.6250\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.4763 - acc: 0.7698 - val_loss: 0.6824 - val_acc: 0.6354\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5191 - acc: 0.7268 - val_loss: 0.6665 - val_acc: 0.6354\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5148 - acc: 0.7526 - val_loss: 0.6674 - val_acc: 0.6562\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.4998 - acc: 0.7560 - val_loss: 0.6762 - val_acc: 0.6354\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5262 - acc: 0.7165 - val_loss: 0.6859 - val_acc: 0.6146\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5112 - acc: 0.7371 - val_loss: 0.6780 - val_acc: 0.6354\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5136 - acc: 0.7423 - val_loss: 0.6680 - val_acc: 0.6250\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5263 - acc: 0.7388 - val_loss: 0.6609 - val_acc: 0.6458\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4789 - acc: 0.7543 - val_loss: 0.6626 - val_acc: 0.6458\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4958 - acc: 0.7405 - val_loss: 0.6878 - val_acc: 0.6146\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5168 - acc: 0.7526 - val_loss: 0.6759 - val_acc: 0.6250\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5225 - acc: 0.7302 - val_loss: 0.6865 - val_acc: 0.6354\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5384 - acc: 0.7354 - val_loss: 0.6705 - val_acc: 0.6562\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5398 - acc: 0.7354 - val_loss: 0.6809 - val_acc: 0.6458\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5107 - acc: 0.7354 - val_loss: 0.6884 - val_acc: 0.6458\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5020 - acc: 0.7526 - val_loss: 0.6702 - val_acc: 0.6562\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5177 - acc: 0.7457 - val_loss: 0.6848 - val_acc: 0.6354\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 0.4871 - acc: 0.7646 - val_loss: 0.6972 - val_acc: 0.6354\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4930 - acc: 0.7543 - val_loss: 0.6738 - val_acc: 0.6562\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5332 - acc: 0.7302 - val_loss: 0.6833 - val_acc: 0.6250\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5139 - acc: 0.7577 - val_loss: 0.6576 - val_acc: 0.6458\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4909 - acc: 0.7337 - val_loss: 0.6750 - val_acc: 0.6250\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5210 - acc: 0.7371 - val_loss: 0.6846 - val_acc: 0.6146\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5192 - acc: 0.7216 - val_loss: 0.6863 - val_acc: 0.6354\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5216 - acc: 0.7543 - val_loss: 0.6414 - val_acc: 0.6562\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5044 - acc: 0.7405 - val_loss: 0.6633 - val_acc: 0.6458\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5440 - acc: 0.7182 - val_loss: 0.6757 - val_acc: 0.6250\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4932 - acc: 0.7526 - val_loss: 0.6614 - val_acc: 0.6458\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 0.5358 - acc: 0.7331 - val_loss: 0.6681 - val_acc: 0.6354\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4906 - acc: 0.7595 - val_loss: 0.6753 - val_acc: 0.6250\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4796 - acc: 0.7715 - val_loss: 0.6783 - val_acc: 0.6250\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5286 - acc: 0.7285 - val_loss: 0.6483 - val_acc: 0.6458\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5014 - acc: 0.7416 - val_loss: 0.6650 - val_acc: 0.6354\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5103 - acc: 0.7595 - val_loss: 0.6949 - val_acc: 0.6354\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5154 - acc: 0.7354 - val_loss: 0.6785 - val_acc: 0.6250\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5095 - acc: 0.7457 - val_loss: 0.6690 - val_acc: 0.6354\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5225 - acc: 0.7491 - val_loss: 0.6592 - val_acc: 0.6458\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5073 - acc: 0.7646 - val_loss: 0.6664 - val_acc: 0.6667\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 8s 224ms/step - loss: 0.4914 - acc: 0.7663 - val_loss: 0.6576 - val_acc: 0.6458\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5181 - acc: 0.7268 - val_loss: 0.6675 - val_acc: 0.6354\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5253 - acc: 0.7388 - val_loss: 0.6718 - val_acc: 0.6250\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4931 - acc: 0.7560 - val_loss: 0.6604 - val_acc: 0.6562\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5249 - acc: 0.7302 - val_loss: 0.6734 - val_acc: 0.6250\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5283 - acc: 0.7199 - val_loss: 0.6405 - val_acc: 0.6667\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5368 - acc: 0.7337 - val_loss: 0.6678 - val_acc: 0.6250\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4765 - acc: 0.7543 - val_loss: 0.6547 - val_acc: 0.6458\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5476 - acc: 0.7285 - val_loss: 0.6761 - val_acc: 0.6354\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5051 - acc: 0.7543 - val_loss: 0.6575 - val_acc: 0.6562\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5310 - acc: 0.7457 - val_loss: 0.6718 - val_acc: 0.6562\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5370 - acc: 0.7577 - val_loss: 0.6678 - val_acc: 0.6458\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5397 - acc: 0.7302 - val_loss: 0.6840 - val_acc: 0.6250\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.5065 - acc: 0.7388 - val_loss: 0.6586 - val_acc: 0.6458\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4921 - acc: 0.7474 - val_loss: 0.6617 - val_acc: 0.6354\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5293 - acc: 0.7251 - val_loss: 0.6816 - val_acc: 0.6146\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5031 - acc: 0.7595 - val_loss: 0.6926 - val_acc: 0.6250\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4920 - acc: 0.7663 - val_loss: 0.6643 - val_acc: 0.6458\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5008 - acc: 0.7440 - val_loss: 0.6550 - val_acc: 0.6458\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5178 - acc: 0.7285 - val_loss: 0.6719 - val_acc: 0.6354\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.4971 - acc: 0.7629 - val_loss: 0.6536 - val_acc: 0.6458\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5670 - acc: 0.7062 - val_loss: 0.6740 - val_acc: 0.6146\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5029 - acc: 0.7337 - val_loss: 0.6758 - val_acc: 0.6250\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5252 - acc: 0.7302 - val_loss: 0.6696 - val_acc: 0.6250\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.4821 - acc: 0.7715 - val_loss: 0.6700 - val_acc: 0.6250\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4944 - acc: 0.7337 - val_loss: 0.6850 - val_acc: 0.6354\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.4925 - acc: 0.7526 - val_loss: 0.6921 - val_acc: 0.6146\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.4983 - acc: 0.7388 - val_loss: 0.6756 - val_acc: 0.6250\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5075 - acc: 0.7440 - val_loss: 0.6547 - val_acc: 0.6667\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5375 - acc: 0.7354 - val_loss: 0.6777 - val_acc: 0.6562\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5031 - acc: 0.7577 - val_loss: 0.6821 - val_acc: 0.6250\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5142 - acc: 0.7337 - val_loss: 0.6700 - val_acc: 0.6354\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5174 - acc: 0.7423 - val_loss: 0.6681 - val_acc: 0.6250\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.4917 - acc: 0.7423 - val_loss: 0.6531 - val_acc: 0.6354\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5260 - acc: 0.7491 - val_loss: 0.6839 - val_acc: 0.6146\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4776 - acc: 0.7595 - val_loss: 0.6783 - val_acc: 0.6562\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4888 - acc: 0.7732 - val_loss: 0.6653 - val_acc: 0.6250\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5114 - acc: 0.7405 - val_loss: 0.6715 - val_acc: 0.6250\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5320 - acc: 0.7113 - val_loss: 0.6828 - val_acc: 0.6250\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4917 - acc: 0.7543 - val_loss: 0.6659 - val_acc: 0.6354\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4710 - acc: 0.7887 - val_loss: 0.6765 - val_acc: 0.6250\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5365 - acc: 0.7337 - val_loss: 0.6727 - val_acc: 0.6250\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5192 - acc: 0.7474 - val_loss: 0.6787 - val_acc: 0.6250\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4879 - acc: 0.7577 - val_loss: 0.6666 - val_acc: 0.6250\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5312 - acc: 0.7354 - val_loss: 0.6686 - val_acc: 0.6354\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4779 - acc: 0.7680 - val_loss: 0.6736 - val_acc: 0.6250\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.4879 - acc: 0.7629 - val_loss: 0.6860 - val_acc: 0.6458\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5036 - acc: 0.7423 - val_loss: 0.6772 - val_acc: 0.6250\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5220 - acc: 0.7543 - val_loss: 0.6751 - val_acc: 0.5729\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4858 - acc: 0.7732 - val_loss: 0.6490 - val_acc: 0.6458\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4900 - acc: 0.7491 - val_loss: 0.6383 - val_acc: 0.6458\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5531 - acc: 0.7337 - val_loss: 0.6752 - val_acc: 0.6250\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5298 - acc: 0.7432 - val_loss: 0.6654 - val_acc: 0.6354\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5194 - acc: 0.7320 - val_loss: 0.6641 - val_acc: 0.6458\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5085 - acc: 0.7560 - val_loss: 0.6808 - val_acc: 0.6146\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5633 - acc: 0.7216 - val_loss: 0.6583 - val_acc: 0.6458\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5358 - acc: 0.7044 - val_loss: 0.6680 - val_acc: 0.6562\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5382 - acc: 0.7440 - val_loss: 0.6483 - val_acc: 0.6458\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5346 - acc: 0.7320 - val_loss: 0.6791 - val_acc: 0.6146\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5106 - acc: 0.7405 - val_loss: 0.6594 - val_acc: 0.6458\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5155 - acc: 0.7423 - val_loss: 0.6848 - val_acc: 0.6354\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5495 - acc: 0.7199 - val_loss: 0.6729 - val_acc: 0.6250\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4853 - acc: 0.7732 - val_loss: 0.6591 - val_acc: 0.6458\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5044 - acc: 0.7612 - val_loss: 0.6541 - val_acc: 0.6354\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4843 - acc: 0.7595 - val_loss: 0.6689 - val_acc: 0.6250\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5132 - acc: 0.7371 - val_loss: 0.6667 - val_acc: 0.6354\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5171 - acc: 0.7388 - val_loss: 0.6591 - val_acc: 0.6562\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5320 - acc: 0.7405 - val_loss: 0.6760 - val_acc: 0.6146\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5085 - acc: 0.7457 - val_loss: 0.6795 - val_acc: 0.6458\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5170 - acc: 0.7388 - val_loss: 0.6613 - val_acc: 0.6354\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5102 - acc: 0.7577 - val_loss: 0.6569 - val_acc: 0.6354\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5215 - acc: 0.7440 - val_loss: 0.6738 - val_acc: 0.6458\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4853 - acc: 0.7680 - val_loss: 0.6593 - val_acc: 0.6562\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5117 - acc: 0.7354 - val_loss: 0.6632 - val_acc: 0.6354\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5105 - acc: 0.7526 - val_loss: 0.6606 - val_acc: 0.6458\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5110 - acc: 0.7629 - val_loss: 0.6733 - val_acc: 0.6146\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5281 - acc: 0.7388 - val_loss: 0.6500 - val_acc: 0.6354\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5338 - acc: 0.7423 - val_loss: 0.6637 - val_acc: 0.5938\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5276 - acc: 0.7320 - val_loss: 0.6521 - val_acc: 0.6250\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5145 - acc: 0.7491 - val_loss: 0.6489 - val_acc: 0.6562\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4999 - acc: 0.7646 - val_loss: 0.6696 - val_acc: 0.6250\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5330 - acc: 0.7354 - val_loss: 0.6340 - val_acc: 0.6562\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5093 - acc: 0.7440 - val_loss: 0.6464 - val_acc: 0.6562\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4826 - acc: 0.7595 - val_loss: 0.6800 - val_acc: 0.6250\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5224 - acc: 0.7423 - val_loss: 0.6649 - val_acc: 0.5938\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5408 - acc: 0.7268 - val_loss: 0.6607 - val_acc: 0.6146\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4936 - acc: 0.7715 - val_loss: 0.6568 - val_acc: 0.6250\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4938 - acc: 0.7560 - val_loss: 0.6631 - val_acc: 0.6354\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5166 - acc: 0.7320 - val_loss: 0.6809 - val_acc: 0.6146\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5125 - acc: 0.7354 - val_loss: 0.6617 - val_acc: 0.6250\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4930 - acc: 0.7491 - val_loss: 0.6599 - val_acc: 0.6458\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5375 - acc: 0.7113 - val_loss: 0.6761 - val_acc: 0.6146\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4882 - acc: 0.7405 - val_loss: 0.6635 - val_acc: 0.6250\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4982 - acc: 0.7646 - val_loss: 0.6657 - val_acc: 0.6146\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5313 - acc: 0.7268 - val_loss: 0.6853 - val_acc: 0.6042\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4965 - acc: 0.7474 - val_loss: 0.6612 - val_acc: 0.6354\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5421 - acc: 0.7199 - val_loss: 0.6683 - val_acc: 0.6354\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5030 - acc: 0.7423 - val_loss: 0.6539 - val_acc: 0.6354\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4892 - acc: 0.7560 - val_loss: 0.6603 - val_acc: 0.6250\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5166 - acc: 0.7440 - val_loss: 0.6666 - val_acc: 0.6354\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 0.5230 - acc: 0.7457 - val_loss: 0.6739 - val_acc: 0.6250\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5321 - acc: 0.7354 - val_loss: 0.6625 - val_acc: 0.6146\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5094 - acc: 0.7423 - val_loss: 0.6603 - val_acc: 0.6250\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.5104 - acc: 0.7440 - val_loss: 0.6591 - val_acc: 0.6354\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.5140 - acc: 0.7457 - val_loss: 0.6545 - val_acc: 0.6667\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.4968 - acc: 0.7423 - val_loss: 0.6503 - val_acc: 0.6458\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4833 - acc: 0.7732 - val_loss: 0.6616 - val_acc: 0.6562\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5080 - acc: 0.7646 - val_loss: 0.6621 - val_acc: 0.6250\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4918 - acc: 0.7612 - val_loss: 0.6411 - val_acc: 0.6562\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5140 - acc: 0.7680 - val_loss: 0.6765 - val_acc: 0.6458\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5370 - acc: 0.7182 - val_loss: 0.6567 - val_acc: 0.6458\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5087 - acc: 0.7526 - val_loss: 0.6661 - val_acc: 0.6562\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5055 - acc: 0.7182 - val_loss: 0.6696 - val_acc: 0.6250\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5062 - acc: 0.7440 - val_loss: 0.6371 - val_acc: 0.6562\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5188 - acc: 0.7388 - val_loss: 0.6566 - val_acc: 0.5938\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5022 - acc: 0.7543 - val_loss: 0.6696 - val_acc: 0.6354\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5002 - acc: 0.7509 - val_loss: 0.6678 - val_acc: 0.5729\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5187 - acc: 0.7405 - val_loss: 0.6539 - val_acc: 0.6042\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.5209 - acc: 0.7234 - val_loss: 0.6667 - val_acc: 0.6354\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5486 - acc: 0.7113 - val_loss: 0.6857 - val_acc: 0.6250\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4690 - acc: 0.7732 - val_loss: 0.6554 - val_acc: 0.6354\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5114 - acc: 0.7405 - val_loss: 0.6693 - val_acc: 0.6250\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5075 - acc: 0.7543 - val_loss: 0.6644 - val_acc: 0.6354\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4992 - acc: 0.7234 - val_loss: 0.6479 - val_acc: 0.6562\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5007 - acc: 0.7526 - val_loss: 0.6704 - val_acc: 0.6562\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4995 - acc: 0.7612 - val_loss: 0.6713 - val_acc: 0.6250\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4974 - acc: 0.7199 - val_loss: 0.6678 - val_acc: 0.6562\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5654 - acc: 0.7062 - val_loss: 0.6709 - val_acc: 0.6354\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5013 - acc: 0.7440 - val_loss: 0.6836 - val_acc: 0.6250\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4860 - acc: 0.7818 - val_loss: 0.6566 - val_acc: 0.6458\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.4972 - acc: 0.7560 - val_loss: 0.6641 - val_acc: 0.6354\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4995 - acc: 0.7577 - val_loss: 0.6687 - val_acc: 0.6250\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5079 - acc: 0.7629 - val_loss: 0.6534 - val_acc: 0.6458\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5293 - acc: 0.7337 - val_loss: 0.6530 - val_acc: 0.6458\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5022 - acc: 0.7371 - val_loss: 0.6489 - val_acc: 0.6250\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5038 - acc: 0.7491 - val_loss: 0.6515 - val_acc: 0.6562\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5216 - acc: 0.7337 - val_loss: 0.6783 - val_acc: 0.6146\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 0.4849 - acc: 0.7491 - val_loss: 0.6577 - val_acc: 0.6458\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.5605 - acc: 0.7216 - val_loss: 0.6577 - val_acc: 0.6354\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4957 - acc: 0.7629 - val_loss: 0.6631 - val_acc: 0.6250\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5201 - acc: 0.7543 - val_loss: 0.6509 - val_acc: 0.6354\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4661 - acc: 0.7646 - val_loss: 0.6500 - val_acc: 0.6458\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5190 - acc: 0.7405 - val_loss: 0.6710 - val_acc: 0.6250\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5042 - acc: 0.7698 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5697 - acc: 0.7010 - val_loss: 0.6362 - val_acc: 0.6875\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4926 - acc: 0.7354 - val_loss: 0.6538 - val_acc: 0.6354\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.4768 - acc: 0.7818 - val_loss: 0.6515 - val_acc: 0.6354\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5234 - acc: 0.7509 - val_loss: 0.6620 - val_acc: 0.6250\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4794 - acc: 0.7680 - val_loss: 0.6548 - val_acc: 0.6562\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5277 - acc: 0.7646 - val_loss: 0.6449 - val_acc: 0.6562\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5123 - acc: 0.7509 - val_loss: 0.6580 - val_acc: 0.6354\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5047 - acc: 0.7595 - val_loss: 0.6545 - val_acc: 0.6667\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5055 - acc: 0.7457 - val_loss: 0.6664 - val_acc: 0.6458\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5031 - acc: 0.7526 - val_loss: 0.6659 - val_acc: 0.6354\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.5359 - acc: 0.7268 - val_loss: 0.6541 - val_acc: 0.6354\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4854 - acc: 0.7749 - val_loss: 0.6625 - val_acc: 0.6250\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5116 - acc: 0.7612 - val_loss: 0.6597 - val_acc: 0.6562\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.5288 - acc: 0.7440 - val_loss: 0.6407 - val_acc: 0.6458\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5158 - acc: 0.7526 - val_loss: 0.6664 - val_acc: 0.6250\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5007 - acc: 0.7491 - val_loss: 0.6558 - val_acc: 0.6354\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5062 - acc: 0.7526 - val_loss: 0.6504 - val_acc: 0.6354\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5092 - acc: 0.7371 - val_loss: 0.6644 - val_acc: 0.6250\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5049 - acc: 0.7474 - val_loss: 0.6738 - val_acc: 0.6354\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4908 - acc: 0.7715 - val_loss: 0.6629 - val_acc: 0.6458\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5124 - acc: 0.7491 - val_loss: 0.6611 - val_acc: 0.6146\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5115 - acc: 0.7491 - val_loss: 0.6556 - val_acc: 0.6354\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 0.4576 - acc: 0.7784 - val_loss: 0.6305 - val_acc: 0.6458\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5370 - acc: 0.7320 - val_loss: 0.6660 - val_acc: 0.6146\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5200 - acc: 0.7526 - val_loss: 0.6411 - val_acc: 0.6562\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5027 - acc: 0.7595 - val_loss: 0.6771 - val_acc: 0.6354\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5403 - acc: 0.7234 - val_loss: 0.6661 - val_acc: 0.6354\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5061 - acc: 0.7801 - val_loss: 0.6531 - val_acc: 0.6458\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4918 - acc: 0.7818 - val_loss: 0.6540 - val_acc: 0.6354\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5140 - acc: 0.7543 - val_loss: 0.6572 - val_acc: 0.6458\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5062 - acc: 0.7483 - val_loss: 0.6478 - val_acc: 0.6562\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5254 - acc: 0.7509 - val_loss: 0.6437 - val_acc: 0.6458\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4818 - acc: 0.7629 - val_loss: 0.6737 - val_acc: 0.6354\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5309 - acc: 0.7165 - val_loss: 0.6591 - val_acc: 0.6458\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5111 - acc: 0.7526 - val_loss: 0.6491 - val_acc: 0.6562\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5050 - acc: 0.7457 - val_loss: 0.6719 - val_acc: 0.6250\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5055 - acc: 0.7663 - val_loss: 0.6755 - val_acc: 0.6458\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5181 - acc: 0.7543 - val_loss: 0.6645 - val_acc: 0.6354\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4899 - acc: 0.7526 - val_loss: 0.6557 - val_acc: 0.6458\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4884 - acc: 0.7543 - val_loss: 0.6691 - val_acc: 0.6458\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4912 - acc: 0.7491 - val_loss: 0.6595 - val_acc: 0.6562\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5168 - acc: 0.7297 - val_loss: 0.6722 - val_acc: 0.6146\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4977 - acc: 0.7577 - val_loss: 0.6583 - val_acc: 0.6562\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5045 - acc: 0.7423 - val_loss: 0.6748 - val_acc: 0.6146\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.4604 - acc: 0.7680 - val_loss: 0.6623 - val_acc: 0.6250\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5058 - acc: 0.7457 - val_loss: 0.6641 - val_acc: 0.6354\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5077 - acc: 0.7457 - val_loss: 0.6520 - val_acc: 0.6458\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4931 - acc: 0.7801 - val_loss: 0.6622 - val_acc: 0.6146\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5388 - acc: 0.7285 - val_loss: 0.6540 - val_acc: 0.6354\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5434 - acc: 0.7251 - val_loss: 0.6620 - val_acc: 0.6042\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4742 - acc: 0.7698 - val_loss: 0.6222 - val_acc: 0.6562\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5397 - acc: 0.7320 - val_loss: 0.6451 - val_acc: 0.6354\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5097 - acc: 0.7474 - val_loss: 0.6653 - val_acc: 0.6354\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4915 - acc: 0.7766 - val_loss: 0.6500 - val_acc: 0.6354\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.5044 - acc: 0.7526 - val_loss: 0.6335 - val_acc: 0.6458\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4969 - acc: 0.7543 - val_loss: 0.6647 - val_acc: 0.6146\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5276 - acc: 0.7491 - val_loss: 0.6491 - val_acc: 0.6250\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5073 - acc: 0.7474 - val_loss: 0.6518 - val_acc: 0.6250\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4902 - acc: 0.7618 - val_loss: 0.6542 - val_acc: 0.6250\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.5369 - acc: 0.7320 - val_loss: 0.6391 - val_acc: 0.6354\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5657 - acc: 0.7010 - val_loss: 0.6422 - val_acc: 0.6354\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4982 - acc: 0.7337 - val_loss: 0.6436 - val_acc: 0.6354\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.5154 - acc: 0.7423 - val_loss: 0.6510 - val_acc: 0.6146\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5223 - acc: 0.7371 - val_loss: 0.6486 - val_acc: 0.6458\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5377 - acc: 0.7354 - val_loss: 0.6490 - val_acc: 0.6250\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5102 - acc: 0.7629 - val_loss: 0.6437 - val_acc: 0.6250\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5013 - acc: 0.7474 - val_loss: 0.6485 - val_acc: 0.6250\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4875 - acc: 0.7354 - val_loss: 0.6580 - val_acc: 0.6250\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5199 - acc: 0.7371 - val_loss: 0.6436 - val_acc: 0.6354\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4943 - acc: 0.7423 - val_loss: 0.6590 - val_acc: 0.6250\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5140 - acc: 0.7388 - val_loss: 0.6407 - val_acc: 0.6458\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4998 - acc: 0.7474 - val_loss: 0.6507 - val_acc: 0.6458\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4952 - acc: 0.7680 - val_loss: 0.6468 - val_acc: 0.6562\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4984 - acc: 0.7491 - val_loss: 0.6366 - val_acc: 0.6354\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.4923 - acc: 0.7543 - val_loss: 0.6510 - val_acc: 0.6250\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4972 - acc: 0.7669 - val_loss: 0.6666 - val_acc: 0.6354\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.4948 - acc: 0.7509 - val_loss: 0.6734 - val_acc: 0.6354\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4884 - acc: 0.7618 - val_loss: 0.6618 - val_acc: 0.6250\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5007 - acc: 0.7509 - val_loss: 0.6548 - val_acc: 0.6562\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5110 - acc: 0.7337 - val_loss: 0.6607 - val_acc: 0.6562\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5265 - acc: 0.7491 - val_loss: 0.6600 - val_acc: 0.6250\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5145 - acc: 0.7388 - val_loss: 0.6608 - val_acc: 0.6250\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5070 - acc: 0.7646 - val_loss: 0.6351 - val_acc: 0.6562\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5047 - acc: 0.7577 - val_loss: 0.6503 - val_acc: 0.6250\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5136 - acc: 0.7491 - val_loss: 0.6574 - val_acc: 0.6354\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5499 - acc: 0.7285 - val_loss: 0.6569 - val_acc: 0.6354\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5036 - acc: 0.7629 - val_loss: 0.6471 - val_acc: 0.6250\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4935 - acc: 0.7801 - val_loss: 0.6537 - val_acc: 0.6667\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5081 - acc: 0.7509 - val_loss: 0.6437 - val_acc: 0.6458\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5072 - acc: 0.7491 - val_loss: 0.6606 - val_acc: 0.6250\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5248 - acc: 0.7543 - val_loss: 0.6486 - val_acc: 0.6354\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5014 - acc: 0.7595 - val_loss: 0.6709 - val_acc: 0.6458\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5182 - acc: 0.7474 - val_loss: 0.6341 - val_acc: 0.6458\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5240 - acc: 0.7457 - val_loss: 0.6558 - val_acc: 0.6354\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5077 - acc: 0.7234 - val_loss: 0.6625 - val_acc: 0.6354\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5195 - acc: 0.7509 - val_loss: 0.6547 - val_acc: 0.6562\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4883 - acc: 0.7646 - val_loss: 0.6397 - val_acc: 0.6667\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5112 - acc: 0.7371 - val_loss: 0.6445 - val_acc: 0.6354\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.4993 - acc: 0.7491 - val_loss: 0.6601 - val_acc: 0.6458\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5105 - acc: 0.7440 - val_loss: 0.6438 - val_acc: 0.6562\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5212 - acc: 0.7371 - val_loss: 0.6493 - val_acc: 0.6354\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.5350 - acc: 0.7251 - val_loss: 0.6463 - val_acc: 0.6562\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4809 - acc: 0.7698 - val_loss: 0.6364 - val_acc: 0.6771\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5180 - acc: 0.7371 - val_loss: 0.6665 - val_acc: 0.6458\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5291 - acc: 0.7320 - val_loss: 0.6395 - val_acc: 0.6354\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5116 - acc: 0.7405 - val_loss: 0.6620 - val_acc: 0.6458\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4907 - acc: 0.7509 - val_loss: 0.6529 - val_acc: 0.6354\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5274 - acc: 0.7337 - val_loss: 0.6565 - val_acc: 0.6146\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.4972 - acc: 0.7526 - val_loss: 0.6433 - val_acc: 0.6562\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5424 - acc: 0.7234 - val_loss: 0.6607 - val_acc: 0.6250\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4804 - acc: 0.7801 - val_loss: 0.6433 - val_acc: 0.6354\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5284 - acc: 0.7388 - val_loss: 0.6445 - val_acc: 0.6458\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5129 - acc: 0.7543 - val_loss: 0.6517 - val_acc: 0.6250\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5135 - acc: 0.7302 - val_loss: 0.6435 - val_acc: 0.6458\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5177 - acc: 0.7509 - val_loss: 0.6658 - val_acc: 0.6562\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.4737 - acc: 0.7646 - val_loss: 0.6298 - val_acc: 0.6562\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.4857 - acc: 0.7629 - val_loss: 0.6605 - val_acc: 0.6562\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4963 - acc: 0.7652 - val_loss: 0.6540 - val_acc: 0.6354\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5008 - acc: 0.7509 - val_loss: 0.6839 - val_acc: 0.6354\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4789 - acc: 0.7887 - val_loss: 0.6777 - val_acc: 0.6354\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5297 - acc: 0.7320 - val_loss: 0.6611 - val_acc: 0.6458\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5317 - acc: 0.7337 - val_loss: 0.6518 - val_acc: 0.6354\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4815 - acc: 0.7715 - val_loss: 0.6683 - val_acc: 0.6146\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4906 - acc: 0.7543 - val_loss: 0.6591 - val_acc: 0.6250\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5329 - acc: 0.7337 - val_loss: 0.6483 - val_acc: 0.6667\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 0.5031 - acc: 0.7629 - val_loss: 0.6548 - val_acc: 0.6458\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4612 - acc: 0.7749 - val_loss: 0.6270 - val_acc: 0.6667\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4663 - acc: 0.7698 - val_loss: 0.6520 - val_acc: 0.6562\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4905 - acc: 0.7577 - val_loss: 0.6459 - val_acc: 0.6667\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5688 - acc: 0.7079 - val_loss: 0.6326 - val_acc: 0.6562\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5323 - acc: 0.7354 - val_loss: 0.6469 - val_acc: 0.6354\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5284 - acc: 0.7371 - val_loss: 0.6523 - val_acc: 0.6562\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.4873 - acc: 0.7732 - val_loss: 0.6539 - val_acc: 0.6458\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5244 - acc: 0.7612 - val_loss: 0.6555 - val_acc: 0.6146\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5312 - acc: 0.7302 - val_loss: 0.6374 - val_acc: 0.6458\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5303 - acc: 0.7491 - val_loss: 0.6531 - val_acc: 0.6354\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5027 - acc: 0.7371 - val_loss: 0.6752 - val_acc: 0.6354\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5125 - acc: 0.7457 - val_loss: 0.6534 - val_acc: 0.6667\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 0.4956 - acc: 0.7526 - val_loss: 0.6531 - val_acc: 0.6562\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5072 - acc: 0.7577 - val_loss: 0.6664 - val_acc: 0.6562\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4971 - acc: 0.7371 - val_loss: 0.6448 - val_acc: 0.6354\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4683 - acc: 0.7990 - val_loss: 0.6531 - val_acc: 0.6354\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4945 - acc: 0.7629 - val_loss: 0.6428 - val_acc: 0.6458\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5097 - acc: 0.7388 - val_loss: 0.6364 - val_acc: 0.6667\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5671 - acc: 0.7371 - val_loss: 0.6430 - val_acc: 0.6354\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5436 - acc: 0.7388 - val_loss: 0.6598 - val_acc: 0.6354\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5181 - acc: 0.7543 - val_loss: 0.6662 - val_acc: 0.6146\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5066 - acc: 0.7354 - val_loss: 0.6525 - val_acc: 0.6667\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4995 - acc: 0.7595 - val_loss: 0.6619 - val_acc: 0.6250\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4691 - acc: 0.7869 - val_loss: 0.6374 - val_acc: 0.6562\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5102 - acc: 0.7560 - val_loss: 0.6669 - val_acc: 0.6146\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5219 - acc: 0.7423 - val_loss: 0.6387 - val_acc: 0.6354\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4933 - acc: 0.7543 - val_loss: 0.6520 - val_acc: 0.6354\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.4748 - acc: 0.7818 - val_loss: 0.6697 - val_acc: 0.6146\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5514 - acc: 0.7216 - val_loss: 0.6577 - val_acc: 0.6354\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5312 - acc: 0.7371 - val_loss: 0.6656 - val_acc: 0.6250\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5143 - acc: 0.7423 - val_loss: 0.6383 - val_acc: 0.6562\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4995 - acc: 0.7560 - val_loss: 0.6655 - val_acc: 0.6562\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4881 - acc: 0.7612 - val_loss: 0.6704 - val_acc: 0.6458\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5111 - acc: 0.7371 - val_loss: 0.6683 - val_acc: 0.6354\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5406 - acc: 0.7440 - val_loss: 0.6630 - val_acc: 0.6562\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5200 - acc: 0.7457 - val_loss: 0.6713 - val_acc: 0.6458\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5170 - acc: 0.7371 - val_loss: 0.6657 - val_acc: 0.6042\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5002 - acc: 0.7543 - val_loss: 0.6619 - val_acc: 0.6250\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4969 - acc: 0.7440 - val_loss: 0.6632 - val_acc: 0.6354\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4602 - acc: 0.7818 - val_loss: 0.6554 - val_acc: 0.6458\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4947 - acc: 0.7663 - val_loss: 0.6420 - val_acc: 0.6354\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4907 - acc: 0.7680 - val_loss: 0.6589 - val_acc: 0.6250\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5337 - acc: 0.7388 - val_loss: 0.6683 - val_acc: 0.6250\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5257 - acc: 0.7474 - val_loss: 0.6624 - val_acc: 0.6250\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5081 - acc: 0.7440 - val_loss: 0.6605 - val_acc: 0.6250\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5286 - acc: 0.7199 - val_loss: 0.6446 - val_acc: 0.6458\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5087 - acc: 0.7663 - val_loss: 0.6520 - val_acc: 0.6250\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5031 - acc: 0.7560 - val_loss: 0.6738 - val_acc: 0.6250\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5018 - acc: 0.7595 - val_loss: 0.6632 - val_acc: 0.6146\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5148 - acc: 0.7371 - val_loss: 0.6610 - val_acc: 0.6354\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4952 - acc: 0.7491 - val_loss: 0.6706 - val_acc: 0.6146\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5273 - acc: 0.7509 - val_loss: 0.6675 - val_acc: 0.6458\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5241 - acc: 0.7491 - val_loss: 0.6561 - val_acc: 0.6354\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5311 - acc: 0.7320 - val_loss: 0.6485 - val_acc: 0.6354\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5265 - acc: 0.7423 - val_loss: 0.6545 - val_acc: 0.6354\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5121 - acc: 0.7371 - val_loss: 0.6636 - val_acc: 0.6354\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4892 - acc: 0.7646 - val_loss: 0.6389 - val_acc: 0.6458\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5157 - acc: 0.7388 - val_loss: 0.6599 - val_acc: 0.6562\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5342 - acc: 0.7663 - val_loss: 0.6598 - val_acc: 0.6458\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4984 - acc: 0.7457 - val_loss: 0.6677 - val_acc: 0.6250\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5356 - acc: 0.7423 - val_loss: 0.6652 - val_acc: 0.6354\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5428 - acc: 0.7474 - val_loss: 0.6728 - val_acc: 0.6354\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4832 - acc: 0.7491 - val_loss: 0.6552 - val_acc: 0.6667\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.5135 - acc: 0.7457 - val_loss: 0.6542 - val_acc: 0.6562\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5354 - acc: 0.7388 - val_loss: 0.6676 - val_acc: 0.6458\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5148 - acc: 0.7199 - val_loss: 0.6650 - val_acc: 0.6458\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5429 - acc: 0.7234 - val_loss: 0.6625 - val_acc: 0.6562\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5095 - acc: 0.7388 - val_loss: 0.6589 - val_acc: 0.6562\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5107 - acc: 0.7423 - val_loss: 0.6629 - val_acc: 0.6354\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5391 - acc: 0.7234 - val_loss: 0.6442 - val_acc: 0.6458\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5270 - acc: 0.7595 - val_loss: 0.6571 - val_acc: 0.6562\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5011 - acc: 0.7732 - val_loss: 0.6639 - val_acc: 0.6250\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5123 - acc: 0.7405 - val_loss: 0.6549 - val_acc: 0.6354\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5339 - acc: 0.7285 - val_loss: 0.6633 - val_acc: 0.6354\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.4973 - acc: 0.7698 - val_loss: 0.6518 - val_acc: 0.6458\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 0.4978 - acc: 0.7577 - val_loss: 0.6670 - val_acc: 0.6354\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4989 - acc: 0.7449 - val_loss: 0.6581 - val_acc: 0.6354\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5090 - acc: 0.7629 - val_loss: 0.6547 - val_acc: 0.6354\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4924 - acc: 0.7509 - val_loss: 0.6655 - val_acc: 0.6250\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4997 - acc: 0.7526 - val_loss: 0.6495 - val_acc: 0.6354\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.4702 - acc: 0.7703 - val_loss: 0.6492 - val_acc: 0.6354\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5260 - acc: 0.7388 - val_loss: 0.6643 - val_acc: 0.6562\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4764 - acc: 0.7715 - val_loss: 0.6632 - val_acc: 0.6458\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.5188 - acc: 0.7474 - val_loss: 0.6400 - val_acc: 0.6562\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5023 - acc: 0.7354 - val_loss: 0.6403 - val_acc: 0.6458\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4947 - acc: 0.7629 - val_loss: 0.6629 - val_acc: 0.6146\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5035 - acc: 0.7440 - val_loss: 0.6560 - val_acc: 0.6562\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4821 - acc: 0.7698 - val_loss: 0.6631 - val_acc: 0.6250\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4960 - acc: 0.7595 - val_loss: 0.6744 - val_acc: 0.6250\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5096 - acc: 0.7457 - val_loss: 0.6571 - val_acc: 0.6354\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5059 - acc: 0.7509 - val_loss: 0.6431 - val_acc: 0.6458\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5334 - acc: 0.7320 - val_loss: 0.6465 - val_acc: 0.6458\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5203 - acc: 0.7560 - val_loss: 0.6510 - val_acc: 0.6354\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.4993 - acc: 0.7474 - val_loss: 0.6561 - val_acc: 0.5729\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5373 - acc: 0.7182 - val_loss: 0.6627 - val_acc: 0.5938\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5044 - acc: 0.7416 - val_loss: 0.6499 - val_acc: 0.6458\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5246 - acc: 0.7268 - val_loss: 0.6294 - val_acc: 0.6667\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5084 - acc: 0.7423 - val_loss: 0.6528 - val_acc: 0.6354\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5094 - acc: 0.7285 - val_loss: 0.6609 - val_acc: 0.6354\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.5131 - acc: 0.7388 - val_loss: 0.6378 - val_acc: 0.6458\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4862 - acc: 0.7543 - val_loss: 0.6544 - val_acc: 0.6458\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5168 - acc: 0.7612 - val_loss: 0.6488 - val_acc: 0.6354\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5000 - acc: 0.7405 - val_loss: 0.6674 - val_acc: 0.6146\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4728 - acc: 0.7698 - val_loss: 0.6769 - val_acc: 0.6250\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4961 - acc: 0.7663 - val_loss: 0.6638 - val_acc: 0.6354\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5115 - acc: 0.7612 - val_loss: 0.6641 - val_acc: 0.6458\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5015 - acc: 0.7663 - val_loss: 0.6663 - val_acc: 0.6146\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5155 - acc: 0.7388 - val_loss: 0.6537 - val_acc: 0.6250\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5200 - acc: 0.7320 - val_loss: 0.6592 - val_acc: 0.6354\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4837 - acc: 0.7766 - val_loss: 0.6593 - val_acc: 0.6562\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5166 - acc: 0.7543 - val_loss: 0.6585 - val_acc: 0.6250\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5222 - acc: 0.7457 - val_loss: 0.6602 - val_acc: 0.6250\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5090 - acc: 0.7526 - val_loss: 0.6680 - val_acc: 0.6354\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5227 - acc: 0.7440 - val_loss: 0.6544 - val_acc: 0.6250\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5409 - acc: 0.7354 - val_loss: 0.6431 - val_acc: 0.6562\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4993 - acc: 0.7440 - val_loss: 0.6646 - val_acc: 0.6250\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5167 - acc: 0.7474 - val_loss: 0.6555 - val_acc: 0.6354\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5098 - acc: 0.7371 - val_loss: 0.6566 - val_acc: 0.6562\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4990 - acc: 0.7526 - val_loss: 0.6640 - val_acc: 0.6250\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4799 - acc: 0.7715 - val_loss: 0.6501 - val_acc: 0.6667\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4955 - acc: 0.7526 - val_loss: 0.6498 - val_acc: 0.6667\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5042 - acc: 0.7560 - val_loss: 0.6592 - val_acc: 0.6562\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5333 - acc: 0.7302 - val_loss: 0.6653 - val_acc: 0.6354\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5096 - acc: 0.7337 - val_loss: 0.6710 - val_acc: 0.6250\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5089 - acc: 0.7474 - val_loss: 0.6530 - val_acc: 0.6354\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.5298 - acc: 0.7354 - val_loss: 0.6503 - val_acc: 0.6562\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5157 - acc: 0.7612 - val_loss: 0.6548 - val_acc: 0.6354\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4941 - acc: 0.7491 - val_loss: 0.6631 - val_acc: 0.6250\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4914 - acc: 0.7526 - val_loss: 0.6497 - val_acc: 0.6458\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4795 - acc: 0.7663 - val_loss: 0.6740 - val_acc: 0.6250\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5074 - acc: 0.7577 - val_loss: 0.6522 - val_acc: 0.6458\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5112 - acc: 0.7629 - val_loss: 0.6769 - val_acc: 0.6354\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5227 - acc: 0.7302 - val_loss: 0.6562 - val_acc: 0.6250\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.4949 - acc: 0.7543 - val_loss: 0.6554 - val_acc: 0.6458\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5363 - acc: 0.7423 - val_loss: 0.6706 - val_acc: 0.6146\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4945 - acc: 0.7560 - val_loss: 0.6381 - val_acc: 0.6458\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5036 - acc: 0.7405 - val_loss: 0.6626 - val_acc: 0.6458\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5335 - acc: 0.7320 - val_loss: 0.6384 - val_acc: 0.6667\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5306 - acc: 0.7182 - val_loss: 0.6657 - val_acc: 0.6458\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4952 - acc: 0.7457 - val_loss: 0.6544 - val_acc: 0.6250\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5092 - acc: 0.7337 - val_loss: 0.6391 - val_acc: 0.6667\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5060 - acc: 0.7595 - val_loss: 0.6650 - val_acc: 0.6250\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5238 - acc: 0.7182 - val_loss: 0.6545 - val_acc: 0.6250\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4866 - acc: 0.7749 - val_loss: 0.6518 - val_acc: 0.6354\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.4831 - acc: 0.7629 - val_loss: 0.6563 - val_acc: 0.6354\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5277 - acc: 0.7491 - val_loss: 0.6400 - val_acc: 0.6667\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5209 - acc: 0.7509 - val_loss: 0.6483 - val_acc: 0.6458\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5176 - acc: 0.7320 - val_loss: 0.6535 - val_acc: 0.6250\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.4942 - acc: 0.7440 - val_loss: 0.6734 - val_acc: 0.6250\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5137 - acc: 0.7371 - val_loss: 0.6683 - val_acc: 0.6458\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5022 - acc: 0.7698 - val_loss: 0.6527 - val_acc: 0.6458\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5035 - acc: 0.7526 - val_loss: 0.6632 - val_acc: 0.6458\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 0.5419 - acc: 0.7251 - val_loss: 0.6807 - val_acc: 0.6250\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5005 - acc: 0.7680 - val_loss: 0.6604 - val_acc: 0.6562\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5277 - acc: 0.7285 - val_loss: 0.6672 - val_acc: 0.6458\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.4825 - acc: 0.7715 - val_loss: 0.6544 - val_acc: 0.6667\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5097 - acc: 0.7302 - val_loss: 0.6836 - val_acc: 0.6250\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.5062 - acc: 0.7646 - val_loss: 0.6714 - val_acc: 0.6354\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5202 - acc: 0.7405 - val_loss: 0.6718 - val_acc: 0.6562\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4938 - acc: 0.7652 - val_loss: 0.6735 - val_acc: 0.6458\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5258 - acc: 0.7371 - val_loss: 0.6825 - val_acc: 0.6250\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4797 - acc: 0.7835 - val_loss: 0.6682 - val_acc: 0.6458\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.5152 - acc: 0.7337 - val_loss: 0.6767 - val_acc: 0.6354\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5095 - acc: 0.7491 - val_loss: 0.6729 - val_acc: 0.6146\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.5288 - acc: 0.7354 - val_loss: 0.6706 - val_acc: 0.6250\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5222 - acc: 0.7440 - val_loss: 0.6384 - val_acc: 0.6458\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.4977 - acc: 0.7646 - val_loss: 0.6446 - val_acc: 0.6458\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4663 - acc: 0.7852 - val_loss: 0.6644 - val_acc: 0.6354\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4868 - acc: 0.7543 - val_loss: 0.6507 - val_acc: 0.6562\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4628 - acc: 0.7732 - val_loss: 0.6449 - val_acc: 0.6458\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4819 - acc: 0.7801 - val_loss: 0.6760 - val_acc: 0.6250\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4931 - acc: 0.7595 - val_loss: 0.6581 - val_acc: 0.6458\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5219 - acc: 0.7423 - val_loss: 0.6395 - val_acc: 0.6771\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4884 - acc: 0.7629 - val_loss: 0.6715 - val_acc: 0.6458\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4824 - acc: 0.7818 - val_loss: 0.6457 - val_acc: 0.6562\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.4985 - acc: 0.7440 - val_loss: 0.6588 - val_acc: 0.6458\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5259 - acc: 0.7423 - val_loss: 0.6463 - val_acc: 0.6562\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5425 - acc: 0.7405 - val_loss: 0.6628 - val_acc: 0.6458\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5265 - acc: 0.7405 - val_loss: 0.6356 - val_acc: 0.6562\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5120 - acc: 0.7526 - val_loss: 0.6618 - val_acc: 0.6354\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.4927 - acc: 0.7543 - val_loss: 0.6570 - val_acc: 0.6562\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5235 - acc: 0.7251 - val_loss: 0.6548 - val_acc: 0.6667\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4943 - acc: 0.7509 - val_loss: 0.6814 - val_acc: 0.6458\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5197 - acc: 0.7285 - val_loss: 0.6617 - val_acc: 0.6458\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.5074 - acc: 0.7440 - val_loss: 0.6495 - val_acc: 0.6458\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4971 - acc: 0.7577 - val_loss: 0.6647 - val_acc: 0.6458\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5141 - acc: 0.7432 - val_loss: 0.6601 - val_acc: 0.6458\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5056 - acc: 0.7509 - val_loss: 0.6632 - val_acc: 0.6458\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.4707 - acc: 0.7543 - val_loss: 0.6481 - val_acc: 0.6771\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5289 - acc: 0.7337 - val_loss: 0.6610 - val_acc: 0.6458\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4725 - acc: 0.7749 - val_loss: 0.6559 - val_acc: 0.6458\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5364 - acc: 0.7405 - val_loss: 0.6663 - val_acc: 0.6354\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5186 - acc: 0.7491 - val_loss: 0.6719 - val_acc: 0.6250\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5274 - acc: 0.7354 - val_loss: 0.6590 - val_acc: 0.6354\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5113 - acc: 0.7285 - val_loss: 0.6722 - val_acc: 0.6458\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.4792 - acc: 0.7491 - val_loss: 0.6515 - val_acc: 0.6667\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5074 - acc: 0.7474 - val_loss: 0.6782 - val_acc: 0.6458\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.4929 - acc: 0.7199 - val_loss: 0.6593 - val_acc: 0.6667\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.4965 - acc: 0.7457 - val_loss: 0.6656 - val_acc: 0.6458\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4938 - acc: 0.7526 - val_loss: 0.6491 - val_acc: 0.6458\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5252 - acc: 0.7251 - val_loss: 0.6636 - val_acc: 0.6562\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.4991 - acc: 0.7268 - val_loss: 0.6660 - val_acc: 0.6458\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.4911 - acc: 0.7388 - val_loss: 0.6611 - val_acc: 0.6354\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5089 - acc: 0.7388 - val_loss: 0.6551 - val_acc: 0.6458\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4848 - acc: 0.7784 - val_loss: 0.6770 - val_acc: 0.6354\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5065 - acc: 0.7432 - val_loss: 0.6514 - val_acc: 0.6458\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5114 - acc: 0.7500 - val_loss: 0.6545 - val_acc: 0.6458\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5004 - acc: 0.7405 - val_loss: 0.6567 - val_acc: 0.6146\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4993 - acc: 0.7560 - val_loss: 0.6430 - val_acc: 0.6667\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5149 - acc: 0.7509 - val_loss: 0.6498 - val_acc: 0.6458\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5177 - acc: 0.7577 - val_loss: 0.6478 - val_acc: 0.6458\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4890 - acc: 0.7560 - val_loss: 0.6437 - val_acc: 0.6562\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.5085 - acc: 0.7423 - val_loss: 0.6515 - val_acc: 0.6667\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5124 - acc: 0.7405 - val_loss: 0.6641 - val_acc: 0.6562\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5077 - acc: 0.7698 - val_loss: 0.6817 - val_acc: 0.6458\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4959 - acc: 0.7732 - val_loss: 0.6602 - val_acc: 0.6458\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5114 - acc: 0.7491 - val_loss: 0.6677 - val_acc: 0.6458\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5607 - acc: 0.7113 - val_loss: 0.6635 - val_acc: 0.6458\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5343 - acc: 0.7440 - val_loss: 0.6453 - val_acc: 0.6667\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4829 - acc: 0.7784 - val_loss: 0.6591 - val_acc: 0.6562\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4888 - acc: 0.7612 - val_loss: 0.6678 - val_acc: 0.6354\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4684 - acc: 0.7663 - val_loss: 0.6516 - val_acc: 0.6562\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5511 - acc: 0.7388 - val_loss: 0.6680 - val_acc: 0.6354\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5126 - acc: 0.7320 - val_loss: 0.6735 - val_acc: 0.6458\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.5194 - acc: 0.7251 - val_loss: 0.6434 - val_acc: 0.6667\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5082 - acc: 0.7560 - val_loss: 0.6420 - val_acc: 0.6562\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5036 - acc: 0.7457 - val_loss: 0.6551 - val_acc: 0.6458\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5185 - acc: 0.7440 - val_loss: 0.6469 - val_acc: 0.6354\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5062 - acc: 0.7234 - val_loss: 0.6522 - val_acc: 0.6458\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.5118 - acc: 0.7457 - val_loss: 0.6572 - val_acc: 0.6458\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5559 - acc: 0.7096 - val_loss: 0.6515 - val_acc: 0.6458\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5279 - acc: 0.7337 - val_loss: 0.6435 - val_acc: 0.6562\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4886 - acc: 0.7766 - val_loss: 0.6571 - val_acc: 0.6458\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5194 - acc: 0.7354 - val_loss: 0.6529 - val_acc: 0.6458\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.5082 - acc: 0.7732 - val_loss: 0.6413 - val_acc: 0.6667\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.4831 - acc: 0.7663 - val_loss: 0.6520 - val_acc: 0.6562\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4930 - acc: 0.7543 - val_loss: 0.6616 - val_acc: 0.6354\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.5294 - acc: 0.7354 - val_loss: 0.6418 - val_acc: 0.6562\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5358 - acc: 0.7216 - val_loss: 0.6241 - val_acc: 0.6458\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5136 - acc: 0.7577 - val_loss: 0.6421 - val_acc: 0.6458\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5082 - acc: 0.7440 - val_loss: 0.6449 - val_acc: 0.6667\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5066 - acc: 0.7629 - val_loss: 0.6609 - val_acc: 0.6458\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5261 - acc: 0.7526 - val_loss: 0.6558 - val_acc: 0.6562\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5060 - acc: 0.7577 - val_loss: 0.6358 - val_acc: 0.6562\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5148 - acc: 0.7405 - val_loss: 0.6649 - val_acc: 0.6354\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5005 - acc: 0.7457 - val_loss: 0.6583 - val_acc: 0.6562\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.4973 - acc: 0.7577 - val_loss: 0.6627 - val_acc: 0.6458\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5456 - acc: 0.7079 - val_loss: 0.6398 - val_acc: 0.6562\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4850 - acc: 0.7663 - val_loss: 0.6600 - val_acc: 0.6250\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5203 - acc: 0.7423 - val_loss: 0.6462 - val_acc: 0.6250\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5112 - acc: 0.7646 - val_loss: 0.6426 - val_acc: 0.6458\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5203 - acc: 0.7543 - val_loss: 0.6372 - val_acc: 0.6667\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5208 - acc: 0.7440 - val_loss: 0.6485 - val_acc: 0.6458\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5374 - acc: 0.7509 - val_loss: 0.6625 - val_acc: 0.6354\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5435 - acc: 0.7199 - val_loss: 0.6685 - val_acc: 0.6458\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4900 - acc: 0.7577 - val_loss: 0.6335 - val_acc: 0.6562\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5337 - acc: 0.7612 - val_loss: 0.6573 - val_acc: 0.6354\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.4852 - acc: 0.7509 - val_loss: 0.6546 - val_acc: 0.6562\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.5152 - acc: 0.7388 - val_loss: 0.6478 - val_acc: 0.6458\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5067 - acc: 0.7371 - val_loss: 0.6583 - val_acc: 0.6562\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.4849 - acc: 0.7784 - val_loss: 0.6562 - val_acc: 0.6354\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5387 - acc: 0.7199 - val_loss: 0.6369 - val_acc: 0.6458\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5128 - acc: 0.7440 - val_loss: 0.6814 - val_acc: 0.6354\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5073 - acc: 0.7251 - val_loss: 0.6676 - val_acc: 0.6458\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5134 - acc: 0.7371 - val_loss: 0.6820 - val_acc: 0.6354\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5044 - acc: 0.7766 - val_loss: 0.6612 - val_acc: 0.6562\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 0.5090 - acc: 0.7629 - val_loss: 0.6496 - val_acc: 0.6667\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.5375 - acc: 0.7268 - val_loss: 0.6667 - val_acc: 0.6458\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5760 - acc: 0.7113 - val_loss: 0.6596 - val_acc: 0.6667\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4846 - acc: 0.7749 - val_loss: 0.6641 - val_acc: 0.6458\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5082 - acc: 0.7560 - val_loss: 0.6556 - val_acc: 0.6562\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5275 - acc: 0.7423 - val_loss: 0.6479 - val_acc: 0.6667\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5185 - acc: 0.7457 - val_loss: 0.6624 - val_acc: 0.6354\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5207 - acc: 0.7365 - val_loss: 0.6495 - val_acc: 0.6458\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5301 - acc: 0.7199 - val_loss: 0.6408 - val_acc: 0.6771\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5167 - acc: 0.7440 - val_loss: 0.6606 - val_acc: 0.6562\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5138 - acc: 0.7405 - val_loss: 0.6795 - val_acc: 0.6458\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5094 - acc: 0.7457 - val_loss: 0.6658 - val_acc: 0.6458\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5061 - acc: 0.7388 - val_loss: 0.6701 - val_acc: 0.6458\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5470 - acc: 0.7388 - val_loss: 0.6598 - val_acc: 0.6562\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4928 - acc: 0.7715 - val_loss: 0.6559 - val_acc: 0.6667\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5029 - acc: 0.7405 - val_loss: 0.6595 - val_acc: 0.6667\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4994 - acc: 0.7440 - val_loss: 0.6664 - val_acc: 0.6562\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5307 - acc: 0.7062 - val_loss: 0.6618 - val_acc: 0.6562\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5073 - acc: 0.7595 - val_loss: 0.6793 - val_acc: 0.6354\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5267 - acc: 0.7320 - val_loss: 0.6582 - val_acc: 0.6458\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.4996 - acc: 0.7388 - val_loss: 0.6563 - val_acc: 0.6667\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.5165 - acc: 0.7302 - val_loss: 0.6559 - val_acc: 0.6667\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5117 - acc: 0.7285 - val_loss: 0.6570 - val_acc: 0.6562\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5286 - acc: 0.7113 - val_loss: 0.6735 - val_acc: 0.6354\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5048 - acc: 0.7577 - val_loss: 0.6820 - val_acc: 0.6354\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5139 - acc: 0.7629 - val_loss: 0.6539 - val_acc: 0.6562\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5316 - acc: 0.7354 - val_loss: 0.6447 - val_acc: 0.6458\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4871 - acc: 0.7921 - val_loss: 0.6597 - val_acc: 0.6250\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5035 - acc: 0.7534 - val_loss: 0.6425 - val_acc: 0.6562\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4941 - acc: 0.7629 - val_loss: 0.6695 - val_acc: 0.6146\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4989 - acc: 0.7405 - val_loss: 0.6633 - val_acc: 0.6562\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.4962 - acc: 0.7749 - val_loss: 0.6650 - val_acc: 0.6562\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.4876 - acc: 0.7698 - val_loss: 0.6509 - val_acc: 0.6562\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5414 - acc: 0.7354 - val_loss: 0.6557 - val_acc: 0.6562\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5355 - acc: 0.7371 - val_loss: 0.6662 - val_acc: 0.6458\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5007 - acc: 0.7663 - val_loss: 0.6586 - val_acc: 0.6562\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5321 - acc: 0.7320 - val_loss: 0.6450 - val_acc: 0.6458\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.4833 - acc: 0.7784 - val_loss: 0.6585 - val_acc: 0.6562\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5152 - acc: 0.7423 - val_loss: 0.6555 - val_acc: 0.6562\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.4808 - acc: 0.7509 - val_loss: 0.6591 - val_acc: 0.6562\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5107 - acc: 0.7388 - val_loss: 0.6602 - val_acc: 0.6458\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4973 - acc: 0.7526 - val_loss: 0.6599 - val_acc: 0.6562\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5314 - acc: 0.7302 - val_loss: 0.6584 - val_acc: 0.6458\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5110 - acc: 0.7680 - val_loss: 0.6387 - val_acc: 0.6667\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5034 - acc: 0.7629 - val_loss: 0.6644 - val_acc: 0.6354\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5296 - acc: 0.7320 - val_loss: 0.6560 - val_acc: 0.6146\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5236 - acc: 0.7491 - val_loss: 0.6438 - val_acc: 0.6354\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 0.5062 - acc: 0.7732 - val_loss: 0.6473 - val_acc: 0.6146\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5155 - acc: 0.7302 - val_loss: 0.6489 - val_acc: 0.6354\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5091 - acc: 0.7251 - val_loss: 0.6559 - val_acc: 0.6562\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5242 - acc: 0.7526 - val_loss: 0.6543 - val_acc: 0.6562\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4657 - acc: 0.7766 - val_loss: 0.6507 - val_acc: 0.6562\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5159 - acc: 0.7560 - val_loss: 0.6558 - val_acc: 0.6667\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5331 - acc: 0.7354 - val_loss: 0.6500 - val_acc: 0.6667\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.5184 - acc: 0.7560 - val_loss: 0.6555 - val_acc: 0.6562\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4992 - acc: 0.7577 - val_loss: 0.6749 - val_acc: 0.6354\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5097 - acc: 0.7560 - val_loss: 0.6552 - val_acc: 0.6458\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4837 - acc: 0.7646 - val_loss: 0.6617 - val_acc: 0.6458\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.4742 - acc: 0.7698 - val_loss: 0.6534 - val_acc: 0.6667\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5246 - acc: 0.7560 - val_loss: 0.6491 - val_acc: 0.6146\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4982 - acc: 0.7577 - val_loss: 0.6503 - val_acc: 0.6250\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4902 - acc: 0.7491 - val_loss: 0.6667 - val_acc: 0.6458\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5316 - acc: 0.7423 - val_loss: 0.6438 - val_acc: 0.6562\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5451 - acc: 0.7526 - val_loss: 0.6467 - val_acc: 0.6458\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5025 - acc: 0.7526 - val_loss: 0.6440 - val_acc: 0.6458\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.5205 - acc: 0.7595 - val_loss: 0.6642 - val_acc: 0.6562\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5096 - acc: 0.7577 - val_loss: 0.6644 - val_acc: 0.6562\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5176 - acc: 0.7337 - val_loss: 0.6796 - val_acc: 0.6458\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5182 - acc: 0.7354 - val_loss: 0.6643 - val_acc: 0.6458\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4796 - acc: 0.7835 - val_loss: 0.6525 - val_acc: 0.6667\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5008 - acc: 0.7560 - val_loss: 0.6574 - val_acc: 0.6667\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.4918 - acc: 0.7629 - val_loss: 0.6501 - val_acc: 0.6771\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.4857 - acc: 0.7491 - val_loss: 0.6484 - val_acc: 0.6667\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.5354 - acc: 0.7405 - val_loss: 0.6545 - val_acc: 0.6667\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.5332 - acc: 0.7354 - val_loss: 0.6591 - val_acc: 0.6562\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.4822 - acc: 0.7766 - val_loss: 0.6459 - val_acc: 0.6667\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5008 - acc: 0.7440 - val_loss: 0.6546 - val_acc: 0.6250\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.4670 - acc: 0.7766 - val_loss: 0.6622 - val_acc: 0.6458\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5132 - acc: 0.7268 - val_loss: 0.6499 - val_acc: 0.6667\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5187 - acc: 0.7491 - val_loss: 0.6519 - val_acc: 0.6562\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4992 - acc: 0.7577 - val_loss: 0.6548 - val_acc: 0.6458\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4809 - acc: 0.7732 - val_loss: 0.6705 - val_acc: 0.6458\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 0.5222 - acc: 0.7388 - val_loss: 0.6603 - val_acc: 0.6562\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5140 - acc: 0.7354 - val_loss: 0.6561 - val_acc: 0.6562\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.5154 - acc: 0.7440 - val_loss: 0.6555 - val_acc: 0.6562\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5290 - acc: 0.7371 - val_loss: 0.6555 - val_acc: 0.6562\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5084 - acc: 0.7474 - val_loss: 0.6679 - val_acc: 0.6250\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4996 - acc: 0.7474 - val_loss: 0.6617 - val_acc: 0.6354\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5341 - acc: 0.7388 - val_loss: 0.6517 - val_acc: 0.6562\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4956 - acc: 0.7560 - val_loss: 0.6384 - val_acc: 0.6771\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5247 - acc: 0.7595 - val_loss: 0.6720 - val_acc: 0.6354\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 0.4920 - acc: 0.7509 - val_loss: 0.6700 - val_acc: 0.6562\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5310 - acc: 0.7320 - val_loss: 0.6626 - val_acc: 0.6458\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5091 - acc: 0.7543 - val_loss: 0.6634 - val_acc: 0.6458\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4946 - acc: 0.7698 - val_loss: 0.6510 - val_acc: 0.6667\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5061 - acc: 0.7405 - val_loss: 0.6722 - val_acc: 0.6458\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 0.5078 - acc: 0.7543 - val_loss: 0.6622 - val_acc: 0.6667\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5119 - acc: 0.7457 - val_loss: 0.6554 - val_acc: 0.6562\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5018 - acc: 0.7612 - val_loss: 0.6581 - val_acc: 0.6458\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5289 - acc: 0.7337 - val_loss: 0.6699 - val_acc: 0.6250\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5063 - acc: 0.7509 - val_loss: 0.6658 - val_acc: 0.6458\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5303 - acc: 0.7474 - val_loss: 0.6452 - val_acc: 0.6458\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5028 - acc: 0.7491 - val_loss: 0.6321 - val_acc: 0.6458\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.5014 - acc: 0.7474 - val_loss: 0.6729 - val_acc: 0.6146\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5417 - acc: 0.7302 - val_loss: 0.6502 - val_acc: 0.6458\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5321 - acc: 0.7405 - val_loss: 0.6475 - val_acc: 0.6562\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.4872 - acc: 0.7973 - val_loss: 0.6413 - val_acc: 0.6667\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5281 - acc: 0.7474 - val_loss: 0.6589 - val_acc: 0.6458\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.4842 - acc: 0.7698 - val_loss: 0.6618 - val_acc: 0.6354\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5168 - acc: 0.7526 - val_loss: 0.6618 - val_acc: 0.6354\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5445 - acc: 0.7216 - val_loss: 0.6676 - val_acc: 0.6250\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.5173 - acc: 0.7491 - val_loss: 0.6497 - val_acc: 0.6458\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5182 - acc: 0.7302 - val_loss: 0.6578 - val_acc: 0.6354\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5024 - acc: 0.7595 - val_loss: 0.6489 - val_acc: 0.6667\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4862 - acc: 0.7766 - val_loss: 0.6686 - val_acc: 0.6354\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4959 - acc: 0.7440 - val_loss: 0.6597 - val_acc: 0.6562\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.5019 - acc: 0.7423 - val_loss: 0.6689 - val_acc: 0.6250\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5102 - acc: 0.7320 - val_loss: 0.6457 - val_acc: 0.6562\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5040 - acc: 0.7526 - val_loss: 0.6546 - val_acc: 0.6458\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5195 - acc: 0.7354 - val_loss: 0.6454 - val_acc: 0.6250\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.5221 - acc: 0.7509 - val_loss: 0.6593 - val_acc: 0.6250\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5168 - acc: 0.7560 - val_loss: 0.6626 - val_acc: 0.6354\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5063 - acc: 0.7526 - val_loss: 0.6616 - val_acc: 0.6250\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5215 - acc: 0.7405 - val_loss: 0.6714 - val_acc: 0.6250\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.5166 - acc: 0.7337 - val_loss: 0.6725 - val_acc: 0.6458\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5056 - acc: 0.7629 - val_loss: 0.6576 - val_acc: 0.6562\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5388 - acc: 0.7234 - val_loss: 0.6599 - val_acc: 0.6458\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.4975 - acc: 0.7577 - val_loss: 0.6594 - val_acc: 0.6354\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5147 - acc: 0.7371 - val_loss: 0.6502 - val_acc: 0.6354\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.4941 - acc: 0.7560 - val_loss: 0.6405 - val_acc: 0.6458\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5190 - acc: 0.7354 - val_loss: 0.6577 - val_acc: 0.6562\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.4871 - acc: 0.7904 - val_loss: 0.6524 - val_acc: 0.6562\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5157 - acc: 0.7337 - val_loss: 0.6643 - val_acc: 0.6562\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.4977 - acc: 0.7663 - val_loss: 0.6569 - val_acc: 0.6562\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5264 - acc: 0.7320 - val_loss: 0.6568 - val_acc: 0.6354\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.5282 - acc: 0.7285 - val_loss: 0.6617 - val_acc: 0.6458\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5051 - acc: 0.7509 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5037 - acc: 0.7526 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 0.4832 - acc: 0.7646 - val_loss: 0.6696 - val_acc: 0.6354\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.4957 - acc: 0.7405 - val_loss: 0.6636 - val_acc: 0.6354\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.5222 - acc: 0.7371 - val_loss: 0.6749 - val_acc: 0.6354\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5267 - acc: 0.7268 - val_loss: 0.6492 - val_acc: 0.6354\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.5246 - acc: 0.7474 - val_loss: 0.6559 - val_acc: 0.6146\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.4638 - acc: 0.7612 - val_loss: 0.6608 - val_acc: 0.6354\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4979 - acc: 0.7715 - val_loss: 0.6599 - val_acc: 0.6562\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 0.5286 - acc: 0.7285 - val_loss: 0.6759 - val_acc: 0.6458\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4740 - acc: 0.7646 - val_loss: 0.6582 - val_acc: 0.6458\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.4937 - acc: 0.7595 - val_loss: 0.6646 - val_acc: 0.6562\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5320 - acc: 0.7302 - val_loss: 0.6795 - val_acc: 0.6354\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.4860 - acc: 0.7801 - val_loss: 0.6461 - val_acc: 0.6875\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.5100 - acc: 0.7268 - val_loss: 0.6582 - val_acc: 0.6562\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4870 - acc: 0.7612 - val_loss: 0.6657 - val_acc: 0.6562\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5210 - acc: 0.7526 - val_loss: 0.6521 - val_acc: 0.6667\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5151 - acc: 0.7285 - val_loss: 0.6499 - val_acc: 0.6458\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5457 - acc: 0.7234 - val_loss: 0.6778 - val_acc: 0.6354\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4948 - acc: 0.7835 - val_loss: 0.6813 - val_acc: 0.6354\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5330 - acc: 0.7320 - val_loss: 0.6764 - val_acc: 0.6458\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.5042 - acc: 0.7612 - val_loss: 0.6785 - val_acc: 0.6146\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5203 - acc: 0.7371 - val_loss: 0.6546 - val_acc: 0.6562\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5385 - acc: 0.7371 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.4672 - acc: 0.7869 - val_loss: 0.6808 - val_acc: 0.6458\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5306 - acc: 0.7216 - val_loss: 0.6652 - val_acc: 0.6562\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5349 - acc: 0.7285 - val_loss: 0.6601 - val_acc: 0.6562\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5267 - acc: 0.7234 - val_loss: 0.6513 - val_acc: 0.6667\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.4821 - acc: 0.7801 - val_loss: 0.6672 - val_acc: 0.6562\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4915 - acc: 0.7491 - val_loss: 0.6571 - val_acc: 0.6562\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5523 - acc: 0.7234 - val_loss: 0.6680 - val_acc: 0.6562\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4943 - acc: 0.7491 - val_loss: 0.6731 - val_acc: 0.6562\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4836 - acc: 0.7595 - val_loss: 0.6706 - val_acc: 0.6562\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4983 - acc: 0.7577 - val_loss: 0.6764 - val_acc: 0.6458\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5249 - acc: 0.7423 - val_loss: 0.6721 - val_acc: 0.6562\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.5127 - acc: 0.7526 - val_loss: 0.6704 - val_acc: 0.6562\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5271 - acc: 0.7113 - val_loss: 0.6570 - val_acc: 0.6667\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4964 - acc: 0.7474 - val_loss: 0.6668 - val_acc: 0.6458\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.5118 - acc: 0.7388 - val_loss: 0.6619 - val_acc: 0.6562\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5176 - acc: 0.7440 - val_loss: 0.6555 - val_acc: 0.6458\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4876 - acc: 0.7698 - val_loss: 0.6756 - val_acc: 0.6458\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5050 - acc: 0.7577 - val_loss: 0.6610 - val_acc: 0.6354\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.4956 - acc: 0.7509 - val_loss: 0.6610 - val_acc: 0.6562\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4991 - acc: 0.7749 - val_loss: 0.6483 - val_acc: 0.6771\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5132 - acc: 0.7302 - val_loss: 0.6801 - val_acc: 0.6458\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4950 - acc: 0.7577 - val_loss: 0.6575 - val_acc: 0.6458\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5034 - acc: 0.7474 - val_loss: 0.6613 - val_acc: 0.6667\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.5215 - acc: 0.7388 - val_loss: 0.6564 - val_acc: 0.6562\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5134 - acc: 0.7423 - val_loss: 0.6700 - val_acc: 0.6250\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4941 - acc: 0.7766 - val_loss: 0.6609 - val_acc: 0.6458\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.4697 - acc: 0.7749 - val_loss: 0.6715 - val_acc: 0.6562\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5180 - acc: 0.7405 - val_loss: 0.6781 - val_acc: 0.6250\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.5596 - acc: 0.7113 - val_loss: 0.6789 - val_acc: 0.6458\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.5126 - acc: 0.7560 - val_loss: 0.6641 - val_acc: 0.6354\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5389 - acc: 0.7320 - val_loss: 0.6292 - val_acc: 0.6875\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5206 - acc: 0.7199 - val_loss: 0.6780 - val_acc: 0.6354\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5060 - acc: 0.7680 - val_loss: 0.6709 - val_acc: 0.6458\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.5223 - acc: 0.7577 - val_loss: 0.6704 - val_acc: 0.6458\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4720 - acc: 0.7749 - val_loss: 0.6697 - val_acc: 0.6146\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.4900 - acc: 0.7474 - val_loss: 0.6732 - val_acc: 0.6146\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.5085 - acc: 0.7509 - val_loss: 0.6533 - val_acc: 0.6250\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5052 - acc: 0.7577 - val_loss: 0.6589 - val_acc: 0.6354\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5328 - acc: 0.7268 - val_loss: 0.6594 - val_acc: 0.5938\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.5213 - acc: 0.7423 - val_loss: 0.6729 - val_acc: 0.6250\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5251 - acc: 0.7423 - val_loss: 0.6511 - val_acc: 0.6458\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5283 - acc: 0.7371 - val_loss: 0.6536 - val_acc: 0.6562\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.5219 - acc: 0.7302 - val_loss: 0.6435 - val_acc: 0.6667\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.5080 - acc: 0.7457 - val_loss: 0.6551 - val_acc: 0.6458\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.5043 - acc: 0.7646 - val_loss: 0.6707 - val_acc: 0.6458\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.4918 - acc: 0.7457 - val_loss: 0.6697 - val_acc: 0.6250\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.4823 - acc: 0.7732 - val_loss: 0.6559 - val_acc: 0.6458\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4899 - acc: 0.7612 - val_loss: 0.6519 - val_acc: 0.6562\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5062 - acc: 0.7595 - val_loss: 0.6537 - val_acc: 0.6667\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.4908 - acc: 0.7457 - val_loss: 0.6659 - val_acc: 0.6562\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.5037 - acc: 0.7320 - val_loss: 0.6628 - val_acc: 0.6562\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.4986 - acc: 0.7354 - val_loss: 0.6818 - val_acc: 0.6458\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5164 - acc: 0.7251 - val_loss: 0.6537 - val_acc: 0.6562\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.4948 - acc: 0.7543 - val_loss: 0.6627 - val_acc: 0.6354\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4999 - acc: 0.7629 - val_loss: 0.6572 - val_acc: 0.6667\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5128 - acc: 0.7543 - val_loss: 0.6720 - val_acc: 0.6354\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5021 - acc: 0.7526 - val_loss: 0.6400 - val_acc: 0.6354\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.5004 - acc: 0.7629 - val_loss: 0.6646 - val_acc: 0.6250\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.4850 - acc: 0.7543 - val_loss: 0.6472 - val_acc: 0.6458\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4801 - acc: 0.7766 - val_loss: 0.6654 - val_acc: 0.6250\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5148 - acc: 0.7354 - val_loss: 0.6641 - val_acc: 0.6458\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.5056 - acc: 0.7474 - val_loss: 0.6449 - val_acc: 0.6771\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.4938 - acc: 0.7423 - val_loss: 0.6567 - val_acc: 0.6562\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.4709 - acc: 0.7646 - val_loss: 0.6658 - val_acc: 0.6562\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4995 - acc: 0.7595 - val_loss: 0.6383 - val_acc: 0.6458\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.4580 - acc: 0.7715 - val_loss: 0.6376 - val_acc: 0.6042\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.5253 - acc: 0.7388 - val_loss: 0.6585 - val_acc: 0.6042\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4983 - acc: 0.7629 - val_loss: 0.6330 - val_acc: 0.6667\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5547 - acc: 0.7285 - val_loss: 0.6485 - val_acc: 0.6458\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4906 - acc: 0.7629 - val_loss: 0.6615 - val_acc: 0.6458\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.5183 - acc: 0.7337 - val_loss: 0.6649 - val_acc: 0.6146\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5044 - acc: 0.7371 - val_loss: 0.6688 - val_acc: 0.6250\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5105 - acc: 0.7509 - val_loss: 0.6672 - val_acc: 0.6458\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5092 - acc: 0.7234 - val_loss: 0.6649 - val_acc: 0.6562\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 0.5244 - acc: 0.7405 - val_loss: 0.6549 - val_acc: 0.6562\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5262 - acc: 0.7405 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.4854 - acc: 0.7629 - val_loss: 0.6429 - val_acc: 0.6667\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5272 - acc: 0.7440 - val_loss: 0.6609 - val_acc: 0.6250\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5023 - acc: 0.7577 - val_loss: 0.6737 - val_acc: 0.6458\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5113 - acc: 0.7440 - val_loss: 0.6713 - val_acc: 0.6562\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5138 - acc: 0.7251 - val_loss: 0.6919 - val_acc: 0.6354\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5287 - acc: 0.7268 - val_loss: 0.6703 - val_acc: 0.6562\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.5388 - acc: 0.7027 - val_loss: 0.6651 - val_acc: 0.6562\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.5290 - acc: 0.7440 - val_loss: 0.6662 - val_acc: 0.6667\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.5080 - acc: 0.7526 - val_loss: 0.6595 - val_acc: 0.6458\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.5089 - acc: 0.7526 - val_loss: 0.6631 - val_acc: 0.6458\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.5309 - acc: 0.7474 - val_loss: 0.6624 - val_acc: 0.6458\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.5012 - acc: 0.7560 - val_loss: 0.6632 - val_acc: 0.6354\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.5570 - acc: 0.7302 - val_loss: 0.6729 - val_acc: 0.6458\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.5047 - acc: 0.7509 - val_loss: 0.6725 - val_acc: 0.6458\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.5084 - acc: 0.7595 - val_loss: 0.6721 - val_acc: 0.6458\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4990 - acc: 0.7509 - val_loss: 0.6666 - val_acc: 0.6667\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.5021 - acc: 0.7405 - val_loss: 0.6707 - val_acc: 0.6562\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5030 - acc: 0.7285 - val_loss: 0.6823 - val_acc: 0.6354\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.5354 - acc: 0.7423 - val_loss: 0.6826 - val_acc: 0.6458\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5159 - acc: 0.7285 - val_loss: 0.6642 - val_acc: 0.6562\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5092 - acc: 0.7560 - val_loss: 0.6736 - val_acc: 0.6562\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.4793 - acc: 0.7663 - val_loss: 0.6622 - val_acc: 0.6562\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5207 - acc: 0.7577 - val_loss: 0.6641 - val_acc: 0.6146\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.5179 - acc: 0.7440 - val_loss: 0.6681 - val_acc: 0.6562\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.4911 - acc: 0.7491 - val_loss: 0.6661 - val_acc: 0.6667\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5128 - acc: 0.7423 - val_loss: 0.6665 - val_acc: 0.6667\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4899 - acc: 0.7612 - val_loss: 0.6630 - val_acc: 0.6354\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.5004 - acc: 0.7663 - val_loss: 0.6556 - val_acc: 0.6667\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4778 - acc: 0.7921 - val_loss: 0.6685 - val_acc: 0.6250\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5259 - acc: 0.7320 - val_loss: 0.6677 - val_acc: 0.6667\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5246 - acc: 0.7285 - val_loss: 0.6930 - val_acc: 0.6354\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.5183 - acc: 0.7474 - val_loss: 0.6738 - val_acc: 0.6562\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.5238 - acc: 0.7285 - val_loss: 0.6578 - val_acc: 0.6667\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.5408 - acc: 0.7388 - val_loss: 0.6771 - val_acc: 0.6458\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.4837 - acc: 0.7629 - val_loss: 0.6653 - val_acc: 0.6562\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.5467 - acc: 0.7268 - val_loss: 0.6514 - val_acc: 0.6667\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.5155 - acc: 0.7680 - val_loss: 0.6603 - val_acc: 0.6562\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5140 - acc: 0.7423 - val_loss: 0.6564 - val_acc: 0.6667\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.5293 - acc: 0.7062 - val_loss: 0.6441 - val_acc: 0.6667\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.5012 - acc: 0.7388 - val_loss: 0.6624 - val_acc: 0.6667\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.5197 - acc: 0.7474 - val_loss: 0.6853 - val_acc: 0.6458\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.4760 - acc: 0.7835 - val_loss: 0.6747 - val_acc: 0.6562\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.5383 - acc: 0.7251 - val_loss: 0.6777 - val_acc: 0.6458\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.5038 - acc: 0.7234 - val_loss: 0.6642 - val_acc: 0.6562\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.4962 - acc: 0.7646 - val_loss: 0.6448 - val_acc: 0.6667\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.4939 - acc: 0.7543 - val_loss: 0.6754 - val_acc: 0.6458\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5022 - acc: 0.7560 - val_loss: 0.6638 - val_acc: 0.6562\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.5083 - acc: 0.7320 - val_loss: 0.6786 - val_acc: 0.6458\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.4907 - acc: 0.7680 - val_loss: 0.6750 - val_acc: 0.6458\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.4922 - acc: 0.7577 - val_loss: 0.6719 - val_acc: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training performance"
      ],
      "metadata": {
        "id": "YFxMJycOpEew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "l9PYKbRNpEK8",
        "outputId": "abcdefe0-a875-4323-fd5a-9ffecc918c7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAVE model"
      ],
      "metadata": {
        "id": "kNtlq_LZpKNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_UNfreeze_2000.h5')"
      ],
      "metadata": {
        "id": "TegSVqblpLEZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "KdsdaIgMpMeG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_UNfreeze_2000.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sZFZ1c_kpMcN",
        "outputId": "9cbf4b57-0606-4047-e7cc-341b902a0c87"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef179e8a-31fa-426a-af51-07c1765cb771\", \"2Class_UNfreeze_2000.h5\", 16604952)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEfCB1RfpMZ6"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}