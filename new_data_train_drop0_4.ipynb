{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/DEg5Q7eO54RkKCYU21Ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train_drop0_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "916d3462-10f9-46e5-8273-c7e37d6526de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "7634f082-2234-43c8-fcb5-3b432025804c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "840  841  1-s2.0-S2095268622000210-main   \n",
              "841  842  1-s2.0-S2095268622000210-main   \n",
              "842  843  1-s2.0-S2095268622000210-main   \n",
              "843  844  1-s2.0-S2095268622000210-main   \n",
              "844  845  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "840  Integration of preparation of K, Na-embedded a...   \n",
              "841  Integration of preparation of K, Na-embedded a...   \n",
              "842  Integration of preparation of K, Na-embedded a...   \n",
              "843  Integration of preparation of K, Na-embedded a...   \n",
              "844  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "840  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "841  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "842  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "843  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "844  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture    detail  Class  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...  original  0-500   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom1  0-500   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom2  0-500   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom3  0-500   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom4  0-500   \n",
              "..                                                 ...       ...    ...   \n",
              "840  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom21  0-500   \n",
              "841  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom22  0-500   \n",
              "842  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom23  0-500   \n",
              "843  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom24  0-500   \n",
              "844  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom25  0-500   \n",
              "\n",
              "        BET  Size(mico)  \n",
              "0    135.06           5  \n",
              "1    135.06          10  \n",
              "2    135.06          10  \n",
              "3    135.06          10  \n",
              "4    135.06          10  \n",
              "..      ...         ...  \n",
              "840  301.70          10  \n",
              "841  301.70          10  \n",
              "842  301.70          10  \n",
              "843  301.70          10  \n",
              "844  301.70          10  \n",
              "\n",
              "[845 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9eb79b54-0a31-4d76-bcd8-8a9ebe71a0c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>841</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>842</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>843</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>844</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>845</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>845 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9eb79b54-0a31-4d76-bcd8-8a9ebe71a0c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9eb79b54-0a31-4d76-bcd8-8a9ebe71a0c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9eb79b54-0a31-4d76-bcd8-8a9ebe71a0c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "0d7fc5b5-b104-4e47-d7e6-2a24a0f1ac3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "d5eae007-40fe-4153-864a-013d22b4c561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "d8317c7f-fe54-414b-beec-9a425b9e8edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "aa2241f4-f1fa-4aa6-e207-b4c5eda4012c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 653, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 653 (delta 143), reused 103 (delta 103), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (653/653), 12.00 MiB | 23.49 MiB/s, done.\n",
            "Resolving deltas: 100% (383/383), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)]  \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "47a605b9-b0ac-4257-fdb0-4d52cb4e4994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 59\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 62\n",
            "total test 2 images: 6\n",
            "total test 3 images: 15 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 681  # จำนวนภาพ Train\n",
        "NUM_TEST = 82 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.4\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "4d9078a2-190b-4242-d8db-6f78af2b03ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "e3677771-f47f-4c67-d325-cf72ac0f7372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "e182cb66-4259-4c8e-e2b2-76bc94d56c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "38b0bebb-c2aa-4318-f6f7-693c9cfea3e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "a0e79314-2684-4879-98c9-cc5cca3e4911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 81 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a2cfe5-499d-4275-cf38-63c4aa3ca3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 35s 2s/step - loss: 1.5869 - acc: 0.4587 - val_loss: 2.1734 - val_acc: 0.0938\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.6109 - acc: 0.4327 - val_loss: 2.0694 - val_acc: 0.0938\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.6912 - acc: 0.4003 - val_loss: 1.9078 - val_acc: 0.1094\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5310 - acc: 0.4109 - val_loss: 1.8743 - val_acc: 0.1094\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5675 - acc: 0.4246 - val_loss: 1.8119 - val_acc: 0.0781\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.6239 - acc: 0.3938 - val_loss: 1.7496 - val_acc: 0.1250\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.6107 - acc: 0.3647 - val_loss: 1.7404 - val_acc: 0.1250\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5821 - acc: 0.4182 - val_loss: 1.5981 - val_acc: 0.1562\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.5510 - acc: 0.3955 - val_loss: 1.6889 - val_acc: 0.1406\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.4821 - acc: 0.4187 - val_loss: 1.6641 - val_acc: 0.1562\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5028 - acc: 0.4084 - val_loss: 1.5942 - val_acc: 0.1250\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5826 - acc: 0.3776 - val_loss: 1.6986 - val_acc: 0.0938\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4847 - acc: 0.4198 - val_loss: 1.7184 - val_acc: 0.1250\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4835 - acc: 0.3841 - val_loss: 1.6036 - val_acc: 0.1562\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4672 - acc: 0.4149 - val_loss: 1.6583 - val_acc: 0.1562\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4793 - acc: 0.4052 - val_loss: 1.6271 - val_acc: 0.1250\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4195 - acc: 0.4203 - val_loss: 1.7068 - val_acc: 0.1094\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 34s 2s/step - loss: 1.4878 - acc: 0.4182 - val_loss: 1.6985 - val_acc: 0.1250\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5144 - acc: 0.3712 - val_loss: 1.6519 - val_acc: 0.1250\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5055 - acc: 0.4117 - val_loss: 1.6377 - val_acc: 0.1250\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4867 - acc: 0.4263 - val_loss: 1.6139 - val_acc: 0.1719\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4705 - acc: 0.4230 - val_loss: 1.6325 - val_acc: 0.1562\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5256 - acc: 0.4117 - val_loss: 1.6677 - val_acc: 0.1094\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5692 - acc: 0.4036 - val_loss: 1.6807 - val_acc: 0.0938\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3909 - acc: 0.4198 - val_loss: 1.6111 - val_acc: 0.1094\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.6028 - acc: 0.3728 - val_loss: 1.6527 - val_acc: 0.0938\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4434 - acc: 0.4165 - val_loss: 1.6555 - val_acc: 0.0938\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.5495 - acc: 0.4279 - val_loss: 1.6735 - val_acc: 0.0781\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4203 - acc: 0.4538 - val_loss: 1.5890 - val_acc: 0.1250\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5000 - acc: 0.4182 - val_loss: 1.6702 - val_acc: 0.1094\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3975 - acc: 0.4344 - val_loss: 1.6535 - val_acc: 0.0781\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4749 - acc: 0.4133 - val_loss: 1.6829 - val_acc: 0.0938\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5633 - acc: 0.4036 - val_loss: 1.6947 - val_acc: 0.1094\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4859 - acc: 0.4068 - val_loss: 1.6371 - val_acc: 0.1250\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.4593 - acc: 0.4392 - val_loss: 1.6563 - val_acc: 0.1250\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4691 - acc: 0.4019 - val_loss: 1.6904 - val_acc: 0.0781\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4108 - acc: 0.3955 - val_loss: 1.6531 - val_acc: 0.1094\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.4821 - acc: 0.4246 - val_loss: 1.5897 - val_acc: 0.1094\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4268 - acc: 0.4036 - val_loss: 1.6717 - val_acc: 0.0938\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4902 - acc: 0.4149 - val_loss: 1.7195 - val_acc: 0.0938\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5216 - acc: 0.4003 - val_loss: 1.7095 - val_acc: 0.1250\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4427 - acc: 0.4133 - val_loss: 1.6843 - val_acc: 0.1094\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4214 - acc: 0.4214 - val_loss: 1.6685 - val_acc: 0.1250\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4918 - acc: 0.4117 - val_loss: 1.6566 - val_acc: 0.1094\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2986 - acc: 0.4609 - val_loss: 1.7539 - val_acc: 0.0781\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4497 - acc: 0.4182 - val_loss: 1.6913 - val_acc: 0.1094\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4372 - acc: 0.4263 - val_loss: 1.7279 - val_acc: 0.0781\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4464 - acc: 0.4016 - val_loss: 1.6558 - val_acc: 0.1406\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4487 - acc: 0.4117 - val_loss: 1.6911 - val_acc: 0.1250\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.4458 - acc: 0.4344 - val_loss: 1.6198 - val_acc: 0.1094\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3087 - acc: 0.4684 - val_loss: 1.7211 - val_acc: 0.0938\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4637 - acc: 0.4230 - val_loss: 1.7315 - val_acc: 0.1094\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3635 - acc: 0.4327 - val_loss: 1.6466 - val_acc: 0.1250\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4283 - acc: 0.4263 - val_loss: 1.7102 - val_acc: 0.1094\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.3646 - acc: 0.4538 - val_loss: 1.6540 - val_acc: 0.1406\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4535 - acc: 0.4263 - val_loss: 1.6695 - val_acc: 0.1250\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3752 - acc: 0.4230 - val_loss: 1.6786 - val_acc: 0.1094\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4399 - acc: 0.4230 - val_loss: 1.6738 - val_acc: 0.0938\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4044 - acc: 0.4489 - val_loss: 1.6944 - val_acc: 0.1250\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.4087 - acc: 0.4376 - val_loss: 1.6991 - val_acc: 0.0781\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4876 - acc: 0.4230 - val_loss: 1.6186 - val_acc: 0.1094\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2498 - acc: 0.4587 - val_loss: 1.7636 - val_acc: 0.0938\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3740 - acc: 0.4327 - val_loss: 1.7495 - val_acc: 0.1250\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4064 - acc: 0.4327 - val_loss: 1.7252 - val_acc: 0.1406\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3796 - acc: 0.4489 - val_loss: 1.7841 - val_acc: 0.1094\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3603 - acc: 0.4392 - val_loss: 1.7418 - val_acc: 0.0781\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3237 - acc: 0.4425 - val_loss: 1.7910 - val_acc: 0.0781\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4296 - acc: 0.4279 - val_loss: 1.7283 - val_acc: 0.1094\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3221 - acc: 0.4635 - val_loss: 1.6501 - val_acc: 0.1250\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3735 - acc: 0.4376 - val_loss: 1.6855 - val_acc: 0.0938\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3471 - acc: 0.4376 - val_loss: 1.6760 - val_acc: 0.1094\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.3691 - acc: 0.4441 - val_loss: 1.7145 - val_acc: 0.0938\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4075 - acc: 0.4266 - val_loss: 1.6629 - val_acc: 0.1094\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2267 - acc: 0.4700 - val_loss: 1.5928 - val_acc: 0.1094\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4110 - acc: 0.4246 - val_loss: 1.6496 - val_acc: 0.1406\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3236 - acc: 0.4538 - val_loss: 1.7562 - val_acc: 0.0938\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3621 - acc: 0.4641 - val_loss: 1.6542 - val_acc: 0.1250\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3576 - acc: 0.4473 - val_loss: 1.6416 - val_acc: 0.1094\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3766 - acc: 0.4360 - val_loss: 1.6419 - val_acc: 0.1094\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3337 - acc: 0.4311 - val_loss: 1.6698 - val_acc: 0.0938\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3165 - acc: 0.4522 - val_loss: 1.5972 - val_acc: 0.1094\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3189 - acc: 0.4457 - val_loss: 1.7102 - val_acc: 0.0938\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3416 - acc: 0.4603 - val_loss: 1.6583 - val_acc: 0.1250\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3787 - acc: 0.4149 - val_loss: 1.6781 - val_acc: 0.1094\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3395 - acc: 0.4506 - val_loss: 1.5937 - val_acc: 0.1250\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3047 - acc: 0.4554 - val_loss: 1.6531 - val_acc: 0.1094\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3129 - acc: 0.4571 - val_loss: 1.7276 - val_acc: 0.0781\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3846 - acc: 0.4311 - val_loss: 1.6919 - val_acc: 0.0938\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 31s 3s/step - loss: 1.3586 - acc: 0.4531 - val_loss: 1.5200 - val_acc: 0.1406\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.3494 - acc: 0.4408 - val_loss: 1.7026 - val_acc: 0.0625\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3317 - acc: 0.4716 - val_loss: 1.6170 - val_acc: 0.1250\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3203 - acc: 0.4538 - val_loss: 1.7144 - val_acc: 0.0781\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2579 - acc: 0.4716 - val_loss: 1.7505 - val_acc: 0.0781\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3547 - acc: 0.4408 - val_loss: 1.7375 - val_acc: 0.0938\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2350 - acc: 0.4814 - val_loss: 1.6517 - val_acc: 0.1250\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2826 - acc: 0.4716 - val_loss: 1.6307 - val_acc: 0.1094\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3652 - acc: 0.4473 - val_loss: 1.6409 - val_acc: 0.0938\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3089 - acc: 0.4538 - val_loss: 1.7015 - val_acc: 0.0938\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2114 - acc: 0.4927 - val_loss: 1.7216 - val_acc: 0.0938\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3112 - acc: 0.4441 - val_loss: 1.6776 - val_acc: 0.1094\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2461 - acc: 0.4571 - val_loss: 1.6507 - val_acc: 0.1094\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3129 - acc: 0.4165 - val_loss: 1.6304 - val_acc: 0.1094\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4139 - acc: 0.4016 - val_loss: 1.7534 - val_acc: 0.0625\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3372 - acc: 0.4165 - val_loss: 1.7109 - val_acc: 0.1094\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3897 - acc: 0.4187 - val_loss: 1.6868 - val_acc: 0.0938\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2574 - acc: 0.4749 - val_loss: 1.6982 - val_acc: 0.0938\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3240 - acc: 0.4522 - val_loss: 1.6988 - val_acc: 0.1094\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2614 - acc: 0.4522 - val_loss: 1.7085 - val_acc: 0.1250\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2592 - acc: 0.4895 - val_loss: 1.7179 - val_acc: 0.1094\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2708 - acc: 0.4635 - val_loss: 1.6471 - val_acc: 0.1406\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2740 - acc: 0.4376 - val_loss: 1.6881 - val_acc: 0.1406\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2584 - acc: 0.4700 - val_loss: 1.6341 - val_acc: 0.1250\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2760 - acc: 0.4938 - val_loss: 1.6451 - val_acc: 0.1250\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2678 - acc: 0.4425 - val_loss: 1.7050 - val_acc: 0.1250\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2918 - acc: 0.4571 - val_loss: 1.6807 - val_acc: 0.1250\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3376 - acc: 0.4554 - val_loss: 1.6726 - val_acc: 0.1562\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3218 - acc: 0.4554 - val_loss: 1.6693 - val_acc: 0.1094\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3150 - acc: 0.4635 - val_loss: 1.7149 - val_acc: 0.1250\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3228 - acc: 0.4554 - val_loss: 1.6852 - val_acc: 0.1250\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2695 - acc: 0.4668 - val_loss: 1.6711 - val_acc: 0.1406\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2661 - acc: 0.4635 - val_loss: 1.6492 - val_acc: 0.1094\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2960 - acc: 0.4360 - val_loss: 1.6200 - val_acc: 0.1094\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2812 - acc: 0.4538 - val_loss: 1.7627 - val_acc: 0.1094\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2735 - acc: 0.4700 - val_loss: 1.7011 - val_acc: 0.1562\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2528 - acc: 0.4538 - val_loss: 1.6788 - val_acc: 0.1250\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2238 - acc: 0.4700 - val_loss: 1.7008 - val_acc: 0.0938\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2691 - acc: 0.4587 - val_loss: 1.7056 - val_acc: 0.0938\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2774 - acc: 0.4571 - val_loss: 1.6421 - val_acc: 0.1562\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2561 - acc: 0.4668 - val_loss: 1.6513 - val_acc: 0.1094\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.2727 - acc: 0.4749 - val_loss: 1.6632 - val_acc: 0.1250\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3206 - acc: 0.4117 - val_loss: 1.6618 - val_acc: 0.1406\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3466 - acc: 0.4619 - val_loss: 1.6894 - val_acc: 0.0938\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2684 - acc: 0.4668 - val_loss: 1.6916 - val_acc: 0.1094\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2598 - acc: 0.4684 - val_loss: 1.5695 - val_acc: 0.1562\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2037 - acc: 0.5008 - val_loss: 1.5734 - val_acc: 0.1250\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2813 - acc: 0.4911 - val_loss: 1.6758 - val_acc: 0.1250\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3440 - acc: 0.4266 - val_loss: 1.7542 - val_acc: 0.1250\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2934 - acc: 0.4619 - val_loss: 1.6402 - val_acc: 0.1562\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2540 - acc: 0.4716 - val_loss: 1.7503 - val_acc: 0.0938\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1703 - acc: 0.5219 - val_loss: 1.6404 - val_acc: 0.1094\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2252 - acc: 0.4749 - val_loss: 1.6268 - val_acc: 0.1250\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1818 - acc: 0.4959 - val_loss: 1.6639 - val_acc: 0.1406\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2703 - acc: 0.4571 - val_loss: 1.6748 - val_acc: 0.0938\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2792 - acc: 0.4749 - val_loss: 1.7483 - val_acc: 0.1094\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2308 - acc: 0.4716 - val_loss: 1.6240 - val_acc: 0.1250\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3007 - acc: 0.4376 - val_loss: 1.6235 - val_acc: 0.0938\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2954 - acc: 0.4441 - val_loss: 1.6929 - val_acc: 0.1094\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1922 - acc: 0.4571 - val_loss: 1.6706 - val_acc: 0.0938\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1474 - acc: 0.5316 - val_loss: 1.7262 - val_acc: 0.0938\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2118 - acc: 0.4700 - val_loss: 1.7247 - val_acc: 0.1250\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2585 - acc: 0.4619 - val_loss: 1.6934 - val_acc: 0.1406\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1900 - acc: 0.5089 - val_loss: 1.7650 - val_acc: 0.1406\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.3084 - acc: 0.4684 - val_loss: 1.6946 - val_acc: 0.1406\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2698 - acc: 0.4571 - val_loss: 1.7966 - val_acc: 0.1094\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1177 - acc: 0.5154 - val_loss: 1.6922 - val_acc: 0.1250\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2078 - acc: 0.4587 - val_loss: 1.7573 - val_acc: 0.1094\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2141 - acc: 0.4473 - val_loss: 1.7776 - val_acc: 0.1250\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1636 - acc: 0.5089 - val_loss: 1.7014 - val_acc: 0.1406\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2645 - acc: 0.4716 - val_loss: 1.7734 - val_acc: 0.1250\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2601 - acc: 0.4716 - val_loss: 1.5825 - val_acc: 0.1562\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2257 - acc: 0.4781 - val_loss: 1.6851 - val_acc: 0.1250\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2022 - acc: 0.4927 - val_loss: 1.6831 - val_acc: 0.1250\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2022 - acc: 0.4927 - val_loss: 1.6336 - val_acc: 0.1094\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2397 - acc: 0.4781 - val_loss: 1.6970 - val_acc: 0.1406\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2231 - acc: 0.4862 - val_loss: 1.6548 - val_acc: 0.1406\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.2021 - acc: 0.4959 - val_loss: 1.6801 - val_acc: 0.1250\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2017 - acc: 0.4891 - val_loss: 1.7354 - val_acc: 0.1250\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2423 - acc: 0.4538 - val_loss: 1.6579 - val_acc: 0.1562\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1688 - acc: 0.5057 - val_loss: 1.6946 - val_acc: 0.1250\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2082 - acc: 0.4878 - val_loss: 1.6975 - val_acc: 0.1250\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2060 - acc: 0.5073 - val_loss: 1.7550 - val_acc: 0.1562\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2429 - acc: 0.4895 - val_loss: 1.6749 - val_acc: 0.1406\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2462 - acc: 0.4522 - val_loss: 1.6927 - val_acc: 0.1406\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1821 - acc: 0.4895 - val_loss: 1.7160 - val_acc: 0.1250\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1995 - acc: 0.5024 - val_loss: 1.7985 - val_acc: 0.0781\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1911 - acc: 0.4765 - val_loss: 1.7559 - val_acc: 0.1719\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1907 - acc: 0.5073 - val_loss: 1.6902 - val_acc: 0.0938\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1680 - acc: 0.4959 - val_loss: 1.7420 - val_acc: 0.1562\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2510 - acc: 0.4895 - val_loss: 1.7331 - val_acc: 0.1250\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1318 - acc: 0.5284 - val_loss: 1.7347 - val_acc: 0.1094\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2020 - acc: 0.5122 - val_loss: 1.5918 - val_acc: 0.1562\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1950 - acc: 0.4927 - val_loss: 1.6649 - val_acc: 0.1406\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2211 - acc: 0.4922 - val_loss: 1.6869 - val_acc: 0.0938\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1268 - acc: 0.4878 - val_loss: 1.7568 - val_acc: 0.1250\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1945 - acc: 0.4959 - val_loss: 1.7848 - val_acc: 0.1094\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1506 - acc: 0.5008 - val_loss: 1.7165 - val_acc: 0.1406\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2175 - acc: 0.4684 - val_loss: 1.7188 - val_acc: 0.1406\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1849 - acc: 0.5008 - val_loss: 1.7035 - val_acc: 0.1250\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1918 - acc: 0.4828 - val_loss: 1.6749 - val_acc: 0.1406\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1086 - acc: 0.5234 - val_loss: 1.7221 - val_acc: 0.1094\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1999 - acc: 0.4797 - val_loss: 1.6334 - val_acc: 0.1719\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1696 - acc: 0.4976 - val_loss: 1.6376 - val_acc: 0.1406\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1335 - acc: 0.5138 - val_loss: 1.6437 - val_acc: 0.1719\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2306 - acc: 0.4830 - val_loss: 1.6080 - val_acc: 0.1250\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1108 - acc: 0.5186 - val_loss: 1.6909 - val_acc: 0.1250\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0985 - acc: 0.5170 - val_loss: 1.7694 - val_acc: 0.0625\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2018 - acc: 0.4927 - val_loss: 1.6155 - val_acc: 0.1562\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1549 - acc: 0.5024 - val_loss: 1.6646 - val_acc: 0.1250\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1802 - acc: 0.4927 - val_loss: 1.7789 - val_acc: 0.1406\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1994 - acc: 0.4878 - val_loss: 1.5848 - val_acc: 0.1719\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1655 - acc: 0.4733 - val_loss: 1.6985 - val_acc: 0.1094\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1895 - acc: 0.4984 - val_loss: 1.7774 - val_acc: 0.1094\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2011 - acc: 0.5089 - val_loss: 1.6755 - val_acc: 0.1094\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2004 - acc: 0.4846 - val_loss: 1.6161 - val_acc: 0.1406\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1508 - acc: 0.4668 - val_loss: 1.6776 - val_acc: 0.1406\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2035 - acc: 0.4781 - val_loss: 1.7069 - val_acc: 0.1250\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1479 - acc: 0.5016 - val_loss: 1.6498 - val_acc: 0.1562\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0759 - acc: 0.5592 - val_loss: 1.6050 - val_acc: 0.1250\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1594 - acc: 0.4992 - val_loss: 1.6328 - val_acc: 0.1406\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1706 - acc: 0.5024 - val_loss: 1.7112 - val_acc: 0.1250\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1299 - acc: 0.5186 - val_loss: 1.7289 - val_acc: 0.1094\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1631 - acc: 0.5041 - val_loss: 1.6874 - val_acc: 0.1250\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1890 - acc: 0.4733 - val_loss: 1.7311 - val_acc: 0.1250\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1288 - acc: 0.4765 - val_loss: 1.7737 - val_acc: 0.1562\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2068 - acc: 0.4797 - val_loss: 1.5891 - val_acc: 0.1562\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1330 - acc: 0.5235 - val_loss: 1.6498 - val_acc: 0.1562\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1790 - acc: 0.4830 - val_loss: 1.6293 - val_acc: 0.1406\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1483 - acc: 0.5170 - val_loss: 1.7087 - val_acc: 0.1094\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.2048 - acc: 0.4781 - val_loss: 1.6468 - val_acc: 0.1406\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1582 - acc: 0.4878 - val_loss: 1.6547 - val_acc: 0.1562\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1963 - acc: 0.4846 - val_loss: 1.6118 - val_acc: 0.1719\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1121 - acc: 0.5186 - val_loss: 1.6994 - val_acc: 0.1406\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1180 - acc: 0.5203 - val_loss: 1.6161 - val_acc: 0.1562\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1439 - acc: 0.5170 - val_loss: 1.7224 - val_acc: 0.1094\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1152 - acc: 0.5365 - val_loss: 1.6478 - val_acc: 0.1719\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1848 - acc: 0.4814 - val_loss: 1.5913 - val_acc: 0.1719\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1694 - acc: 0.5156 - val_loss: 1.5723 - val_acc: 0.1719\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1747 - acc: 0.4878 - val_loss: 1.7303 - val_acc: 0.1562\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1668 - acc: 0.4992 - val_loss: 1.6174 - val_acc: 0.1719\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1560 - acc: 0.5122 - val_loss: 1.6890 - val_acc: 0.2031\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0824 - acc: 0.5235 - val_loss: 1.6755 - val_acc: 0.1406\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1205 - acc: 0.4862 - val_loss: 1.6370 - val_acc: 0.1406\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1292 - acc: 0.5284 - val_loss: 1.6047 - val_acc: 0.1719\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1058 - acc: 0.5186 - val_loss: 1.7665 - val_acc: 0.1094\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1477 - acc: 0.5031 - val_loss: 1.6325 - val_acc: 0.1719\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1409 - acc: 0.5008 - val_loss: 1.6682 - val_acc: 0.1719\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1456 - acc: 0.4927 - val_loss: 1.5649 - val_acc: 0.2031\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1933 - acc: 0.4943 - val_loss: 1.7257 - val_acc: 0.1406\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2296 - acc: 0.4765 - val_loss: 1.7171 - val_acc: 0.1719\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1903 - acc: 0.4895 - val_loss: 1.6927 - val_acc: 0.1719\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0640 - acc: 0.5413 - val_loss: 1.6981 - val_acc: 0.1719\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1321 - acc: 0.5170 - val_loss: 1.7075 - val_acc: 0.1406\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1001 - acc: 0.5494 - val_loss: 1.6129 - val_acc: 0.1562\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1280 - acc: 0.4953 - val_loss: 1.6860 - val_acc: 0.1406\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1491 - acc: 0.5105 - val_loss: 1.7222 - val_acc: 0.1562\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1108 - acc: 0.5462 - val_loss: 1.6024 - val_acc: 0.1875\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1308 - acc: 0.5235 - val_loss: 1.6127 - val_acc: 0.1562\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1030 - acc: 0.5316 - val_loss: 1.6081 - val_acc: 0.2188\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1298 - acc: 0.5543 - val_loss: 1.6808 - val_acc: 0.1719\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1199 - acc: 0.4959 - val_loss: 1.7079 - val_acc: 0.1406\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0924 - acc: 0.5186 - val_loss: 1.6713 - val_acc: 0.1406\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0791 - acc: 0.5251 - val_loss: 1.6737 - val_acc: 0.1719\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1313 - acc: 0.5073 - val_loss: 1.7073 - val_acc: 0.1562\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0791 - acc: 0.5235 - val_loss: 1.5990 - val_acc: 0.1875\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1341 - acc: 0.5381 - val_loss: 1.6356 - val_acc: 0.2031\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1237 - acc: 0.5186 - val_loss: 1.7177 - val_acc: 0.1719\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1653 - acc: 0.5041 - val_loss: 1.6963 - val_acc: 0.1719\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1394 - acc: 0.4938 - val_loss: 1.5987 - val_acc: 0.1875\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1346 - acc: 0.4943 - val_loss: 1.7396 - val_acc: 0.1406\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1499 - acc: 0.5008 - val_loss: 1.6438 - val_acc: 0.1719\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0865 - acc: 0.5138 - val_loss: 1.6539 - val_acc: 0.1562\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1082 - acc: 0.5156 - val_loss: 1.6907 - val_acc: 0.1562\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1044 - acc: 0.5300 - val_loss: 1.6236 - val_acc: 0.1719\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1296 - acc: 0.5332 - val_loss: 1.6550 - val_acc: 0.1875\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0830 - acc: 0.5575 - val_loss: 1.6643 - val_acc: 0.1719\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1287 - acc: 0.5186 - val_loss: 1.7504 - val_acc: 0.1562\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0736 - acc: 0.5381 - val_loss: 1.6601 - val_acc: 0.1562\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1044 - acc: 0.5186 - val_loss: 1.6343 - val_acc: 0.1875\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0992 - acc: 0.5251 - val_loss: 1.7481 - val_acc: 0.1406\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0064 - acc: 0.5608 - val_loss: 1.5958 - val_acc: 0.2031\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0718 - acc: 0.5446 - val_loss: 1.6413 - val_acc: 0.2031\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0566 - acc: 0.5413 - val_loss: 1.6844 - val_acc: 0.1406\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0586 - acc: 0.5575 - val_loss: 1.6506 - val_acc: 0.1562\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0730 - acc: 0.5527 - val_loss: 1.6945 - val_acc: 0.1562\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1027 - acc: 0.5332 - val_loss: 1.6867 - val_acc: 0.1719\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0788 - acc: 0.5397 - val_loss: 1.7034 - val_acc: 0.1562\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1257 - acc: 0.5365 - val_loss: 1.5615 - val_acc: 0.1719\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0663 - acc: 0.5575 - val_loss: 1.6712 - val_acc: 0.1719\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0771 - acc: 0.5391 - val_loss: 1.6000 - val_acc: 0.1562\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0854 - acc: 0.5348 - val_loss: 1.7015 - val_acc: 0.1562\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0672 - acc: 0.4976 - val_loss: 1.7186 - val_acc: 0.1719\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1179 - acc: 0.5073 - val_loss: 1.7181 - val_acc: 0.1562\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1242 - acc: 0.5154 - val_loss: 1.6228 - val_acc: 0.1719\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0691 - acc: 0.5300 - val_loss: 1.6811 - val_acc: 0.1562\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1002 - acc: 0.5332 - val_loss: 1.7137 - val_acc: 0.1406\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1371 - acc: 0.5138 - val_loss: 1.5781 - val_acc: 0.1719\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0461 - acc: 0.5300 - val_loss: 1.6021 - val_acc: 0.1875\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0639 - acc: 0.5105 - val_loss: 1.7523 - val_acc: 0.1250\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0580 - acc: 0.5397 - val_loss: 1.5749 - val_acc: 0.2188\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0696 - acc: 0.5365 - val_loss: 1.6782 - val_acc: 0.1406\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0832 - acc: 0.5462 - val_loss: 1.7431 - val_acc: 0.1250\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1128 - acc: 0.5316 - val_loss: 1.6577 - val_acc: 0.2031\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0738 - acc: 0.5575 - val_loss: 1.7687 - val_acc: 0.1094\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0786 - acc: 0.5089 - val_loss: 1.6466 - val_acc: 0.1875\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0472 - acc: 0.5406 - val_loss: 1.7014 - val_acc: 0.1562\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0738 - acc: 0.5300 - val_loss: 1.6257 - val_acc: 0.1875\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0558 - acc: 0.5316 - val_loss: 1.6893 - val_acc: 0.1562\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0841 - acc: 0.5170 - val_loss: 1.7029 - val_acc: 0.1719\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0865 - acc: 0.5365 - val_loss: 1.6882 - val_acc: 0.1562\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0765 - acc: 0.5300 - val_loss: 1.6242 - val_acc: 0.1719\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0369 - acc: 0.5478 - val_loss: 1.7254 - val_acc: 0.1562\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1216 - acc: 0.5024 - val_loss: 1.7083 - val_acc: 0.1875\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0274 - acc: 0.5673 - val_loss: 1.7330 - val_acc: 0.1562\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0177 - acc: 0.5575 - val_loss: 1.6207 - val_acc: 0.1875\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0402 - acc: 0.5462 - val_loss: 1.6358 - val_acc: 0.1875\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.1273 - acc: 0.5235 - val_loss: 1.6238 - val_acc: 0.1875\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0283 - acc: 0.5446 - val_loss: 1.6062 - val_acc: 0.1875\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0683 - acc: 0.5494 - val_loss: 1.6101 - val_acc: 0.1562\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0344 - acc: 0.5543 - val_loss: 1.6647 - val_acc: 0.1562\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0595 - acc: 0.5429 - val_loss: 1.6506 - val_acc: 0.1406\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0702 - acc: 0.5219 - val_loss: 1.6350 - val_acc: 0.1406\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0503 - acc: 0.5624 - val_loss: 1.5960 - val_acc: 0.1875\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1054 - acc: 0.5300 - val_loss: 1.6186 - val_acc: 0.1562\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0970 - acc: 0.5365 - val_loss: 1.5608 - val_acc: 0.1562\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1191 - acc: 0.4927 - val_loss: 1.6586 - val_acc: 0.1875\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0332 - acc: 0.5640 - val_loss: 1.6073 - val_acc: 0.1406\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0708 - acc: 0.5437 - val_loss: 1.6439 - val_acc: 0.1562\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0836 - acc: 0.5284 - val_loss: 1.5404 - val_acc: 0.2031\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0619 - acc: 0.5381 - val_loss: 1.6458 - val_acc: 0.1719\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0611 - acc: 0.5186 - val_loss: 1.6684 - val_acc: 0.1875\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0570 - acc: 0.5348 - val_loss: 1.6209 - val_acc: 0.1406\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0359 - acc: 0.5559 - val_loss: 1.6096 - val_acc: 0.1875\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0346 - acc: 0.5609 - val_loss: 1.6848 - val_acc: 0.1719\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.0268 - acc: 0.5640 - val_loss: 1.5881 - val_acc: 0.1875\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0816 - acc: 0.5381 - val_loss: 1.6865 - val_acc: 0.1562\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0700 - acc: 0.5344 - val_loss: 1.6870 - val_acc: 0.1719\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9948 - acc: 0.5391 - val_loss: 1.6514 - val_acc: 0.1562\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0391 - acc: 0.5673 - val_loss: 1.6740 - val_acc: 0.1719\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0909 - acc: 0.5170 - val_loss: 1.6424 - val_acc: 0.1875\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0797 - acc: 0.5429 - val_loss: 1.6703 - val_acc: 0.1562\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0743 - acc: 0.5138 - val_loss: 1.5291 - val_acc: 0.2031\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0803 - acc: 0.5381 - val_loss: 1.6426 - val_acc: 0.1562\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9936 - acc: 0.5484 - val_loss: 1.6617 - val_acc: 0.1562\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0550 - acc: 0.5267 - val_loss: 1.6636 - val_acc: 0.1719\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0685 - acc: 0.5365 - val_loss: 1.5887 - val_acc: 0.2031\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9883 - acc: 0.5624 - val_loss: 1.6117 - val_acc: 0.1719\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1531 - acc: 0.4878 - val_loss: 1.6266 - val_acc: 0.1875\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0995 - acc: 0.5203 - val_loss: 1.6522 - val_acc: 0.1719\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0489 - acc: 0.5446 - val_loss: 1.7737 - val_acc: 0.1406\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0904 - acc: 0.5446 - val_loss: 1.6098 - val_acc: 0.2031\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0518 - acc: 0.5381 - val_loss: 1.6859 - val_acc: 0.1719\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0174 - acc: 0.5705 - val_loss: 1.6550 - val_acc: 0.1719\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0391 - acc: 0.5429 - val_loss: 1.6205 - val_acc: 0.1875\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0933 - acc: 0.5284 - val_loss: 1.5405 - val_acc: 0.2031\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0389 - acc: 0.5575 - val_loss: 1.6493 - val_acc: 0.1875\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0094 - acc: 0.5527 - val_loss: 1.6131 - val_acc: 0.2031\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9986 - acc: 0.5624 - val_loss: 1.6091 - val_acc: 0.1719\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0756 - acc: 0.5640 - val_loss: 1.6399 - val_acc: 0.1875\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1130 - acc: 0.5138 - val_loss: 1.5553 - val_acc: 0.1719\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1005 - acc: 0.5348 - val_loss: 1.5369 - val_acc: 0.1719\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0357 - acc: 0.5397 - val_loss: 1.6180 - val_acc: 0.1875\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0406 - acc: 0.5397 - val_loss: 1.7086 - val_acc: 0.1719\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0512 - acc: 0.5462 - val_loss: 1.6493 - val_acc: 0.1719\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0549 - acc: 0.5332 - val_loss: 1.5723 - val_acc: 0.1875\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9793 - acc: 0.5673 - val_loss: 1.6036 - val_acc: 0.1875\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0582 - acc: 0.5284 - val_loss: 1.5343 - val_acc: 0.1875\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0770 - acc: 0.5235 - val_loss: 1.5577 - val_acc: 0.1719\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0156 - acc: 0.5559 - val_loss: 1.6089 - val_acc: 0.1719\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0100 - acc: 0.5592 - val_loss: 1.6668 - val_acc: 0.1719\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0126 - acc: 0.5624 - val_loss: 1.6949 - val_acc: 0.1719\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0311 - acc: 0.5484 - val_loss: 1.6748 - val_acc: 0.1250\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0558 - acc: 0.5422 - val_loss: 1.6865 - val_acc: 0.1406\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0061 - acc: 0.5575 - val_loss: 1.6944 - val_acc: 0.1562\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0602 - acc: 0.5608 - val_loss: 1.6898 - val_acc: 0.1562\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0844 - acc: 0.5170 - val_loss: 1.6182 - val_acc: 0.1406\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0791 - acc: 0.5267 - val_loss: 1.6029 - val_acc: 0.1562\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9651 - acc: 0.5673 - val_loss: 1.6205 - val_acc: 0.1562\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0392 - acc: 0.5413 - val_loss: 1.7092 - val_acc: 0.1406\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0051 - acc: 0.5592 - val_loss: 1.6293 - val_acc: 0.2031\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0427 - acc: 0.5559 - val_loss: 1.6623 - val_acc: 0.1875\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0283 - acc: 0.5478 - val_loss: 1.6822 - val_acc: 0.1562\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0139 - acc: 0.5786 - val_loss: 1.6242 - val_acc: 0.1875\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0614 - acc: 0.5462 - val_loss: 1.6480 - val_acc: 0.1875\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9640 - acc: 0.5802 - val_loss: 1.6338 - val_acc: 0.1875\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0020 - acc: 0.5624 - val_loss: 1.6617 - val_acc: 0.1562\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0033 - acc: 0.5916 - val_loss: 1.6665 - val_acc: 0.1562\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0152 - acc: 0.5494 - val_loss: 1.5897 - val_acc: 0.2031\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0622 - acc: 0.5365 - val_loss: 1.6017 - val_acc: 0.1875\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0185 - acc: 0.5656 - val_loss: 1.5536 - val_acc: 0.2031\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0068 - acc: 0.5624 - val_loss: 1.7106 - val_acc: 0.0938\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0141 - acc: 0.5397 - val_loss: 1.4586 - val_acc: 0.2344\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0312 - acc: 0.5478 - val_loss: 1.6515 - val_acc: 0.1562\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0697 - acc: 0.5543 - val_loss: 1.5832 - val_acc: 0.1875\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9834 - acc: 0.5737 - val_loss: 1.5584 - val_acc: 0.2188\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0934 - acc: 0.5073 - val_loss: 1.5913 - val_acc: 0.2031\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0592 - acc: 0.5429 - val_loss: 1.5948 - val_acc: 0.1562\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9676 - acc: 0.5828 - val_loss: 1.5550 - val_acc: 0.2031\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9741 - acc: 0.5689 - val_loss: 1.5854 - val_acc: 0.2031\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0742 - acc: 0.5462 - val_loss: 1.6160 - val_acc: 0.1875\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0460 - acc: 0.5689 - val_loss: 1.6109 - val_acc: 0.1562\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0336 - acc: 0.5625 - val_loss: 1.6356 - val_acc: 0.2031\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9965 - acc: 0.5527 - val_loss: 1.6322 - val_acc: 0.1875\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9825 - acc: 0.5705 - val_loss: 1.7077 - val_acc: 0.1719\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9898 - acc: 0.5689 - val_loss: 1.5959 - val_acc: 0.1875\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9912 - acc: 0.5737 - val_loss: 1.7015 - val_acc: 0.1719\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0644 - acc: 0.5203 - val_loss: 1.6745 - val_acc: 0.1562\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0738 - acc: 0.5365 - val_loss: 1.6591 - val_acc: 0.1719\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9878 - acc: 0.5883 - val_loss: 1.7243 - val_acc: 0.1719\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9790 - acc: 0.5737 - val_loss: 1.6275 - val_acc: 0.1562\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.0635 - acc: 0.5413 - val_loss: 1.6431 - val_acc: 0.1562\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9902 - acc: 0.5609 - val_loss: 1.5617 - val_acc: 0.1875\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0367 - acc: 0.5413 - val_loss: 1.7010 - val_acc: 0.1719\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0162 - acc: 0.5608 - val_loss: 1.5989 - val_acc: 0.1719\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0123 - acc: 0.5511 - val_loss: 1.6590 - val_acc: 0.1719\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0186 - acc: 0.5316 - val_loss: 1.6778 - val_acc: 0.1875\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9705 - acc: 0.5609 - val_loss: 1.7177 - val_acc: 0.1719\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9988 - acc: 0.5543 - val_loss: 1.6482 - val_acc: 0.1719\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9890 - acc: 0.5413 - val_loss: 1.6056 - val_acc: 0.1719\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0852 - acc: 0.5267 - val_loss: 1.6151 - val_acc: 0.1875\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0151 - acc: 0.5721 - val_loss: 1.6532 - val_acc: 0.1719\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0023 - acc: 0.5932 - val_loss: 1.5556 - val_acc: 0.1875\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0144 - acc: 0.5656 - val_loss: 1.6783 - val_acc: 0.1406\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9726 - acc: 0.5624 - val_loss: 1.6494 - val_acc: 0.1719\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9350 - acc: 0.5900 - val_loss: 1.6624 - val_acc: 0.1562\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9029 - acc: 0.5948 - val_loss: 1.5761 - val_acc: 0.1875\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9358 - acc: 0.5867 - val_loss: 1.5427 - val_acc: 0.1875\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9868 - acc: 0.5624 - val_loss: 1.6044 - val_acc: 0.1875\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9855 - acc: 0.5721 - val_loss: 1.6266 - val_acc: 0.1719\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9454 - acc: 0.5900 - val_loss: 1.5898 - val_acc: 0.2031\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0264 - acc: 0.5624 - val_loss: 1.5743 - val_acc: 0.1719\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9774 - acc: 0.5705 - val_loss: 1.5971 - val_acc: 0.2031\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0321 - acc: 0.5348 - val_loss: 1.5878 - val_acc: 0.1875\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0374 - acc: 0.5527 - val_loss: 1.5790 - val_acc: 0.1719\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0201 - acc: 0.5511 - val_loss: 1.6535 - val_acc: 0.1875\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9906 - acc: 0.5818 - val_loss: 1.5165 - val_acc: 0.2031\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0721 - acc: 0.5413 - val_loss: 1.5635 - val_acc: 0.1875\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0626 - acc: 0.5462 - val_loss: 1.6831 - val_acc: 0.1562\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0013 - acc: 0.5494 - val_loss: 1.6604 - val_acc: 0.1406\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0415 - acc: 0.5640 - val_loss: 1.5088 - val_acc: 0.2188\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0336 - acc: 0.5559 - val_loss: 1.4903 - val_acc: 0.2188\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0248 - acc: 0.5437 - val_loss: 1.5382 - val_acc: 0.2031\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0365 - acc: 0.5478 - val_loss: 1.6023 - val_acc: 0.1719\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9759 - acc: 0.5672 - val_loss: 1.6089 - val_acc: 0.1875\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9690 - acc: 0.5867 - val_loss: 1.6217 - val_acc: 0.1250\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9847 - acc: 0.5575 - val_loss: 1.5644 - val_acc: 0.2344\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0807 - acc: 0.5332 - val_loss: 1.6198 - val_acc: 0.1719\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0236 - acc: 0.5413 - val_loss: 1.5841 - val_acc: 0.2031\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0320 - acc: 0.5640 - val_loss: 1.6712 - val_acc: 0.1719\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9780 - acc: 0.5511 - val_loss: 1.5870 - val_acc: 0.2031\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9365 - acc: 0.5932 - val_loss: 1.6829 - val_acc: 0.1719\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9897 - acc: 0.5673 - val_loss: 1.5243 - val_acc: 0.2500\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0265 - acc: 0.5429 - val_loss: 1.5961 - val_acc: 0.1719\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9539 - acc: 0.5656 - val_loss: 1.6172 - val_acc: 0.1719\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0289 - acc: 0.5527 - val_loss: 1.6307 - val_acc: 0.1875\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9995 - acc: 0.5381 - val_loss: 1.6923 - val_acc: 0.1875\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9726 - acc: 0.5948 - val_loss: 1.6156 - val_acc: 0.1719\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0197 - acc: 0.5527 - val_loss: 1.5787 - val_acc: 0.1875\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9618 - acc: 0.5844 - val_loss: 1.6266 - val_acc: 0.1719\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0025 - acc: 0.5656 - val_loss: 1.6339 - val_acc: 0.1719\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9146 - acc: 0.5964 - val_loss: 1.5398 - val_acc: 0.2344\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0459 - acc: 0.5640 - val_loss: 1.6024 - val_acc: 0.1875\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0509 - acc: 0.5365 - val_loss: 1.5585 - val_acc: 0.2031\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9526 - acc: 0.5754 - val_loss: 1.6387 - val_acc: 0.2188\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9525 - acc: 0.5916 - val_loss: 1.5439 - val_acc: 0.2656\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9559 - acc: 0.5754 - val_loss: 1.6534 - val_acc: 0.1875\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9173 - acc: 0.5932 - val_loss: 1.6122 - val_acc: 0.2031\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9646 - acc: 0.5851 - val_loss: 1.6997 - val_acc: 0.1719\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9648 - acc: 0.5656 - val_loss: 1.6117 - val_acc: 0.1875\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0047 - acc: 0.5575 - val_loss: 1.6304 - val_acc: 0.1719\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0162 - acc: 0.5559 - val_loss: 1.6348 - val_acc: 0.1875\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0096 - acc: 0.5562 - val_loss: 1.6710 - val_acc: 0.2031\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0019 - acc: 0.5640 - val_loss: 1.6586 - val_acc: 0.1875\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9566 - acc: 0.5932 - val_loss: 1.5541 - val_acc: 0.1875\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9505 - acc: 0.5737 - val_loss: 1.6482 - val_acc: 0.1562\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9678 - acc: 0.5867 - val_loss: 1.6183 - val_acc: 0.2031\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0137 - acc: 0.5721 - val_loss: 1.6051 - val_acc: 0.1875\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9598 - acc: 0.5721 - val_loss: 1.7504 - val_acc: 0.1406\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9895 - acc: 0.5559 - val_loss: 1.6678 - val_acc: 0.2031\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0048 - acc: 0.5624 - val_loss: 1.6154 - val_acc: 0.1875\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9543 - acc: 0.5828 - val_loss: 1.5914 - val_acc: 0.2344\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9958 - acc: 0.5543 - val_loss: 1.5690 - val_acc: 0.2500\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0443 - acc: 0.5494 - val_loss: 1.6557 - val_acc: 0.1719\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9946 - acc: 0.5543 - val_loss: 1.6447 - val_acc: 0.1719\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9824 - acc: 0.5754 - val_loss: 1.6174 - val_acc: 0.1875\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9728 - acc: 0.5721 - val_loss: 1.6012 - val_acc: 0.1875\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9351 - acc: 0.5932 - val_loss: 1.5053 - val_acc: 0.2344\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9875 - acc: 0.5673 - val_loss: 1.6019 - val_acc: 0.2031\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9440 - acc: 0.5997 - val_loss: 1.5649 - val_acc: 0.2188\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9454 - acc: 0.5948 - val_loss: 1.6819 - val_acc: 0.2031\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9737 - acc: 0.5397 - val_loss: 1.6592 - val_acc: 0.1875\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9775 - acc: 0.5786 - val_loss: 1.6169 - val_acc: 0.1875\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9601 - acc: 0.5818 - val_loss: 1.5733 - val_acc: 0.2344\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0434 - acc: 0.5511 - val_loss: 1.6266 - val_acc: 0.2188\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0070 - acc: 0.5575 - val_loss: 1.5110 - val_acc: 0.2500\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9855 - acc: 0.5575 - val_loss: 1.5586 - val_acc: 0.2188\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9742 - acc: 0.5673 - val_loss: 1.6643 - val_acc: 0.1562\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9891 - acc: 0.5932 - val_loss: 1.6154 - val_acc: 0.1875\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9856 - acc: 0.5770 - val_loss: 1.6177 - val_acc: 0.1875\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9485 - acc: 0.5705 - val_loss: 1.6748 - val_acc: 0.1875\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9673 - acc: 0.5656 - val_loss: 1.5832 - val_acc: 0.2500\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9940 - acc: 0.5835 - val_loss: 1.6254 - val_acc: 0.2031\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9643 - acc: 0.5705 - val_loss: 1.6053 - val_acc: 0.1875\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9175 - acc: 0.5964 - val_loss: 1.5757 - val_acc: 0.2344\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9541 - acc: 0.5867 - val_loss: 1.5980 - val_acc: 0.2188\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0222 - acc: 0.5527 - val_loss: 1.6973 - val_acc: 0.1875\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9939 - acc: 0.5721 - val_loss: 1.5533 - val_acc: 0.2500\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9070 - acc: 0.5948 - val_loss: 1.7125 - val_acc: 0.1562\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9956 - acc: 0.5608 - val_loss: 1.5803 - val_acc: 0.2344\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9785 - acc: 0.5802 - val_loss: 1.5397 - val_acc: 0.2500\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9674 - acc: 0.5624 - val_loss: 1.4863 - val_acc: 0.2500\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9658 - acc: 0.5608 - val_loss: 1.5475 - val_acc: 0.2812\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9230 - acc: 0.5851 - val_loss: 1.6429 - val_acc: 0.2188\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9652 - acc: 0.5851 - val_loss: 1.6515 - val_acc: 0.1875\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9093 - acc: 0.6094 - val_loss: 1.6190 - val_acc: 0.2031\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9827 - acc: 0.5835 - val_loss: 1.5721 - val_acc: 0.2500\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9731 - acc: 0.5786 - val_loss: 1.6517 - val_acc: 0.2188\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9497 - acc: 0.5802 - val_loss: 1.6241 - val_acc: 0.2031\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9245 - acc: 0.5867 - val_loss: 1.7160 - val_acc: 0.1719\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9517 - acc: 0.5786 - val_loss: 1.6742 - val_acc: 0.2031\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9592 - acc: 0.5964 - val_loss: 1.7058 - val_acc: 0.1875\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9614 - acc: 0.5608 - val_loss: 1.6641 - val_acc: 0.2031\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9665 - acc: 0.5883 - val_loss: 1.6614 - val_acc: 0.1875\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9464 - acc: 0.5867 - val_loss: 1.6074 - val_acc: 0.2188\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9911 - acc: 0.5494 - val_loss: 1.5552 - val_acc: 0.2500\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9769 - acc: 0.5737 - val_loss: 1.5771 - val_acc: 0.2500\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9940 - acc: 0.5640 - val_loss: 1.6048 - val_acc: 0.1875\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9015 - acc: 0.5867 - val_loss: 1.5783 - val_acc: 0.2188\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9879 - acc: 0.5721 - val_loss: 1.6008 - val_acc: 0.2031\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.0014 - acc: 0.5170 - val_loss: 1.5715 - val_acc: 0.2344\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8915 - acc: 0.6078 - val_loss: 1.5172 - val_acc: 0.2344\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9985 - acc: 0.5494 - val_loss: 1.5240 - val_acc: 0.2500\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9753 - acc: 0.5754 - val_loss: 1.6817 - val_acc: 0.1875\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9589 - acc: 0.5754 - val_loss: 1.5894 - val_acc: 0.2188\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9815 - acc: 0.5705 - val_loss: 1.6330 - val_acc: 0.2344\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9606 - acc: 0.5656 - val_loss: 1.5543 - val_acc: 0.2344\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9241 - acc: 0.5932 - val_loss: 1.5521 - val_acc: 0.2500\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9657 - acc: 0.5818 - val_loss: 1.6823 - val_acc: 0.2188\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9556 - acc: 0.5527 - val_loss: 1.5996 - val_acc: 0.2344\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9592 - acc: 0.5575 - val_loss: 1.6403 - val_acc: 0.2188\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9384 - acc: 0.5900 - val_loss: 1.6332 - val_acc: 0.2188\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9352 - acc: 0.5786 - val_loss: 1.5950 - val_acc: 0.2500\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9654 - acc: 0.5786 - val_loss: 1.4934 - val_acc: 0.2344\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9702 - acc: 0.5981 - val_loss: 1.6480 - val_acc: 0.2031\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9188 - acc: 0.5981 - val_loss: 1.6332 - val_acc: 0.2031\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8664 - acc: 0.6402 - val_loss: 1.6064 - val_acc: 0.2031\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9408 - acc: 0.5922 - val_loss: 1.5680 - val_acc: 0.2500\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9466 - acc: 0.5997 - val_loss: 1.5768 - val_acc: 0.2500\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9143 - acc: 0.6305 - val_loss: 1.4348 - val_acc: 0.2656\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9858 - acc: 0.5948 - val_loss: 1.6055 - val_acc: 0.2500\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9213 - acc: 0.5802 - val_loss: 1.5511 - val_acc: 0.2656\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9617 - acc: 0.5640 - val_loss: 1.5469 - val_acc: 0.2344\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9358 - acc: 0.5900 - val_loss: 1.5017 - val_acc: 0.2500\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8566 - acc: 0.6434 - val_loss: 1.5702 - val_acc: 0.2500\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9587 - acc: 0.5511 - val_loss: 1.5975 - val_acc: 0.2812\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9071 - acc: 0.5802 - val_loss: 1.6142 - val_acc: 0.2031\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9496 - acc: 0.5900 - val_loss: 1.5500 - val_acc: 0.2656\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9288 - acc: 0.5922 - val_loss: 1.5764 - val_acc: 0.2188\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9920 - acc: 0.5640 - val_loss: 1.5665 - val_acc: 0.2344\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9521 - acc: 0.5656 - val_loss: 1.6963 - val_acc: 0.2031\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9753 - acc: 0.5656 - val_loss: 1.5829 - val_acc: 0.2344\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9865 - acc: 0.5754 - val_loss: 1.5964 - val_acc: 0.2344\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9865 - acc: 0.5737 - val_loss: 1.5334 - val_acc: 0.2500\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8838 - acc: 0.6013 - val_loss: 1.6062 - val_acc: 0.2031\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9639 - acc: 0.5559 - val_loss: 1.5731 - val_acc: 0.2344\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9477 - acc: 0.5754 - val_loss: 1.5363 - val_acc: 0.2344\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9640 - acc: 0.5705 - val_loss: 1.5933 - val_acc: 0.2344\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9607 - acc: 0.5835 - val_loss: 1.6781 - val_acc: 0.1875\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9324 - acc: 0.6094 - val_loss: 1.5725 - val_acc: 0.2344\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9197 - acc: 0.6094 - val_loss: 1.6537 - val_acc: 0.2188\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9453 - acc: 0.5964 - val_loss: 1.5670 - val_acc: 0.2031\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9691 - acc: 0.5964 - val_loss: 1.6029 - val_acc: 0.2031\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9230 - acc: 0.5802 - val_loss: 1.5331 - val_acc: 0.2656\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8570 - acc: 0.6240 - val_loss: 1.6441 - val_acc: 0.1719\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9354 - acc: 0.5802 - val_loss: 1.6197 - val_acc: 0.2188\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9100 - acc: 0.5984 - val_loss: 1.5591 - val_acc: 0.2656\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9544 - acc: 0.5900 - val_loss: 1.5856 - val_acc: 0.2031\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9113 - acc: 0.6094 - val_loss: 1.6135 - val_acc: 0.1875\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9152 - acc: 0.5900 - val_loss: 1.5365 - val_acc: 0.2188\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9588 - acc: 0.5766 - val_loss: 1.5226 - val_acc: 0.2500\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9543 - acc: 0.5981 - val_loss: 1.5567 - val_acc: 0.2500\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9321 - acc: 0.5786 - val_loss: 1.5660 - val_acc: 0.2031\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9158 - acc: 0.6029 - val_loss: 1.5408 - val_acc: 0.2500\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9135 - acc: 0.5900 - val_loss: 1.4948 - val_acc: 0.2812\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9003 - acc: 0.5932 - val_loss: 1.5844 - val_acc: 0.2344\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9332 - acc: 0.6029 - val_loss: 1.6282 - val_acc: 0.2344\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9436 - acc: 0.5900 - val_loss: 1.5818 - val_acc: 0.2344\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9015 - acc: 0.5770 - val_loss: 1.5748 - val_acc: 0.2656\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9612 - acc: 0.5916 - val_loss: 1.5076 - val_acc: 0.2344\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9308 - acc: 0.5932 - val_loss: 1.5449 - val_acc: 0.2344\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9560 - acc: 0.5781 - val_loss: 1.5514 - val_acc: 0.2656\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9029 - acc: 0.6126 - val_loss: 1.5778 - val_acc: 0.2500\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9337 - acc: 0.6029 - val_loss: 1.5451 - val_acc: 0.2344\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9040 - acc: 0.5922 - val_loss: 1.5321 - val_acc: 0.2656\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9632 - acc: 0.5543 - val_loss: 1.6125 - val_acc: 0.2344\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9442 - acc: 0.5900 - val_loss: 1.5958 - val_acc: 0.2188\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9297 - acc: 0.5916 - val_loss: 1.5795 - val_acc: 0.2500\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9676 - acc: 0.5673 - val_loss: 1.6647 - val_acc: 0.2031\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9169 - acc: 0.5786 - val_loss: 1.5044 - val_acc: 0.2656\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8951 - acc: 0.6062 - val_loss: 1.6052 - val_acc: 0.2344\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9529 - acc: 0.5786 - val_loss: 1.5997 - val_acc: 0.2031\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9101 - acc: 0.6045 - val_loss: 1.5148 - val_acc: 0.2656\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9328 - acc: 0.5916 - val_loss: 1.5870 - val_acc: 0.2344\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9469 - acc: 0.5754 - val_loss: 1.5121 - val_acc: 0.2656\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9138 - acc: 0.5997 - val_loss: 1.5393 - val_acc: 0.2500\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9171 - acc: 0.5997 - val_loss: 1.5606 - val_acc: 0.2031\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9485 - acc: 0.5867 - val_loss: 1.5883 - val_acc: 0.2500\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9474 - acc: 0.5721 - val_loss: 1.5298 - val_acc: 0.2500\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8612 - acc: 0.6078 - val_loss: 1.5518 - val_acc: 0.2344\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9320 - acc: 0.6013 - val_loss: 1.5753 - val_acc: 0.2500\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9029 - acc: 0.6062 - val_loss: 1.4570 - val_acc: 0.2344\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9447 - acc: 0.5883 - val_loss: 1.5742 - val_acc: 0.2031\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9320 - acc: 0.5900 - val_loss: 1.5171 - val_acc: 0.2500\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8622 - acc: 0.6156 - val_loss: 1.5783 - val_acc: 0.2188\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9514 - acc: 0.5818 - val_loss: 1.4852 - val_acc: 0.2656\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9324 - acc: 0.5883 - val_loss: 1.5831 - val_acc: 0.2344\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9110 - acc: 0.6029 - val_loss: 1.6027 - val_acc: 0.2031\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9851 - acc: 0.5737 - val_loss: 1.5671 - val_acc: 0.2500\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9102 - acc: 0.6110 - val_loss: 1.5549 - val_acc: 0.2500\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9393 - acc: 0.5851 - val_loss: 1.6231 - val_acc: 0.2344\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8859 - acc: 0.6029 - val_loss: 1.6662 - val_acc: 0.1875\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9317 - acc: 0.5851 - val_loss: 1.6343 - val_acc: 0.2344\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8923 - acc: 0.6159 - val_loss: 1.5385 - val_acc: 0.2500\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9099 - acc: 0.6013 - val_loss: 1.5677 - val_acc: 0.1875\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9386 - acc: 0.5737 - val_loss: 1.6847 - val_acc: 0.1875\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8894 - acc: 0.5948 - val_loss: 1.7089 - val_acc: 0.2188\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8845 - acc: 0.6045 - val_loss: 1.5431 - val_acc: 0.2500\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9420 - acc: 0.5867 - val_loss: 1.6035 - val_acc: 0.2188\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8604 - acc: 0.6370 - val_loss: 1.5798 - val_acc: 0.2500\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8701 - acc: 0.6045 - val_loss: 1.5887 - val_acc: 0.2656\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9752 - acc: 0.5737 - val_loss: 1.5474 - val_acc: 0.2344\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8649 - acc: 0.6110 - val_loss: 1.6707 - val_acc: 0.2188\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8925 - acc: 0.6256 - val_loss: 1.6941 - val_acc: 0.1875\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9220 - acc: 0.6045 - val_loss: 1.6472 - val_acc: 0.2344\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9501 - acc: 0.5851 - val_loss: 1.6037 - val_acc: 0.2188\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8845 - acc: 0.6143 - val_loss: 1.5611 - val_acc: 0.2031\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9360 - acc: 0.5754 - val_loss: 1.5796 - val_acc: 0.2344\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9199 - acc: 0.5883 - val_loss: 1.5271 - val_acc: 0.2812\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9108 - acc: 0.5835 - val_loss: 1.6425 - val_acc: 0.2500\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8915 - acc: 0.6224 - val_loss: 1.4894 - val_acc: 0.2656\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9669 - acc: 0.5734 - val_loss: 1.6211 - val_acc: 0.2500\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8804 - acc: 0.6191 - val_loss: 1.6437 - val_acc: 0.2188\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8569 - acc: 0.6062 - val_loss: 1.5849 - val_acc: 0.2500\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8209 - acc: 0.6434 - val_loss: 1.6888 - val_acc: 0.2031\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9231 - acc: 0.6045 - val_loss: 1.5276 - val_acc: 0.2812\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8706 - acc: 0.6062 - val_loss: 1.6205 - val_acc: 0.2031\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9165 - acc: 0.6078 - val_loss: 1.6705 - val_acc: 0.1875\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8640 - acc: 0.6159 - val_loss: 1.6358 - val_acc: 0.2344\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9077 - acc: 0.6013 - val_loss: 1.5669 - val_acc: 0.2188\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9166 - acc: 0.5916 - val_loss: 1.6806 - val_acc: 0.2344\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8849 - acc: 0.6094 - val_loss: 1.5497 - val_acc: 0.2656\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9022 - acc: 0.5770 - val_loss: 1.5649 - val_acc: 0.2344\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9222 - acc: 0.6078 - val_loss: 1.5869 - val_acc: 0.2656\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9161 - acc: 0.5948 - val_loss: 1.6792 - val_acc: 0.2188\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9027 - acc: 0.6175 - val_loss: 1.5712 - val_acc: 0.2812\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9249 - acc: 0.5883 - val_loss: 1.6036 - val_acc: 0.2500\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8886 - acc: 0.6224 - val_loss: 1.5640 - val_acc: 0.2500\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9072 - acc: 0.6078 - val_loss: 1.5089 - val_acc: 0.2656\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 28s 3s/step - loss: 0.9452 - acc: 0.6191 - val_loss: 1.5836 - val_acc: 0.2344\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9357 - acc: 0.6029 - val_loss: 1.5906 - val_acc: 0.2500\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8949 - acc: 0.6029 - val_loss: 1.6210 - val_acc: 0.2188\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9029 - acc: 0.5932 - val_loss: 1.5493 - val_acc: 0.2656\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8718 - acc: 0.6029 - val_loss: 1.6204 - val_acc: 0.2344\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8917 - acc: 0.6094 - val_loss: 1.6208 - val_acc: 0.2344\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9251 - acc: 0.6175 - val_loss: 1.5296 - val_acc: 0.2969\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9125 - acc: 0.6159 - val_loss: 1.6049 - val_acc: 0.2344\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9364 - acc: 0.5737 - val_loss: 1.4957 - val_acc: 0.2656\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8825 - acc: 0.6062 - val_loss: 1.6841 - val_acc: 0.1875\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9365 - acc: 0.5867 - val_loss: 1.5731 - val_acc: 0.2500\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8918 - acc: 0.5948 - val_loss: 1.5545 - val_acc: 0.2344\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8776 - acc: 0.6110 - val_loss: 1.6213 - val_acc: 0.2031\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9268 - acc: 0.5964 - val_loss: 1.6412 - val_acc: 0.2344\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8864 - acc: 0.6045 - val_loss: 1.5667 - val_acc: 0.2656\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9242 - acc: 0.6013 - val_loss: 1.4759 - val_acc: 0.2969\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8857 - acc: 0.6321 - val_loss: 1.5713 - val_acc: 0.2500\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9170 - acc: 0.6110 - val_loss: 1.6083 - val_acc: 0.2188\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8812 - acc: 0.6110 - val_loss: 1.5626 - val_acc: 0.2500\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8826 - acc: 0.6078 - val_loss: 1.4969 - val_acc: 0.2500\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8526 - acc: 0.6175 - val_loss: 1.5270 - val_acc: 0.2812\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8823 - acc: 0.6126 - val_loss: 1.5965 - val_acc: 0.2500\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9438 - acc: 0.6045 - val_loss: 1.5598 - val_acc: 0.2656\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9216 - acc: 0.6078 - val_loss: 1.5343 - val_acc: 0.2812\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8958 - acc: 0.6110 - val_loss: 1.6022 - val_acc: 0.2344\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8821 - acc: 0.6000 - val_loss: 1.5734 - val_acc: 0.2812\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9223 - acc: 0.5922 - val_loss: 1.6144 - val_acc: 0.2344\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8651 - acc: 0.6191 - val_loss: 1.5669 - val_acc: 0.2344\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9474 - acc: 0.5922 - val_loss: 1.6108 - val_acc: 0.2656\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9636 - acc: 0.5932 - val_loss: 1.5282 - val_acc: 0.2656\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9414 - acc: 0.5964 - val_loss: 1.5398 - val_acc: 0.2656\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9031 - acc: 0.5835 - val_loss: 1.5594 - val_acc: 0.2656\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8743 - acc: 0.6078 - val_loss: 1.5743 - val_acc: 0.2656\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8459 - acc: 0.6375 - val_loss: 1.6362 - val_acc: 0.2188\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8310 - acc: 0.6288 - val_loss: 1.6075 - val_acc: 0.2500\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9130 - acc: 0.5964 - val_loss: 1.5366 - val_acc: 0.2812\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9037 - acc: 0.5981 - val_loss: 1.6062 - val_acc: 0.2500\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8980 - acc: 0.5900 - val_loss: 1.5653 - val_acc: 0.2656\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8521 - acc: 0.6483 - val_loss: 1.5164 - val_acc: 0.2656\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9282 - acc: 0.5802 - val_loss: 1.6173 - val_acc: 0.2500\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8808 - acc: 0.6191 - val_loss: 1.5805 - val_acc: 0.2344\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8993 - acc: 0.6266 - val_loss: 1.6008 - val_acc: 0.2344\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8465 - acc: 0.6207 - val_loss: 1.5870 - val_acc: 0.2500\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8905 - acc: 0.6045 - val_loss: 1.5031 - val_acc: 0.2500\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8563 - acc: 0.6172 - val_loss: 1.5488 - val_acc: 0.2500\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8921 - acc: 0.6047 - val_loss: 1.5565 - val_acc: 0.2500\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9070 - acc: 0.6029 - val_loss: 1.6030 - val_acc: 0.2188\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8231 - acc: 0.6321 - val_loss: 1.6686 - val_acc: 0.2031\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9214 - acc: 0.6337 - val_loss: 1.5811 - val_acc: 0.2344\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9490 - acc: 0.5932 - val_loss: 1.6292 - val_acc: 0.2656\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8930 - acc: 0.6062 - val_loss: 1.5949 - val_acc: 0.2344\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8798 - acc: 0.6013 - val_loss: 1.5869 - val_acc: 0.2812\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8702 - acc: 0.6013 - val_loss: 1.5974 - val_acc: 0.2500\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8969 - acc: 0.5948 - val_loss: 1.5308 - val_acc: 0.2500\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8973 - acc: 0.6029 - val_loss: 1.5797 - val_acc: 0.2344\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9240 - acc: 0.5851 - val_loss: 1.5799 - val_acc: 0.2344\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8769 - acc: 0.6078 - val_loss: 1.5595 - val_acc: 0.2344\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8467 - acc: 0.6191 - val_loss: 1.5431 - val_acc: 0.2656\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8833 - acc: 0.6159 - val_loss: 1.5839 - val_acc: 0.2656\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8848 - acc: 0.5964 - val_loss: 1.5622 - val_acc: 0.2812\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8834 - acc: 0.6143 - val_loss: 1.5557 - val_acc: 0.2500\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9158 - acc: 0.6045 - val_loss: 1.6370 - val_acc: 0.2188\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9228 - acc: 0.5828 - val_loss: 1.5515 - val_acc: 0.2344\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8782 - acc: 0.5948 - val_loss: 1.5985 - val_acc: 0.2344\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8954 - acc: 0.6110 - val_loss: 1.6106 - val_acc: 0.2500\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8563 - acc: 0.6156 - val_loss: 1.6197 - val_acc: 0.1875\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8618 - acc: 0.6256 - val_loss: 1.5371 - val_acc: 0.2812\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9031 - acc: 0.6109 - val_loss: 1.6082 - val_acc: 0.2344\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8692 - acc: 0.6062 - val_loss: 1.6064 - val_acc: 0.2344\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9036 - acc: 0.6159 - val_loss: 1.5951 - val_acc: 0.2344\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8539 - acc: 0.6353 - val_loss: 1.5476 - val_acc: 0.2969\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9125 - acc: 0.6029 - val_loss: 1.5239 - val_acc: 0.2812\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8534 - acc: 0.6109 - val_loss: 1.6288 - val_acc: 0.2344\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8679 - acc: 0.6353 - val_loss: 1.6704 - val_acc: 0.2188\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8798 - acc: 0.6062 - val_loss: 1.6119 - val_acc: 0.2188\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9426 - acc: 0.5802 - val_loss: 1.5873 - val_acc: 0.2500\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8832 - acc: 0.6191 - val_loss: 1.5972 - val_acc: 0.2188\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9022 - acc: 0.6078 - val_loss: 1.5031 - val_acc: 0.2656\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8671 - acc: 0.6337 - val_loss: 1.4785 - val_acc: 0.2812\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8884 - acc: 0.6000 - val_loss: 1.5555 - val_acc: 0.2500\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9275 - acc: 0.5981 - val_loss: 1.5735 - val_acc: 0.2500\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8946 - acc: 0.5900 - val_loss: 1.5389 - val_acc: 0.2812\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8473 - acc: 0.6451 - val_loss: 1.4871 - val_acc: 0.2656\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8591 - acc: 0.6207 - val_loss: 1.6044 - val_acc: 0.2344\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9039 - acc: 0.6110 - val_loss: 1.5137 - val_acc: 0.2344\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8662 - acc: 0.6094 - val_loss: 1.6003 - val_acc: 0.2344\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8715 - acc: 0.6191 - val_loss: 1.5380 - val_acc: 0.2500\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8939 - acc: 0.6143 - val_loss: 1.6164 - val_acc: 0.2188\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8576 - acc: 0.6321 - val_loss: 1.5705 - val_acc: 0.2188\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8329 - acc: 0.6402 - val_loss: 1.6263 - val_acc: 0.2188\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9259 - acc: 0.5867 - val_loss: 1.5672 - val_acc: 0.2812\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8481 - acc: 0.6337 - val_loss: 1.5749 - val_acc: 0.2500\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8585 - acc: 0.6094 - val_loss: 1.5076 - val_acc: 0.2812\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9182 - acc: 0.5851 - val_loss: 1.5767 - val_acc: 0.2656\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9256 - acc: 0.5867 - val_loss: 1.5282 - val_acc: 0.2656\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9040 - acc: 0.5953 - val_loss: 1.5917 - val_acc: 0.2500\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8632 - acc: 0.6207 - val_loss: 1.5875 - val_acc: 0.2344\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8546 - acc: 0.6272 - val_loss: 1.5524 - val_acc: 0.2656\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8697 - acc: 0.6256 - val_loss: 1.5380 - val_acc: 0.2500\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9201 - acc: 0.6141 - val_loss: 1.5282 - val_acc: 0.2500\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8142 - acc: 0.6288 - val_loss: 1.5834 - val_acc: 0.2500\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8246 - acc: 0.6353 - val_loss: 1.5715 - val_acc: 0.2656\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8816 - acc: 0.6045 - val_loss: 1.5763 - val_acc: 0.2656\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8624 - acc: 0.6224 - val_loss: 1.4947 - val_acc: 0.2969\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8796 - acc: 0.6094 - val_loss: 1.5616 - val_acc: 0.2812\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8541 - acc: 0.6256 - val_loss: 1.5875 - val_acc: 0.2656\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8789 - acc: 0.6110 - val_loss: 1.6078 - val_acc: 0.2344\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8871 - acc: 0.6062 - val_loss: 1.5077 - val_acc: 0.2812\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9077 - acc: 0.5916 - val_loss: 1.5716 - val_acc: 0.2344\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8909 - acc: 0.5948 - val_loss: 1.5031 - val_acc: 0.2656\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8947 - acc: 0.6207 - val_loss: 1.4570 - val_acc: 0.2656\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9080 - acc: 0.6062 - val_loss: 1.5917 - val_acc: 0.1875\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8363 - acc: 0.6288 - val_loss: 1.4900 - val_acc: 0.2656\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8141 - acc: 0.6564 - val_loss: 1.6130 - val_acc: 0.2500\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8500 - acc: 0.6062 - val_loss: 1.5491 - val_acc: 0.2656\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8789 - acc: 0.6078 - val_loss: 1.4471 - val_acc: 0.2812\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8016 - acc: 0.6532 - val_loss: 1.5070 - val_acc: 0.2656\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8720 - acc: 0.6297 - val_loss: 1.5534 - val_acc: 0.2500\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8792 - acc: 0.6143 - val_loss: 1.6227 - val_acc: 0.2031\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8786 - acc: 0.6175 - val_loss: 1.5398 - val_acc: 0.2812\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8809 - acc: 0.6094 - val_loss: 1.5758 - val_acc: 0.2500\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8243 - acc: 0.6418 - val_loss: 1.5674 - val_acc: 0.2344\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8550 - acc: 0.6159 - val_loss: 1.4997 - val_acc: 0.2812\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9909 - acc: 0.5672 - val_loss: 1.5759 - val_acc: 0.2188\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8624 - acc: 0.6256 - val_loss: 1.5145 - val_acc: 0.2344\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8242 - acc: 0.6207 - val_loss: 1.4236 - val_acc: 0.3125\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8796 - acc: 0.6159 - val_loss: 1.5910 - val_acc: 0.2188\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8712 - acc: 0.6094 - val_loss: 1.4592 - val_acc: 0.2656\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8352 - acc: 0.6272 - val_loss: 1.5475 - val_acc: 0.2812\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8426 - acc: 0.6402 - val_loss: 1.6631 - val_acc: 0.2344\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8672 - acc: 0.6272 - val_loss: 1.5249 - val_acc: 0.2500\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8549 - acc: 0.6207 - val_loss: 1.5397 - val_acc: 0.2500\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9122 - acc: 0.5981 - val_loss: 1.5918 - val_acc: 0.2344\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8701 - acc: 0.6337 - val_loss: 1.5550 - val_acc: 0.2500\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8153 - acc: 0.6126 - val_loss: 1.5079 - val_acc: 0.2344\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8857 - acc: 0.6078 - val_loss: 1.5331 - val_acc: 0.2500\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8392 - acc: 0.6256 - val_loss: 1.5199 - val_acc: 0.2812\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8092 - acc: 0.6288 - val_loss: 1.4658 - val_acc: 0.2812\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9146 - acc: 0.5948 - val_loss: 1.5605 - val_acc: 0.2344\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8894 - acc: 0.6207 - val_loss: 1.5470 - val_acc: 0.2500\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8832 - acc: 0.6159 - val_loss: 1.5619 - val_acc: 0.2812\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8670 - acc: 0.6143 - val_loss: 1.6423 - val_acc: 0.1875\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8897 - acc: 0.6207 - val_loss: 1.5890 - val_acc: 0.2500\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8862 - acc: 0.6219 - val_loss: 1.5692 - val_acc: 0.2656\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8976 - acc: 0.5883 - val_loss: 1.6080 - val_acc: 0.2188\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8212 - acc: 0.6240 - val_loss: 1.6046 - val_acc: 0.2188\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8437 - acc: 0.6240 - val_loss: 1.5083 - val_acc: 0.2812\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8685 - acc: 0.6256 - val_loss: 1.6041 - val_acc: 0.2188\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8865 - acc: 0.5981 - val_loss: 1.6000 - val_acc: 0.2500\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8336 - acc: 0.6126 - val_loss: 1.5703 - val_acc: 0.2188\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8860 - acc: 0.6172 - val_loss: 1.5557 - val_acc: 0.2500\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8487 - acc: 0.6175 - val_loss: 1.4987 - val_acc: 0.2812\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8551 - acc: 0.6045 - val_loss: 1.4779 - val_acc: 0.2656\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8758 - acc: 0.6159 - val_loss: 1.6579 - val_acc: 0.2031\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8630 - acc: 0.6224 - val_loss: 1.5627 - val_acc: 0.2500\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8137 - acc: 0.6499 - val_loss: 1.5963 - val_acc: 0.2031\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8895 - acc: 0.6045 - val_loss: 1.5492 - val_acc: 0.2344\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8362 - acc: 0.6207 - val_loss: 1.5471 - val_acc: 0.2500\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8724 - acc: 0.6402 - val_loss: 1.6188 - val_acc: 0.2031\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8655 - acc: 0.6110 - val_loss: 1.5583 - val_acc: 0.2500\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8577 - acc: 0.6224 - val_loss: 1.5786 - val_acc: 0.2344\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8582 - acc: 0.6143 - val_loss: 1.5806 - val_acc: 0.2656\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8811 - acc: 0.6062 - val_loss: 1.6395 - val_acc: 0.2344\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8457 - acc: 0.6467 - val_loss: 1.5377 - val_acc: 0.2500\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8714 - acc: 0.6143 - val_loss: 1.5674 - val_acc: 0.2656\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8180 - acc: 0.6499 - val_loss: 1.5575 - val_acc: 0.2500\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8432 - acc: 0.6126 - val_loss: 1.6064 - val_acc: 0.2031\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8834 - acc: 0.6224 - val_loss: 1.5129 - val_acc: 0.2656\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8367 - acc: 0.6328 - val_loss: 1.5978 - val_acc: 0.2031\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9095 - acc: 0.6094 - val_loss: 1.5775 - val_acc: 0.2344\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8155 - acc: 0.6434 - val_loss: 1.4522 - val_acc: 0.2969\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8683 - acc: 0.6045 - val_loss: 1.5793 - val_acc: 0.2500\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.9039 - acc: 0.6191 - val_loss: 1.6317 - val_acc: 0.2344\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8336 - acc: 0.6370 - val_loss: 1.5990 - val_acc: 0.2812\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8649 - acc: 0.6187 - val_loss: 1.4941 - val_acc: 0.2656\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8249 - acc: 0.6256 - val_loss: 1.5607 - val_acc: 0.2344\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8427 - acc: 0.6353 - val_loss: 1.5030 - val_acc: 0.2656\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8521 - acc: 0.6219 - val_loss: 1.5769 - val_acc: 0.2031\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8049 - acc: 0.6288 - val_loss: 1.6186 - val_acc: 0.2656\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8007 - acc: 0.6515 - val_loss: 1.7252 - val_acc: 0.1875\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8342 - acc: 0.6203 - val_loss: 1.6167 - val_acc: 0.2188\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8865 - acc: 0.5948 - val_loss: 1.6120 - val_acc: 0.2500\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8297 - acc: 0.6548 - val_loss: 1.5120 - val_acc: 0.2500\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8535 - acc: 0.6337 - val_loss: 1.6182 - val_acc: 0.2344\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8644 - acc: 0.6272 - val_loss: 1.5502 - val_acc: 0.2500\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8521 - acc: 0.6240 - val_loss: 1.6934 - val_acc: 0.1875\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7884 - acc: 0.6515 - val_loss: 1.6436 - val_acc: 0.2031\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8383 - acc: 0.6328 - val_loss: 1.4995 - val_acc: 0.2656\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8733 - acc: 0.6240 - val_loss: 1.5823 - val_acc: 0.2500\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8589 - acc: 0.6386 - val_loss: 1.5578 - val_acc: 0.2812\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8047 - acc: 0.6629 - val_loss: 1.6086 - val_acc: 0.2344\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8253 - acc: 0.6418 - val_loss: 1.5867 - val_acc: 0.2344\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8342 - acc: 0.6207 - val_loss: 1.6066 - val_acc: 0.2500\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8223 - acc: 0.6337 - val_loss: 1.5710 - val_acc: 0.2656\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8381 - acc: 0.6321 - val_loss: 1.5127 - val_acc: 0.2500\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8688 - acc: 0.6045 - val_loss: 1.5628 - val_acc: 0.2500\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8609 - acc: 0.6078 - val_loss: 1.5298 - val_acc: 0.2344\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8810 - acc: 0.6078 - val_loss: 1.5407 - val_acc: 0.2656\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8423 - acc: 0.6434 - val_loss: 1.6125 - val_acc: 0.2344\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7749 - acc: 0.6807 - val_loss: 1.5845 - val_acc: 0.2500\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8598 - acc: 0.6207 - val_loss: 1.5343 - val_acc: 0.2188\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8732 - acc: 0.6094 - val_loss: 1.5461 - val_acc: 0.2656\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8538 - acc: 0.6159 - val_loss: 1.6519 - val_acc: 0.2031\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8736 - acc: 0.6191 - val_loss: 1.6794 - val_acc: 0.2031\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8249 - acc: 0.6564 - val_loss: 1.5624 - val_acc: 0.2344\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8262 - acc: 0.6564 - val_loss: 1.5398 - val_acc: 0.2656\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8281 - acc: 0.6386 - val_loss: 1.5181 - val_acc: 0.2344\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8440 - acc: 0.6143 - val_loss: 1.6260 - val_acc: 0.2344\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8277 - acc: 0.6272 - val_loss: 1.7137 - val_acc: 0.1875\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8830 - acc: 0.6143 - val_loss: 1.5835 - val_acc: 0.2344\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8747 - acc: 0.6159 - val_loss: 1.5212 - val_acc: 0.2656\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8421 - acc: 0.6191 - val_loss: 1.5990 - val_acc: 0.2031\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 0.8424 - acc: 0.6013 - val_loss: 1.5325 - val_acc: 0.3125\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 0.8706 - acc: 0.6172 - val_loss: 1.5301 - val_acc: 0.2656\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7861 - acc: 0.6499 - val_loss: 1.5268 - val_acc: 0.2656\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8657 - acc: 0.6402 - val_loss: 1.5564 - val_acc: 0.2344\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8904 - acc: 0.6224 - val_loss: 1.5041 - val_acc: 0.2812\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8856 - acc: 0.6045 - val_loss: 1.6394 - val_acc: 0.2344\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8572 - acc: 0.6272 - val_loss: 1.5966 - val_acc: 0.2500\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8749 - acc: 0.6288 - val_loss: 1.5641 - val_acc: 0.2500\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8093 - acc: 0.6250 - val_loss: 1.5083 - val_acc: 0.2656\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8628 - acc: 0.6094 - val_loss: 1.5548 - val_acc: 0.2344\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8777 - acc: 0.5900 - val_loss: 1.5283 - val_acc: 0.2188\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8415 - acc: 0.6240 - val_loss: 1.5620 - val_acc: 0.2344\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8587 - acc: 0.6386 - val_loss: 1.5304 - val_acc: 0.2812\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8348 - acc: 0.6532 - val_loss: 1.4816 - val_acc: 0.2969\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8187 - acc: 0.6353 - val_loss: 1.5928 - val_acc: 0.2500\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7890 - acc: 0.6469 - val_loss: 1.5810 - val_acc: 0.2500\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8481 - acc: 0.6483 - val_loss: 1.4999 - val_acc: 0.2812\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8673 - acc: 0.6013 - val_loss: 1.5050 - val_acc: 0.2656\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8406 - acc: 0.6159 - val_loss: 1.5405 - val_acc: 0.2500\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8394 - acc: 0.6305 - val_loss: 1.5895 - val_acc: 0.2344\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8425 - acc: 0.6240 - val_loss: 1.6228 - val_acc: 0.2188\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7968 - acc: 0.6515 - val_loss: 1.6189 - val_acc: 0.2344\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8822 - acc: 0.6110 - val_loss: 1.6422 - val_acc: 0.2344\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8148 - acc: 0.6609 - val_loss: 1.4807 - val_acc: 0.2500\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8342 - acc: 0.6677 - val_loss: 1.5635 - val_acc: 0.2344\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8252 - acc: 0.6402 - val_loss: 1.5752 - val_acc: 0.2656\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8847 - acc: 0.6062 - val_loss: 1.5634 - val_acc: 0.2656\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8231 - acc: 0.6434 - val_loss: 1.6246 - val_acc: 0.2188\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9028 - acc: 0.6078 - val_loss: 1.5451 - val_acc: 0.2344\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7973 - acc: 0.6564 - val_loss: 1.5449 - val_acc: 0.2500\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7993 - acc: 0.6483 - val_loss: 1.6055 - val_acc: 0.2500\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8282 - acc: 0.6297 - val_loss: 1.5324 - val_acc: 0.2656\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8369 - acc: 0.6483 - val_loss: 1.5971 - val_acc: 0.1875\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8408 - acc: 0.6266 - val_loss: 1.5116 - val_acc: 0.2188\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8111 - acc: 0.6613 - val_loss: 1.5096 - val_acc: 0.2500\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8550 - acc: 0.6143 - val_loss: 1.5691 - val_acc: 0.2344\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8724 - acc: 0.6256 - val_loss: 1.6354 - val_acc: 0.2031\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8427 - acc: 0.6175 - val_loss: 1.4646 - val_acc: 0.2812\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8154 - acc: 0.6224 - val_loss: 1.6442 - val_acc: 0.1875\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8480 - acc: 0.6207 - val_loss: 1.5293 - val_acc: 0.2344\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8088 - acc: 0.6175 - val_loss: 1.5942 - val_acc: 0.2344\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8396 - acc: 0.6313 - val_loss: 1.5786 - val_acc: 0.2500\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8660 - acc: 0.6013 - val_loss: 1.5226 - val_acc: 0.2656\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8019 - acc: 0.6386 - val_loss: 1.5105 - val_acc: 0.2500\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8254 - acc: 0.6353 - val_loss: 1.5313 - val_acc: 0.2188\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8752 - acc: 0.6143 - val_loss: 1.5717 - val_acc: 0.2031\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8433 - acc: 0.6159 - val_loss: 1.5191 - val_acc: 0.2500\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8228 - acc: 0.6580 - val_loss: 1.5839 - val_acc: 0.2188\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8317 - acc: 0.6256 - val_loss: 1.5878 - val_acc: 0.2188\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8331 - acc: 0.6272 - val_loss: 1.4589 - val_acc: 0.2969\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.7920 - acc: 0.6469 - val_loss: 1.5209 - val_acc: 0.2812\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8291 - acc: 0.6191 - val_loss: 1.6029 - val_acc: 0.2656\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8338 - acc: 0.6240 - val_loss: 1.5157 - val_acc: 0.2812\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8117 - acc: 0.6353 - val_loss: 1.5130 - val_acc: 0.2812\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8272 - acc: 0.6451 - val_loss: 1.6098 - val_acc: 0.2188\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8510 - acc: 0.6110 - val_loss: 1.5988 - val_acc: 0.2188\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8486 - acc: 0.6305 - val_loss: 1.5688 - val_acc: 0.2031\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8126 - acc: 0.6370 - val_loss: 1.5172 - val_acc: 0.2656\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8288 - acc: 0.6207 - val_loss: 1.4429 - val_acc: 0.2656\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8440 - acc: 0.6207 - val_loss: 1.4170 - val_acc: 0.2656\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8499 - acc: 0.6094 - val_loss: 1.5171 - val_acc: 0.2500\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8067 - acc: 0.6532 - val_loss: 1.4400 - val_acc: 0.2344\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8368 - acc: 0.6434 - val_loss: 1.4845 - val_acc: 0.2656\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7779 - acc: 0.6499 - val_loss: 1.6326 - val_acc: 0.2188\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8380 - acc: 0.6207 - val_loss: 1.5465 - val_acc: 0.2188\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8028 - acc: 0.6499 - val_loss: 1.6043 - val_acc: 0.2188\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8657 - acc: 0.6159 - val_loss: 1.5090 - val_acc: 0.2500\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8271 - acc: 0.6483 - val_loss: 1.4685 - val_acc: 0.2656\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7954 - acc: 0.6418 - val_loss: 1.6207 - val_acc: 0.2188\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8317 - acc: 0.6499 - val_loss: 1.5680 - val_acc: 0.2188\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8610 - acc: 0.6143 - val_loss: 1.4994 - val_acc: 0.2656\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8208 - acc: 0.6353 - val_loss: 1.5008 - val_acc: 0.2500\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8183 - acc: 0.6418 - val_loss: 1.5340 - val_acc: 0.2500\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8672 - acc: 0.6110 - val_loss: 1.5990 - val_acc: 0.2188\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8324 - acc: 0.6547 - val_loss: 1.4942 - val_acc: 0.2812\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8042 - acc: 0.6337 - val_loss: 1.5657 - val_acc: 0.2656\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8882 - acc: 0.5964 - val_loss: 1.5626 - val_acc: 0.2500\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8087 - acc: 0.6531 - val_loss: 1.5406 - val_acc: 0.2500\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8402 - acc: 0.6451 - val_loss: 1.5663 - val_acc: 0.2344\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8050 - acc: 0.6240 - val_loss: 1.5998 - val_acc: 0.2344\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7983 - acc: 0.6483 - val_loss: 1.5916 - val_acc: 0.1719\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7967 - acc: 0.6499 - val_loss: 1.6289 - val_acc: 0.2188\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8219 - acc: 0.6321 - val_loss: 1.6380 - val_acc: 0.2500\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8631 - acc: 0.6159 - val_loss: 1.5304 - val_acc: 0.2500\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8265 - acc: 0.6337 - val_loss: 1.5228 - val_acc: 0.2188\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8370 - acc: 0.6418 - val_loss: 1.5198 - val_acc: 0.2500\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 0.8397 - acc: 0.6191 - val_loss: 1.5442 - val_acc: 0.2500\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7936 - acc: 0.6710 - val_loss: 1.5845 - val_acc: 0.2188\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8103 - acc: 0.6564 - val_loss: 1.5854 - val_acc: 0.2188\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8778 - acc: 0.6375 - val_loss: 1.5413 - val_acc: 0.2500\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8351 - acc: 0.6159 - val_loss: 1.5783 - val_acc: 0.2500\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7979 - acc: 0.6645 - val_loss: 1.5596 - val_acc: 0.2656\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8212 - acc: 0.6532 - val_loss: 1.4741 - val_acc: 0.2656\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8363 - acc: 0.6234 - val_loss: 1.5236 - val_acc: 0.2344\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8522 - acc: 0.6224 - val_loss: 1.4575 - val_acc: 0.2812\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8125 - acc: 0.6613 - val_loss: 1.5875 - val_acc: 0.2031\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8176 - acc: 0.6467 - val_loss: 1.4509 - val_acc: 0.2344\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8235 - acc: 0.6434 - val_loss: 1.5413 - val_acc: 0.2188\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8222 - acc: 0.6451 - val_loss: 1.5986 - val_acc: 0.2188\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8377 - acc: 0.6094 - val_loss: 1.4906 - val_acc: 0.2656\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7962 - acc: 0.6337 - val_loss: 1.4139 - val_acc: 0.2812\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8219 - acc: 0.6386 - val_loss: 1.5692 - val_acc: 0.2500\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8318 - acc: 0.6386 - val_loss: 1.5613 - val_acc: 0.2656\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8394 - acc: 0.6110 - val_loss: 1.5664 - val_acc: 0.2656\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8234 - acc: 0.6224 - val_loss: 1.5736 - val_acc: 0.2344\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8186 - acc: 0.6402 - val_loss: 1.6436 - val_acc: 0.2188\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8474 - acc: 0.6386 - val_loss: 1.5802 - val_acc: 0.2188\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.7854 - acc: 0.6596 - val_loss: 1.5237 - val_acc: 0.2188\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8145 - acc: 0.6305 - val_loss: 1.5465 - val_acc: 0.2344\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8044 - acc: 0.6451 - val_loss: 1.6296 - val_acc: 0.2344\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8597 - acc: 0.6272 - val_loss: 1.4830 - val_acc: 0.2500\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8474 - acc: 0.6159 - val_loss: 1.5007 - val_acc: 0.2812\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8729 - acc: 0.6013 - val_loss: 1.5588 - val_acc: 0.1719\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8334 - acc: 0.6451 - val_loss: 1.5199 - val_acc: 0.2344\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8099 - acc: 0.6515 - val_loss: 1.5387 - val_acc: 0.2031\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 0.8483 - acc: 0.6305 - val_loss: 1.5562 - val_acc: 0.2344\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8010 - acc: 0.6483 - val_loss: 1.4939 - val_acc: 0.2500\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8222 - acc: 0.6234 - val_loss: 1.5334 - val_acc: 0.2188\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8111 - acc: 0.6159 - val_loss: 1.5513 - val_acc: 0.2344\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7670 - acc: 0.6532 - val_loss: 1.5226 - val_acc: 0.2500\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8418 - acc: 0.6272 - val_loss: 1.4716 - val_acc: 0.2656\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8234 - acc: 0.6438 - val_loss: 1.5310 - val_acc: 0.2656\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8279 - acc: 0.6240 - val_loss: 1.5901 - val_acc: 0.2188\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 0.8167 - acc: 0.6370 - val_loss: 1.4961 - val_acc: 0.2500\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7874 - acc: 0.6483 - val_loss: 1.5506 - val_acc: 0.2812\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8551 - acc: 0.6353 - val_loss: 1.4751 - val_acc: 0.2656\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8336 - acc: 0.6305 - val_loss: 1.5496 - val_acc: 0.2344\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8173 - acc: 0.6580 - val_loss: 1.5996 - val_acc: 0.2031\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8130 - acc: 0.6434 - val_loss: 1.5739 - val_acc: 0.2031\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.7986 - acc: 0.6548 - val_loss: 1.4994 - val_acc: 0.2656\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.8092 - acc: 0.6516 - val_loss: 1.4802 - val_acc: 0.2969\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8147 - acc: 0.6224 - val_loss: 1.5972 - val_acc: 0.2344\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8218 - acc: 0.6370 - val_loss: 1.4599 - val_acc: 0.2969\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8201 - acc: 0.6175 - val_loss: 1.5024 - val_acc: 0.2656\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.7905 - acc: 0.6467 - val_loss: 1.4419 - val_acc: 0.2656\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7642 - acc: 0.6629 - val_loss: 1.6462 - val_acc: 0.2031\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8731 - acc: 0.5997 - val_loss: 1.6237 - val_acc: 0.2188\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7875 - acc: 0.6224 - val_loss: 1.4880 - val_acc: 0.2656\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.7914 - acc: 0.6386 - val_loss: 1.5878 - val_acc: 0.2031\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8953 - acc: 0.6078 - val_loss: 1.5511 - val_acc: 0.2344\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.8682 - acc: 0.6207 - val_loss: 1.5579 - val_acc: 0.2344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "a1b770e1-df21-4b0d-8aa8-63cc774a79cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.586937665939331,\n",
              "  1.6109180450439453,\n",
              "  1.6911612749099731,\n",
              "  1.5310198068618774,\n",
              "  1.5675312280654907,\n",
              "  1.6239404678344727,\n",
              "  1.6106798648834229,\n",
              "  1.582107424736023,\n",
              "  1.5509700775146484,\n",
              "  1.48212730884552,\n",
              "  1.502788782119751,\n",
              "  1.5826313495635986,\n",
              "  1.4847478866577148,\n",
              "  1.4834849834442139,\n",
              "  1.4672093391418457,\n",
              "  1.479291319847107,\n",
              "  1.4194648265838623,\n",
              "  1.4877556562423706,\n",
              "  1.5144129991531372,\n",
              "  1.5055042505264282,\n",
              "  1.4867295026779175,\n",
              "  1.4705429077148438,\n",
              "  1.5255705118179321,\n",
              "  1.5692129135131836,\n",
              "  1.390872836112976,\n",
              "  1.6028132438659668,\n",
              "  1.4433526992797852,\n",
              "  1.5494800806045532,\n",
              "  1.4203133583068848,\n",
              "  1.499953269958496,\n",
              "  1.3975251913070679,\n",
              "  1.4749054908752441,\n",
              "  1.5633374452590942,\n",
              "  1.4858925342559814,\n",
              "  1.459256649017334,\n",
              "  1.4690760374069214,\n",
              "  1.4108179807662964,\n",
              "  1.4820826053619385,\n",
              "  1.42679762840271,\n",
              "  1.4901719093322754,\n",
              "  1.5215516090393066,\n",
              "  1.4427168369293213,\n",
              "  1.4214123487472534,\n",
              "  1.4917715787887573,\n",
              "  1.2985788583755493,\n",
              "  1.4497003555297852,\n",
              "  1.437220573425293,\n",
              "  1.4463592767715454,\n",
              "  1.4487022161483765,\n",
              "  1.4458295106887817,\n",
              "  1.3087341785430908,\n",
              "  1.4636937379837036,\n",
              "  1.3635280132293701,\n",
              "  1.4283002614974976,\n",
              "  1.364585041999817,\n",
              "  1.4534573554992676,\n",
              "  1.3752001523971558,\n",
              "  1.4399222135543823,\n",
              "  1.4043906927108765,\n",
              "  1.4086681604385376,\n",
              "  1.487640142440796,\n",
              "  1.249790906906128,\n",
              "  1.3740109205245972,\n",
              "  1.4064221382141113,\n",
              "  1.3795634508132935,\n",
              "  1.3602595329284668,\n",
              "  1.323652744293213,\n",
              "  1.4295594692230225,\n",
              "  1.3220921754837036,\n",
              "  1.3735049962997437,\n",
              "  1.3470661640167236,\n",
              "  1.369064211845398,\n",
              "  1.4074976444244385,\n",
              "  1.226664662361145,\n",
              "  1.411044716835022,\n",
              "  1.3235927820205688,\n",
              "  1.3621373176574707,\n",
              "  1.357628583908081,\n",
              "  1.3765785694122314,\n",
              "  1.3337297439575195,\n",
              "  1.3165262937545776,\n",
              "  1.318892478942871,\n",
              "  1.3416153192520142,\n",
              "  1.3787158727645874,\n",
              "  1.3395423889160156,\n",
              "  1.3047322034835815,\n",
              "  1.3128920793533325,\n",
              "  1.3845999240875244,\n",
              "  1.3585774898529053,\n",
              "  1.3493651151657104,\n",
              "  1.331695556640625,\n",
              "  1.320299744606018,\n",
              "  1.2578784227371216,\n",
              "  1.35465669631958,\n",
              "  1.234979271888733,\n",
              "  1.2825636863708496,\n",
              "  1.3651807308197021,\n",
              "  1.3088830709457397,\n",
              "  1.2113871574401855,\n",
              "  1.3111903667449951,\n",
              "  1.2460658550262451,\n",
              "  1.3129040002822876,\n",
              "  1.41385018825531,\n",
              "  1.3371927738189697,\n",
              "  1.3897123336791992,\n",
              "  1.2574045658111572,\n",
              "  1.3239965438842773,\n",
              "  1.261381983757019,\n",
              "  1.259167194366455,\n",
              "  1.2708425521850586,\n",
              "  1.2739975452423096,\n",
              "  1.2584227323532104,\n",
              "  1.2760487794876099,\n",
              "  1.2678196430206299,\n",
              "  1.291757345199585,\n",
              "  1.337569236755371,\n",
              "  1.3218363523483276,\n",
              "  1.3150484561920166,\n",
              "  1.3227953910827637,\n",
              "  1.2694889307022095,\n",
              "  1.2661124467849731,\n",
              "  1.2960227727890015,\n",
              "  1.2811729907989502,\n",
              "  1.2734825611114502,\n",
              "  1.2528437376022339,\n",
              "  1.2237818241119385,\n",
              "  1.269110083580017,\n",
              "  1.277440071105957,\n",
              "  1.2561136484146118,\n",
              "  1.2727255821228027,\n",
              "  1.3206312656402588,\n",
              "  1.34656822681427,\n",
              "  1.268352746963501,\n",
              "  1.2597525119781494,\n",
              "  1.2036644220352173,\n",
              "  1.2812528610229492,\n",
              "  1.3440123796463013,\n",
              "  1.293364405632019,\n",
              "  1.2540053129196167,\n",
              "  1.1702824831008911,\n",
              "  1.225171446800232,\n",
              "  1.1817935705184937,\n",
              "  1.2703099250793457,\n",
              "  1.279160976409912,\n",
              "  1.2308123111724854,\n",
              "  1.3007138967514038,\n",
              "  1.2954068183898926,\n",
              "  1.1921664476394653,\n",
              "  1.1474061012268066,\n",
              "  1.2118029594421387,\n",
              "  1.2585349082946777,\n",
              "  1.1900302171707153,\n",
              "  1.3083585500717163,\n",
              "  1.2697652578353882,\n",
              "  1.1177078485488892,\n",
              "  1.2078038454055786,\n",
              "  1.214072346687317,\n",
              "  1.163610577583313,\n",
              "  1.264464259147644,\n",
              "  1.260092854499817,\n",
              "  1.2256762981414795,\n",
              "  1.2021652460098267,\n",
              "  1.202224850654602,\n",
              "  1.2397011518478394,\n",
              "  1.2230861186981201,\n",
              "  1.2020987272262573,\n",
              "  1.2017418146133423,\n",
              "  1.2422547340393066,\n",
              "  1.1688402891159058,\n",
              "  1.208213448524475,\n",
              "  1.206002116203308,\n",
              "  1.242891788482666,\n",
              "  1.2461507320404053,\n",
              "  1.182105541229248,\n",
              "  1.1994951963424683,\n",
              "  1.1911126375198364,\n",
              "  1.1907120943069458,\n",
              "  1.1680306196212769,\n",
              "  1.2510172128677368,\n",
              "  1.1317803859710693,\n",
              "  1.2019771337509155,\n",
              "  1.1950405836105347,\n",
              "  1.221050500869751,\n",
              "  1.126761555671692,\n",
              "  1.1945464611053467,\n",
              "  1.150592565536499,\n",
              "  1.217540979385376,\n",
              "  1.184946060180664,\n",
              "  1.1917777061462402,\n",
              "  1.1086094379425049,\n",
              "  1.199877381324768,\n",
              "  1.1695932149887085,\n",
              "  1.1335186958312988,\n",
              "  1.2306294441223145,\n",
              "  1.110832929611206,\n",
              "  1.0985347032546997,\n",
              "  1.2018214464187622,\n",
              "  1.1549488306045532,\n",
              "  1.180155873298645,\n",
              "  1.199402928352356,\n",
              "  1.165474534034729,\n",
              "  1.1895105838775635,\n",
              "  1.201138973236084,\n",
              "  1.200351357460022,\n",
              "  1.1507604122161865,\n",
              "  1.2035049200057983,\n",
              "  1.1479132175445557,\n",
              "  1.0759154558181763,\n",
              "  1.1594032049179077,\n",
              "  1.1705918312072754,\n",
              "  1.1299105882644653,\n",
              "  1.1631056070327759,\n",
              "  1.1890194416046143,\n",
              "  1.1287912130355835,\n",
              "  1.2068275213241577,\n",
              "  1.1330020427703857,\n",
              "  1.1790492534637451,\n",
              "  1.1482605934143066,\n",
              "  1.2047810554504395,\n",
              "  1.158164143562317,\n",
              "  1.1962767839431763,\n",
              "  1.1120579242706299,\n",
              "  1.1180191040039062,\n",
              "  1.1438825130462646,\n",
              "  1.1152242422103882,\n",
              "  1.1847645044326782,\n",
              "  1.1693655252456665,\n",
              "  1.1747121810913086,\n",
              "  1.1667752265930176,\n",
              "  1.1560125350952148,\n",
              "  1.0824085474014282,\n",
              "  1.1204880475997925,\n",
              "  1.1292427778244019,\n",
              "  1.1058017015457153,\n",
              "  1.1476598978042603,\n",
              "  1.140929102897644,\n",
              "  1.14560866355896,\n",
              "  1.1932649612426758,\n",
              "  1.2296079397201538,\n",
              "  1.190282940864563,\n",
              "  1.0639969110488892,\n",
              "  1.132051944732666,\n",
              "  1.1001110076904297,\n",
              "  1.1280437707901,\n",
              "  1.1490660905838013,\n",
              "  1.1107885837554932,\n",
              "  1.1307991743087769,\n",
              "  1.10302734375,\n",
              "  1.1297760009765625,\n",
              "  1.1198599338531494,\n",
              "  1.092437744140625,\n",
              "  1.0790681838989258,\n",
              "  1.1313053369522095,\n",
              "  1.0790750980377197,\n",
              "  1.1340758800506592,\n",
              "  1.1236515045166016,\n",
              "  1.165252447128296,\n",
              "  1.139422059059143,\n",
              "  1.1345763206481934,\n",
              "  1.1498812437057495,\n",
              "  1.0864557027816772,\n",
              "  1.1081583499908447,\n",
              "  1.1043697595596313,\n",
              "  1.1296048164367676,\n",
              "  1.083042860031128,\n",
              "  1.128719449043274,\n",
              "  1.0736005306243896,\n",
              "  1.104374885559082,\n",
              "  1.0991625785827637,\n",
              "  1.0064082145690918,\n",
              "  1.0718022584915161,\n",
              "  1.056557536125183,\n",
              "  1.0586493015289307,\n",
              "  1.0730366706848145,\n",
              "  1.1026835441589355,\n",
              "  1.0788077116012573,\n",
              "  1.1257460117340088,\n",
              "  1.0662676095962524,\n",
              "  1.0771188735961914,\n",
              "  1.0854036808013916,\n",
              "  1.0671863555908203,\n",
              "  1.117864727973938,\n",
              "  1.124178409576416,\n",
              "  1.0690605640411377,\n",
              "  1.1001583337783813,\n",
              "  1.137149453163147,\n",
              "  1.0460690259933472,\n",
              "  1.0639088153839111,\n",
              "  1.0579960346221924,\n",
              "  1.0695728063583374,\n",
              "  1.0832278728485107,\n",
              "  1.112767219543457,\n",
              "  1.0738121271133423,\n",
              "  1.0785688161849976,\n",
              "  1.0471655130386353,\n",
              "  1.0738393068313599,\n",
              "  1.055820107460022,\n",
              "  1.0840847492218018,\n",
              "  1.0865496397018433,\n",
              "  1.076537847518921,\n",
              "  1.0368643999099731,\n",
              "  1.1216199398040771,\n",
              "  1.0274003744125366,\n",
              "  1.0177040100097656,\n",
              "  1.040163516998291,\n",
              "  1.1272813081741333,\n",
              "  1.0283002853393555,\n",
              "  1.0683091878890991,\n",
              "  1.0343965291976929,\n",
              "  1.059546709060669,\n",
              "  1.0702263116836548,\n",
              "  1.0503194332122803,\n",
              "  1.1053842306137085,\n",
              "  1.0969642400741577,\n",
              "  1.1190853118896484,\n",
              "  1.0332050323486328,\n",
              "  1.0708301067352295,\n",
              "  1.0836453437805176,\n",
              "  1.061882734298706,\n",
              "  1.0610979795455933,\n",
              "  1.0570178031921387,\n",
              "  1.035875678062439,\n",
              "  1.0345512628555298,\n",
              "  1.0268356800079346,\n",
              "  1.0815792083740234,\n",
              "  1.0700434446334839,\n",
              "  0.9948105812072754,\n",
              "  1.0390541553497314,\n",
              "  1.0909478664398193,\n",
              "  1.0797092914581299,\n",
              "  1.0743228197097778,\n",
              "  1.0803335905075073,\n",
              "  0.9935951232910156,\n",
              "  1.0550071001052856,\n",
              "  1.0684826374053955,\n",
              "  0.988285481929779,\n",
              "  1.1531033515930176,\n",
              "  1.0994739532470703,\n",
              "  1.0489243268966675,\n",
              "  1.0904477834701538,\n",
              "  1.0517505407333374,\n",
              "  1.0173872709274292,\n",
              "  1.0391101837158203,\n",
              "  1.0933027267456055,\n",
              "  1.0389220714569092,\n",
              "  1.0093860626220703,\n",
              "  0.9986094236373901,\n",
              "  1.0756211280822754,\n",
              "  1.1129913330078125,\n",
              "  1.1004638671875,\n",
              "  1.0357269048690796,\n",
              "  1.0406306982040405,\n",
              "  1.0512115955352783,\n",
              "  1.0548510551452637,\n",
              "  0.9792895913124084,\n",
              "  1.0582196712493896,\n",
              "  1.0770364999771118,\n",
              "  1.015596628189087,\n",
              "  1.009964108467102,\n",
              "  1.0125685930252075,\n",
              "  1.0310770273208618,\n",
              "  1.0558135509490967,\n",
              "  1.0060758590698242,\n",
              "  1.0602284669876099,\n",
              "  1.0843626260757446,\n",
              "  1.0790902376174927,\n",
              "  0.9650980830192566,\n",
              "  1.0392488241195679,\n",
              "  1.0050595998764038,\n",
              "  1.0426597595214844,\n",
              "  1.028300166130066,\n",
              "  1.013922095298767,\n",
              "  1.0614274740219116,\n",
              "  0.9639571905136108,\n",
              "  1.0020389556884766,\n",
              "  1.0032814741134644,\n",
              "  1.0151982307434082,\n",
              "  1.06218421459198,\n",
              "  1.0185189247131348,\n",
              "  1.0067793130874634,\n",
              "  1.014096736907959,\n",
              "  1.0311589241027832,\n",
              "  1.0696834325790405,\n",
              "  0.9833865761756897,\n",
              "  1.0934232473373413,\n",
              "  1.059167742729187,\n",
              "  0.967578113079071,\n",
              "  0.9741251468658447,\n",
              "  1.0741586685180664,\n",
              "  1.0460424423217773,\n",
              "  1.0336464643478394,\n",
              "  0.9964988231658936,\n",
              "  0.9825424551963806,\n",
              "  0.9898357391357422,\n",
              "  0.9912418127059937,\n",
              "  1.0644402503967285,\n",
              "  1.0738410949707031,\n",
              "  0.9878358840942383,\n",
              "  0.9790021181106567,\n",
              "  1.0635381937026978,\n",
              "  0.9901716113090515,\n",
              "  1.0367215871810913,\n",
              "  1.0161809921264648,\n",
              "  1.0122607946395874,\n",
              "  1.0186281204223633,\n",
              "  0.9705194234848022,\n",
              "  0.9987552762031555,\n",
              "  0.9889509677886963,\n",
              "  1.0851924419403076,\n",
              "  1.0151472091674805,\n",
              "  1.0023144483566284,\n",
              "  1.014404535293579,\n",
              "  0.9725653529167175,\n",
              "  0.9349555373191833,\n",
              "  0.9029092192649841,\n",
              "  0.9358353614807129,\n",
              "  0.986763596534729,\n",
              "  0.9854702949523926,\n",
              "  0.9454444646835327,\n",
              "  1.0263973474502563,\n",
              "  0.9773942232131958,\n",
              "  1.0321069955825806,\n",
              "  1.0373687744140625,\n",
              "  1.020066738128662,\n",
              "  0.9906083941459656,\n",
              "  1.0721495151519775,\n",
              "  1.0625723600387573,\n",
              "  1.0012816190719604,\n",
              "  1.04152250289917,\n",
              "  1.0336085557937622,\n",
              "  1.0248278379440308,\n",
              "  1.0364689826965332,\n",
              "  0.9758630990982056,\n",
              "  0.9690098762512207,\n",
              "  0.9846590161323547,\n",
              "  1.0806899070739746,\n",
              "  1.0236115455627441,\n",
              "  1.031973123550415,\n",
              "  0.9779712557792664,\n",
              "  0.9364795088768005,\n",
              "  0.9897357225418091,\n",
              "  1.0264559984207153,\n",
              "  0.9538819789886475,\n",
              "  1.0288928747177124,\n",
              "  0.9994664192199707,\n",
              "  0.9725822806358337,\n",
              "  1.0196633338928223,\n",
              "  0.9617804288864136,\n",
              "  1.0024950504302979,\n",
              "  0.9145875573158264,\n",
              "  1.045937180519104,\n",
              "  1.0508794784545898,\n",
              "  0.9526044726371765,\n",
              "  0.9524968266487122,\n",
              "  0.9559176564216614,\n",
              "  0.9172683954238892,\n",
              "  0.9645544290542603,\n",
              "  0.9647764563560486,\n",
              "  1.0046827793121338,\n",
              "  1.0161833763122559,\n",
              "  1.0096052885055542,\n",
              "  1.0019402503967285,\n",
              "  0.9566031098365784,\n",
              "  0.9505206942558289,\n",
              "  0.9678026437759399,\n",
              "  1.0137029886245728,\n",
              "  0.9597613215446472,\n",
              "  0.9895241260528564,\n",
              "  1.004823923110962,\n",
              "  0.9542841911315918,\n",
              "  0.9958104491233826,\n",
              "  1.0443495512008667,\n",
              "  0.9945650100708008,\n",
              "  0.9824056625366211,\n",
              "  0.9727717638015747,\n",
              "  0.9351127743721008,\n",
              "  0.9875026345252991,\n",
              "  0.9439780116081238,\n",
              "  0.9453547596931458,\n",
              "  0.9736572504043579,\n",
              "  0.9774805903434753,\n",
              "  0.9600831270217896,\n",
              "  1.043398380279541,\n",
              "  1.0070158243179321,\n",
              "  0.9855393171310425,\n",
              "  0.9742234945297241,\n",
              "  0.9890517592430115,\n",
              "  0.9856336712837219,\n",
              "  0.9484948515892029,\n",
              "  0.9673312306404114,\n",
              "  0.9939683079719543,\n",
              "  0.9642803072929382,\n",
              "  0.9175136089324951,\n",
              "  0.9540749788284302,\n",
              "  1.0221986770629883,\n",
              "  0.9939329028129578,\n",
              "  0.9070172309875488,\n",
              "  0.995561420917511,\n",
              "  0.9784958362579346,\n",
              "  0.9674232602119446,\n",
              "  0.965836226940155,\n",
              "  0.9229617714881897,\n",
              "  0.9651818871498108,\n",
              "  0.9093105792999268,\n",
              "  0.9826976656913757,\n",
              "  0.9730740785598755,\n",
              "  0.9496816992759705,\n",
              "  0.9245207905769348,\n",
              "  0.9516646265983582,\n",
              "  0.9591808319091797,\n",
              "  0.961414098739624,\n",
              "  0.9664966464042664,\n",
              "  0.9464071989059448,\n",
              "  0.9910686612129211,\n",
              "  0.976911723613739,\n",
              "  0.9939846992492676,\n",
              "  0.9014793634414673,\n",
              "  0.9879411458969116,\n",
              "  1.0014362335205078,\n",
              "  0.8914751410484314,\n",
              "  0.9984868764877319,\n",
              "  0.9753256440162659,\n",
              "  0.9589095711708069,\n",
              "  0.9814870357513428,\n",
              "  0.960585355758667,\n",
              "  0.9240809679031372,\n",
              "  0.965688943862915,\n",
              "  0.9555533528327942,\n",
              "  0.9592024087905884,\n",
              "  0.9383642673492432,\n",
              "  0.9352353811264038,\n",
              "  0.9654173254966736,\n",
              "  0.9701734185218811,\n",
              "  0.918796718120575,\n",
              "  0.8663754463195801,\n",
              "  0.9408013224601746,\n",
              "  0.9465940594673157,\n",
              "  0.9143088459968567,\n",
              "  0.9858425259590149,\n",
              "  0.9213313460350037,\n",
              "  0.9617468118667603,\n",
              "  0.9358421564102173,\n",
              "  0.8566195964813232,\n",
              "  0.9586751461029053,\n",
              "  0.9070704579353333,\n",
              "  0.9495728015899658,\n",
              "  0.9287546277046204,\n",
              "  0.9919868111610413,\n",
              "  0.9520754218101501,\n",
              "  0.9752988815307617,\n",
              "  0.9864978790283203,\n",
              "  0.9865212440490723,\n",
              "  0.8837631344795227,\n",
              "  0.9639138579368591,\n",
              "  0.9476500153541565,\n",
              "  0.9639906287193298,\n",
              "  0.960654079914093,\n",
              "  0.9323852062225342,\n",
              "  0.9196504354476929,\n",
              "  0.9452540278434753,\n",
              "  0.9690547585487366,\n",
              "  0.9229500889778137,\n",
              "  0.8570407032966614,\n",
              "  0.9354456663131714,\n",
              "  0.9099944829940796,\n",
              "  0.9544167518615723,\n",
              "  0.9112778306007385,\n",
              "  0.915244996547699,\n",
              "  0.9588327407836914,\n",
              "  0.9542825818061829,\n",
              "  0.9321152567863464,\n",
              "  0.9158245921134949,\n",
              "  0.9134758114814758,\n",
              "  0.9003259539604187,\n",
              "  0.9332250952720642,\n",
              "  0.9435815811157227,\n",
              "  0.9015210270881653,\n",
              "  0.9611690640449524,\n",
              "  0.9308201670646667,\n",
              "  0.9560095071792603,\n",
              "  0.9029211401939392,\n",
              "  0.9336853623390198,\n",
              "  0.9040063619613647,\n",
              "  0.963196873664856,\n",
              "  0.9441802501678467,\n",
              "  0.9297459721565247,\n",
              "  0.9676083326339722,\n",
              "  0.9169281125068665,\n",
              "  0.8950711488723755,\n",
              "  0.9529354572296143,\n",
              "  0.9100656509399414,\n",
              "  0.9327502250671387,\n",
              "  0.9468771815299988,\n",
              "  0.9138073921203613,\n",
              "  0.9170603156089783,\n",
              "  0.9484525918960571,\n",
              "  0.9473881125450134,\n",
              "  0.861184298992157,\n",
              "  0.9319961667060852,\n",
              "  0.9029488563537598,\n",
              "  0.9446995854377747,\n",
              "  0.9319902062416077,\n",
              "  0.8621563911437988,\n",
              "  0.9513936638832092,\n",
              "  0.9324215054512024,\n",
              "  0.9109619855880737,\n",
              "  0.9851320385932922,\n",
              "  0.910236656665802,\n",
              "  0.9393291473388672,\n",
              "  0.8859374523162842,\n",
              "  0.9316874146461487,\n",
              "  0.8923358917236328,\n",
              "  0.9098871946334839,\n",
              "  0.9386041760444641,\n",
              "  0.8893772959709167,\n",
              "  0.8844980597496033,\n",
              "  0.9420174956321716,\n",
              "  0.8604316115379333,\n",
              "  0.8701087832450867,\n",
              "  0.9751936197280884,\n",
              "  0.8649274110794067,\n",
              "  0.8925157785415649,\n",
              "  0.9220066666603088,\n",
              "  0.9500640630722046,\n",
              "  0.8844565153121948,\n",
              "  0.9360367655754089,\n",
              "  0.9199042916297913,\n",
              "  0.9108057618141174,\n",
              "  0.8915295600891113,\n",
              "  0.9668798446655273,\n",
              "  0.8803767561912537,\n",
              "  0.856916069984436,\n",
              "  0.8209357261657715,\n",
              "  0.9230592250823975,\n",
              "  0.8705596923828125,\n",
              "  0.9164664149284363,\n",
              "  0.8639909029006958,\n",
              "  0.9077251553535461,\n",
              "  0.9165763258934021,\n",
              "  0.8849257826805115,\n",
              "  0.9021985530853271,\n",
              "  0.9221770167350769,\n",
              "  0.9160604476928711,\n",
              "  0.9026731848716736,\n",
              "  0.9249440431594849,\n",
              "  0.8886153697967529,\n",
              "  0.9071572422981262,\n",
              "  0.9452285170555115,\n",
              "  0.9356577396392822,\n",
              "  0.8949316143989563,\n",
              "  0.9028618931770325,\n",
              "  0.8717952966690063,\n",
              "  0.8916699290275574,\n",
              "  0.9250987768173218,\n",
              "  0.9124979376792908,\n",
              "  0.9363971948623657,\n",
              "  0.8824507594108582,\n",
              "  0.9365459084510803,\n",
              "  0.8917776346206665,\n",
              "  0.8776306509971619,\n",
              "  0.9267930388450623,\n",
              "  0.8864363431930542,\n",
              "  0.9241938591003418,\n",
              "  0.8856877088546753,\n",
              "  0.9170477986335754,\n",
              "  0.8812118172645569,\n",
              "  0.8825531005859375,\n",
              "  0.8526256084442139,\n",
              "  0.8822709918022156,\n",
              "  0.9438009858131409,\n",
              "  0.9215689301490784,\n",
              "  0.8958438634872437,\n",
              "  0.882066547870636,\n",
              "  0.9223414659500122,\n",
              "  0.8651259541511536,\n",
              "  0.9474425315856934,\n",
              "  0.9635743498802185,\n",
              "  0.9414011836051941,\n",
              "  0.9030905365943909,\n",
              "  0.874274492263794,\n",
              "  0.8459495306015015,\n",
              "  0.8309574723243713,\n",
              "  0.9130204319953918,\n",
              "  0.9037217497825623,\n",
              "  0.8979625701904297,\n",
              "  0.8520843982696533,\n",
              "  0.9281955361366272,\n",
              "  0.8807797431945801,\n",
              "  0.8993147015571594,\n",
              "  0.8465002179145813,\n",
              "  0.8905285596847534,\n",
              "  0.8563206791877747,\n",
              "  0.8921292424201965,\n",
              "  0.90701824426651,\n",
              "  0.8230947852134705,\n",
              "  0.9214284420013428,\n",
              "  0.9490041136741638,\n",
              "  0.8929941654205322,\n",
              "  0.8798465132713318,\n",
              "  0.8702459931373596,\n",
              "  0.8969323039054871,\n",
              "  0.8973168134689331,\n",
              "  0.9239806532859802,\n",
              "  0.8769315481185913,\n",
              "  0.8466947078704834,\n",
              "  0.8832605481147766,\n",
              "  0.8847743272781372,\n",
              "  0.8834421634674072,\n",
              "  0.9158334136009216,\n",
              "  0.9227527379989624,\n",
              "  0.8781732320785522,\n",
              "  0.8954353928565979,\n",
              "  0.8563340902328491,\n",
              "  0.8617931604385376,\n",
              "  0.9030587077140808,\n",
              "  0.8692334890365601,\n",
              "  0.9036359786987305,\n",
              "  0.8538870811462402,\n",
              "  0.9125454425811768,\n",
              "  0.8534234166145325,\n",
              "  0.8679347634315491,\n",
              "  0.8798478841781616,\n",
              "  0.9426181316375732,\n",
              "  0.8832006454467773,\n",
              "  0.9022310972213745,\n",
              "  0.867055356502533,\n",
              "  0.8884496688842773,\n",
              "  0.9274783730506897,\n",
              "  0.894606351852417,\n",
              "  0.8472985625267029,\n",
              "  0.8590691089630127,\n",
              "  0.903864860534668,\n",
              "  0.866222620010376,\n",
              "  0.8714883327484131,\n",
              "  0.8938583731651306,\n",
              "  0.8575889468193054,\n",
              "  0.8329290747642517,\n",
              "  0.9258742332458496,\n",
              "  0.8481239676475525,\n",
              "  0.8584540486335754,\n",
              "  0.9182441234588623,\n",
              "  0.9255527257919312,\n",
              "  0.9040180444717407,\n",
              "  0.8632347583770752,\n",
              "  0.8546378016471863,\n",
              "  0.8697183132171631,\n",
              "  0.9200569987297058,\n",
              "  0.814175009727478,\n",
              "  0.8245888948440552,\n",
              "  0.8815745115280151,\n",
              "  0.8624281287193298,\n",
              "  0.8796060085296631,\n",
              "  0.8540778756141663,\n",
              "  0.8789083361625671,\n",
              "  0.8871266841888428,\n",
              "  0.9076833128929138,\n",
              "  0.8908801078796387,\n",
              "  0.8946689367294312,\n",
              "  0.9080436825752258,\n",
              "  0.8363116383552551,\n",
              "  0.814073383808136,\n",
              "  0.8500010967254639,\n",
              "  0.8788792490959167,\n",
              "  0.801556408405304,\n",
              "  0.8719698786735535,\n",
              "  0.8791552186012268,\n",
              "  0.8786253929138184,\n",
              "  0.880933940410614,\n",
              "  0.8242581486701965,\n",
              "  0.854960560798645,\n",
              "  0.9908820986747742,\n",
              "  0.8623902797698975,\n",
              "  0.8241767287254333,\n",
              "  0.8795605301856995,\n",
              "  0.8711519241333008,\n",
              "  0.8351566791534424,\n",
              "  0.8425936102867126,\n",
              "  0.867159903049469,\n",
              "  0.8548951148986816,\n",
              "  0.9122118353843689,\n",
              "  0.8701011538505554,\n",
              "  0.8153106570243835,\n",
              "  0.885661244392395,\n",
              "  0.8391939997673035,\n",
              "  0.8091756701469421,\n",
              "  0.9146371483802795,\n",
              "  0.889384925365448,\n",
              "  0.8831750750541687,\n",
              "  0.8670057654380798,\n",
              "  0.8897469639778137,\n",
              "  0.8861724138259888,\n",
              "  0.8976104855537415,\n",
              "  0.8211928009986877,\n",
              "  0.8436663150787354,\n",
              "  0.868463397026062,\n",
              "  0.8864949941635132,\n",
              "  0.8335784673690796,\n",
              "  0.8859708905220032,\n",
              "  0.8487407565116882,\n",
              "  0.8550575375556946,\n",
              "  0.87576824426651,\n",
              "  0.8629541993141174,\n",
              "  0.8137487173080444,\n",
              "  0.8894942402839661,\n",
              "  0.8362407684326172,\n",
              "  0.8723609447479248,\n",
              "  0.865540623664856,\n",
              "  0.857709527015686,\n",
              "  0.8582411408424377,\n",
              "  0.8811237812042236,\n",
              "  0.8456506729125977,\n",
              "  0.8714403510093689,\n",
              "  0.8179875016212463,\n",
              "  0.8431558012962341,\n",
              "  0.8834437131881714,\n",
              "  0.8367231488227844,\n",
              "  0.9095127582550049,\n",
              "  0.8154748678207397,\n",
              "  0.8682945370674133,\n",
              "  0.903893768787384,\n",
              "  0.8336235880851746,\n",
              "  0.8648587465286255,\n",
              "  0.8248673677444458,\n",
              "  0.8427335619926453,\n",
              "  0.8521479368209839,\n",
              "  0.804940402507782,\n",
              "  0.800653874874115,\n",
              "  0.8341954350471497,\n",
              "  0.8864670991897583,\n",
              "  0.829727292060852,\n",
              "  0.8535246253013611,\n",
              "  0.8644090890884399,\n",
              "  0.8520500063896179,\n",
              "  0.7883961796760559,\n",
              "  0.8382935523986816,\n",
              "  0.8732617497444153,\n",
              "  0.8589033484458923,\n",
              "  0.8047273755073547,\n",
              "  0.8253424167633057,\n",
              "  0.8342454433441162,\n",
              "  0.8223410248756409,\n",
              "  0.8381434679031372,\n",
              "  0.8688061833381653,\n",
              "  0.8608946204185486,\n",
              "  0.880973219871521,\n",
              "  0.8422542810440063,\n",
              "  0.7748652696609497,\n",
              "  0.8597500324249268,\n",
              "  0.8731765747070312,\n",
              "  0.8537530303001404,\n",
              "  0.8735555410385132,\n",
              "  0.8248578310012817,\n",
              "  0.8261855244636536,\n",
              "  0.8280782103538513,\n",
              "  0.8439752459526062,\n",
              "  0.8277497291564941,\n",
              "  0.8829665184020996,\n",
              "  0.8746531009674072,\n",
              "  0.8421341776847839,\n",
              "  0.8424438238143921,\n",
              "  0.8705675005912781,\n",
              "  0.7860991358757019,\n",
              "  0.8656643629074097,\n",
              "  0.8904411792755127,\n",
              "  0.8856070637702942,\n",
              "  0.857243001461029,\n",
              "  0.874921977519989,\n",
              "  0.8092643022537231,\n",
              "  0.8628475666046143,\n",
              "  0.8776810169219971,\n",
              "  0.8414778709411621,\n",
              "  0.8586743474006653,\n",
              "  0.834796130657196,\n",
              "  0.8187227249145508,\n",
              "  0.7890065908432007,\n",
              "  0.8481385707855225,\n",
              "  0.867271363735199,\n",
              "  0.8405502438545227,\n",
              "  0.8393990993499756,\n",
              "  0.842472493648529,\n",
              "  0.7967901229858398,\n",
              "  0.8822039365768433,\n",
              "  0.8147575259208679,\n",
              "  0.8341787457466125,\n",
              "  0.8252026438713074,\n",
              "  0.8847194314002991,\n",
              "  0.8231346607208252,\n",
              "  0.9027856588363647,\n",
              "  0.797288715839386,\n",
              "  0.7993445992469788,\n",
              "  0.8281587362289429,\n",
              "  0.8369243741035461,\n",
              "  0.8408002853393555,\n",
              "  0.8111447691917419,\n",
              "  0.8550404906272888,\n",
              "  0.8724273443222046,\n",
              "  0.8426843881607056,\n",
              "  0.8154084086418152,\n",
              "  0.8479635119438171,\n",
              "  0.8087987303733826,\n",
              "  0.8396092653274536,\n",
              "  0.8659874796867371,\n",
              "  0.8018929958343506,\n",
              "  0.8253581523895264,\n",
              "  0.8751801252365112,\n",
              "  0.8432531356811523,\n",
              "  0.8227904438972473,\n",
              "  0.8317276835441589,\n",
              "  0.8331321477890015,\n",
              "  0.7919875979423523,\n",
              "  0.8290600776672363,\n",
              "  0.8337998986244202,\n",
              "  0.8117002844810486,\n",
              "  0.8272128701210022,\n",
              "  0.850971519947052,\n",
              "  0.8486390709877014,\n",
              "  0.8125797510147095,\n",
              "  0.828813910484314,\n",
              "  0.8439977765083313,\n",
              "  0.84988933801651,\n",
              "  0.8067361116409302,\n",
              "  0.8368180990219116,\n",
              "  0.7778721451759338,\n",
              "  0.8379627466201782,\n",
              "  0.8028140664100647,\n",
              "  0.8656922578811646,\n",
              "  0.8270987272262573,\n",
              "  0.7953931093215942,\n",
              "  0.831684947013855,\n",
              "  0.8610434532165527,\n",
              "  0.8208374381065369,\n",
              "  0.8183391690254211,\n",
              "  0.867225706577301,\n",
              "  0.8324011564254761,\n",
              "  0.8041955232620239,\n",
              "  0.888231098651886,\n",
              "  0.808735191822052,\n",
              "  0.840176522731781,\n",
              "  0.8049986362457275,\n",
              "  0.7983362078666687,\n",
              "  0.7967243194580078,\n",
              "  0.8219271302223206,\n",
              "  0.8630552291870117,\n",
              "  0.8265225887298584,\n",
              "  0.8369514346122742,\n",
              "  0.8397483825683594,\n",
              "  0.7936475276947021,\n",
              "  0.8103051781654358,\n",
              "  0.8778139352798462,\n",
              "  0.8350648283958435,\n",
              "  0.797883927822113,\n",
              "  0.8211845755577087,\n",
              "  0.8362704515457153,\n",
              "  0.8521528840065002,\n",
              "  0.8125057220458984,\n",
              "  0.8175937533378601,\n",
              "  0.8234862685203552,\n",
              "  0.8221520781517029,\n",
              "  0.8377129435539246,\n",
              "  0.7962136268615723,\n",
              "  0.8218663334846497,\n",
              "  0.8318031430244446,\n",
              "  0.8394367098808289,\n",
              "  0.8233561515808105,\n",
              "  0.8185818195343018,\n",
              "  0.8473724126815796,\n",
              "  0.7854235172271729,\n",
              "  0.8145283460617065,\n",
              "  0.804374635219574,\n",
              "  0.8596875667572021,\n",
              "  0.8474460244178772,\n",
              "  0.8728957176208496,\n",
              "  0.8334488868713379,\n",
              "  0.8098753690719604,\n",
              "  0.8482937812805176,\n",
              "  0.8010064363479614,\n",
              "  0.822169303894043,\n",
              "  0.811090350151062,\n",
              "  0.7670029401779175,\n",
              "  0.841754138469696,\n",
              "  0.8233765363693237,\n",
              "  0.8278664946556091,\n",
              "  0.8166579008102417,\n",
              "  0.7874404788017273,\n",
              "  0.8551496267318726,\n",
              "  0.8335956931114197,\n",
              "  0.8172714710235596,\n",
              "  0.8129609823226929,\n",
              "  0.7986161112785339,\n",
              "  0.8092465400695801,\n",
              "  0.8147496581077576,\n",
              "  0.8217579126358032,\n",
              "  0.8200844526290894,\n",
              "  0.7904874682426453,\n",
              "  0.7641630172729492,\n",
              "  0.8730743527412415,\n",
              "  0.7875314354896545,\n",
              "  0.7913625836372375,\n",
              "  0.8952803015708923,\n",
              "  0.8682295680046082],\n",
              " 'acc': [0.458670973777771,\n",
              "  0.4327390491962433,\n",
              "  0.40032413601875305,\n",
              "  0.41093748807907104,\n",
              "  0.4246353209018707,\n",
              "  0.3938411772251129,\n",
              "  0.36466774344444275,\n",
              "  0.4181523621082306,\n",
              "  0.39546191692352295,\n",
              "  0.41874998807907104,\n",
              "  0.4084278643131256,\n",
              "  0.3776337206363678,\n",
              "  0.4197731018066406,\n",
              "  0.38411667943000793,\n",
              "  0.41491085290908813,\n",
              "  0.40518638491630554,\n",
              "  0.4203124940395355,\n",
              "  0.4181523621082306,\n",
              "  0.3711507320404053,\n",
              "  0.41166937351226807,\n",
              "  0.42625609040260315,\n",
              "  0.4230145812034607,\n",
              "  0.41166937351226807,\n",
              "  0.4035656452178955,\n",
              "  0.4197731018066406,\n",
              "  0.3727714717388153,\n",
              "  0.41653159260749817,\n",
              "  0.4278768301010132,\n",
              "  0.4538087546825409,\n",
              "  0.4181523621082306,\n",
              "  0.4343598186969757,\n",
              "  0.4132901132106781,\n",
              "  0.4035656452178955,\n",
              "  0.4068071246147156,\n",
              "  0.4392220377922058,\n",
              "  0.4019449055194855,\n",
              "  0.39546191692352295,\n",
              "  0.4246353209018707,\n",
              "  0.4035656452178955,\n",
              "  0.41491085290908813,\n",
              "  0.40032413601875305,\n",
              "  0.4132901132106781,\n",
              "  0.42139384150505066,\n",
              "  0.41166937351226807,\n",
              "  0.4609375,\n",
              "  0.4181523621082306,\n",
              "  0.42625609040260315,\n",
              "  0.40156251192092896,\n",
              "  0.41166937351226807,\n",
              "  0.4343598186969757,\n",
              "  0.468395471572876,\n",
              "  0.4230145812034607,\n",
              "  0.4327390491962433,\n",
              "  0.42625609040260315,\n",
              "  0.4538087546825409,\n",
              "  0.42625609040260315,\n",
              "  0.4230145812034607,\n",
              "  0.4230145812034607,\n",
              "  0.4489465057849884,\n",
              "  0.4376012980937958,\n",
              "  0.4230145812034607,\n",
              "  0.458670973777771,\n",
              "  0.4327390491962433,\n",
              "  0.4327390491962433,\n",
              "  0.4489465057849884,\n",
              "  0.4392220377922058,\n",
              "  0.44246354699134827,\n",
              "  0.4278768301010132,\n",
              "  0.4635332226753235,\n",
              "  0.4376012980937958,\n",
              "  0.4376012980937958,\n",
              "  0.4440842866897583,\n",
              "  0.42656248807907104,\n",
              "  0.470016211271286,\n",
              "  0.4246353209018707,\n",
              "  0.4538087546825409,\n",
              "  0.46406251192092896,\n",
              "  0.44732576608657837,\n",
              "  0.43598055839538574,\n",
              "  0.43111830949783325,\n",
              "  0.45218801498413086,\n",
              "  0.44570502638816833,\n",
              "  0.4602917432785034,\n",
              "  0.41491085290908813,\n",
              "  0.4505672752857208,\n",
              "  0.4554294943809509,\n",
              "  0.45705023407936096,\n",
              "  0.43111830949783325,\n",
              "  0.453125,\n",
              "  0.44084277749061584,\n",
              "  0.47163695096969604,\n",
              "  0.4538087546825409,\n",
              "  0.47163695096969604,\n",
              "  0.44084277749061584,\n",
              "  0.48136141896247864,\n",
              "  0.47163695096969604,\n",
              "  0.44732576608657837,\n",
              "  0.4538087546825409,\n",
              "  0.49270665645599365,\n",
              "  0.4440842866897583,\n",
              "  0.45705023407936096,\n",
              "  0.41653159260749817,\n",
              "  0.40156251192092896,\n",
              "  0.41653159260749817,\n",
              "  0.41874998807907104,\n",
              "  0.4748784303665161,\n",
              "  0.45218801498413086,\n",
              "  0.45218801498413086,\n",
              "  0.4894651472568512,\n",
              "  0.4635332226753235,\n",
              "  0.4376012980937958,\n",
              "  0.470016211271286,\n",
              "  0.4937500059604645,\n",
              "  0.44246354699134827,\n",
              "  0.45705023407936096,\n",
              "  0.4554294943809509,\n",
              "  0.4554294943809509,\n",
              "  0.4635332226753235,\n",
              "  0.4554294943809509,\n",
              "  0.46677470207214355,\n",
              "  0.4635332226753235,\n",
              "  0.43598055839538574,\n",
              "  0.4538087546825409,\n",
              "  0.470016211271286,\n",
              "  0.4538087546825409,\n",
              "  0.470016211271286,\n",
              "  0.458670973777771,\n",
              "  0.45705023407936096,\n",
              "  0.46677470207214355,\n",
              "  0.4748784303665161,\n",
              "  0.41166937351226807,\n",
              "  0.46191248297691345,\n",
              "  0.46677470207214355,\n",
              "  0.468395471572876,\n",
              "  0.5008103847503662,\n",
              "  0.49108588695526123,\n",
              "  0.42656248807907104,\n",
              "  0.46191248297691345,\n",
              "  0.47163695096969604,\n",
              "  0.5218800902366638,\n",
              "  0.4748784303665161,\n",
              "  0.4959481358528137,\n",
              "  0.45705023407936096,\n",
              "  0.4748784303665161,\n",
              "  0.47163695096969604,\n",
              "  0.4376012980937958,\n",
              "  0.4440842866897583,\n",
              "  0.45705023407936096,\n",
              "  0.531604528427124,\n",
              "  0.470016211271286,\n",
              "  0.46191248297691345,\n",
              "  0.5089141130447388,\n",
              "  0.468395471572876,\n",
              "  0.45705023407936096,\n",
              "  0.5153970718383789,\n",
              "  0.458670973777771,\n",
              "  0.44732576608657837,\n",
              "  0.5089141130447388,\n",
              "  0.47163695096969604,\n",
              "  0.47163695096969604,\n",
              "  0.47811993956565857,\n",
              "  0.49270665645599365,\n",
              "  0.49270665645599365,\n",
              "  0.47811993956565857,\n",
              "  0.48622366786003113,\n",
              "  0.4959481358528137,\n",
              "  0.48906248807907104,\n",
              "  0.4538087546825409,\n",
              "  0.5056726336479187,\n",
              "  0.48784440755844116,\n",
              "  0.5072933435440063,\n",
              "  0.4894651472568512,\n",
              "  0.45218801498413086,\n",
              "  0.4894651472568512,\n",
              "  0.5024310946464539,\n",
              "  0.47649919986724854,\n",
              "  0.5072933435440063,\n",
              "  0.4959481358528137,\n",
              "  0.4894651472568512,\n",
              "  0.528363049030304,\n",
              "  0.5121555924415588,\n",
              "  0.49270665645599365,\n",
              "  0.4921875,\n",
              "  0.48784440755844116,\n",
              "  0.4959481358528137,\n",
              "  0.5008103847503662,\n",
              "  0.468395471572876,\n",
              "  0.5008103847503662,\n",
              "  0.4828124940395355,\n",
              "  0.5234375,\n",
              "  0.4797406792640686,\n",
              "  0.49756887555122375,\n",
              "  0.5137763619422913,\n",
              "  0.48298215866088867,\n",
              "  0.518638551235199,\n",
              "  0.5170178413391113,\n",
              "  0.49270665645599365,\n",
              "  0.5024310946464539,\n",
              "  0.49270665645599365,\n",
              "  0.48784440755844116,\n",
              "  0.4732576906681061,\n",
              "  0.4984374940395355,\n",
              "  0.5089141130447388,\n",
              "  0.4846029281616211,\n",
              "  0.46677470207214355,\n",
              "  0.47811993956565857,\n",
              "  0.5015624761581421,\n",
              "  0.5591571927070618,\n",
              "  0.4991896152496338,\n",
              "  0.5024310946464539,\n",
              "  0.518638551235199,\n",
              "  0.5040518641471863,\n",
              "  0.4732576906681061,\n",
              "  0.47649919986724854,\n",
              "  0.4797406792640686,\n",
              "  0.5235008001327515,\n",
              "  0.48298215866088867,\n",
              "  0.5170178413391113,\n",
              "  0.47811993956565857,\n",
              "  0.48784440755844116,\n",
              "  0.4846029281616211,\n",
              "  0.518638551235199,\n",
              "  0.520312488079071,\n",
              "  0.5170178413391113,\n",
              "  0.5364667773246765,\n",
              "  0.48136141896247864,\n",
              "  0.515625,\n",
              "  0.48784440755844116,\n",
              "  0.4991896152496338,\n",
              "  0.5121555924415588,\n",
              "  0.5235008001327515,\n",
              "  0.48622366786003113,\n",
              "  0.528363049030304,\n",
              "  0.518638551235199,\n",
              "  0.503125011920929,\n",
              "  0.5008103847503662,\n",
              "  0.49270665645599365,\n",
              "  0.4943273961544037,\n",
              "  0.47649919986724854,\n",
              "  0.4894651472568512,\n",
              "  0.541329026222229,\n",
              "  0.5170178413391113,\n",
              "  0.5494327545166016,\n",
              "  0.49531251192092896,\n",
              "  0.5105348229408264,\n",
              "  0.5461912751197815,\n",
              "  0.5235008001327515,\n",
              "  0.531604528427124,\n",
              "  0.554295003414154,\n",
              "  0.4959481358528137,\n",
              "  0.518638551235199,\n",
              "  0.5251215696334839,\n",
              "  0.5072933435440063,\n",
              "  0.5235008001327515,\n",
              "  0.5380875468254089,\n",
              "  0.518638551235199,\n",
              "  0.5040518641471863,\n",
              "  0.4937500059604645,\n",
              "  0.4943273961544037,\n",
              "  0.5008103847503662,\n",
              "  0.5137763619422913,\n",
              "  0.515625,\n",
              "  0.5299838185310364,\n",
              "  0.5332252979278564,\n",
              "  0.5575364828109741,\n",
              "  0.518638551235199,\n",
              "  0.5380875468254089,\n",
              "  0.518638551235199,\n",
              "  0.5251215696334839,\n",
              "  0.5607779622077942,\n",
              "  0.5445705056190491,\n",
              "  0.541329026222229,\n",
              "  0.5575364828109741,\n",
              "  0.5526742339134216,\n",
              "  0.5332252979278564,\n",
              "  0.5397082567214966,\n",
              "  0.5364667773246765,\n",
              "  0.5575364828109741,\n",
              "  0.5390625,\n",
              "  0.5348460078239441,\n",
              "  0.49756887555122375,\n",
              "  0.5072933435440063,\n",
              "  0.5153970718383789,\n",
              "  0.5299838185310364,\n",
              "  0.5332252979278564,\n",
              "  0.5137763619422913,\n",
              "  0.5299838185310364,\n",
              "  0.5105348229408264,\n",
              "  0.5397082567214966,\n",
              "  0.5364667773246765,\n",
              "  0.5461912751197815,\n",
              "  0.531604528427124,\n",
              "  0.5575364828109741,\n",
              "  0.5089141130447388,\n",
              "  0.5406249761581421,\n",
              "  0.5299838185310364,\n",
              "  0.531604528427124,\n",
              "  0.5170178413391113,\n",
              "  0.5364667773246765,\n",
              "  0.5299838185310364,\n",
              "  0.5478119850158691,\n",
              "  0.5024310946464539,\n",
              "  0.5672609210014343,\n",
              "  0.5575364828109741,\n",
              "  0.5461912751197815,\n",
              "  0.5235008001327515,\n",
              "  0.5445705056190491,\n",
              "  0.5494327545166016,\n",
              "  0.554295003414154,\n",
              "  0.5429497361183167,\n",
              "  0.5218800902366638,\n",
              "  0.5623987317085266,\n",
              "  0.5299838185310364,\n",
              "  0.5364667773246765,\n",
              "  0.49270665645599365,\n",
              "  0.5640194416046143,\n",
              "  0.543749988079071,\n",
              "  0.528363049030304,\n",
              "  0.5380875468254089,\n",
              "  0.518638551235199,\n",
              "  0.5348460078239441,\n",
              "  0.5559157133102417,\n",
              "  0.5609375238418579,\n",
              "  0.5640194416046143,\n",
              "  0.5380875468254089,\n",
              "  0.534375011920929,\n",
              "  0.5390625,\n",
              "  0.5672609210014343,\n",
              "  0.5170178413391113,\n",
              "  0.5429497361183167,\n",
              "  0.5137763619422913,\n",
              "  0.5380875468254089,\n",
              "  0.5484374761581421,\n",
              "  0.5267422795295715,\n",
              "  0.5364667773246765,\n",
              "  0.5623987317085266,\n",
              "  0.48784440755844116,\n",
              "  0.5202593207359314,\n",
              "  0.5445705056190491,\n",
              "  0.5445705056190491,\n",
              "  0.5380875468254089,\n",
              "  0.5705024600028992,\n",
              "  0.5429497361183167,\n",
              "  0.528363049030304,\n",
              "  0.5575364828109741,\n",
              "  0.5526742339134216,\n",
              "  0.5623987317085266,\n",
              "  0.5640194416046143,\n",
              "  0.5137763619422913,\n",
              "  0.5348460078239441,\n",
              "  0.5397082567214966,\n",
              "  0.5397082567214966,\n",
              "  0.5461912751197815,\n",
              "  0.5332252979278564,\n",
              "  0.5672609210014343,\n",
              "  0.528363049030304,\n",
              "  0.5235008001327515,\n",
              "  0.5559157133102417,\n",
              "  0.5591571927070618,\n",
              "  0.5623987317085266,\n",
              "  0.5484374761581421,\n",
              "  0.542187511920929,\n",
              "  0.5575364828109741,\n",
              "  0.5607779622077942,\n",
              "  0.5170178413391113,\n",
              "  0.5267422795295715,\n",
              "  0.5672609210014343,\n",
              "  0.541329026222229,\n",
              "  0.5591571927070618,\n",
              "  0.5559157133102417,\n",
              "  0.5478119850158691,\n",
              "  0.5786061882972717,\n",
              "  0.5461912751197815,\n",
              "  0.5802268981933594,\n",
              "  0.5623987317085266,\n",
              "  0.591572105884552,\n",
              "  0.5494327545166016,\n",
              "  0.5364667773246765,\n",
              "  0.5656402111053467,\n",
              "  0.5623987317085266,\n",
              "  0.5397082567214966,\n",
              "  0.5478119850158691,\n",
              "  0.554295003414154,\n",
              "  0.5737439393997192,\n",
              "  0.5072933435440063,\n",
              "  0.5429497361183167,\n",
              "  0.582812488079071,\n",
              "  0.5688816905021667,\n",
              "  0.5461912751197815,\n",
              "  0.5688816905021667,\n",
              "  0.5625,\n",
              "  0.5526742339134216,\n",
              "  0.5705024600028992,\n",
              "  0.5688816905021667,\n",
              "  0.5737439393997192,\n",
              "  0.5202593207359314,\n",
              "  0.5364667773246765,\n",
              "  0.5883306264877319,\n",
              "  0.5737439393997192,\n",
              "  0.541329026222229,\n",
              "  0.5609375238418579,\n",
              "  0.541329026222229,\n",
              "  0.5607779622077942,\n",
              "  0.5510534644126892,\n",
              "  0.531604528427124,\n",
              "  0.5609375238418579,\n",
              "  0.554295003414154,\n",
              "  0.541329026222229,\n",
              "  0.5267422795295715,\n",
              "  0.5721231698989868,\n",
              "  0.5931928753852844,\n",
              "  0.5656402111053467,\n",
              "  0.5623987317085266,\n",
              "  0.5899513959884644,\n",
              "  0.5948135852813721,\n",
              "  0.5867098569869995,\n",
              "  0.5623987317085266,\n",
              "  0.5721231698989868,\n",
              "  0.5899513959884644,\n",
              "  0.5623987317085266,\n",
              "  0.5705024600028992,\n",
              "  0.5348460078239441,\n",
              "  0.5526742339134216,\n",
              "  0.5510534644126892,\n",
              "  0.5818476676940918,\n",
              "  0.541329026222229,\n",
              "  0.5461912751197815,\n",
              "  0.5494327545166016,\n",
              "  0.5640194416046143,\n",
              "  0.5559157133102417,\n",
              "  0.543749988079071,\n",
              "  0.5478119850158691,\n",
              "  0.567187488079071,\n",
              "  0.5867098569869995,\n",
              "  0.5575364828109741,\n",
              "  0.5332252979278564,\n",
              "  0.541329026222229,\n",
              "  0.5640194416046143,\n",
              "  0.5510534644126892,\n",
              "  0.5931928753852844,\n",
              "  0.5672609210014343,\n",
              "  0.5429497361183167,\n",
              "  0.5656402111053467,\n",
              "  0.5526742339134216,\n",
              "  0.5380875468254089,\n",
              "  0.5948135852813721,\n",
              "  0.5526742339134216,\n",
              "  0.5843750238418579,\n",
              "  0.5656402111053467,\n",
              "  0.5964343547821045,\n",
              "  0.5640194416046143,\n",
              "  0.5364667773246765,\n",
              "  0.5753646492958069,\n",
              "  0.591572105884552,\n",
              "  0.5753646492958069,\n",
              "  0.5931928753852844,\n",
              "  0.5850891470909119,\n",
              "  0.5656402111053467,\n",
              "  0.5575364828109741,\n",
              "  0.5559157133102417,\n",
              "  0.5562499761581421,\n",
              "  0.5640194416046143,\n",
              "  0.5931928753852844,\n",
              "  0.5737439393997192,\n",
              "  0.5867098569869995,\n",
              "  0.5721231698989868,\n",
              "  0.5721231698989868,\n",
              "  0.5559157133102417,\n",
              "  0.5623987317085266,\n",
              "  0.582812488079071,\n",
              "  0.554295003414154,\n",
              "  0.5494327545166016,\n",
              "  0.554295003414154,\n",
              "  0.5753646492958069,\n",
              "  0.5721231698989868,\n",
              "  0.5931928753852844,\n",
              "  0.5672609210014343,\n",
              "  0.5996758341789246,\n",
              "  0.5948135852813721,\n",
              "  0.5397082567214966,\n",
              "  0.5786061882972717,\n",
              "  0.5818476676940918,\n",
              "  0.5510534644126892,\n",
              "  0.5575364828109741,\n",
              "  0.5575364828109741,\n",
              "  0.5672609210014343,\n",
              "  0.5931928753852844,\n",
              "  0.5769854187965393,\n",
              "  0.5705024600028992,\n",
              "  0.5656402111053467,\n",
              "  0.5834683775901794,\n",
              "  0.5705024600028992,\n",
              "  0.5964343547821045,\n",
              "  0.5867098569869995,\n",
              "  0.5526742339134216,\n",
              "  0.5721231698989868,\n",
              "  0.5948135852813721,\n",
              "  0.5607779622077942,\n",
              "  0.5802268981933594,\n",
              "  0.5623987317085266,\n",
              "  0.5607779622077942,\n",
              "  0.5850891470909119,\n",
              "  0.5850891470909119,\n",
              "  0.6094003319740295,\n",
              "  0.5834683775901794,\n",
              "  0.5786061882972717,\n",
              "  0.5802268981933594,\n",
              "  0.5867098569869995,\n",
              "  0.5786061882972717,\n",
              "  0.5964343547821045,\n",
              "  0.5607779622077942,\n",
              "  0.5883306264877319,\n",
              "  0.5867098569869995,\n",
              "  0.5494327545166016,\n",
              "  0.5737439393997192,\n",
              "  0.5640194416046143,\n",
              "  0.5867098569869995,\n",
              "  0.5721231698989868,\n",
              "  0.5170178413391113,\n",
              "  0.6077795624732971,\n",
              "  0.5494327545166016,\n",
              "  0.5753646492958069,\n",
              "  0.5753646492958069,\n",
              "  0.5705024600028992,\n",
              "  0.5656402111053467,\n",
              "  0.5931928753852844,\n",
              "  0.5818476676940918,\n",
              "  0.5526742339134216,\n",
              "  0.5575364828109741,\n",
              "  0.5899513959884644,\n",
              "  0.5786061882972717,\n",
              "  0.5786061882972717,\n",
              "  0.5980551242828369,\n",
              "  0.5980551242828369,\n",
              "  0.6401944756507874,\n",
              "  0.5921875238418579,\n",
              "  0.5996758341789246,\n",
              "  0.6304700374603271,\n",
              "  0.5948135852813721,\n",
              "  0.5802268981933594,\n",
              "  0.5640194416046143,\n",
              "  0.5899513959884644,\n",
              "  0.6434359550476074,\n",
              "  0.5510534644126892,\n",
              "  0.5802268981933594,\n",
              "  0.5899513959884644,\n",
              "  0.5921875238418579,\n",
              "  0.5640194416046143,\n",
              "  0.5656402111053467,\n",
              "  0.5656402111053467,\n",
              "  0.5753646492958069,\n",
              "  0.5737439393997192,\n",
              "  0.601296603679657,\n",
              "  0.5559157133102417,\n",
              "  0.5753646492958069,\n",
              "  0.5705024600028992,\n",
              "  0.5834683775901794,\n",
              "  0.6094003319740295,\n",
              "  0.6094003319740295,\n",
              "  0.5964343547821045,\n",
              "  0.5964343547821045,\n",
              "  0.5802268981933594,\n",
              "  0.6239870190620422,\n",
              "  0.5802268981933594,\n",
              "  0.598437488079071,\n",
              "  0.5899513959884644,\n",
              "  0.6094003319740295,\n",
              "  0.5899513959884644,\n",
              "  0.5765625238418579,\n",
              "  0.5980551242828369,\n",
              "  0.5786061882972717,\n",
              "  0.6029173135757446,\n",
              "  0.5899513959884644,\n",
              "  0.5931928753852844,\n",
              "  0.6029173135757446,\n",
              "  0.5899513959884644,\n",
              "  0.5769854187965393,\n",
              "  0.591572105884552,\n",
              "  0.5931928753852844,\n",
              "  0.578125,\n",
              "  0.6126418113708496,\n",
              "  0.6029173135757446,\n",
              "  0.5921875238418579,\n",
              "  0.554295003414154,\n",
              "  0.5899513959884644,\n",
              "  0.591572105884552,\n",
              "  0.5672609210014343,\n",
              "  0.5786061882972717,\n",
              "  0.6061588525772095,\n",
              "  0.5786061882972717,\n",
              "  0.604538083076477,\n",
              "  0.591572105884552,\n",
              "  0.5753646492958069,\n",
              "  0.5996758341789246,\n",
              "  0.5996758341789246,\n",
              "  0.5867098569869995,\n",
              "  0.5721231698989868,\n",
              "  0.6077795624732971,\n",
              "  0.601296603679657,\n",
              "  0.6061588525772095,\n",
              "  0.5883306264877319,\n",
              "  0.5899513959884644,\n",
              "  0.6156250238418579,\n",
              "  0.5818476676940918,\n",
              "  0.5883306264877319,\n",
              "  0.6029173135757446,\n",
              "  0.5737439393997192,\n",
              "  0.6110210418701172,\n",
              "  0.5850891470909119,\n",
              "  0.6029173135757446,\n",
              "  0.5850891470909119,\n",
              "  0.6158832907676697,\n",
              "  0.601296603679657,\n",
              "  0.5737439393997192,\n",
              "  0.5948135852813721,\n",
              "  0.604538083076477,\n",
              "  0.5867098569869995,\n",
              "  0.6369529962539673,\n",
              "  0.604538083076477,\n",
              "  0.5737439393997192,\n",
              "  0.6110210418701172,\n",
              "  0.6256077885627747,\n",
              "  0.604538083076477,\n",
              "  0.5850891470909119,\n",
              "  0.614262580871582,\n",
              "  0.5753646492958069,\n",
              "  0.5883306264877319,\n",
              "  0.5834683775901794,\n",
              "  0.6223663091659546,\n",
              "  0.573437511920929,\n",
              "  0.6191247701644897,\n",
              "  0.6061588525772095,\n",
              "  0.6434359550476074,\n",
              "  0.604538083076477,\n",
              "  0.6061588525772095,\n",
              "  0.6077795624732971,\n",
              "  0.6158832907676697,\n",
              "  0.601296603679657,\n",
              "  0.591572105884552,\n",
              "  0.6094003319740295,\n",
              "  0.5769854187965393,\n",
              "  0.6077795624732971,\n",
              "  0.5948135852813721,\n",
              "  0.6175040602684021,\n",
              "  0.5883306264877319,\n",
              "  0.6223663091659546,\n",
              "  0.6077795624732971,\n",
              "  0.6191247701644897,\n",
              "  0.6029173135757446,\n",
              "  0.6029173135757446,\n",
              "  0.5931928753852844,\n",
              "  0.6029173135757446,\n",
              "  0.6094003319740295,\n",
              "  0.6175040602684021,\n",
              "  0.6158832907676697,\n",
              "  0.5737439393997192,\n",
              "  0.6061588525772095,\n",
              "  0.5867098569869995,\n",
              "  0.5948135852813721,\n",
              "  0.6110210418701172,\n",
              "  0.5964343547821045,\n",
              "  0.604538083076477,\n",
              "  0.601296603679657,\n",
              "  0.6320907473564148,\n",
              "  0.6110210418701172,\n",
              "  0.6110210418701172,\n",
              "  0.6077795624732971,\n",
              "  0.6175040602684021,\n",
              "  0.6126418113708496,\n",
              "  0.604538083076477,\n",
              "  0.6077795624732971,\n",
              "  0.6110210418701172,\n",
              "  0.6000000238418579,\n",
              "  0.5921875238418579,\n",
              "  0.6191247701644897,\n",
              "  0.5921875238418579,\n",
              "  0.5931928753852844,\n",
              "  0.5964343547821045,\n",
              "  0.5834683775901794,\n",
              "  0.6077795624732971,\n",
              "  0.637499988079071,\n",
              "  0.6288492679595947,\n",
              "  0.5964343547821045,\n",
              "  0.5980551242828369,\n",
              "  0.5899513959884644,\n",
              "  0.6482982039451599,\n",
              "  0.5802268981933594,\n",
              "  0.6191247701644897,\n",
              "  0.6265624761581421,\n",
              "  0.6207455396652222,\n",
              "  0.604538083076477,\n",
              "  0.6171875,\n",
              "  0.604687511920929,\n",
              "  0.6029173135757446,\n",
              "  0.6320907473564148,\n",
              "  0.6337115168571472,\n",
              "  0.5931928753852844,\n",
              "  0.6061588525772095,\n",
              "  0.601296603679657,\n",
              "  0.601296603679657,\n",
              "  0.5948135852813721,\n",
              "  0.6029173135757446,\n",
              "  0.5850891470909119,\n",
              "  0.6077795624732971,\n",
              "  0.6191247701644897,\n",
              "  0.6158832907676697,\n",
              "  0.5964343547821045,\n",
              "  0.614262580871582,\n",
              "  0.604538083076477,\n",
              "  0.582812488079071,\n",
              "  0.5948135852813721,\n",
              "  0.6110210418701172,\n",
              "  0.6156250238418579,\n",
              "  0.6256077885627747,\n",
              "  0.6109374761581421,\n",
              "  0.6061588525772095,\n",
              "  0.6158832907676697,\n",
              "  0.6353322267532349,\n",
              "  0.6029173135757446,\n",
              "  0.6109374761581421,\n",
              "  0.6353322267532349,\n",
              "  0.6061588525772095,\n",
              "  0.5802268981933594,\n",
              "  0.6191247701644897,\n",
              "  0.6077795624732971,\n",
              "  0.6337115168571472,\n",
              "  0.6000000238418579,\n",
              "  0.5980551242828369,\n",
              "  0.5899513959884644,\n",
              "  0.6450567245483398,\n",
              "  0.6207455396652222,\n",
              "  0.6110210418701172,\n",
              "  0.6094003319740295,\n",
              "  0.6191247701644897,\n",
              "  0.614262580871582,\n",
              "  0.6320907473564148,\n",
              "  0.6401944756507874,\n",
              "  0.5867098569869995,\n",
              "  0.6337115168571472,\n",
              "  0.6094003319740295,\n",
              "  0.5850891470909119,\n",
              "  0.5867098569869995,\n",
              "  0.5953124761581421,\n",
              "  0.6207455396652222,\n",
              "  0.6272284984588623,\n",
              "  0.6256077885627747,\n",
              "  0.614062488079071,\n",
              "  0.6288492679595947,\n",
              "  0.6353322267532349,\n",
              "  0.604538083076477,\n",
              "  0.6223663091659546,\n",
              "  0.6094003319740295,\n",
              "  0.6256077885627747,\n",
              "  0.6110210418701172,\n",
              "  0.6061588525772095,\n",
              "  0.591572105884552,\n",
              "  0.5948135852813721,\n",
              "  0.6207455396652222,\n",
              "  0.6061588525772095,\n",
              "  0.6288492679595947,\n",
              "  0.6564019322395325,\n",
              "  0.6061588525772095,\n",
              "  0.6077795624732971,\n",
              "  0.6531604528427124,\n",
              "  0.629687488079071,\n",
              "  0.614262580871582,\n",
              "  0.6175040602684021,\n",
              "  0.6094003319740295,\n",
              "  0.6418152451515198,\n",
              "  0.6158832907676697,\n",
              "  0.567187488079071,\n",
              "  0.6256077885627747,\n",
              "  0.6207455396652222,\n",
              "  0.6158832907676697,\n",
              "  0.609375,\n",
              "  0.6272284984588623,\n",
              "  0.6401944756507874,\n",
              "  0.6272284984588623,\n",
              "  0.6207455396652222,\n",
              "  0.5980551242828369,\n",
              "  0.6337115168571472,\n",
              "  0.6126418113708496,\n",
              "  0.6077795624732971,\n",
              "  0.6256077885627747,\n",
              "  0.6288492679595947,\n",
              "  0.5948135852813721,\n",
              "  0.6207455396652222,\n",
              "  0.6158832907676697,\n",
              "  0.614262580871582,\n",
              "  0.6207455396652222,\n",
              "  0.621874988079071,\n",
              "  0.5883306264877319,\n",
              "  0.6239870190620422,\n",
              "  0.6239870190620422,\n",
              "  0.6256077885627747,\n",
              "  0.5980551242828369,\n",
              "  0.6126418113708496,\n",
              "  0.6171875,\n",
              "  0.6175040602684021,\n",
              "  0.604538083076477,\n",
              "  0.6158832907676697,\n",
              "  0.6223663091659546,\n",
              "  0.6499189734458923,\n",
              "  0.604538083076477,\n",
              "  0.6207455396652222,\n",
              "  0.6401944756507874,\n",
              "  0.6110210418701172,\n",
              "  0.6223663091659546,\n",
              "  0.614262580871582,\n",
              "  0.6061588525772095,\n",
              "  0.6466774940490723,\n",
              "  0.614262580871582,\n",
              "  0.6499189734458923,\n",
              "  0.6126418113708496,\n",
              "  0.6223663091659546,\n",
              "  0.6328125,\n",
              "  0.6094003319740295,\n",
              "  0.6434359550476074,\n",
              "  0.604538083076477,\n",
              "  0.6191247701644897,\n",
              "  0.6369529962539673,\n",
              "  0.6187499761581421,\n",
              "  0.6256077885627747,\n",
              "  0.6353322267532349,\n",
              "  0.621874988079071,\n",
              "  0.6288492679595947,\n",
              "  0.65153968334198,\n",
              "  0.620312511920929,\n",
              "  0.5948135852813721,\n",
              "  0.6547812223434448,\n",
              "  0.6337115168571472,\n",
              "  0.6272284984588623,\n",
              "  0.6239870190620422,\n",
              "  0.65153968334198,\n",
              "  0.6328125,\n",
              "  0.6239870190620422,\n",
              "  0.6385737657546997,\n",
              "  0.6628849506378174,\n",
              "  0.6418152451515198,\n",
              "  0.6207455396652222,\n",
              "  0.6337115168571472,\n",
              "  0.6320907473564148,\n",
              "  0.604538083076477,\n",
              "  0.6077795624732971,\n",
              "  0.6077795624732971,\n",
              "  0.6434359550476074,\n",
              "  0.6807131171226501,\n",
              "  0.6207455396652222,\n",
              "  0.6094003319740295,\n",
              "  0.6158832907676697,\n",
              "  0.6191247701644897,\n",
              "  0.6564019322395325,\n",
              "  0.6564019322395325,\n",
              "  0.6385737657546997,\n",
              "  0.614262580871582,\n",
              "  0.6272284984588623,\n",
              "  0.614262580871582,\n",
              "  0.6158832907676697,\n",
              "  0.6191247701644897,\n",
              "  0.601296603679657,\n",
              "  0.6171875,\n",
              "  0.6499189734458923,\n",
              "  0.6401944756507874,\n",
              "  0.6223663091659546,\n",
              "  0.604538083076477,\n",
              "  0.6272284984588623,\n",
              "  0.6288492679595947,\n",
              "  0.625,\n",
              "  0.6094003319740295,\n",
              "  0.5899513959884644,\n",
              "  0.6239870190620422,\n",
              "  0.6385737657546997,\n",
              "  0.6531604528427124,\n",
              "  0.6353322267532349,\n",
              "  0.6468750238418579,\n",
              "  0.6482982039451599,\n",
              "  0.601296603679657,\n",
              "  0.6158832907676697,\n",
              "  0.6304700374603271,\n",
              "  0.6239870190620422,\n",
              "  0.65153968334198,\n",
              "  0.6110210418701172,\n",
              "  0.660937488079071,\n",
              "  0.6677471399307251,\n",
              "  0.6401944756507874,\n",
              "  0.6061588525772095,\n",
              "  0.6434359550476074,\n",
              "  0.6077795624732971,\n",
              "  0.6564019322395325,\n",
              "  0.6482982039451599,\n",
              "  0.629687488079071,\n",
              "  0.6482982039451599,\n",
              "  0.6265624761581421,\n",
              "  0.661264181137085,\n",
              "  0.614262580871582,\n",
              "  0.6256077885627747,\n",
              "  0.6175040602684021,\n",
              "  0.6223663091659546,\n",
              "  0.6207455396652222,\n",
              "  0.6175040602684021,\n",
              "  0.6312500238418579,\n",
              "  0.601296603679657,\n",
              "  0.6385737657546997,\n",
              "  0.6353322267532349,\n",
              "  0.614262580871582,\n",
              "  0.6158832907676697,\n",
              "  0.6580227017402649,\n",
              "  0.6256077885627747,\n",
              "  0.6272284984588623,\n",
              "  0.6468750238418579,\n",
              "  0.6191247701644897,\n",
              "  0.6239870190620422,\n",
              "  0.6353322267532349,\n",
              "  0.6450567245483398,\n",
              "  0.6110210418701172,\n",
              "  0.6304700374603271,\n",
              "  0.6369529962539673,\n",
              "  0.6207455396652222,\n",
              "  0.6207455396652222,\n",
              "  0.6094003319740295,\n",
              "  0.6531604528427124,\n",
              "  0.6434359550476074,\n",
              "  0.6499189734458923,\n",
              "  0.6207455396652222,\n",
              "  0.6499189734458923,\n",
              "  0.6158832907676697,\n",
              "  0.6482982039451599,\n",
              "  0.6418152451515198,\n",
              "  0.6499189734458923,\n",
              "  0.614262580871582,\n",
              "  0.6353322267532349,\n",
              "  0.6418152451515198,\n",
              "  0.6110210418701172,\n",
              "  0.6546875238418579,\n",
              "  0.6337115168571472,\n",
              "  0.5964343547821045,\n",
              "  0.653124988079071,\n",
              "  0.6450567245483398,\n",
              "  0.6239870190620422,\n",
              "  0.6482982039451599,\n",
              "  0.6499189734458923,\n",
              "  0.6320907473564148,\n",
              "  0.6158832907676697,\n",
              "  0.6337115168571472,\n",
              "  0.6418152451515198,\n",
              "  0.6191247701644897,\n",
              "  0.6709886789321899,\n",
              "  0.6564019322395325,\n",
              "  0.637499988079071,\n",
              "  0.6158832907676697,\n",
              "  0.664505660533905,\n",
              "  0.6531604528427124,\n",
              "  0.6234375238418579,\n",
              "  0.6223663091659546,\n",
              "  0.661264181137085,\n",
              "  0.6466774940490723,\n",
              "  0.6434359550476074,\n",
              "  0.6450567245483398,\n",
              "  0.6094003319740295,\n",
              "  0.6337115168571472,\n",
              "  0.6385737657546997,\n",
              "  0.6385737657546997,\n",
              "  0.6110210418701172,\n",
              "  0.6223663091659546,\n",
              "  0.6401944756507874,\n",
              "  0.6385737657546997,\n",
              "  0.6596434116363525,\n",
              "  0.6304700374603271,\n",
              "  0.6450567245483398,\n",
              "  0.6272284984588623,\n",
              "  0.6158832907676697,\n",
              "  0.601296603679657,\n",
              "  0.6450567245483398,\n",
              "  0.65153968334198,\n",
              "  0.6304700374603271,\n",
              "  0.6482982039451599,\n",
              "  0.6234375238418579,\n",
              "  0.6158832907676697,\n",
              "  0.6531604528427124,\n",
              "  0.6272284984588623,\n",
              "  0.643750011920929,\n",
              "  0.6239870190620422,\n",
              "  0.6369529962539673,\n",
              "  0.6482982039451599,\n",
              "  0.6353322267532349,\n",
              "  0.6304700374603271,\n",
              "  0.6580227017402649,\n",
              "  0.6434359550476074,\n",
              "  0.6547812223434448,\n",
              "  0.651562511920929,\n",
              "  0.6223663091659546,\n",
              "  0.6369529962539673,\n",
              "  0.6175040602684021,\n",
              "  0.6466774940490723,\n",
              "  0.6628849506378174,\n",
              "  0.5996758341789246,\n",
              "  0.6223663091659546,\n",
              "  0.6385737657546997,\n",
              "  0.6077795624732971,\n",
              "  0.6207455396652222],\n",
              " 'val_loss': [2.173442840576172,\n",
              "  2.0693655014038086,\n",
              "  1.9078119993209839,\n",
              "  1.8742777109146118,\n",
              "  1.8119105100631714,\n",
              "  1.7495976686477661,\n",
              "  1.7403795719146729,\n",
              "  1.598128080368042,\n",
              "  1.688873052597046,\n",
              "  1.6641383171081543,\n",
              "  1.5941903591156006,\n",
              "  1.698602318763733,\n",
              "  1.718433141708374,\n",
              "  1.6036272048950195,\n",
              "  1.6582872867584229,\n",
              "  1.627070426940918,\n",
              "  1.706770420074463,\n",
              "  1.6984872817993164,\n",
              "  1.6518728733062744,\n",
              "  1.6377009153366089,\n",
              "  1.6138744354248047,\n",
              "  1.6325032711029053,\n",
              "  1.6676733493804932,\n",
              "  1.6806516647338867,\n",
              "  1.6110771894454956,\n",
              "  1.6527235507965088,\n",
              "  1.655511736869812,\n",
              "  1.673528790473938,\n",
              "  1.5890305042266846,\n",
              "  1.6701841354370117,\n",
              "  1.65354323387146,\n",
              "  1.6829183101654053,\n",
              "  1.6947144269943237,\n",
              "  1.6370726823806763,\n",
              "  1.6562604904174805,\n",
              "  1.69040846824646,\n",
              "  1.6530606746673584,\n",
              "  1.5897445678710938,\n",
              "  1.6717395782470703,\n",
              "  1.719475507736206,\n",
              "  1.7095179557800293,\n",
              "  1.684259057044983,\n",
              "  1.6684867143630981,\n",
              "  1.656585693359375,\n",
              "  1.7539005279541016,\n",
              "  1.6913141012191772,\n",
              "  1.7278929948806763,\n",
              "  1.6557739973068237,\n",
              "  1.691076636314392,\n",
              "  1.6198108196258545,\n",
              "  1.7210655212402344,\n",
              "  1.7314977645874023,\n",
              "  1.6466341018676758,\n",
              "  1.7102370262145996,\n",
              "  1.6539911031723022,\n",
              "  1.6694527864456177,\n",
              "  1.6786348819732666,\n",
              "  1.673828363418579,\n",
              "  1.6943655014038086,\n",
              "  1.6990652084350586,\n",
              "  1.6186414957046509,\n",
              "  1.7635562419891357,\n",
              "  1.749503254890442,\n",
              "  1.7252107858657837,\n",
              "  1.7840754985809326,\n",
              "  1.7417984008789062,\n",
              "  1.790987491607666,\n",
              "  1.7283352613449097,\n",
              "  1.6501438617706299,\n",
              "  1.6854853630065918,\n",
              "  1.6760399341583252,\n",
              "  1.7145462036132812,\n",
              "  1.6629111766815186,\n",
              "  1.5927714109420776,\n",
              "  1.6496224403381348,\n",
              "  1.7561724185943604,\n",
              "  1.6541571617126465,\n",
              "  1.6415743827819824,\n",
              "  1.6418921947479248,\n",
              "  1.6697940826416016,\n",
              "  1.597241997718811,\n",
              "  1.7102285623550415,\n",
              "  1.6582577228546143,\n",
              "  1.678135633468628,\n",
              "  1.5937104225158691,\n",
              "  1.6531035900115967,\n",
              "  1.7275598049163818,\n",
              "  1.69192636013031,\n",
              "  1.520038366317749,\n",
              "  1.7026431560516357,\n",
              "  1.617046594619751,\n",
              "  1.7143995761871338,\n",
              "  1.7504651546478271,\n",
              "  1.737461805343628,\n",
              "  1.6516773700714111,\n",
              "  1.6306793689727783,\n",
              "  1.6408569812774658,\n",
              "  1.7014808654785156,\n",
              "  1.721590280532837,\n",
              "  1.677643060684204,\n",
              "  1.6506621837615967,\n",
              "  1.6304385662078857,\n",
              "  1.7534286975860596,\n",
              "  1.71085786819458,\n",
              "  1.686765193939209,\n",
              "  1.6981756687164307,\n",
              "  1.698805809020996,\n",
              "  1.7085137367248535,\n",
              "  1.7179327011108398,\n",
              "  1.6470831632614136,\n",
              "  1.6881335973739624,\n",
              "  1.6341209411621094,\n",
              "  1.645117998123169,\n",
              "  1.704977035522461,\n",
              "  1.6806609630584717,\n",
              "  1.6725742816925049,\n",
              "  1.6693201065063477,\n",
              "  1.7149150371551514,\n",
              "  1.685168981552124,\n",
              "  1.6710593700408936,\n",
              "  1.6492412090301514,\n",
              "  1.6200435161590576,\n",
              "  1.7627190351486206,\n",
              "  1.7010562419891357,\n",
              "  1.6788192987442017,\n",
              "  1.700767993927002,\n",
              "  1.705580472946167,\n",
              "  1.642063856124878,\n",
              "  1.6512830257415771,\n",
              "  1.6632022857666016,\n",
              "  1.6617804765701294,\n",
              "  1.6893668174743652,\n",
              "  1.6915879249572754,\n",
              "  1.5694971084594727,\n",
              "  1.5734272003173828,\n",
              "  1.675750732421875,\n",
              "  1.7542165517807007,\n",
              "  1.6402435302734375,\n",
              "  1.7502739429473877,\n",
              "  1.6404131650924683,\n",
              "  1.6268184185028076,\n",
              "  1.663912296295166,\n",
              "  1.6748461723327637,\n",
              "  1.7483079433441162,\n",
              "  1.6240218877792358,\n",
              "  1.6235227584838867,\n",
              "  1.6929463148117065,\n",
              "  1.6706016063690186,\n",
              "  1.7261743545532227,\n",
              "  1.7246818542480469,\n",
              "  1.6933867931365967,\n",
              "  1.7650203704833984,\n",
              "  1.6945621967315674,\n",
              "  1.7966128587722778,\n",
              "  1.6921848058700562,\n",
              "  1.7572506666183472,\n",
              "  1.777613878250122,\n",
              "  1.7014086246490479,\n",
              "  1.7734386920928955,\n",
              "  1.5825083255767822,\n",
              "  1.6850996017456055,\n",
              "  1.6830816268920898,\n",
              "  1.6336160898208618,\n",
              "  1.696983814239502,\n",
              "  1.6548125743865967,\n",
              "  1.6801071166992188,\n",
              "  1.735437035560608,\n",
              "  1.6578798294067383,\n",
              "  1.6945682764053345,\n",
              "  1.6974611282348633,\n",
              "  1.7550122737884521,\n",
              "  1.6748671531677246,\n",
              "  1.6927021741867065,\n",
              "  1.716045618057251,\n",
              "  1.7985349893569946,\n",
              "  1.7558817863464355,\n",
              "  1.6901848316192627,\n",
              "  1.7420403957366943,\n",
              "  1.733077883720398,\n",
              "  1.7346749305725098,\n",
              "  1.5918349027633667,\n",
              "  1.6649223566055298,\n",
              "  1.6868816614151,\n",
              "  1.7567509412765503,\n",
              "  1.784836769104004,\n",
              "  1.7164809703826904,\n",
              "  1.7187737226486206,\n",
              "  1.7034573554992676,\n",
              "  1.6749370098114014,\n",
              "  1.7221131324768066,\n",
              "  1.6333550214767456,\n",
              "  1.6375967264175415,\n",
              "  1.6437251567840576,\n",
              "  1.6080248355865479,\n",
              "  1.6908702850341797,\n",
              "  1.7693694829940796,\n",
              "  1.615525722503662,\n",
              "  1.6646137237548828,\n",
              "  1.778868556022644,\n",
              "  1.5847653150558472,\n",
              "  1.6985340118408203,\n",
              "  1.777442455291748,\n",
              "  1.6754953861236572,\n",
              "  1.6160671710968018,\n",
              "  1.6776318550109863,\n",
              "  1.7069432735443115,\n",
              "  1.6498019695281982,\n",
              "  1.6050119400024414,\n",
              "  1.632783055305481,\n",
              "  1.7112452983856201,\n",
              "  1.7289354801177979,\n",
              "  1.6873681545257568,\n",
              "  1.7310895919799805,\n",
              "  1.7736806869506836,\n",
              "  1.5891038179397583,\n",
              "  1.6497893333435059,\n",
              "  1.629305362701416,\n",
              "  1.7086801528930664,\n",
              "  1.6467814445495605,\n",
              "  1.6547354459762573,\n",
              "  1.6118104457855225,\n",
              "  1.699373722076416,\n",
              "  1.6161015033721924,\n",
              "  1.7224382162094116,\n",
              "  1.6477569341659546,\n",
              "  1.5913279056549072,\n",
              "  1.5722898244857788,\n",
              "  1.730330467224121,\n",
              "  1.6173758506774902,\n",
              "  1.6889780759811401,\n",
              "  1.67550790309906,\n",
              "  1.637006163597107,\n",
              "  1.6047273874282837,\n",
              "  1.7665467262268066,\n",
              "  1.632515549659729,\n",
              "  1.6682193279266357,\n",
              "  1.5649060010910034,\n",
              "  1.7256606817245483,\n",
              "  1.7170886993408203,\n",
              "  1.6926956176757812,\n",
              "  1.6980807781219482,\n",
              "  1.7074800729751587,\n",
              "  1.6129238605499268,\n",
              "  1.6860201358795166,\n",
              "  1.7221705913543701,\n",
              "  1.6024484634399414,\n",
              "  1.6127104759216309,\n",
              "  1.608134388923645,\n",
              "  1.6807705163955688,\n",
              "  1.707889199256897,\n",
              "  1.6712591648101807,\n",
              "  1.67365300655365,\n",
              "  1.7073359489440918,\n",
              "  1.59903883934021,\n",
              "  1.6355572938919067,\n",
              "  1.7177284955978394,\n",
              "  1.6963307857513428,\n",
              "  1.5986971855163574,\n",
              "  1.7395620346069336,\n",
              "  1.643779993057251,\n",
              "  1.653923749923706,\n",
              "  1.6906654834747314,\n",
              "  1.6235624551773071,\n",
              "  1.655008316040039,\n",
              "  1.6642978191375732,\n",
              "  1.750432014465332,\n",
              "  1.6601412296295166,\n",
              "  1.634321689605713,\n",
              "  1.7481317520141602,\n",
              "  1.5957554578781128,\n",
              "  1.64131498336792,\n",
              "  1.684428334236145,\n",
              "  1.6505928039550781,\n",
              "  1.6945178508758545,\n",
              "  1.6866811513900757,\n",
              "  1.7033768892288208,\n",
              "  1.5614513158798218,\n",
              "  1.6711578369140625,\n",
              "  1.599992275238037,\n",
              "  1.7014715671539307,\n",
              "  1.7186388969421387,\n",
              "  1.7181414365768433,\n",
              "  1.6227936744689941,\n",
              "  1.6811251640319824,\n",
              "  1.7137153148651123,\n",
              "  1.5780603885650635,\n",
              "  1.602128028869629,\n",
              "  1.7522928714752197,\n",
              "  1.5748708248138428,\n",
              "  1.6782243251800537,\n",
              "  1.7431390285491943,\n",
              "  1.6576547622680664,\n",
              "  1.7687361240386963,\n",
              "  1.6465990543365479,\n",
              "  1.701395034790039,\n",
              "  1.625670075416565,\n",
              "  1.6893041133880615,\n",
              "  1.7028769254684448,\n",
              "  1.6881864070892334,\n",
              "  1.6241811513900757,\n",
              "  1.7253586053848267,\n",
              "  1.7083137035369873,\n",
              "  1.7330307960510254,\n",
              "  1.6207486391067505,\n",
              "  1.6358363628387451,\n",
              "  1.6237854957580566,\n",
              "  1.6061577796936035,\n",
              "  1.610051155090332,\n",
              "  1.664747714996338,\n",
              "  1.6505590677261353,\n",
              "  1.6349515914916992,\n",
              "  1.5959832668304443,\n",
              "  1.6185581684112549,\n",
              "  1.5607733726501465,\n",
              "  1.6586480140686035,\n",
              "  1.607262372970581,\n",
              "  1.6438534259796143,\n",
              "  1.5403509140014648,\n",
              "  1.645770788192749,\n",
              "  1.668405294418335,\n",
              "  1.6209230422973633,\n",
              "  1.609649419784546,\n",
              "  1.6847609281539917,\n",
              "  1.5881344079971313,\n",
              "  1.6864759922027588,\n",
              "  1.6869630813598633,\n",
              "  1.6513586044311523,\n",
              "  1.6739919185638428,\n",
              "  1.6423609256744385,\n",
              "  1.6703318357467651,\n",
              "  1.5290608406066895,\n",
              "  1.6425812244415283,\n",
              "  1.6616716384887695,\n",
              "  1.6636325120925903,\n",
              "  1.5887025594711304,\n",
              "  1.6117010116577148,\n",
              "  1.6265664100646973,\n",
              "  1.6522184610366821,\n",
              "  1.773655652999878,\n",
              "  1.609837532043457,\n",
              "  1.6858834028244019,\n",
              "  1.6550084352493286,\n",
              "  1.6204822063446045,\n",
              "  1.540541648864746,\n",
              "  1.649327278137207,\n",
              "  1.6130924224853516,\n",
              "  1.6090576648712158,\n",
              "  1.639926791191101,\n",
              "  1.5552692413330078,\n",
              "  1.5368866920471191,\n",
              "  1.6180057525634766,\n",
              "  1.7086491584777832,\n",
              "  1.6493477821350098,\n",
              "  1.5723464488983154,\n",
              "  1.6036460399627686,\n",
              "  1.5342872142791748,\n",
              "  1.5576691627502441,\n",
              "  1.6089305877685547,\n",
              "  1.6668422222137451,\n",
              "  1.694936990737915,\n",
              "  1.6748015880584717,\n",
              "  1.6865179538726807,\n",
              "  1.694406270980835,\n",
              "  1.6898460388183594,\n",
              "  1.6182119846343994,\n",
              "  1.6029276847839355,\n",
              "  1.62053382396698,\n",
              "  1.7091832160949707,\n",
              "  1.6292998790740967,\n",
              "  1.6622792482376099,\n",
              "  1.6821798086166382,\n",
              "  1.6241894960403442,\n",
              "  1.6480209827423096,\n",
              "  1.6337512731552124,\n",
              "  1.6617250442504883,\n",
              "  1.6664741039276123,\n",
              "  1.589732050895691,\n",
              "  1.6016854047775269,\n",
              "  1.553633213043213,\n",
              "  1.7105741500854492,\n",
              "  1.4586174488067627,\n",
              "  1.6515074968338013,\n",
              "  1.5831685066223145,\n",
              "  1.558427333831787,\n",
              "  1.5913114547729492,\n",
              "  1.5948147773742676,\n",
              "  1.5550310611724854,\n",
              "  1.585397481918335,\n",
              "  1.6160224676132202,\n",
              "  1.6109473705291748,\n",
              "  1.6356443166732788,\n",
              "  1.6321873664855957,\n",
              "  1.7076966762542725,\n",
              "  1.5959150791168213,\n",
              "  1.7014949321746826,\n",
              "  1.674481749534607,\n",
              "  1.659050703048706,\n",
              "  1.7242984771728516,\n",
              "  1.6275368928909302,\n",
              "  1.643094778060913,\n",
              "  1.5616891384124756,\n",
              "  1.7010244131088257,\n",
              "  1.5988881587982178,\n",
              "  1.6589616537094116,\n",
              "  1.677777647972107,\n",
              "  1.7177373170852661,\n",
              "  1.6481621265411377,\n",
              "  1.6056292057037354,\n",
              "  1.6151015758514404,\n",
              "  1.6532106399536133,\n",
              "  1.5555737018585205,\n",
              "  1.6782821416854858,\n",
              "  1.6494303941726685,\n",
              "  1.662388801574707,\n",
              "  1.5761239528656006,\n",
              "  1.542722225189209,\n",
              "  1.6044490337371826,\n",
              "  1.626573920249939,\n",
              "  1.5898088216781616,\n",
              "  1.5742626190185547,\n",
              "  1.5971096754074097,\n",
              "  1.5878493785858154,\n",
              "  1.5789930820465088,\n",
              "  1.653544306755066,\n",
              "  1.5164852142333984,\n",
              "  1.5634738206863403,\n",
              "  1.6830917596817017,\n",
              "  1.660386085510254,\n",
              "  1.5088324546813965,\n",
              "  1.4903199672698975,\n",
              "  1.538207769393921,\n",
              "  1.602297306060791,\n",
              "  1.6088886260986328,\n",
              "  1.621681809425354,\n",
              "  1.5643739700317383,\n",
              "  1.6198488473892212,\n",
              "  1.584092617034912,\n",
              "  1.671189546585083,\n",
              "  1.5870249271392822,\n",
              "  1.682916283607483,\n",
              "  1.524289608001709,\n",
              "  1.5961329936981201,\n",
              "  1.6171660423278809,\n",
              "  1.6307220458984375,\n",
              "  1.6923184394836426,\n",
              "  1.6155998706817627,\n",
              "  1.578749418258667,\n",
              "  1.6265923976898193,\n",
              "  1.6338648796081543,\n",
              "  1.5397676229476929,\n",
              "  1.6024459600448608,\n",
              "  1.5584967136383057,\n",
              "  1.638655424118042,\n",
              "  1.5438756942749023,\n",
              "  1.653380274772644,\n",
              "  1.612209677696228,\n",
              "  1.699700117111206,\n",
              "  1.611703872680664,\n",
              "  1.630370855331421,\n",
              "  1.6347639560699463,\n",
              "  1.6709516048431396,\n",
              "  1.65860116481781,\n",
              "  1.5541234016418457,\n",
              "  1.6482012271881104,\n",
              "  1.6182706356048584,\n",
              "  1.605064868927002,\n",
              "  1.7504152059555054,\n",
              "  1.6678214073181152,\n",
              "  1.615448236465454,\n",
              "  1.591374397277832,\n",
              "  1.5690033435821533,\n",
              "  1.6556917428970337,\n",
              "  1.644749641418457,\n",
              "  1.6174300909042358,\n",
              "  1.6012449264526367,\n",
              "  1.5052695274353027,\n",
              "  1.6019468307495117,\n",
              "  1.5648623704910278,\n",
              "  1.6818989515304565,\n",
              "  1.659170150756836,\n",
              "  1.6169058084487915,\n",
              "  1.5732953548431396,\n",
              "  1.6266298294067383,\n",
              "  1.511035442352295,\n",
              "  1.5586307048797607,\n",
              "  1.664283275604248,\n",
              "  1.6154497861862183,\n",
              "  1.617675542831421,\n",
              "  1.674838900566101,\n",
              "  1.583207368850708,\n",
              "  1.6253576278686523,\n",
              "  1.605281114578247,\n",
              "  1.5757472515106201,\n",
              "  1.597996473312378,\n",
              "  1.6973499059677124,\n",
              "  1.5533056259155273,\n",
              "  1.712475299835205,\n",
              "  1.5802699327468872,\n",
              "  1.5397205352783203,\n",
              "  1.486336350440979,\n",
              "  1.5475013256072998,\n",
              "  1.6429232358932495,\n",
              "  1.6515278816223145,\n",
              "  1.6190428733825684,\n",
              "  1.5721290111541748,\n",
              "  1.6516741514205933,\n",
              "  1.6241295337677002,\n",
              "  1.7160043716430664,\n",
              "  1.6741564273834229,\n",
              "  1.705812692642212,\n",
              "  1.6640840768814087,\n",
              "  1.6614432334899902,\n",
              "  1.6073589324951172,\n",
              "  1.5551677942276,\n",
              "  1.5771143436431885,\n",
              "  1.6047773361206055,\n",
              "  1.5783157348632812,\n",
              "  1.6008186340332031,\n",
              "  1.5714638233184814,\n",
              "  1.5172219276428223,\n",
              "  1.5239580869674683,\n",
              "  1.681738018989563,\n",
              "  1.5893909931182861,\n",
              "  1.6329907178878784,\n",
              "  1.5543320178985596,\n",
              "  1.552108883857727,\n",
              "  1.6823458671569824,\n",
              "  1.5996284484863281,\n",
              "  1.6403028964996338,\n",
              "  1.6331517696380615,\n",
              "  1.5950343608856201,\n",
              "  1.4934327602386475,\n",
              "  1.6480252742767334,\n",
              "  1.6331795454025269,\n",
              "  1.606362223625183,\n",
              "  1.568018913269043,\n",
              "  1.5767848491668701,\n",
              "  1.4347844123840332,\n",
              "  1.6055318117141724,\n",
              "  1.5511441230773926,\n",
              "  1.546941876411438,\n",
              "  1.5016862154006958,\n",
              "  1.5702484846115112,\n",
              "  1.597536563873291,\n",
              "  1.6142115592956543,\n",
              "  1.5499619245529175,\n",
              "  1.5763771533966064,\n",
              "  1.5665076971054077,\n",
              "  1.6963438987731934,\n",
              "  1.5828953981399536,\n",
              "  1.5964365005493164,\n",
              "  1.5334309339523315,\n",
              "  1.606212854385376,\n",
              "  1.5730814933776855,\n",
              "  1.5363470315933228,\n",
              "  1.593348503112793,\n",
              "  1.678123116493225,\n",
              "  1.5725293159484863,\n",
              "  1.6536903381347656,\n",
              "  1.5670127868652344,\n",
              "  1.60286283493042,\n",
              "  1.533078908920288,\n",
              "  1.644075870513916,\n",
              "  1.6197280883789062,\n",
              "  1.5591096878051758,\n",
              "  1.58555006980896,\n",
              "  1.613470435142517,\n",
              "  1.536545753479004,\n",
              "  1.5226430892944336,\n",
              "  1.556734323501587,\n",
              "  1.5660260915756226,\n",
              "  1.5408060550689697,\n",
              "  1.49477219581604,\n",
              "  1.584359884262085,\n",
              "  1.6282471418380737,\n",
              "  1.581817626953125,\n",
              "  1.5747952461242676,\n",
              "  1.5076048374176025,\n",
              "  1.5449321269989014,\n",
              "  1.5513973236083984,\n",
              "  1.5778441429138184,\n",
              "  1.5451130867004395,\n",
              "  1.5321414470672607,\n",
              "  1.6124982833862305,\n",
              "  1.5958092212677002,\n",
              "  1.5795390605926514,\n",
              "  1.6646820306777954,\n",
              "  1.5043933391571045,\n",
              "  1.6051735877990723,\n",
              "  1.5996508598327637,\n",
              "  1.5148011445999146,\n",
              "  1.5869534015655518,\n",
              "  1.5120726823806763,\n",
              "  1.539292573928833,\n",
              "  1.560612440109253,\n",
              "  1.5882892608642578,\n",
              "  1.529832124710083,\n",
              "  1.55184006690979,\n",
              "  1.5753107070922852,\n",
              "  1.4569764137268066,\n",
              "  1.5741820335388184,\n",
              "  1.5170607566833496,\n",
              "  1.5783405303955078,\n",
              "  1.4852110147476196,\n",
              "  1.5830824375152588,\n",
              "  1.6026588678359985,\n",
              "  1.5671119689941406,\n",
              "  1.5549397468566895,\n",
              "  1.623141884803772,\n",
              "  1.6662263870239258,\n",
              "  1.6343188285827637,\n",
              "  1.5385043621063232,\n",
              "  1.5677096843719482,\n",
              "  1.6846623420715332,\n",
              "  1.7089476585388184,\n",
              "  1.543086051940918,\n",
              "  1.6034986972808838,\n",
              "  1.5797803401947021,\n",
              "  1.588710069656372,\n",
              "  1.547377347946167,\n",
              "  1.6706582307815552,\n",
              "  1.6940749883651733,\n",
              "  1.6472318172454834,\n",
              "  1.603737711906433,\n",
              "  1.5611014366149902,\n",
              "  1.5796252489089966,\n",
              "  1.5270791053771973,\n",
              "  1.6425409317016602,\n",
              "  1.4893749952316284,\n",
              "  1.621130108833313,\n",
              "  1.643690824508667,\n",
              "  1.5849145650863647,\n",
              "  1.6888371706008911,\n",
              "  1.5275564193725586,\n",
              "  1.6204919815063477,\n",
              "  1.6705055236816406,\n",
              "  1.635766863822937,\n",
              "  1.566935658454895,\n",
              "  1.6805893182754517,\n",
              "  1.5497214794158936,\n",
              "  1.5649423599243164,\n",
              "  1.5869419574737549,\n",
              "  1.6792042255401611,\n",
              "  1.571183681488037,\n",
              "  1.6035701036453247,\n",
              "  1.5640249252319336,\n",
              "  1.5088527202606201,\n",
              "  1.5836124420166016,\n",
              "  1.5905793905258179,\n",
              "  1.6210416555404663,\n",
              "  1.5492992401123047,\n",
              "  1.6204097270965576,\n",
              "  1.6207695007324219,\n",
              "  1.5295904874801636,\n",
              "  1.6049394607543945,\n",
              "  1.4957209825515747,\n",
              "  1.6840943098068237,\n",
              "  1.5730568170547485,\n",
              "  1.5545060634613037,\n",
              "  1.6212999820709229,\n",
              "  1.6412086486816406,\n",
              "  1.5667431354522705,\n",
              "  1.4759066104888916,\n",
              "  1.571274995803833,\n",
              "  1.6083284616470337,\n",
              "  1.5625730752944946,\n",
              "  1.4969154596328735,\n",
              "  1.5270323753356934,\n",
              "  1.5965063571929932,\n",
              "  1.5597975254058838,\n",
              "  1.534348964691162,\n",
              "  1.6021976470947266,\n",
              "  1.5733933448791504,\n",
              "  1.614393949508667,\n",
              "  1.5668681859970093,\n",
              "  1.610750675201416,\n",
              "  1.5281736850738525,\n",
              "  1.539820909500122,\n",
              "  1.5593688488006592,\n",
              "  1.5743319988250732,\n",
              "  1.6362156867980957,\n",
              "  1.6074934005737305,\n",
              "  1.5365636348724365,\n",
              "  1.6061701774597168,\n",
              "  1.565279483795166,\n",
              "  1.5163710117340088,\n",
              "  1.6172926425933838,\n",
              "  1.5805001258850098,\n",
              "  1.6007627248764038,\n",
              "  1.587040901184082,\n",
              "  1.5030547380447388,\n",
              "  1.548817753791809,\n",
              "  1.5565149784088135,\n",
              "  1.6030468940734863,\n",
              "  1.6685826778411865,\n",
              "  1.5811251401901245,\n",
              "  1.6291861534118652,\n",
              "  1.5948998928070068,\n",
              "  1.586850881576538,\n",
              "  1.5974278450012207,\n",
              "  1.530842661857605,\n",
              "  1.5797479152679443,\n",
              "  1.5798935890197754,\n",
              "  1.5594890117645264,\n",
              "  1.5430594682693481,\n",
              "  1.583883285522461,\n",
              "  1.562178373336792,\n",
              "  1.5557494163513184,\n",
              "  1.6369776725769043,\n",
              "  1.5514557361602783,\n",
              "  1.5985158681869507,\n",
              "  1.6105618476867676,\n",
              "  1.6197173595428467,\n",
              "  1.5371077060699463,\n",
              "  1.6081808805465698,\n",
              "  1.6064066886901855,\n",
              "  1.5951040983200073,\n",
              "  1.5476055145263672,\n",
              "  1.5238885879516602,\n",
              "  1.6288344860076904,\n",
              "  1.6704286336898804,\n",
              "  1.61186683177948,\n",
              "  1.5872740745544434,\n",
              "  1.5971500873565674,\n",
              "  1.503101110458374,\n",
              "  1.478520393371582,\n",
              "  1.555450439453125,\n",
              "  1.5735186338424683,\n",
              "  1.538851261138916,\n",
              "  1.4871482849121094,\n",
              "  1.604435920715332,\n",
              "  1.5137172937393188,\n",
              "  1.6003329753875732,\n",
              "  1.538015365600586,\n",
              "  1.6164082288742065,\n",
              "  1.5704690217971802,\n",
              "  1.6263171434402466,\n",
              "  1.5672214031219482,\n",
              "  1.574937105178833,\n",
              "  1.5076179504394531,\n",
              "  1.5767419338226318,\n",
              "  1.5282354354858398,\n",
              "  1.5917385816574097,\n",
              "  1.5874577760696411,\n",
              "  1.5523955821990967,\n",
              "  1.5379829406738281,\n",
              "  1.5281720161437988,\n",
              "  1.5834399461746216,\n",
              "  1.5715292692184448,\n",
              "  1.576322317123413,\n",
              "  1.4946861267089844,\n",
              "  1.5616257190704346,\n",
              "  1.5874507427215576,\n",
              "  1.607804775238037,\n",
              "  1.507684350013733,\n",
              "  1.5716323852539062,\n",
              "  1.5030806064605713,\n",
              "  1.4569541215896606,\n",
              "  1.5916916131973267,\n",
              "  1.4900189638137817,\n",
              "  1.612987756729126,\n",
              "  1.5491244792938232,\n",
              "  1.4470951557159424,\n",
              "  1.5070056915283203,\n",
              "  1.553445816040039,\n",
              "  1.6226606369018555,\n",
              "  1.5398011207580566,\n",
              "  1.5758297443389893,\n",
              "  1.5674262046813965,\n",
              "  1.4997358322143555,\n",
              "  1.5759131908416748,\n",
              "  1.5144984722137451,\n",
              "  1.4235508441925049,\n",
              "  1.5910186767578125,\n",
              "  1.459244728088379,\n",
              "  1.5475010871887207,\n",
              "  1.6631308794021606,\n",
              "  1.5249006748199463,\n",
              "  1.5396533012390137,\n",
              "  1.591782808303833,\n",
              "  1.5549695491790771,\n",
              "  1.5079329013824463,\n",
              "  1.5331473350524902,\n",
              "  1.5199346542358398,\n",
              "  1.4658178091049194,\n",
              "  1.5604777336120605,\n",
              "  1.546950101852417,\n",
              "  1.5618579387664795,\n",
              "  1.6422885656356812,\n",
              "  1.588998794555664,\n",
              "  1.5692462921142578,\n",
              "  1.6079938411712646,\n",
              "  1.6045560836791992,\n",
              "  1.5083187818527222,\n",
              "  1.604100227355957,\n",
              "  1.600035309791565,\n",
              "  1.5702884197235107,\n",
              "  1.555659294128418,\n",
              "  1.498711109161377,\n",
              "  1.477920651435852,\n",
              "  1.6578772068023682,\n",
              "  1.5627107620239258,\n",
              "  1.596272587776184,\n",
              "  1.5491830110549927,\n",
              "  1.5470558404922485,\n",
              "  1.618826150894165,\n",
              "  1.5582804679870605,\n",
              "  1.5786466598510742,\n",
              "  1.580641269683838,\n",
              "  1.6395337581634521,\n",
              "  1.5377360582351685,\n",
              "  1.5673978328704834,\n",
              "  1.5575416088104248,\n",
              "  1.606403112411499,\n",
              "  1.512927532196045,\n",
              "  1.5977725982666016,\n",
              "  1.5775010585784912,\n",
              "  1.452213168144226,\n",
              "  1.5792932510375977,\n",
              "  1.6317201852798462,\n",
              "  1.5990321636199951,\n",
              "  1.4940581321716309,\n",
              "  1.5607157945632935,\n",
              "  1.502994418144226,\n",
              "  1.5769391059875488,\n",
              "  1.6186161041259766,\n",
              "  1.725203037261963,\n",
              "  1.6166698932647705,\n",
              "  1.6119706630706787,\n",
              "  1.512021541595459,\n",
              "  1.6181987524032593,\n",
              "  1.5501971244812012,\n",
              "  1.6934213638305664,\n",
              "  1.6436400413513184,\n",
              "  1.4994614124298096,\n",
              "  1.5822827816009521,\n",
              "  1.5578304529190063,\n",
              "  1.608586311340332,\n",
              "  1.5866620540618896,\n",
              "  1.6065866947174072,\n",
              "  1.5709939002990723,\n",
              "  1.512735366821289,\n",
              "  1.5627601146697998,\n",
              "  1.5297799110412598,\n",
              "  1.5406526327133179,\n",
              "  1.6124937534332275,\n",
              "  1.5844858884811401,\n",
              "  1.5342810153961182,\n",
              "  1.5460741519927979,\n",
              "  1.6519310474395752,\n",
              "  1.679368257522583,\n",
              "  1.5624282360076904,\n",
              "  1.5398023128509521,\n",
              "  1.5181126594543457,\n",
              "  1.6259900331497192,\n",
              "  1.7136943340301514,\n",
              "  1.5835285186767578,\n",
              "  1.521233320236206,\n",
              "  1.5989758968353271,\n",
              "  1.532462239265442,\n",
              "  1.530142068862915,\n",
              "  1.526829719543457,\n",
              "  1.556413173675537,\n",
              "  1.5041382312774658,\n",
              "  1.6394447088241577,\n",
              "  1.5966243743896484,\n",
              "  1.5641283988952637,\n",
              "  1.508292317390442,\n",
              "  1.5547914505004883,\n",
              "  1.5283348560333252,\n",
              "  1.562016487121582,\n",
              "  1.5303655862808228,\n",
              "  1.4816107749938965,\n",
              "  1.5928406715393066,\n",
              "  1.581006407737732,\n",
              "  1.4999239444732666,\n",
              "  1.5050417184829712,\n",
              "  1.5404837131500244,\n",
              "  1.5895006656646729,\n",
              "  1.6228399276733398,\n",
              "  1.618916392326355,\n",
              "  1.6422028541564941,\n",
              "  1.4807372093200684,\n",
              "  1.5635168552398682,\n",
              "  1.575242519378662,\n",
              "  1.56343412399292,\n",
              "  1.6246346235275269,\n",
              "  1.5450540781021118,\n",
              "  1.5449061393737793,\n",
              "  1.6054680347442627,\n",
              "  1.5323588848114014,\n",
              "  1.5971158742904663,\n",
              "  1.5115609169006348,\n",
              "  1.5096325874328613,\n",
              "  1.569105625152588,\n",
              "  1.6353890895843506,\n",
              "  1.4646152257919312,\n",
              "  1.64417564868927,\n",
              "  1.5292768478393555,\n",
              "  1.5941784381866455,\n",
              "  1.5786019563674927,\n",
              "  1.5225739479064941,\n",
              "  1.5104761123657227,\n",
              "  1.5312926769256592,\n",
              "  1.5717216730117798,\n",
              "  1.5191020965576172,\n",
              "  1.583893060684204,\n",
              "  1.5877745151519775,\n",
              "  1.458880066871643,\n",
              "  1.5209457874298096,\n",
              "  1.6028988361358643,\n",
              "  1.5157480239868164,\n",
              "  1.5130281448364258,\n",
              "  1.6098060607910156,\n",
              "  1.598841905593872,\n",
              "  1.5687823295593262,\n",
              "  1.5171582698822021,\n",
              "  1.4429023265838623,\n",
              "  1.4169756174087524,\n",
              "  1.5170642137527466,\n",
              "  1.43996000289917,\n",
              "  1.4844980239868164,\n",
              "  1.6326318979263306,\n",
              "  1.5465490818023682,\n",
              "  1.6042639017105103,\n",
              "  1.5090134143829346,\n",
              "  1.4685145616531372,\n",
              "  1.6206867694854736,\n",
              "  1.56795334815979,\n",
              "  1.4994463920593262,\n",
              "  1.5007638931274414,\n",
              "  1.534002423286438,\n",
              "  1.5990073680877686,\n",
              "  1.4942387342453003,\n",
              "  1.5657044649124146,\n",
              "  1.5626323223114014,\n",
              "  1.5406473875045776,\n",
              "  1.5663175582885742,\n",
              "  1.5998445749282837,\n",
              "  1.5915701389312744,\n",
              "  1.6288819313049316,\n",
              "  1.6379542350769043,\n",
              "  1.5304309129714966,\n",
              "  1.522837519645691,\n",
              "  1.5197809934616089,\n",
              "  1.5441570281982422,\n",
              "  1.5845048427581787,\n",
              "  1.5854017734527588,\n",
              "  1.541340947151184,\n",
              "  1.5782802104949951,\n",
              "  1.5595862865447998,\n",
              "  1.4741475582122803,\n",
              "  1.5236129760742188,\n",
              "  1.4574817419052124,\n",
              "  1.5875134468078613,\n",
              "  1.4509284496307373,\n",
              "  1.541313886642456,\n",
              "  1.598581314086914,\n",
              "  1.4906225204467773,\n",
              "  1.4139405488967896,\n",
              "  1.5691643953323364,\n",
              "  1.5612695217132568,\n",
              "  1.5663999319076538,\n",
              "  1.5735561847686768,\n",
              "  1.6436445713043213,\n",
              "  1.5802321434020996,\n",
              "  1.5236573219299316,\n",
              "  1.546455979347229,\n",
              "  1.6295623779296875,\n",
              "  1.483044147491455,\n",
              "  1.500719428062439,\n",
              "  1.5588260889053345,\n",
              "  1.5198893547058105,\n",
              "  1.538658857345581,\n",
              "  1.5562165975570679,\n",
              "  1.4939247369766235,\n",
              "  1.533351182937622,\n",
              "  1.5513476133346558,\n",
              "  1.5225830078125,\n",
              "  1.4715683460235596,\n",
              "  1.5309607982635498,\n",
              "  1.5900923013687134,\n",
              "  1.4961154460906982,\n",
              "  1.5505847930908203,\n",
              "  1.4751085042953491,\n",
              "  1.5495702028274536,\n",
              "  1.5996308326721191,\n",
              "  1.5739426612854004,\n",
              "  1.4993977546691895,\n",
              "  1.4802029132843018,\n",
              "  1.5971940755844116,\n",
              "  1.4599334001541138,\n",
              "  1.5023808479309082,\n",
              "  1.4418715238571167,\n",
              "  1.646225929260254,\n",
              "  1.6236518621444702,\n",
              "  1.4879698753356934,\n",
              "  1.5877741575241089,\n",
              "  1.5510914325714111,\n",
              "  1.5578519105911255],\n",
              " 'val_acc': [0.09375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.078125,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.078125,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.078125,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.078125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.078125,\n",
              "  0.09375,\n",
              "  0.140625,\n",
              "  0.0625,\n",
              "  0.125,\n",
              "  0.078125,\n",
              "  0.078125,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.0625,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.15625,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.09375,\n",
              "  0.109375,\n",
              "  0.09375,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.078125,\n",
              "  0.171875,\n",
              "  0.09375,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.09375,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.109375,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.0625,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.109375,\n",
              "  0.125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.109375,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.109375,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.109375,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.125,\n",
              "  0.21875,\n",
              "  0.140625,\n",
              "  0.125,\n",
              "  0.203125,\n",
              "  0.109375,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.125,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.09375,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.125,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.25,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.15625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "c9ebadfd-bb44-4bab-8194-8588f9d77a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "189d9451-1599-414d-dc74-8b8be5f2fc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e6942e8-b3fe-467f-9530-77e41db7fad9\", \"newdata_SEM1.h5\", 16615536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}