{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/DEg5Q7eO54RkKCYU21Ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train_drop0_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "ea35bfd9-b5e0-419f-ad9d-ca5e6c458d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "2d14eddb-9b67-4206-eea2-d0de47cd2677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "840  841  1-s2.0-S2095268622000210-main   \n",
              "841  842  1-s2.0-S2095268622000210-main   \n",
              "842  843  1-s2.0-S2095268622000210-main   \n",
              "843  844  1-s2.0-S2095268622000210-main   \n",
              "844  845  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "840  Integration of preparation of K, Na-embedded a...   \n",
              "841  Integration of preparation of K, Na-embedded a...   \n",
              "842  Integration of preparation of K, Na-embedded a...   \n",
              "843  Integration of preparation of K, Na-embedded a...   \n",
              "844  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "840  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "841  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "842  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "843  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "844  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture    detail  Class  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...  original  0-500   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom1  0-500   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom2  0-500   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom3  0-500   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom4  0-500   \n",
              "..                                                 ...       ...    ...   \n",
              "840  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom21  0-500   \n",
              "841  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom22  0-500   \n",
              "842  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom23  0-500   \n",
              "843  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom24  0-500   \n",
              "844  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom25  0-500   \n",
              "\n",
              "        BET  Size(mico)  \n",
              "0    135.06           5  \n",
              "1    135.06          10  \n",
              "2    135.06          10  \n",
              "3    135.06          10  \n",
              "4    135.06          10  \n",
              "..      ...         ...  \n",
              "840  301.70          10  \n",
              "841  301.70          10  \n",
              "842  301.70          10  \n",
              "843  301.70          10  \n",
              "844  301.70          10  \n",
              "\n",
              "[845 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea3ae546-15ed-475d-8c6e-259b3170b0ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>841</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>842</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>843</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>844</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>845</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>845 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea3ae546-15ed-475d-8c6e-259b3170b0ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea3ae546-15ed-475d-8c6e-259b3170b0ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea3ae546-15ed-475d-8c6e-259b3170b0ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "726b321e-3d21-423f-81ca-5a3cee7b3089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "a4a046c2-d950-4933-baaa-f4e99d507205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "e8dd2015-a93e-413b-dc47-b3c51b8f2de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "5bd45ab8-312f-4c4b-f53e-90ff4a3ac4e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 680, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 680 (delta 160), reused 160 (delta 136), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (680/680), 12.25 MiB | 32.84 MiB/s, done.\n",
            "Resolving deltas: 100% (400/400), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)]  \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "01747a7e-afc2-40c7-da93-a9d9f16c1159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 98\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 71\n",
            "total test 2 images: 6\n",
            "total test 3 images: 46 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 681  # จำนวนภาพ Train\n",
        "NUM_TEST = 82 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.4\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "4ff9b81d-b8c0-4121-ceb1-ff3e01791485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "2ed181fc-0192-4ef7-f078-6c1efd296c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "9d76a5ac-babb-467d-d128-1dbc335dd1c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "caa3004f-bfcc-4a92-dd9f-78462aaf8ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "98588082-a8cf-4596-c3d9-f5ca0f7ce2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 120 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d776e958-faf9-4c83-dc1a-47b0bf8838b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 23s 981ms/step - loss: 1.8107 - acc: 0.3987 - val_loss: 1.6325 - val_acc: 0.1875\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 4s 310ms/step - loss: 1.9065 - acc: 0.3809 - val_loss: 1.6087 - val_acc: 0.1406\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.8535 - acc: 0.3874 - val_loss: 1.5799 - val_acc: 0.2031\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.6960 - acc: 0.4036 - val_loss: 1.5218 - val_acc: 0.1875\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.9395 - acc: 0.3549 - val_loss: 1.4041 - val_acc: 0.2188\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 3s 187ms/step - loss: 1.9326 - acc: 0.3688 - val_loss: 1.4676 - val_acc: 0.2344\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.8296 - acc: 0.3663 - val_loss: 1.3109 - val_acc: 0.2500\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.6671 - acc: 0.4117 - val_loss: 1.4387 - val_acc: 0.2344\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.8023 - acc: 0.4133 - val_loss: 1.3659 - val_acc: 0.2656\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.7545 - acc: 0.3728 - val_loss: 1.4090 - val_acc: 0.2812\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.8592 - acc: 0.3793 - val_loss: 1.3336 - val_acc: 0.2969\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.8094 - acc: 0.3776 - val_loss: 1.3629 - val_acc: 0.2656\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.8199 - acc: 0.4100 - val_loss: 1.3950 - val_acc: 0.2500\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.8421 - acc: 0.3582 - val_loss: 1.4394 - val_acc: 0.2344\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.7764 - acc: 0.3922 - val_loss: 1.2896 - val_acc: 0.3594\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.7156 - acc: 0.3955 - val_loss: 1.3009 - val_acc: 0.3125\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.6703 - acc: 0.4117 - val_loss: 1.3660 - val_acc: 0.2969\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.7451 - acc: 0.3938 - val_loss: 1.2834 - val_acc: 0.2812\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.7420 - acc: 0.3809 - val_loss: 1.2661 - val_acc: 0.3125\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.7311 - acc: 0.4036 - val_loss: 1.2923 - val_acc: 0.2500\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.7058 - acc: 0.4052 - val_loss: 1.3944 - val_acc: 0.2031\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.7344 - acc: 0.3971 - val_loss: 1.2084 - val_acc: 0.2812\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.7397 - acc: 0.3825 - val_loss: 1.2702 - val_acc: 0.2500\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.8194 - acc: 0.3630 - val_loss: 1.3337 - val_acc: 0.3125\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.7345 - acc: 0.3857 - val_loss: 1.3586 - val_acc: 0.2656\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.6949 - acc: 0.4031 - val_loss: 1.2214 - val_acc: 0.3281\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.7743 - acc: 0.4068 - val_loss: 1.3561 - val_acc: 0.2812\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.7057 - acc: 0.4182 - val_loss: 1.3637 - val_acc: 0.2656\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.8002 - acc: 0.3987 - val_loss: 1.3228 - val_acc: 0.2969\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.6757 - acc: 0.4117 - val_loss: 1.2607 - val_acc: 0.3438\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.6716 - acc: 0.4198 - val_loss: 1.2403 - val_acc: 0.3594\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.7976 - acc: 0.3987 - val_loss: 1.3384 - val_acc: 0.3125\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.6183 - acc: 0.4078 - val_loss: 1.2288 - val_acc: 0.3438\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.6972 - acc: 0.3793 - val_loss: 1.3234 - val_acc: 0.2812\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.7888 - acc: 0.3728 - val_loss: 1.2826 - val_acc: 0.3438\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.7266 - acc: 0.3955 - val_loss: 1.2634 - val_acc: 0.2812\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.7996 - acc: 0.3906 - val_loss: 1.2743 - val_acc: 0.3125\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.6343 - acc: 0.3938 - val_loss: 1.2241 - val_acc: 0.3281\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.7071 - acc: 0.4003 - val_loss: 1.1730 - val_acc: 0.3750\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.8017 - acc: 0.3776 - val_loss: 1.2737 - val_acc: 0.2500\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.7111 - acc: 0.3874 - val_loss: 1.2458 - val_acc: 0.3438\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.7966 - acc: 0.3712 - val_loss: 1.1419 - val_acc: 0.3438\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.6705 - acc: 0.4214 - val_loss: 1.2220 - val_acc: 0.2969\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5943 - acc: 0.4295 - val_loss: 1.0972 - val_acc: 0.4062\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.7122 - acc: 0.4068 - val_loss: 1.1496 - val_acc: 0.4219\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6743 - acc: 0.3938 - val_loss: 1.2552 - val_acc: 0.3438\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.6831 - acc: 0.4219 - val_loss: 1.1554 - val_acc: 0.3750\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.6105 - acc: 0.4408 - val_loss: 1.1845 - val_acc: 0.3438\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.7419 - acc: 0.3857 - val_loss: 1.2278 - val_acc: 0.3281\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.7355 - acc: 0.3955 - val_loss: 1.3002 - val_acc: 0.2812\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6410 - acc: 0.3875 - val_loss: 1.2364 - val_acc: 0.3750\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.6946 - acc: 0.3760 - val_loss: 1.1787 - val_acc: 0.4062\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.6591 - acc: 0.4019 - val_loss: 1.1981 - val_acc: 0.3438\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.7404 - acc: 0.3971 - val_loss: 1.1648 - val_acc: 0.3594\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.5498 - acc: 0.4198 - val_loss: 1.0929 - val_acc: 0.4375\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.6558 - acc: 0.3809 - val_loss: 1.1548 - val_acc: 0.3281\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6160 - acc: 0.3969 - val_loss: 1.0961 - val_acc: 0.4062\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.6545 - acc: 0.4117 - val_loss: 1.2849 - val_acc: 0.2812\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.6192 - acc: 0.4198 - val_loss: 1.1501 - val_acc: 0.3750\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6926 - acc: 0.4019 - val_loss: 1.2351 - val_acc: 0.3438\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.5352 - acc: 0.4538 - val_loss: 1.0811 - val_acc: 0.4531\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.6658 - acc: 0.4133 - val_loss: 1.1588 - val_acc: 0.3594\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.7401 - acc: 0.3955 - val_loss: 1.2129 - val_acc: 0.4219\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.6753 - acc: 0.4100 - val_loss: 1.1575 - val_acc: 0.3594\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.5197 - acc: 0.4263 - val_loss: 1.1718 - val_acc: 0.3594\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 2s 192ms/step - loss: 1.6847 - acc: 0.4182 - val_loss: 1.2069 - val_acc: 0.3281\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.6618 - acc: 0.3971 - val_loss: 1.1797 - val_acc: 0.3438\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.5804 - acc: 0.3875 - val_loss: 1.2714 - val_acc: 0.2969\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.5431 - acc: 0.4182 - val_loss: 1.0770 - val_acc: 0.4219\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.5570 - acc: 0.4360 - val_loss: 1.1372 - val_acc: 0.3906\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.5806 - acc: 0.4003 - val_loss: 1.2098 - val_acc: 0.2812\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.4915 - acc: 0.4344 - val_loss: 1.2153 - val_acc: 0.3438\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.6408 - acc: 0.3938 - val_loss: 1.2502 - val_acc: 0.3281\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.4874 - acc: 0.4279 - val_loss: 1.1621 - val_acc: 0.4062\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.6058 - acc: 0.3955 - val_loss: 1.1442 - val_acc: 0.3906\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.7052 - acc: 0.3971 - val_loss: 1.1853 - val_acc: 0.2500\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.6226 - acc: 0.4182 - val_loss: 1.1606 - val_acc: 0.3125\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.4946 - acc: 0.4425 - val_loss: 1.0579 - val_acc: 0.4219\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5312 - acc: 0.4441 - val_loss: 1.0869 - val_acc: 0.4219\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.5915 - acc: 0.4084 - val_loss: 1.1112 - val_acc: 0.3281\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.5298 - acc: 0.4360 - val_loss: 1.1037 - val_acc: 0.3906\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5372 - acc: 0.4182 - val_loss: 1.1094 - val_acc: 0.3906\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5953 - acc: 0.4408 - val_loss: 1.0882 - val_acc: 0.3750\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6277 - acc: 0.4125 - val_loss: 1.2266 - val_acc: 0.2969\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5702 - acc: 0.4457 - val_loss: 1.1090 - val_acc: 0.3906\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5266 - acc: 0.4327 - val_loss: 1.2156 - val_acc: 0.3125\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5865 - acc: 0.4263 - val_loss: 1.1381 - val_acc: 0.3281\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5565 - acc: 0.4327 - val_loss: 1.2001 - val_acc: 0.3438\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5926 - acc: 0.4214 - val_loss: 1.1459 - val_acc: 0.2969\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.7373 - acc: 0.3841 - val_loss: 1.1588 - val_acc: 0.3750\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.5689 - acc: 0.4506 - val_loss: 1.0952 - val_acc: 0.3906\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.5824 - acc: 0.3953 - val_loss: 1.1323 - val_acc: 0.3906\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.5351 - acc: 0.4392 - val_loss: 1.1254 - val_acc: 0.4062\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.4108 - acc: 0.4716 - val_loss: 1.2069 - val_acc: 0.2500\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 1.3845 - acc: 0.4635 - val_loss: 1.1505 - val_acc: 0.2969\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4978 - acc: 0.4489 - val_loss: 1.1797 - val_acc: 0.3438\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.5884 - acc: 0.4279 - val_loss: 1.1665 - val_acc: 0.3906\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5067 - acc: 0.4279 - val_loss: 1.0284 - val_acc: 0.4062\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5133 - acc: 0.4376 - val_loss: 1.1281 - val_acc: 0.3750\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.5960 - acc: 0.3874 - val_loss: 1.1272 - val_acc: 0.3438\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4995 - acc: 0.4182 - val_loss: 1.0772 - val_acc: 0.3594\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.5635 - acc: 0.4684 - val_loss: 1.1604 - val_acc: 0.4219\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5209 - acc: 0.4149 - val_loss: 1.1937 - val_acc: 0.2969\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4523 - acc: 0.4425 - val_loss: 1.2194 - val_acc: 0.2656\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4987 - acc: 0.4619 - val_loss: 1.2139 - val_acc: 0.2969\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4793 - acc: 0.4246 - val_loss: 1.0906 - val_acc: 0.3906\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.5096 - acc: 0.4263 - val_loss: 1.2316 - val_acc: 0.2656\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5443 - acc: 0.4311 - val_loss: 1.1876 - val_acc: 0.2969\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5724 - acc: 0.4187 - val_loss: 1.1245 - val_acc: 0.4219\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4810 - acc: 0.4441 - val_loss: 1.1529 - val_acc: 0.3750\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 2s 150ms/step - loss: 1.6391 - acc: 0.4198 - val_loss: 1.2165 - val_acc: 0.2812\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.6214 - acc: 0.3744 - val_loss: 1.1483 - val_acc: 0.3594\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5632 - acc: 0.4214 - val_loss: 1.1318 - val_acc: 0.3594\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5000 - acc: 0.4295 - val_loss: 1.0869 - val_acc: 0.3594\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5038 - acc: 0.4279 - val_loss: 1.1794 - val_acc: 0.3281\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.5212 - acc: 0.4473 - val_loss: 1.1147 - val_acc: 0.2969\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4329 - acc: 0.4408 - val_loss: 1.1292 - val_acc: 0.3906\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.5542 - acc: 0.4117 - val_loss: 1.1114 - val_acc: 0.3594\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5823 - acc: 0.4133 - val_loss: 1.1949 - val_acc: 0.3125\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.5256 - acc: 0.4392 - val_loss: 1.1486 - val_acc: 0.3906\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.5171 - acc: 0.4125 - val_loss: 1.2530 - val_acc: 0.2812\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4540 - acc: 0.4554 - val_loss: 1.1917 - val_acc: 0.3594\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.4156 - acc: 0.4603 - val_loss: 1.0862 - val_acc: 0.3750\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4745 - acc: 0.4422 - val_loss: 1.0926 - val_acc: 0.2969\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4460 - acc: 0.4781 - val_loss: 1.1748 - val_acc: 0.3438\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.5231 - acc: 0.4425 - val_loss: 1.1068 - val_acc: 0.3438\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.4578 - acc: 0.4652 - val_loss: 1.1176 - val_acc: 0.3125\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.4614 - acc: 0.4668 - val_loss: 1.0324 - val_acc: 0.4219\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4706 - acc: 0.4441 - val_loss: 1.1178 - val_acc: 0.3438\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4314 - acc: 0.4376 - val_loss: 1.0575 - val_acc: 0.3750\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3921 - acc: 0.4749 - val_loss: 1.1036 - val_acc: 0.3281\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3838 - acc: 0.4587 - val_loss: 1.1598 - val_acc: 0.3438\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.4925 - acc: 0.4425 - val_loss: 1.1601 - val_acc: 0.3125\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.4052 - acc: 0.4684 - val_loss: 1.1512 - val_acc: 0.2656\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5660 - acc: 0.3938 - val_loss: 1.2274 - val_acc: 0.3125\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3392 - acc: 0.4734 - val_loss: 1.2227 - val_acc: 0.2969\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5111 - acc: 0.4441 - val_loss: 1.2446 - val_acc: 0.3125\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4845 - acc: 0.4392 - val_loss: 1.1469 - val_acc: 0.2188\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4383 - acc: 0.4571 - val_loss: 1.0735 - val_acc: 0.3438\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4028 - acc: 0.4571 - val_loss: 1.2222 - val_acc: 0.2500\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3878 - acc: 0.4878 - val_loss: 1.1039 - val_acc: 0.3125\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.5271 - acc: 0.4359 - val_loss: 1.2505 - val_acc: 0.2500\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3962 - acc: 0.4781 - val_loss: 1.0890 - val_acc: 0.2656\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5061 - acc: 0.4506 - val_loss: 1.0767 - val_acc: 0.3750\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3994 - acc: 0.4554 - val_loss: 1.0513 - val_acc: 0.3594\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3766 - acc: 0.4765 - val_loss: 1.1318 - val_acc: 0.3750\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3418 - acc: 0.4943 - val_loss: 1.2689 - val_acc: 0.2500\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.5201 - acc: 0.4279 - val_loss: 1.1204 - val_acc: 0.2969\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4875 - acc: 0.4437 - val_loss: 1.2358 - val_acc: 0.2969\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5084 - acc: 0.4376 - val_loss: 1.1725 - val_acc: 0.3125\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.3981 - acc: 0.4688 - val_loss: 1.0631 - val_acc: 0.4531\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.4789 - acc: 0.4425 - val_loss: 1.2175 - val_acc: 0.2969\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4750 - acc: 0.4635 - val_loss: 1.1689 - val_acc: 0.2656\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.5240 - acc: 0.4538 - val_loss: 1.1157 - val_acc: 0.3438\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.4682 - acc: 0.4392 - val_loss: 1.2018 - val_acc: 0.3594\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.4540 - acc: 0.4392 - val_loss: 1.1814 - val_acc: 0.3438\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.4849 - acc: 0.4327 - val_loss: 1.1917 - val_acc: 0.2500\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5184 - acc: 0.4538 - val_loss: 1.1710 - val_acc: 0.2969\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5627 - acc: 0.4392 - val_loss: 1.1554 - val_acc: 0.3438\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.3864 - acc: 0.4587 - val_loss: 1.1578 - val_acc: 0.3438\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.3912 - acc: 0.4992 - val_loss: 1.1786 - val_acc: 0.2969\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3790 - acc: 0.4765 - val_loss: 1.1161 - val_acc: 0.3906\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.3474 - acc: 0.4862 - val_loss: 1.0005 - val_acc: 0.4688\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3435 - acc: 0.4797 - val_loss: 1.0995 - val_acc: 0.4062\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.4093 - acc: 0.4684 - val_loss: 1.0704 - val_acc: 0.3906\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3428 - acc: 0.4489 - val_loss: 1.2807 - val_acc: 0.2969\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.4739 - acc: 0.4328 - val_loss: 1.1634 - val_acc: 0.4062\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4160 - acc: 0.4441 - val_loss: 1.0233 - val_acc: 0.3906\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.4458 - acc: 0.4668 - val_loss: 1.0698 - val_acc: 0.4062\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3916 - acc: 0.4700 - val_loss: 1.1151 - val_acc: 0.3750\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3851 - acc: 0.4846 - val_loss: 1.1247 - val_acc: 0.3125\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3722 - acc: 0.4571 - val_loss: 1.0089 - val_acc: 0.4062\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3666 - acc: 0.4814 - val_loss: 1.2121 - val_acc: 0.3125\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.4273 - acc: 0.4408 - val_loss: 1.2165 - val_acc: 0.2812\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3651 - acc: 0.4538 - val_loss: 1.1253 - val_acc: 0.2969\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.4587 - acc: 0.4473 - val_loss: 1.1092 - val_acc: 0.3594\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4449 - acc: 0.4441 - val_loss: 1.1531 - val_acc: 0.3281\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3726 - acc: 0.4652 - val_loss: 1.0889 - val_acc: 0.3438\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2720 - acc: 0.4891 - val_loss: 1.1071 - val_acc: 0.3594\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3766 - acc: 0.4684 - val_loss: 1.0901 - val_acc: 0.3906\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4519 - acc: 0.4635 - val_loss: 1.1621 - val_acc: 0.3438\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.3565 - acc: 0.4733 - val_loss: 1.1427 - val_acc: 0.3281\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3994 - acc: 0.4700 - val_loss: 1.1847 - val_acc: 0.2969\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3919 - acc: 0.4684 - val_loss: 1.2842 - val_acc: 0.2812\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3588 - acc: 0.4862 - val_loss: 1.1321 - val_acc: 0.2969\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.4251 - acc: 0.4587 - val_loss: 1.1247 - val_acc: 0.3594\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4043 - acc: 0.4684 - val_loss: 1.1843 - val_acc: 0.3438\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4035 - acc: 0.4797 - val_loss: 1.2005 - val_acc: 0.2812\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.4095 - acc: 0.4716 - val_loss: 1.1872 - val_acc: 0.3281\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3560 - acc: 0.4797 - val_loss: 1.0690 - val_acc: 0.3125\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4452 - acc: 0.4506 - val_loss: 1.1697 - val_acc: 0.2812\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.3903 - acc: 0.4716 - val_loss: 1.0778 - val_acc: 0.3750\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3855 - acc: 0.4668 - val_loss: 1.1979 - val_acc: 0.2969\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.4299 - acc: 0.4344 - val_loss: 1.1796 - val_acc: 0.3281\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3616 - acc: 0.4571 - val_loss: 1.2638 - val_acc: 0.2812\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.4434 - acc: 0.4684 - val_loss: 1.1713 - val_acc: 0.3281\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3891 - acc: 0.4684 - val_loss: 1.1016 - val_acc: 0.3281\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3189 - acc: 0.4878 - val_loss: 1.1236 - val_acc: 0.3594\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 1.2688 - acc: 0.4668 - val_loss: 1.0949 - val_acc: 0.3594\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2568 - acc: 0.4927 - val_loss: 1.1213 - val_acc: 0.3125\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4264 - acc: 0.4554 - val_loss: 1.0419 - val_acc: 0.3594\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3068 - acc: 0.4652 - val_loss: 1.1418 - val_acc: 0.3750\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.3217 - acc: 0.4911 - val_loss: 0.9665 - val_acc: 0.4531\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3176 - acc: 0.4652 - val_loss: 1.1678 - val_acc: 0.3281\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3323 - acc: 0.4797 - val_loss: 1.0712 - val_acc: 0.3906\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.4470 - acc: 0.4182 - val_loss: 1.0739 - val_acc: 0.3906\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 2s 192ms/step - loss: 1.4849 - acc: 0.4522 - val_loss: 1.1591 - val_acc: 0.3750\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3905 - acc: 0.4684 - val_loss: 1.2087 - val_acc: 0.2656\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3947 - acc: 0.4797 - val_loss: 1.0559 - val_acc: 0.3906\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.3859 - acc: 0.4668 - val_loss: 1.1350 - val_acc: 0.2969\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.4253 - acc: 0.4652 - val_loss: 1.0897 - val_acc: 0.3438\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.3609 - acc: 0.4587 - val_loss: 1.0998 - val_acc: 0.3438\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3775 - acc: 0.4814 - val_loss: 1.0672 - val_acc: 0.4062\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3401 - acc: 0.4716 - val_loss: 1.1484 - val_acc: 0.3438\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3724 - acc: 0.4700 - val_loss: 1.0445 - val_acc: 0.3438\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2830 - acc: 0.4814 - val_loss: 1.1886 - val_acc: 0.2500\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.4190 - acc: 0.4506 - val_loss: 1.2209 - val_acc: 0.2969\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2983 - acc: 0.5031 - val_loss: 1.2230 - val_acc: 0.2812\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4425 - acc: 0.4603 - val_loss: 1.1776 - val_acc: 0.2812\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.3786 - acc: 0.4765 - val_loss: 1.2277 - val_acc: 0.2969\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4562 - acc: 0.4554 - val_loss: 1.1059 - val_acc: 0.3438\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.3199 - acc: 0.4895 - val_loss: 1.2064 - val_acc: 0.3125\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2678 - acc: 0.5122 - val_loss: 1.2066 - val_acc: 0.2969\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3075 - acc: 0.4733 - val_loss: 1.2709 - val_acc: 0.2969\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.3464 - acc: 0.4652 - val_loss: 1.1053 - val_acc: 0.3750\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.3304 - acc: 0.4797 - val_loss: 1.1872 - val_acc: 0.3281\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2819 - acc: 0.4943 - val_loss: 1.2470 - val_acc: 0.2812\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2406 - acc: 0.5105 - val_loss: 1.2428 - val_acc: 0.2500\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3111 - acc: 0.4830 - val_loss: 1.1575 - val_acc: 0.3750\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.3550 - acc: 0.4603 - val_loss: 1.1741 - val_acc: 0.2969\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.2731 - acc: 0.4846 - val_loss: 1.1386 - val_acc: 0.3594\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3989 - acc: 0.4716 - val_loss: 1.1958 - val_acc: 0.2969\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3749 - acc: 0.4716 - val_loss: 1.1157 - val_acc: 0.4062\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.3751 - acc: 0.4922 - val_loss: 1.1175 - val_acc: 0.3281\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.3638 - acc: 0.4700 - val_loss: 1.1857 - val_acc: 0.2812\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.3707 - acc: 0.4846 - val_loss: 1.1101 - val_acc: 0.3438\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3252 - acc: 0.4716 - val_loss: 1.0898 - val_acc: 0.3125\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3757 - acc: 0.4846 - val_loss: 1.1808 - val_acc: 0.3281\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3782 - acc: 0.4719 - val_loss: 1.2003 - val_acc: 0.2969\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3285 - acc: 0.4684 - val_loss: 1.2138 - val_acc: 0.2969\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2778 - acc: 0.5073 - val_loss: 0.9784 - val_acc: 0.4062\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2849 - acc: 0.4830 - val_loss: 1.2594 - val_acc: 0.2344\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3050 - acc: 0.4797 - val_loss: 1.2766 - val_acc: 0.2656\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.3303 - acc: 0.4943 - val_loss: 1.1459 - val_acc: 0.3125\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.2737 - acc: 0.5089 - val_loss: 1.2441 - val_acc: 0.2969\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.3233 - acc: 0.4846 - val_loss: 1.0641 - val_acc: 0.3906\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.3745 - acc: 0.4781 - val_loss: 1.1978 - val_acc: 0.2812\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2865 - acc: 0.4652 - val_loss: 1.1478 - val_acc: 0.3438\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.3527 - acc: 0.4733 - val_loss: 1.1868 - val_acc: 0.2500\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2800 - acc: 0.4862 - val_loss: 1.1816 - val_acc: 0.3438\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3150 - acc: 0.4781 - val_loss: 1.1014 - val_acc: 0.3750\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3414 - acc: 0.4911 - val_loss: 1.1142 - val_acc: 0.3594\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4043 - acc: 0.4668 - val_loss: 1.1791 - val_acc: 0.3438\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3004 - acc: 0.4716 - val_loss: 1.1816 - val_acc: 0.3750\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.3340 - acc: 0.4781 - val_loss: 1.1507 - val_acc: 0.2969\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2629 - acc: 0.4828 - val_loss: 1.0858 - val_acc: 0.3906\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3054 - acc: 0.4668 - val_loss: 1.1535 - val_acc: 0.3594\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2987 - acc: 0.4976 - val_loss: 1.1944 - val_acc: 0.3281\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2077 - acc: 0.5170 - val_loss: 1.0121 - val_acc: 0.4062\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3161 - acc: 0.5008 - val_loss: 1.2119 - val_acc: 0.3281\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3662 - acc: 0.4719 - val_loss: 1.1884 - val_acc: 0.3594\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.2387 - acc: 0.5138 - val_loss: 1.1162 - val_acc: 0.3750\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3283 - acc: 0.4749 - val_loss: 1.1394 - val_acc: 0.3281\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3762 - acc: 0.4473 - val_loss: 1.0946 - val_acc: 0.3594\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2531 - acc: 0.4959 - val_loss: 1.1143 - val_acc: 0.3906\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.2688 - acc: 0.4927 - val_loss: 1.1755 - val_acc: 0.3281\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1821 - acc: 0.5234 - val_loss: 1.1033 - val_acc: 0.3125\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2613 - acc: 0.5057 - val_loss: 1.0243 - val_acc: 0.4531\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.2741 - acc: 0.4668 - val_loss: 1.1446 - val_acc: 0.3750\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2788 - acc: 0.4684 - val_loss: 1.1901 - val_acc: 0.2969\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2899 - acc: 0.4927 - val_loss: 1.1388 - val_acc: 0.3750\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.3504 - acc: 0.4814 - val_loss: 1.0657 - val_acc: 0.4062\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3085 - acc: 0.4635 - val_loss: 1.1898 - val_acc: 0.3281\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2082 - acc: 0.5235 - val_loss: 1.1206 - val_acc: 0.3438\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2584 - acc: 0.5105 - val_loss: 1.2280 - val_acc: 0.3281\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2984 - acc: 0.4668 - val_loss: 1.0662 - val_acc: 0.3281\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2643 - acc: 0.4814 - val_loss: 1.2729 - val_acc: 0.2969\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2984 - acc: 0.4922 - val_loss: 1.1437 - val_acc: 0.3281\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3073 - acc: 0.5024 - val_loss: 1.1821 - val_acc: 0.3438\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3100 - acc: 0.4943 - val_loss: 1.1661 - val_acc: 0.4062\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2740 - acc: 0.4878 - val_loss: 1.2160 - val_acc: 0.3906\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2600 - acc: 0.4959 - val_loss: 1.2004 - val_acc: 0.4062\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.3261 - acc: 0.4765 - val_loss: 1.1414 - val_acc: 0.3906\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2174 - acc: 0.5138 - val_loss: 1.2542 - val_acc: 0.3438\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2996 - acc: 0.4846 - val_loss: 1.1463 - val_acc: 0.3906\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.2790 - acc: 0.4992 - val_loss: 1.1878 - val_acc: 0.4062\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3615 - acc: 0.4814 - val_loss: 1.1557 - val_acc: 0.3750\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3050 - acc: 0.4765 - val_loss: 1.2929 - val_acc: 0.3125\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1688 - acc: 0.5511 - val_loss: 1.1357 - val_acc: 0.3906\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2906 - acc: 0.4969 - val_loss: 1.0785 - val_acc: 0.3750\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.2824 - acc: 0.4862 - val_loss: 1.2213 - val_acc: 0.3125\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2746 - acc: 0.4703 - val_loss: 1.0611 - val_acc: 0.3906\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.2477 - acc: 0.5109 - val_loss: 0.9461 - val_acc: 0.4688\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2049 - acc: 0.4862 - val_loss: 1.1676 - val_acc: 0.3125\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3190 - acc: 0.4538 - val_loss: 1.1802 - val_acc: 0.3125\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2394 - acc: 0.5186 - val_loss: 1.2012 - val_acc: 0.3906\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.2354 - acc: 0.4895 - val_loss: 1.1783 - val_acc: 0.3750\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3103 - acc: 0.4895 - val_loss: 1.1045 - val_acc: 0.3906\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2418 - acc: 0.4927 - val_loss: 1.1707 - val_acc: 0.3594\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2751 - acc: 0.4700 - val_loss: 1.2161 - val_acc: 0.3594\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.2480 - acc: 0.4878 - val_loss: 1.2482 - val_acc: 0.3438\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.3318 - acc: 0.4911 - val_loss: 1.2966 - val_acc: 0.2969\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2031 - acc: 0.5024 - val_loss: 1.2552 - val_acc: 0.3281\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1544 - acc: 0.5381 - val_loss: 1.1533 - val_acc: 0.3906\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2608 - acc: 0.5089 - val_loss: 1.2728 - val_acc: 0.2969\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.2376 - acc: 0.5057 - val_loss: 1.1402 - val_acc: 0.3750\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.2625 - acc: 0.5041 - val_loss: 1.3324 - val_acc: 0.2344\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2714 - acc: 0.4976 - val_loss: 1.0981 - val_acc: 0.3750\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.2159 - acc: 0.5300 - val_loss: 1.2541 - val_acc: 0.2656\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1491 - acc: 0.5381 - val_loss: 1.2574 - val_acc: 0.2500\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2247 - acc: 0.5267 - val_loss: 1.2469 - val_acc: 0.3281\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1965 - acc: 0.5284 - val_loss: 1.3560 - val_acc: 0.2500\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.2894 - acc: 0.4830 - val_loss: 1.1777 - val_acc: 0.2812\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.2627 - acc: 0.4927 - val_loss: 1.1463 - val_acc: 0.4062\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2117 - acc: 0.4862 - val_loss: 1.1280 - val_acc: 0.3750\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1150 - acc: 0.5154 - val_loss: 1.1222 - val_acc: 0.3594\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.2275 - acc: 0.5267 - val_loss: 1.2989 - val_acc: 0.3281\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.1718 - acc: 0.5365 - val_loss: 1.2813 - val_acc: 0.2656\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2577 - acc: 0.5219 - val_loss: 1.3069 - val_acc: 0.3594\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 1.2540 - acc: 0.5008 - val_loss: 1.1961 - val_acc: 0.3438\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1578 - acc: 0.5154 - val_loss: 1.2849 - val_acc: 0.2500\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2253 - acc: 0.5089 - val_loss: 1.2235 - val_acc: 0.3438\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.2080 - acc: 0.5203 - val_loss: 1.1953 - val_acc: 0.3438\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2498 - acc: 0.4943 - val_loss: 1.0757 - val_acc: 0.3906\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1981 - acc: 0.4976 - val_loss: 1.2258 - val_acc: 0.3750\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2107 - acc: 0.5024 - val_loss: 1.1926 - val_acc: 0.3750\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2548 - acc: 0.4992 - val_loss: 1.1441 - val_acc: 0.3438\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2007 - acc: 0.5089 - val_loss: 1.1290 - val_acc: 0.3750\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1869 - acc: 0.5170 - val_loss: 1.1967 - val_acc: 0.3438\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2389 - acc: 0.4830 - val_loss: 1.1435 - val_acc: 0.3906\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.2293 - acc: 0.4684 - val_loss: 1.3124 - val_acc: 0.2969\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2793 - acc: 0.4922 - val_loss: 1.1841 - val_acc: 0.3906\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2382 - acc: 0.4992 - val_loss: 1.1483 - val_acc: 0.3125\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2919 - acc: 0.4959 - val_loss: 1.1879 - val_acc: 0.3281\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2416 - acc: 0.5203 - val_loss: 1.1985 - val_acc: 0.3438\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2176 - acc: 0.5000 - val_loss: 1.2398 - val_acc: 0.3594\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2463 - acc: 0.5235 - val_loss: 1.2603 - val_acc: 0.2969\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1604 - acc: 0.5170 - val_loss: 1.1587 - val_acc: 0.3594\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2891 - acc: 0.4992 - val_loss: 1.1253 - val_acc: 0.3750\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2719 - acc: 0.4959 - val_loss: 1.2380 - val_acc: 0.3594\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2503 - acc: 0.5348 - val_loss: 1.2012 - val_acc: 0.3281\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1464 - acc: 0.5186 - val_loss: 1.2618 - val_acc: 0.3594\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.2535 - acc: 0.4765 - val_loss: 1.2461 - val_acc: 0.3438\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1545 - acc: 0.5154 - val_loss: 1.1981 - val_acc: 0.3750\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2504 - acc: 0.4906 - val_loss: 1.2076 - val_acc: 0.3750\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2165 - acc: 0.5186 - val_loss: 1.3092 - val_acc: 0.2969\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.1868 - acc: 0.5105 - val_loss: 1.2575 - val_acc: 0.3125\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1410 - acc: 0.5267 - val_loss: 1.2416 - val_acc: 0.2344\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1642 - acc: 0.5348 - val_loss: 1.1434 - val_acc: 0.3438\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1637 - acc: 0.5316 - val_loss: 1.1520 - val_acc: 0.3906\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0923 - acc: 0.5511 - val_loss: 1.2221 - val_acc: 0.3281\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1969 - acc: 0.5154 - val_loss: 1.2047 - val_acc: 0.3906\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.1528 - acc: 0.5332 - val_loss: 1.2404 - val_acc: 0.2969\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1972 - acc: 0.5170 - val_loss: 1.1920 - val_acc: 0.2656\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1688 - acc: 0.5203 - val_loss: 1.0907 - val_acc: 0.4375\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1789 - acc: 0.5284 - val_loss: 1.2211 - val_acc: 0.3125\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.1869 - acc: 0.5170 - val_loss: 1.2442 - val_acc: 0.3438\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2173 - acc: 0.5138 - val_loss: 1.1929 - val_acc: 0.3750\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2139 - acc: 0.5041 - val_loss: 1.1828 - val_acc: 0.3125\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1888 - acc: 0.4976 - val_loss: 1.1608 - val_acc: 0.2812\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1702 - acc: 0.4862 - val_loss: 1.1747 - val_acc: 0.4062\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2408 - acc: 0.5266 - val_loss: 1.2075 - val_acc: 0.3125\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1682 - acc: 0.5251 - val_loss: 1.2496 - val_acc: 0.2812\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.1588 - acc: 0.5073 - val_loss: 1.1590 - val_acc: 0.3750\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1974 - acc: 0.5413 - val_loss: 1.0828 - val_acc: 0.4375\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1531 - acc: 0.5543 - val_loss: 1.1787 - val_acc: 0.3594\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1657 - acc: 0.5122 - val_loss: 1.2519 - val_acc: 0.2812\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1926 - acc: 0.5008 - val_loss: 1.2699 - val_acc: 0.3281\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.2190 - acc: 0.4895 - val_loss: 1.2250 - val_acc: 0.3281\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 2s 191ms/step - loss: 1.1751 - acc: 0.5041 - val_loss: 1.2302 - val_acc: 0.2969\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1672 - acc: 0.5138 - val_loss: 1.2741 - val_acc: 0.2500\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.1356 - acc: 0.5203 - val_loss: 1.1570 - val_acc: 0.3594\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1544 - acc: 0.5170 - val_loss: 1.2143 - val_acc: 0.3594\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2073 - acc: 0.5089 - val_loss: 1.1888 - val_acc: 0.3281\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1609 - acc: 0.5186 - val_loss: 1.2755 - val_acc: 0.2500\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1586 - acc: 0.5105 - val_loss: 1.0534 - val_acc: 0.4062\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1643 - acc: 0.5170 - val_loss: 1.1577 - val_acc: 0.3750\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1020 - acc: 0.5365 - val_loss: 1.2028 - val_acc: 0.3281\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1333 - acc: 0.5267 - val_loss: 1.1858 - val_acc: 0.3594\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2191 - acc: 0.5057 - val_loss: 1.2617 - val_acc: 0.3281\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1103 - acc: 0.5300 - val_loss: 1.1852 - val_acc: 0.3594\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2090 - acc: 0.4862 - val_loss: 1.1489 - val_acc: 0.3438\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1237 - acc: 0.5219 - val_loss: 1.1812 - val_acc: 0.3594\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1367 - acc: 0.5527 - val_loss: 1.3508 - val_acc: 0.2656\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2633 - acc: 0.4976 - val_loss: 1.2283 - val_acc: 0.2812\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1435 - acc: 0.5300 - val_loss: 1.2427 - val_acc: 0.3281\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1984 - acc: 0.5122 - val_loss: 1.1645 - val_acc: 0.2969\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2158 - acc: 0.5251 - val_loss: 1.2241 - val_acc: 0.3438\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1408 - acc: 0.5300 - val_loss: 1.1454 - val_acc: 0.3438\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2026 - acc: 0.5170 - val_loss: 1.1593 - val_acc: 0.3281\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1090 - acc: 0.5284 - val_loss: 1.0365 - val_acc: 0.4062\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1542 - acc: 0.5297 - val_loss: 1.2488 - val_acc: 0.3281\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1510 - acc: 0.5316 - val_loss: 1.2144 - val_acc: 0.3438\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1257 - acc: 0.5332 - val_loss: 1.1907 - val_acc: 0.2656\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1648 - acc: 0.5250 - val_loss: 1.2364 - val_acc: 0.3125\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0932 - acc: 0.5562 - val_loss: 1.0858 - val_acc: 0.3906\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2156 - acc: 0.5105 - val_loss: 1.2212 - val_acc: 0.2969\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1336 - acc: 0.5267 - val_loss: 1.2664 - val_acc: 0.2969\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1664 - acc: 0.5170 - val_loss: 1.2509 - val_acc: 0.2656\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1491 - acc: 0.5188 - val_loss: 1.1357 - val_acc: 0.4062\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1845 - acc: 0.5105 - val_loss: 1.3179 - val_acc: 0.2812\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1427 - acc: 0.5332 - val_loss: 1.2493 - val_acc: 0.3125\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1322 - acc: 0.5186 - val_loss: 1.1464 - val_acc: 0.4219\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1549 - acc: 0.5203 - val_loss: 1.2240 - val_acc: 0.2969\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0771 - acc: 0.5547 - val_loss: 1.2047 - val_acc: 0.3438\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1875 - acc: 0.5203 - val_loss: 1.2310 - val_acc: 0.2969\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1826 - acc: 0.5235 - val_loss: 1.1996 - val_acc: 0.3750\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1596 - acc: 0.5281 - val_loss: 1.3338 - val_acc: 0.2969\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2003 - acc: 0.5381 - val_loss: 1.2225 - val_acc: 0.3438\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2556 - acc: 0.5024 - val_loss: 1.2385 - val_acc: 0.3281\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1502 - acc: 0.5188 - val_loss: 1.2260 - val_acc: 0.3594\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0776 - acc: 0.5835 - val_loss: 1.2217 - val_acc: 0.3125\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0880 - acc: 0.5484 - val_loss: 0.9934 - val_acc: 0.4531\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1342 - acc: 0.5413 - val_loss: 1.1810 - val_acc: 0.3750\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2250 - acc: 0.4911 - val_loss: 1.0009 - val_acc: 0.4219\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1674 - acc: 0.5041 - val_loss: 1.1354 - val_acc: 0.3438\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.1038 - acc: 0.5413 - val_loss: 1.1400 - val_acc: 0.3750\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.1361 - acc: 0.5494 - val_loss: 1.1331 - val_acc: 0.4062\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1092 - acc: 0.5316 - val_loss: 1.1489 - val_acc: 0.3281\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 2s 191ms/step - loss: 1.1121 - acc: 0.5332 - val_loss: 1.2719 - val_acc: 0.3281\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.1324 - acc: 0.5284 - val_loss: 1.1020 - val_acc: 0.4219\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1042 - acc: 0.5381 - val_loss: 1.1306 - val_acc: 0.4062\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1424 - acc: 0.5024 - val_loss: 1.2821 - val_acc: 0.3125\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0858 - acc: 0.5348 - val_loss: 1.1510 - val_acc: 0.3125\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0488 - acc: 0.5640 - val_loss: 1.1017 - val_acc: 0.4062\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1432 - acc: 0.4943 - val_loss: 1.1228 - val_acc: 0.3906\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1824 - acc: 0.5316 - val_loss: 1.2980 - val_acc: 0.2812\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1159 - acc: 0.5397 - val_loss: 1.2667 - val_acc: 0.3125\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1474 - acc: 0.5186 - val_loss: 1.1604 - val_acc: 0.3438\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1479 - acc: 0.5446 - val_loss: 1.1209 - val_acc: 0.4219\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1687 - acc: 0.5219 - val_loss: 1.1348 - val_acc: 0.3281\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1076 - acc: 0.5186 - val_loss: 1.2901 - val_acc: 0.3125\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1195 - acc: 0.5462 - val_loss: 1.2659 - val_acc: 0.3281\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1450 - acc: 0.5656 - val_loss: 1.2769 - val_acc: 0.2656\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2135 - acc: 0.5016 - val_loss: 1.2171 - val_acc: 0.3438\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.1800 - acc: 0.5251 - val_loss: 1.1766 - val_acc: 0.3594\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0974 - acc: 0.5608 - val_loss: 1.0044 - val_acc: 0.4531\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.0247 - acc: 0.5413 - val_loss: 1.1509 - val_acc: 0.3438\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1629 - acc: 0.5089 - val_loss: 1.0838 - val_acc: 0.3750\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0851 - acc: 0.5365 - val_loss: 1.2550 - val_acc: 0.3906\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1575 - acc: 0.5219 - val_loss: 1.2872 - val_acc: 0.3281\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.1135 - acc: 0.5429 - val_loss: 1.0861 - val_acc: 0.3906\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1372 - acc: 0.5608 - val_loss: 1.2468 - val_acc: 0.2969\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1348 - acc: 0.5219 - val_loss: 1.1184 - val_acc: 0.3906\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1109 - acc: 0.5494 - val_loss: 1.2308 - val_acc: 0.3594\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1053 - acc: 0.5429 - val_loss: 1.2598 - val_acc: 0.2969\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1002 - acc: 0.5494 - val_loss: 1.1457 - val_acc: 0.3750\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0701 - acc: 0.5446 - val_loss: 1.2698 - val_acc: 0.2500\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1510 - acc: 0.5284 - val_loss: 1.1732 - val_acc: 0.2969\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0830 - acc: 0.5219 - val_loss: 1.1458 - val_acc: 0.3906\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1220 - acc: 0.5332 - val_loss: 1.2037 - val_acc: 0.3438\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1167 - acc: 0.5365 - val_loss: 1.1977 - val_acc: 0.3594\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1325 - acc: 0.5300 - val_loss: 1.2417 - val_acc: 0.3281\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2145 - acc: 0.4765 - val_loss: 1.3020 - val_acc: 0.3125\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1635 - acc: 0.5154 - val_loss: 1.2600 - val_acc: 0.3125\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0443 - acc: 0.5381 - val_loss: 1.3167 - val_acc: 0.3125\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1452 - acc: 0.5365 - val_loss: 1.1933 - val_acc: 0.3438\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1675 - acc: 0.5203 - val_loss: 1.1707 - val_acc: 0.3438\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1130 - acc: 0.4976 - val_loss: 1.2931 - val_acc: 0.3125\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1571 - acc: 0.5332 - val_loss: 1.2779 - val_acc: 0.2969\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1248 - acc: 0.5203 - val_loss: 1.1125 - val_acc: 0.3906\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0951 - acc: 0.5494 - val_loss: 1.2453 - val_acc: 0.2656\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0983 - acc: 0.5429 - val_loss: 1.2092 - val_acc: 0.2812\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1077 - acc: 0.5300 - val_loss: 1.2803 - val_acc: 0.3281\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0735 - acc: 0.5469 - val_loss: 1.1423 - val_acc: 0.3594\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0544 - acc: 0.5478 - val_loss: 1.1730 - val_acc: 0.3750\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1581 - acc: 0.5413 - val_loss: 1.1592 - val_acc: 0.3594\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0495 - acc: 0.5592 - val_loss: 1.1829 - val_acc: 0.3438\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.0600 - acc: 0.5624 - val_loss: 1.3237 - val_acc: 0.3281\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1666 - acc: 0.5332 - val_loss: 1.1143 - val_acc: 0.3750\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0188 - acc: 0.5641 - val_loss: 1.1787 - val_acc: 0.3438\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0668 - acc: 0.5592 - val_loss: 1.2313 - val_acc: 0.3125\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0542 - acc: 0.5592 - val_loss: 1.1294 - val_acc: 0.3281\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0821 - acc: 0.5559 - val_loss: 1.2250 - val_acc: 0.2656\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.0610 - acc: 0.5835 - val_loss: 1.3460 - val_acc: 0.2812\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0868 - acc: 0.5332 - val_loss: 1.2638 - val_acc: 0.2969\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1022 - acc: 0.5170 - val_loss: 1.2110 - val_acc: 0.3125\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1456 - acc: 0.5397 - val_loss: 1.2669 - val_acc: 0.3438\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0974 - acc: 0.5397 - val_loss: 1.1658 - val_acc: 0.2969\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1469 - acc: 0.5266 - val_loss: 1.2810 - val_acc: 0.2812\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0709 - acc: 0.5429 - val_loss: 1.1210 - val_acc: 0.2656\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.1135 - acc: 0.5281 - val_loss: 1.2653 - val_acc: 0.2969\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1422 - acc: 0.5089 - val_loss: 1.3327 - val_acc: 0.2031\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0829 - acc: 0.5332 - val_loss: 1.2343 - val_acc: 0.2812\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1173 - acc: 0.5592 - val_loss: 1.1759 - val_acc: 0.3438\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0583 - acc: 0.5494 - val_loss: 1.2414 - val_acc: 0.3594\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1286 - acc: 0.5105 - val_loss: 1.1774 - val_acc: 0.3281\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0366 - acc: 0.5786 - val_loss: 1.2714 - val_acc: 0.3906\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0645 - acc: 0.5154 - val_loss: 1.2790 - val_acc: 0.2969\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0788 - acc: 0.5559 - val_loss: 1.1984 - val_acc: 0.3125\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1196 - acc: 0.5348 - val_loss: 1.3261 - val_acc: 0.2812\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.1344 - acc: 0.5332 - val_loss: 1.1436 - val_acc: 0.3750\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1176 - acc: 0.5381 - val_loss: 1.3464 - val_acc: 0.2500\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0650 - acc: 0.5413 - val_loss: 1.3378 - val_acc: 0.2500\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1320 - acc: 0.5203 - val_loss: 1.1489 - val_acc: 0.4062\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1185 - acc: 0.5328 - val_loss: 1.3262 - val_acc: 0.3125\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1146 - acc: 0.5284 - val_loss: 1.2490 - val_acc: 0.3281\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1488 - acc: 0.5284 - val_loss: 1.2529 - val_acc: 0.2812\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0605 - acc: 0.5203 - val_loss: 1.3445 - val_acc: 0.2344\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0662 - acc: 0.5640 - val_loss: 1.1630 - val_acc: 0.3594\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1026 - acc: 0.5300 - val_loss: 1.2176 - val_acc: 0.3125\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0719 - acc: 0.5300 - val_loss: 1.3135 - val_acc: 0.2500\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0639 - acc: 0.5770 - val_loss: 1.1552 - val_acc: 0.3750\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1090 - acc: 0.5543 - val_loss: 1.2054 - val_acc: 0.2969\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0395 - acc: 0.5397 - val_loss: 1.1098 - val_acc: 0.3594\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0715 - acc: 0.5297 - val_loss: 1.2528 - val_acc: 0.2656\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0508 - acc: 0.5689 - val_loss: 1.1096 - val_acc: 0.3281\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0038 - acc: 0.5802 - val_loss: 1.1823 - val_acc: 0.3750\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0889 - acc: 0.5332 - val_loss: 1.2186 - val_acc: 0.3281\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0483 - acc: 0.5316 - val_loss: 1.2338 - val_acc: 0.2656\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1179 - acc: 0.5381 - val_loss: 1.2018 - val_acc: 0.3594\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0489 - acc: 0.5656 - val_loss: 1.1605 - val_acc: 0.3281\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0510 - acc: 0.5511 - val_loss: 1.0952 - val_acc: 0.3750\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1252 - acc: 0.5316 - val_loss: 1.1123 - val_acc: 0.3750\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0717 - acc: 0.5446 - val_loss: 1.2488 - val_acc: 0.2812\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1132 - acc: 0.5494 - val_loss: 1.1815 - val_acc: 0.3281\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0290 - acc: 0.5575 - val_loss: 1.2866 - val_acc: 0.2812\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0374 - acc: 0.5689 - val_loss: 1.3184 - val_acc: 0.3281\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0337 - acc: 0.5608 - val_loss: 1.2315 - val_acc: 0.3125\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1015 - acc: 0.5559 - val_loss: 1.2251 - val_acc: 0.3281\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0835 - acc: 0.5406 - val_loss: 1.3556 - val_acc: 0.3125\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1516 - acc: 0.5284 - val_loss: 1.3276 - val_acc: 0.3125\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.1033 - acc: 0.5105 - val_loss: 1.2171 - val_acc: 0.3438\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0901 - acc: 0.5429 - val_loss: 1.1110 - val_acc: 0.3594\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1101 - acc: 0.5494 - val_loss: 1.2220 - val_acc: 0.3438\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1042 - acc: 0.5559 - val_loss: 1.3610 - val_acc: 0.2344\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0609 - acc: 0.5297 - val_loss: 1.2319 - val_acc: 0.2812\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0736 - acc: 0.5478 - val_loss: 1.2075 - val_acc: 0.3438\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0146 - acc: 0.5575 - val_loss: 1.1959 - val_acc: 0.3594\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1201 - acc: 0.5348 - val_loss: 1.2129 - val_acc: 0.3281\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0207 - acc: 0.5575 - val_loss: 1.2759 - val_acc: 0.3125\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0326 - acc: 0.5689 - val_loss: 1.2117 - val_acc: 0.2812\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1018 - acc: 0.5348 - val_loss: 1.2191 - val_acc: 0.3281\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1356 - acc: 0.5251 - val_loss: 1.0865 - val_acc: 0.3750\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2043 - acc: 0.5219 - val_loss: 1.1365 - val_acc: 0.3438\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0862 - acc: 0.5365 - val_loss: 1.2266 - val_acc: 0.2812\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0472 - acc: 0.5511 - val_loss: 1.2405 - val_acc: 0.2812\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0836 - acc: 0.5365 - val_loss: 1.1685 - val_acc: 0.3594\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1073 - acc: 0.5267 - val_loss: 1.2961 - val_acc: 0.3125\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.0203 - acc: 0.5770 - val_loss: 1.1993 - val_acc: 0.3125\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0606 - acc: 0.5429 - val_loss: 1.2238 - val_acc: 0.3594\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0531 - acc: 0.5608 - val_loss: 1.1652 - val_acc: 0.3281\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0817 - acc: 0.5592 - val_loss: 1.2191 - val_acc: 0.2812\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0219 - acc: 0.5705 - val_loss: 1.2355 - val_acc: 0.3125\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0763 - acc: 0.5332 - val_loss: 1.2023 - val_acc: 0.2812\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0269 - acc: 0.5737 - val_loss: 1.1575 - val_acc: 0.3438\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0950 - acc: 0.5348 - val_loss: 1.3395 - val_acc: 0.2656\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0517 - acc: 0.5624 - val_loss: 1.1293 - val_acc: 0.3438\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0501 - acc: 0.5446 - val_loss: 1.1243 - val_acc: 0.3750\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.1283 - acc: 0.5154 - val_loss: 1.1875 - val_acc: 0.4062\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.0412 - acc: 0.5656 - val_loss: 1.0882 - val_acc: 0.3594\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.0824 - acc: 0.5397 - val_loss: 1.1969 - val_acc: 0.3125\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0166 - acc: 0.5673 - val_loss: 1.1772 - val_acc: 0.3281\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0130 - acc: 0.5754 - val_loss: 1.1449 - val_acc: 0.3594\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.0449 - acc: 0.5656 - val_loss: 1.2311 - val_acc: 0.3281\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0974 - acc: 0.5429 - val_loss: 1.2592 - val_acc: 0.2812\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0366 - acc: 0.5219 - val_loss: 1.1939 - val_acc: 0.2656\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0514 - acc: 0.5543 - val_loss: 1.1588 - val_acc: 0.3438\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0046 - acc: 0.5559 - val_loss: 1.1243 - val_acc: 0.3906\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0662 - acc: 0.5462 - val_loss: 1.1931 - val_acc: 0.3438\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0434 - acc: 0.5527 - val_loss: 1.0757 - val_acc: 0.3438\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 0.9777 - acc: 0.5818 - val_loss: 1.2149 - val_acc: 0.3125\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9551 - acc: 0.5851 - val_loss: 1.2552 - val_acc: 0.2188\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0470 - acc: 0.5559 - val_loss: 1.1602 - val_acc: 0.3750\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0944 - acc: 0.5381 - val_loss: 1.0804 - val_acc: 0.3594\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0994 - acc: 0.5348 - val_loss: 1.2018 - val_acc: 0.2812\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0498 - acc: 0.5689 - val_loss: 1.1756 - val_acc: 0.2656\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.0018 - acc: 0.5624 - val_loss: 1.1584 - val_acc: 0.3594\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0226 - acc: 0.5484 - val_loss: 1.2609 - val_acc: 0.2969\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0661 - acc: 0.5381 - val_loss: 1.2022 - val_acc: 0.2812\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0694 - acc: 0.5251 - val_loss: 1.2166 - val_acc: 0.3125\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.0595 - acc: 0.5575 - val_loss: 1.2611 - val_acc: 0.2969\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0574 - acc: 0.5656 - val_loss: 1.1758 - val_acc: 0.3281\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0182 - acc: 0.5689 - val_loss: 1.1476 - val_acc: 0.3750\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0679 - acc: 0.5527 - val_loss: 1.2762 - val_acc: 0.2969\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0640 - acc: 0.5429 - val_loss: 1.2372 - val_acc: 0.2812\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0314 - acc: 0.5705 - val_loss: 1.2393 - val_acc: 0.3125\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0145 - acc: 0.5494 - val_loss: 1.3265 - val_acc: 0.2656\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0160 - acc: 0.5575 - val_loss: 1.2526 - val_acc: 0.3281\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.0796 - acc: 0.5284 - val_loss: 1.1442 - val_acc: 0.3594\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0084 - acc: 0.5673 - val_loss: 1.1680 - val_acc: 0.3438\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0057 - acc: 0.5818 - val_loss: 1.1407 - val_acc: 0.3594\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0957 - acc: 0.5656 - val_loss: 1.2258 - val_acc: 0.3281\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9899 - acc: 0.5640 - val_loss: 1.2481 - val_acc: 0.3281\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0470 - acc: 0.5365 - val_loss: 1.1119 - val_acc: 0.3594\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0191 - acc: 0.5511 - val_loss: 1.1487 - val_acc: 0.3438\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0700 - acc: 0.5462 - val_loss: 1.0916 - val_acc: 0.3438\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0522 - acc: 0.5640 - val_loss: 1.0466 - val_acc: 0.3750\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0152 - acc: 0.5705 - val_loss: 1.2199 - val_acc: 0.2656\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0758 - acc: 0.5797 - val_loss: 1.1067 - val_acc: 0.3906\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0224 - acc: 0.5397 - val_loss: 1.3141 - val_acc: 0.2969\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9922 - acc: 0.5413 - val_loss: 1.2036 - val_acc: 0.3594\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0147 - acc: 0.5531 - val_loss: 1.2801 - val_acc: 0.3438\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0699 - acc: 0.5397 - val_loss: 1.1097 - val_acc: 0.3750\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0959 - acc: 0.5381 - val_loss: 1.1606 - val_acc: 0.2969\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0136 - acc: 0.5608 - val_loss: 1.1568 - val_acc: 0.2969\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 2s 209ms/step - loss: 0.9830 - acc: 0.5786 - val_loss: 1.2436 - val_acc: 0.3438\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0488 - acc: 0.5559 - val_loss: 1.1841 - val_acc: 0.3281\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0381 - acc: 0.5786 - val_loss: 1.2681 - val_acc: 0.3281\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0471 - acc: 0.5592 - val_loss: 1.1290 - val_acc: 0.3750\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9611 - acc: 0.5818 - val_loss: 1.1699 - val_acc: 0.3750\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0118 - acc: 0.5754 - val_loss: 1.1847 - val_acc: 0.3750\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0827 - acc: 0.5397 - val_loss: 1.0810 - val_acc: 0.4062\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0941 - acc: 0.5413 - val_loss: 1.1522 - val_acc: 0.3438\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0067 - acc: 0.5754 - val_loss: 1.2424 - val_acc: 0.2812\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0425 - acc: 0.5462 - val_loss: 1.1558 - val_acc: 0.3438\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0749 - acc: 0.5575 - val_loss: 1.1928 - val_acc: 0.3281\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.1015 - acc: 0.5235 - val_loss: 1.1363 - val_acc: 0.3438\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0199 - acc: 0.5575 - val_loss: 1.1738 - val_acc: 0.3438\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 0.9709 - acc: 0.5851 - val_loss: 1.1393 - val_acc: 0.3125\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0414 - acc: 0.5429 - val_loss: 1.1211 - val_acc: 0.3125\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9728 - acc: 0.5737 - val_loss: 1.1542 - val_acc: 0.3594\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0452 - acc: 0.5640 - val_loss: 1.1839 - val_acc: 0.3750\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0465 - acc: 0.5462 - val_loss: 1.1828 - val_acc: 0.2812\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0504 - acc: 0.5575 - val_loss: 1.1066 - val_acc: 0.3125\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9468 - acc: 0.5867 - val_loss: 1.1203 - val_acc: 0.3125\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9755 - acc: 0.5656 - val_loss: 1.2244 - val_acc: 0.3125\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9972 - acc: 0.5527 - val_loss: 1.3065 - val_acc: 0.2812\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0417 - acc: 0.5186 - val_loss: 1.2380 - val_acc: 0.3125\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0033 - acc: 0.5462 - val_loss: 1.1768 - val_acc: 0.3438\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0529 - acc: 0.5365 - val_loss: 1.0625 - val_acc: 0.4219\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0425 - acc: 0.5494 - val_loss: 1.0736 - val_acc: 0.3906\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0235 - acc: 0.5547 - val_loss: 1.0742 - val_acc: 0.3750\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.0397 - acc: 0.5802 - val_loss: 1.1632 - val_acc: 0.3750\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9677 - acc: 0.5818 - val_loss: 1.1137 - val_acc: 0.3125\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0399 - acc: 0.5381 - val_loss: 1.2036 - val_acc: 0.2812\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9936 - acc: 0.5656 - val_loss: 1.1795 - val_acc: 0.3750\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0101 - acc: 0.5446 - val_loss: 1.1545 - val_acc: 0.3438\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0599 - acc: 0.5429 - val_loss: 1.2262 - val_acc: 0.2969\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.0320 - acc: 0.5640 - val_loss: 1.1926 - val_acc: 0.2969\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0175 - acc: 0.5851 - val_loss: 1.1568 - val_acc: 0.3438\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0425 - acc: 0.5413 - val_loss: 1.2251 - val_acc: 0.3125\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9815 - acc: 0.5624 - val_loss: 1.1890 - val_acc: 0.3438\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9688 - acc: 0.6078 - val_loss: 1.1954 - val_acc: 0.3281\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0144 - acc: 0.5802 - val_loss: 1.1453 - val_acc: 0.3594\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0443 - acc: 0.5348 - val_loss: 1.2164 - val_acc: 0.3125\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0252 - acc: 0.5592 - val_loss: 1.2302 - val_acc: 0.2969\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0224 - acc: 0.5640 - val_loss: 1.1519 - val_acc: 0.3281\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9985 - acc: 0.5818 - val_loss: 1.2227 - val_acc: 0.3281\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0115 - acc: 0.5527 - val_loss: 1.1597 - val_acc: 0.3281\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0196 - acc: 0.5527 - val_loss: 1.2668 - val_acc: 0.2500\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0503 - acc: 0.5575 - val_loss: 1.2209 - val_acc: 0.2969\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0481 - acc: 0.5446 - val_loss: 1.2234 - val_acc: 0.3125\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9463 - acc: 0.5786 - val_loss: 1.2285 - val_acc: 0.2969\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0044 - acc: 0.5608 - val_loss: 1.1624 - val_acc: 0.4062\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.9818 - acc: 0.5608 - val_loss: 1.1979 - val_acc: 0.3594\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0055 - acc: 0.5922 - val_loss: 1.1941 - val_acc: 0.3438\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.9590 - acc: 0.5916 - val_loss: 1.1645 - val_acc: 0.3125\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9276 - acc: 0.6013 - val_loss: 1.1812 - val_acc: 0.3594\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0318 - acc: 0.5656 - val_loss: 1.1655 - val_acc: 0.3125\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9865 - acc: 0.5770 - val_loss: 1.2545 - val_acc: 0.3125\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9127 - acc: 0.6288 - val_loss: 1.1818 - val_acc: 0.3594\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0585 - acc: 0.5543 - val_loss: 1.0993 - val_acc: 0.3750\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0255 - acc: 0.5527 - val_loss: 1.0863 - val_acc: 0.3438\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0181 - acc: 0.5641 - val_loss: 1.3057 - val_acc: 0.2344\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9877 - acc: 0.5835 - val_loss: 1.1077 - val_acc: 0.4062\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9310 - acc: 0.5641 - val_loss: 1.2585 - val_acc: 0.2969\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9777 - acc: 0.5916 - val_loss: 1.0539 - val_acc: 0.3594\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9477 - acc: 0.5786 - val_loss: 1.1267 - val_acc: 0.3438\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0213 - acc: 0.5624 - val_loss: 1.2582 - val_acc: 0.2344\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9703 - acc: 0.5948 - val_loss: 1.1756 - val_acc: 0.3281\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0132 - acc: 0.5446 - val_loss: 1.1964 - val_acc: 0.3125\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0531 - acc: 0.5656 - val_loss: 1.2469 - val_acc: 0.2500\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0012 - acc: 0.5981 - val_loss: 1.1364 - val_acc: 0.3594\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0178 - acc: 0.5656 - val_loss: 1.2114 - val_acc: 0.3438\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9803 - acc: 0.5818 - val_loss: 1.3311 - val_acc: 0.2188\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9783 - acc: 0.5673 - val_loss: 1.2345 - val_acc: 0.3281\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0208 - acc: 0.5640 - val_loss: 1.2770 - val_acc: 0.2656\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9811 - acc: 0.6029 - val_loss: 1.2211 - val_acc: 0.3438\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0185 - acc: 0.5478 - val_loss: 1.0981 - val_acc: 0.4062\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9715 - acc: 0.5916 - val_loss: 1.2372 - val_acc: 0.3281\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9558 - acc: 0.5835 - val_loss: 1.1433 - val_acc: 0.2969\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0224 - acc: 0.5851 - val_loss: 1.2424 - val_acc: 0.3125\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 0.9426 - acc: 0.5948 - val_loss: 1.2465 - val_acc: 0.2656\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9261 - acc: 0.6256 - val_loss: 1.3267 - val_acc: 0.2812\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9765 - acc: 0.5802 - val_loss: 1.1517 - val_acc: 0.3438\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9680 - acc: 0.5900 - val_loss: 1.1909 - val_acc: 0.3438\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0042 - acc: 0.5705 - val_loss: 1.2127 - val_acc: 0.2969\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.0022 - acc: 0.5948 - val_loss: 1.1513 - val_acc: 0.3438\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9637 - acc: 0.6078 - val_loss: 1.2129 - val_acc: 0.3438\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9872 - acc: 0.5543 - val_loss: 1.2448 - val_acc: 0.2812\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9984 - acc: 0.5737 - val_loss: 1.1101 - val_acc: 0.3438\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0265 - acc: 0.5640 - val_loss: 1.1727 - val_acc: 0.3438\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0349 - acc: 0.5673 - val_loss: 1.2052 - val_acc: 0.3281\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9630 - acc: 0.5624 - val_loss: 1.2318 - val_acc: 0.3438\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0468 - acc: 0.5462 - val_loss: 1.1906 - val_acc: 0.3750\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0309 - acc: 0.5802 - val_loss: 1.2178 - val_acc: 0.2969\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9948 - acc: 0.5446 - val_loss: 1.2836 - val_acc: 0.2812\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0059 - acc: 0.5818 - val_loss: 1.2516 - val_acc: 0.2656\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9826 - acc: 0.5818 - val_loss: 1.1107 - val_acc: 0.3594\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9995 - acc: 0.5770 - val_loss: 1.1678 - val_acc: 0.3281\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9423 - acc: 0.6062 - val_loss: 1.2223 - val_acc: 0.3438\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9843 - acc: 0.5543 - val_loss: 1.1904 - val_acc: 0.2969\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9935 - acc: 0.5624 - val_loss: 1.3192 - val_acc: 0.2812\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9922 - acc: 0.5851 - val_loss: 1.1872 - val_acc: 0.3438\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9506 - acc: 0.5900 - val_loss: 1.2625 - val_acc: 0.2969\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9258 - acc: 0.5948 - val_loss: 1.2124 - val_acc: 0.3750\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 2s 213ms/step - loss: 0.9952 - acc: 0.5900 - val_loss: 1.2367 - val_acc: 0.3281\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9130 - acc: 0.6175 - val_loss: 1.3122 - val_acc: 0.2188\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9789 - acc: 0.5981 - val_loss: 1.2362 - val_acc: 0.2969\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0070 - acc: 0.5705 - val_loss: 1.2105 - val_acc: 0.3281\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9786 - acc: 0.5948 - val_loss: 1.2227 - val_acc: 0.3125\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9541 - acc: 0.5900 - val_loss: 1.2486 - val_acc: 0.2500\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9106 - acc: 0.6062 - val_loss: 1.1977 - val_acc: 0.3750\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0356 - acc: 0.5689 - val_loss: 1.1991 - val_acc: 0.2969\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9590 - acc: 0.5802 - val_loss: 1.2430 - val_acc: 0.3125\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9746 - acc: 0.5948 - val_loss: 1.2167 - val_acc: 0.2812\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9286 - acc: 0.5818 - val_loss: 1.3179 - val_acc: 0.2500\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9894 - acc: 0.5624 - val_loss: 1.2148 - val_acc: 0.2969\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9890 - acc: 0.5673 - val_loss: 1.2434 - val_acc: 0.3281\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9198 - acc: 0.5997 - val_loss: 1.2649 - val_acc: 0.3281\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 1.0058 - acc: 0.5656 - val_loss: 1.2966 - val_acc: 0.3125\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0002 - acc: 0.5770 - val_loss: 1.1666 - val_acc: 0.2969\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9537 - acc: 0.6175 - val_loss: 1.0600 - val_acc: 0.4062\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9385 - acc: 0.5656 - val_loss: 1.3223 - val_acc: 0.2812\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9592 - acc: 0.5689 - val_loss: 1.2562 - val_acc: 0.2812\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9182 - acc: 0.5851 - val_loss: 1.2265 - val_acc: 0.3281\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9743 - acc: 0.5527 - val_loss: 1.2460 - val_acc: 0.2656\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9595 - acc: 0.5867 - val_loss: 1.1731 - val_acc: 0.3438\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9498 - acc: 0.5835 - val_loss: 1.1971 - val_acc: 0.3281\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.9558 - acc: 0.5867 - val_loss: 1.1483 - val_acc: 0.3594\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9805 - acc: 0.5932 - val_loss: 1.1745 - val_acc: 0.3750\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9788 - acc: 0.5781 - val_loss: 1.2559 - val_acc: 0.2656\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9448 - acc: 0.5835 - val_loss: 1.1897 - val_acc: 0.3594\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9367 - acc: 0.6013 - val_loss: 1.2005 - val_acc: 0.3281\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9303 - acc: 0.5997 - val_loss: 1.1988 - val_acc: 0.3125\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9382 - acc: 0.6013 - val_loss: 1.1646 - val_acc: 0.2969\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9753 - acc: 0.5770 - val_loss: 1.1131 - val_acc: 0.3594\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0062 - acc: 0.5559 - val_loss: 1.1960 - val_acc: 0.2969\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9648 - acc: 0.5828 - val_loss: 1.1494 - val_acc: 0.3594\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0011 - acc: 0.5624 - val_loss: 1.2618 - val_acc: 0.2969\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0227 - acc: 0.5786 - val_loss: 1.1005 - val_acc: 0.3594\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9089 - acc: 0.5851 - val_loss: 1.1954 - val_acc: 0.2656\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.9967 - acc: 0.5562 - val_loss: 1.2708 - val_acc: 0.2812\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9134 - acc: 0.6110 - val_loss: 1.2712 - val_acc: 0.2500\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0112 - acc: 0.5608 - val_loss: 1.1584 - val_acc: 0.3281\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.8751 - acc: 0.6013 - val_loss: 1.2569 - val_acc: 0.2500\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.9666 - acc: 0.5624 - val_loss: 1.2549 - val_acc: 0.2656\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9678 - acc: 0.5900 - val_loss: 1.0438 - val_acc: 0.3438\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0014 - acc: 0.5851 - val_loss: 1.2084 - val_acc: 0.2812\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9607 - acc: 0.5721 - val_loss: 1.1105 - val_acc: 0.3906\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8950 - acc: 0.6159 - val_loss: 1.1984 - val_acc: 0.2969\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9385 - acc: 0.5818 - val_loss: 1.1367 - val_acc: 0.3281\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 0.9129 - acc: 0.5964 - val_loss: 1.1375 - val_acc: 0.3438\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9288 - acc: 0.5964 - val_loss: 1.1358 - val_acc: 0.3125\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9845 - acc: 0.5705 - val_loss: 1.2257 - val_acc: 0.3438\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9074 - acc: 0.5900 - val_loss: 1.1107 - val_acc: 0.3594\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9580 - acc: 0.5916 - val_loss: 1.1520 - val_acc: 0.3281\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0012 - acc: 0.5608 - val_loss: 1.1876 - val_acc: 0.2969\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8841 - acc: 0.5964 - val_loss: 1.1356 - val_acc: 0.3438\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9611 - acc: 0.5900 - val_loss: 1.1479 - val_acc: 0.3594\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9189 - acc: 0.5981 - val_loss: 1.1618 - val_acc: 0.3594\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0074 - acc: 0.5802 - val_loss: 1.1304 - val_acc: 0.3281\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9678 - acc: 0.5900 - val_loss: 1.1613 - val_acc: 0.3125\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9582 - acc: 0.5802 - val_loss: 1.2438 - val_acc: 0.2812\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9707 - acc: 0.5851 - val_loss: 1.1267 - val_acc: 0.2656\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9120 - acc: 0.5997 - val_loss: 1.2776 - val_acc: 0.2969\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9361 - acc: 0.5981 - val_loss: 1.2442 - val_acc: 0.2656\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8777 - acc: 0.6250 - val_loss: 1.1901 - val_acc: 0.3281\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9651 - acc: 0.5932 - val_loss: 1.1406 - val_acc: 0.3438\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0139 - acc: 0.5527 - val_loss: 1.2637 - val_acc: 0.2812\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9671 - acc: 0.5916 - val_loss: 1.3200 - val_acc: 0.2812\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9300 - acc: 0.5916 - val_loss: 1.1526 - val_acc: 0.3438\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9506 - acc: 0.5673 - val_loss: 1.2428 - val_acc: 0.3438\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9385 - acc: 0.6125 - val_loss: 1.2759 - val_acc: 0.2812\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8738 - acc: 0.5964 - val_loss: 1.1614 - val_acc: 0.3281\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9426 - acc: 0.5964 - val_loss: 1.0976 - val_acc: 0.3281\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0020 - acc: 0.5673 - val_loss: 1.2654 - val_acc: 0.2656\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9599 - acc: 0.5906 - val_loss: 1.2659 - val_acc: 0.2969\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9205 - acc: 0.5703 - val_loss: 1.2398 - val_acc: 0.3438\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9383 - acc: 0.5737 - val_loss: 1.0895 - val_acc: 0.3438\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9926 - acc: 0.5851 - val_loss: 1.1686 - val_acc: 0.3594\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9587 - acc: 0.5964 - val_loss: 1.0984 - val_acc: 0.3594\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9085 - acc: 0.6029 - val_loss: 1.0610 - val_acc: 0.3594\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9131 - acc: 0.5818 - val_loss: 1.2317 - val_acc: 0.2812\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9507 - acc: 0.5786 - val_loss: 1.1009 - val_acc: 0.3594\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9223 - acc: 0.5818 - val_loss: 1.1226 - val_acc: 0.2812\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9654 - acc: 0.5948 - val_loss: 1.0450 - val_acc: 0.4219\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9872 - acc: 0.5867 - val_loss: 1.2489 - val_acc: 0.2812\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9741 - acc: 0.5673 - val_loss: 1.1689 - val_acc: 0.2656\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9378 - acc: 0.5770 - val_loss: 1.1849 - val_acc: 0.3281\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9936 - acc: 0.5721 - val_loss: 1.1199 - val_acc: 0.3281\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9958 - acc: 0.5673 - val_loss: 1.1680 - val_acc: 0.3281\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9849 - acc: 0.5689 - val_loss: 1.1448 - val_acc: 0.3438\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9198 - acc: 0.5984 - val_loss: 1.2743 - val_acc: 0.3125\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9697 - acc: 0.5737 - val_loss: 1.0984 - val_acc: 0.2656\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9924 - acc: 0.5766 - val_loss: 1.1043 - val_acc: 0.3438\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9803 - acc: 0.5719 - val_loss: 1.1680 - val_acc: 0.3125\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9205 - acc: 0.6143 - val_loss: 1.1617 - val_acc: 0.3281\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9793 - acc: 0.5786 - val_loss: 1.2269 - val_acc: 0.3438\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9159 - acc: 0.5844 - val_loss: 1.0914 - val_acc: 0.2969\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9428 - acc: 0.5835 - val_loss: 1.2257 - val_acc: 0.2969\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9439 - acc: 0.5883 - val_loss: 1.1277 - val_acc: 0.3281\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9396 - acc: 0.6141 - val_loss: 1.2595 - val_acc: 0.2969\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9233 - acc: 0.5851 - val_loss: 1.2208 - val_acc: 0.3438\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9537 - acc: 0.5932 - val_loss: 1.1408 - val_acc: 0.3594\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9137 - acc: 0.6062 - val_loss: 1.2174 - val_acc: 0.2812\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9692 - acc: 0.5786 - val_loss: 1.2869 - val_acc: 0.2812\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9159 - acc: 0.6143 - val_loss: 1.2652 - val_acc: 0.2656\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9798 - acc: 0.5656 - val_loss: 1.0492 - val_acc: 0.3906\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9309 - acc: 0.5997 - val_loss: 1.1372 - val_acc: 0.3438\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9130 - acc: 0.6045 - val_loss: 1.2103 - val_acc: 0.2969\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9760 - acc: 0.5948 - val_loss: 1.3291 - val_acc: 0.2656\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9918 - acc: 0.5543 - val_loss: 1.2312 - val_acc: 0.3125\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 3s 189ms/step - loss: 0.9511 - acc: 0.5689 - val_loss: 1.2708 - val_acc: 0.2656\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9164 - acc: 0.5851 - val_loss: 1.2101 - val_acc: 0.2969\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 0.9510 - acc: 0.5867 - val_loss: 1.3133 - val_acc: 0.2656\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9898 - acc: 0.5608 - val_loss: 1.1985 - val_acc: 0.2969\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9486 - acc: 0.5818 - val_loss: 1.1446 - val_acc: 0.3594\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9429 - acc: 0.5916 - val_loss: 1.2431 - val_acc: 0.2812\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9345 - acc: 0.6062 - val_loss: 1.2044 - val_acc: 0.3125\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0241 - acc: 0.5559 - val_loss: 1.1776 - val_acc: 0.2656\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9030 - acc: 0.6224 - val_loss: 1.2444 - val_acc: 0.2656\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8855 - acc: 0.6143 - val_loss: 1.2084 - val_acc: 0.2969\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9013 - acc: 0.5984 - val_loss: 1.2548 - val_acc: 0.2656\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 0.9646 - acc: 0.5640 - val_loss: 1.2281 - val_acc: 0.3125\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9391 - acc: 0.5770 - val_loss: 1.1251 - val_acc: 0.3750\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9795 - acc: 0.5608 - val_loss: 1.2650 - val_acc: 0.2500\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 2s 192ms/step - loss: 0.9522 - acc: 0.5997 - val_loss: 1.2012 - val_acc: 0.3438\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9775 - acc: 0.5754 - val_loss: 1.2203 - val_acc: 0.3438\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9009 - acc: 0.6013 - val_loss: 1.2250 - val_acc: 0.2812\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8935 - acc: 0.6191 - val_loss: 1.1982 - val_acc: 0.3125\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9221 - acc: 0.6029 - val_loss: 1.2545 - val_acc: 0.3125\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9635 - acc: 0.5851 - val_loss: 1.2523 - val_acc: 0.2812\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9276 - acc: 0.6029 - val_loss: 1.2482 - val_acc: 0.2969\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9539 - acc: 0.5835 - val_loss: 1.1560 - val_acc: 0.2969\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9155 - acc: 0.6029 - val_loss: 1.1150 - val_acc: 0.3125\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9716 - acc: 0.5721 - val_loss: 1.2022 - val_acc: 0.3125\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9327 - acc: 0.6094 - val_loss: 1.1225 - val_acc: 0.3281\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8960 - acc: 0.6159 - val_loss: 1.0915 - val_acc: 0.3594\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8825 - acc: 0.6029 - val_loss: 1.1512 - val_acc: 0.3281\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9519 - acc: 0.5688 - val_loss: 1.3537 - val_acc: 0.2500\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9442 - acc: 0.6094 - val_loss: 1.2985 - val_acc: 0.2500\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8921 - acc: 0.6159 - val_loss: 1.1277 - val_acc: 0.3750\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9122 - acc: 0.6000 - val_loss: 1.1987 - val_acc: 0.3125\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9504 - acc: 0.5802 - val_loss: 1.3368 - val_acc: 0.2656\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9528 - acc: 0.5867 - val_loss: 1.1346 - val_acc: 0.3125\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8966 - acc: 0.6203 - val_loss: 1.0993 - val_acc: 0.3125\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9802 - acc: 0.5750 - val_loss: 1.2376 - val_acc: 0.3281\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9424 - acc: 0.5835 - val_loss: 1.3175 - val_acc: 0.2812\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9613 - acc: 0.5851 - val_loss: 1.1867 - val_acc: 0.3281\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9716 - acc: 0.6029 - val_loss: 1.2702 - val_acc: 0.2812\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9330 - acc: 0.6045 - val_loss: 1.2319 - val_acc: 0.2969\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8619 - acc: 0.6078 - val_loss: 1.2210 - val_acc: 0.3125\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9274 - acc: 0.5981 - val_loss: 1.1144 - val_acc: 0.3438\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9543 - acc: 0.5835 - val_loss: 1.2286 - val_acc: 0.2969\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8700 - acc: 0.6256 - val_loss: 1.2152 - val_acc: 0.2500\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9374 - acc: 0.5964 - val_loss: 1.2870 - val_acc: 0.2812\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9089 - acc: 0.5932 - val_loss: 1.1824 - val_acc: 0.3438\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9706 - acc: 0.5737 - val_loss: 1.1777 - val_acc: 0.2969\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9438 - acc: 0.5883 - val_loss: 1.1560 - val_acc: 0.2969\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9177 - acc: 0.5997 - val_loss: 1.1773 - val_acc: 0.3281\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9513 - acc: 0.5997 - val_loss: 1.1956 - val_acc: 0.3594\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9951 - acc: 0.5608 - val_loss: 1.1157 - val_acc: 0.3281\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8968 - acc: 0.6126 - val_loss: 1.2301 - val_acc: 0.2500\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9041 - acc: 0.5900 - val_loss: 1.2240 - val_acc: 0.2656\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9715 - acc: 0.5754 - val_loss: 1.2167 - val_acc: 0.3125\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9686 - acc: 0.5835 - val_loss: 1.0900 - val_acc: 0.3594\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9525 - acc: 0.5835 - val_loss: 1.2841 - val_acc: 0.2656\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8644 - acc: 0.5883 - val_loss: 1.1444 - val_acc: 0.3125\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9023 - acc: 0.6159 - val_loss: 1.1762 - val_acc: 0.3438\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8807 - acc: 0.6126 - val_loss: 1.2066 - val_acc: 0.3125\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0088 - acc: 0.5609 - val_loss: 1.2067 - val_acc: 0.3125\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9108 - acc: 0.6029 - val_loss: 1.0882 - val_acc: 0.3594\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8946 - acc: 0.6029 - val_loss: 1.1177 - val_acc: 0.3594\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8770 - acc: 0.6175 - val_loss: 1.1649 - val_acc: 0.3438\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8895 - acc: 0.6013 - val_loss: 1.1966 - val_acc: 0.3594\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9610 - acc: 0.5818 - val_loss: 1.1104 - val_acc: 0.3906\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9397 - acc: 0.5997 - val_loss: 1.2272 - val_acc: 0.3125\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9069 - acc: 0.5932 - val_loss: 1.1574 - val_acc: 0.3438\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9535 - acc: 0.6062 - val_loss: 1.1928 - val_acc: 0.2656\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8895 - acc: 0.6078 - val_loss: 1.1682 - val_acc: 0.3438\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9272 - acc: 0.6062 - val_loss: 1.1205 - val_acc: 0.3438\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9230 - acc: 0.5948 - val_loss: 1.0921 - val_acc: 0.4062\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9479 - acc: 0.5737 - val_loss: 1.2219 - val_acc: 0.2812\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9120 - acc: 0.6062 - val_loss: 1.1123 - val_acc: 0.3750\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8957 - acc: 0.6109 - val_loss: 1.1572 - val_acc: 0.3125\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.8954 - acc: 0.6224 - val_loss: 1.1101 - val_acc: 0.3438\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8778 - acc: 0.5969 - val_loss: 1.1784 - val_acc: 0.3125\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.9124 - acc: 0.6045 - val_loss: 1.0402 - val_acc: 0.4219\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8572 - acc: 0.6191 - val_loss: 1.1385 - val_acc: 0.3438\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8907 - acc: 0.5948 - val_loss: 1.2386 - val_acc: 0.2969\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9107 - acc: 0.5770 - val_loss: 1.0951 - val_acc: 0.3281\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.9332 - acc: 0.5916 - val_loss: 1.1489 - val_acc: 0.3438\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9617 - acc: 0.5835 - val_loss: 1.2310 - val_acc: 0.2969\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9549 - acc: 0.5575 - val_loss: 1.2840 - val_acc: 0.2969\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8936 - acc: 0.5981 - val_loss: 1.0666 - val_acc: 0.3750\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8698 - acc: 0.6321 - val_loss: 1.1946 - val_acc: 0.3125\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8802 - acc: 0.6240 - val_loss: 1.2212 - val_acc: 0.2031\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8692 - acc: 0.6313 - val_loss: 1.1013 - val_acc: 0.3594\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8866 - acc: 0.5953 - val_loss: 1.2102 - val_acc: 0.3281\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9081 - acc: 0.5997 - val_loss: 1.1418 - val_acc: 0.3281\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9332 - acc: 0.5770 - val_loss: 1.3043 - val_acc: 0.2656\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9257 - acc: 0.5997 - val_loss: 1.2646 - val_acc: 0.2656\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9137 - acc: 0.5953 - val_loss: 1.0988 - val_acc: 0.4375\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8675 - acc: 0.6272 - val_loss: 1.1659 - val_acc: 0.3594\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9562 - acc: 0.5575 - val_loss: 1.2045 - val_acc: 0.3125\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9260 - acc: 0.5969 - val_loss: 1.2832 - val_acc: 0.2969\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9152 - acc: 0.6078 - val_loss: 1.1221 - val_acc: 0.3438\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9047 - acc: 0.6078 - val_loss: 1.2293 - val_acc: 0.2969\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8991 - acc: 0.5997 - val_loss: 1.2159 - val_acc: 0.2969\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9587 - acc: 0.5608 - val_loss: 1.1422 - val_acc: 0.4062\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9267 - acc: 0.5835 - val_loss: 1.1498 - val_acc: 0.3125\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9056 - acc: 0.6240 - val_loss: 1.1212 - val_acc: 0.3125\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9087 - acc: 0.6305 - val_loss: 1.3163 - val_acc: 0.2188\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9374 - acc: 0.5818 - val_loss: 1.0640 - val_acc: 0.3438\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9212 - acc: 0.5844 - val_loss: 1.2518 - val_acc: 0.2969\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9109 - acc: 0.6094 - val_loss: 1.2236 - val_acc: 0.2969\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9293 - acc: 0.5932 - val_loss: 1.1354 - val_acc: 0.3750\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9018 - acc: 0.6126 - val_loss: 1.2160 - val_acc: 0.3438\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8901 - acc: 0.5953 - val_loss: 1.0406 - val_acc: 0.3750\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9071 - acc: 0.6126 - val_loss: 1.2193 - val_acc: 0.2656\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9193 - acc: 0.5964 - val_loss: 1.1669 - val_acc: 0.3438\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8712 - acc: 0.6191 - val_loss: 1.2009 - val_acc: 0.2969\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 0.8744 - acc: 0.6013 - val_loss: 1.2370 - val_acc: 0.2500\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9412 - acc: 0.5835 - val_loss: 1.3980 - val_acc: 0.2500\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8924 - acc: 0.6078 - val_loss: 1.1480 - val_acc: 0.3594\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8709 - acc: 0.6288 - val_loss: 1.1954 - val_acc: 0.3281\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8528 - acc: 0.6224 - val_loss: 1.1515 - val_acc: 0.3750\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9152 - acc: 0.5673 - val_loss: 1.0840 - val_acc: 0.3906\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9105 - acc: 0.5964 - val_loss: 1.1572 - val_acc: 0.3438\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9066 - acc: 0.5851 - val_loss: 1.1624 - val_acc: 0.3594\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8885 - acc: 0.6062 - val_loss: 1.0414 - val_acc: 0.4062\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8784 - acc: 0.5867 - val_loss: 1.2705 - val_acc: 0.3125\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8814 - acc: 0.6240 - val_loss: 1.1854 - val_acc: 0.3594\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8675 - acc: 0.6110 - val_loss: 1.2188 - val_acc: 0.3281\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8832 - acc: 0.6207 - val_loss: 1.1539 - val_acc: 0.3281\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8987 - acc: 0.6045 - val_loss: 1.2049 - val_acc: 0.2969\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8791 - acc: 0.6143 - val_loss: 1.1981 - val_acc: 0.3438\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8896 - acc: 0.5981 - val_loss: 1.2149 - val_acc: 0.3125\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9248 - acc: 0.6013 - val_loss: 1.1750 - val_acc: 0.3281\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9286 - acc: 0.5932 - val_loss: 1.0841 - val_acc: 0.3594\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8926 - acc: 0.5867 - val_loss: 1.1593 - val_acc: 0.3281\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8719 - acc: 0.6234 - val_loss: 1.3376 - val_acc: 0.2500\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.9181 - acc: 0.5981 - val_loss: 1.1963 - val_acc: 0.3125\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8934 - acc: 0.5916 - val_loss: 1.1375 - val_acc: 0.3594\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8588 - acc: 0.6078 - val_loss: 1.1850 - val_acc: 0.3281\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9315 - acc: 0.5883 - val_loss: 1.1133 - val_acc: 0.3594\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8654 - acc: 0.6359 - val_loss: 1.2021 - val_acc: 0.3281\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9284 - acc: 0.6110 - val_loss: 1.0819 - val_acc: 0.4062\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8579 - acc: 0.6207 - val_loss: 1.2926 - val_acc: 0.2344\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8995 - acc: 0.6207 - val_loss: 1.0395 - val_acc: 0.3750\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8747 - acc: 0.6224 - val_loss: 1.2234 - val_acc: 0.3125\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.8674 - acc: 0.6288 - val_loss: 1.0842 - val_acc: 0.3594\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8460 - acc: 0.6353 - val_loss: 1.2051 - val_acc: 0.3594\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9132 - acc: 0.5818 - val_loss: 1.1756 - val_acc: 0.3281\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9166 - acc: 0.5948 - val_loss: 1.1409 - val_acc: 0.3281\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8616 - acc: 0.6240 - val_loss: 1.1398 - val_acc: 0.3281\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9068 - acc: 0.5754 - val_loss: 1.1217 - val_acc: 0.3594\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8894 - acc: 0.5964 - val_loss: 1.1801 - val_acc: 0.3594\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9379 - acc: 0.6013 - val_loss: 1.1531 - val_acc: 0.3594\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9008 - acc: 0.6045 - val_loss: 1.2568 - val_acc: 0.2969\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8565 - acc: 0.6337 - val_loss: 1.1306 - val_acc: 0.3594\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9077 - acc: 0.5932 - val_loss: 1.0180 - val_acc: 0.3906\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8839 - acc: 0.6094 - val_loss: 1.2671 - val_acc: 0.2969\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8726 - acc: 0.6141 - val_loss: 1.0958 - val_acc: 0.3750\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8989 - acc: 0.5818 - val_loss: 1.2224 - val_acc: 0.3438\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9063 - acc: 0.6031 - val_loss: 1.1024 - val_acc: 0.3750\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9074 - acc: 0.6078 - val_loss: 1.1244 - val_acc: 0.3594\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8881 - acc: 0.6143 - val_loss: 1.2788 - val_acc: 0.2969\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9181 - acc: 0.6094 - val_loss: 1.1596 - val_acc: 0.3125\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8885 - acc: 0.6207 - val_loss: 1.1944 - val_acc: 0.3594\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9404 - acc: 0.5818 - val_loss: 1.1748 - val_acc: 0.2969\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.8886 - acc: 0.6175 - val_loss: 1.1305 - val_acc: 0.3750\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8338 - acc: 0.6266 - val_loss: 1.2754 - val_acc: 0.3281\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8350 - acc: 0.6337 - val_loss: 1.1995 - val_acc: 0.3281\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9329 - acc: 0.5770 - val_loss: 1.1150 - val_acc: 0.4219\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9146 - acc: 0.5948 - val_loss: 1.3074 - val_acc: 0.2969\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8712 - acc: 0.6078 - val_loss: 1.1640 - val_acc: 0.3438\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.8740 - acc: 0.5997 - val_loss: 1.1393 - val_acc: 0.3906\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8478 - acc: 0.6451 - val_loss: 1.2626 - val_acc: 0.3125\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9463 - acc: 0.5705 - val_loss: 1.0966 - val_acc: 0.3906\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8832 - acc: 0.5916 - val_loss: 1.1571 - val_acc: 0.3281\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9059 - acc: 0.5916 - val_loss: 1.2155 - val_acc: 0.3438\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8618 - acc: 0.6126 - val_loss: 1.3659 - val_acc: 0.3281\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8784 - acc: 0.5932 - val_loss: 1.2291 - val_acc: 0.3594\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9023 - acc: 0.6094 - val_loss: 1.0960 - val_acc: 0.3594\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9312 - acc: 0.5900 - val_loss: 1.2378 - val_acc: 0.3125\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9786 - acc: 0.5770 - val_loss: 1.1614 - val_acc: 0.3750\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8655 - acc: 0.6159 - val_loss: 1.1573 - val_acc: 0.3125\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8867 - acc: 0.6187 - val_loss: 1.1310 - val_acc: 0.3594\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8783 - acc: 0.6305 - val_loss: 1.1719 - val_acc: 0.3594\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8581 - acc: 0.6256 - val_loss: 1.1721 - val_acc: 0.3125\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.9213 - acc: 0.5981 - val_loss: 1.1516 - val_acc: 0.3281\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9030 - acc: 0.6203 - val_loss: 1.2161 - val_acc: 0.3750\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9091 - acc: 0.6143 - val_loss: 1.2916 - val_acc: 0.2969\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8712 - acc: 0.6143 - val_loss: 1.1885 - val_acc: 0.2969\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9321 - acc: 0.5916 - val_loss: 1.1797 - val_acc: 0.3281\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9015 - acc: 0.6013 - val_loss: 1.1743 - val_acc: 0.3281\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9074 - acc: 0.6062 - val_loss: 1.1479 - val_acc: 0.3125\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8935 - acc: 0.6029 - val_loss: 1.1927 - val_acc: 0.3594\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8676 - acc: 0.6234 - val_loss: 1.1168 - val_acc: 0.3594\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8686 - acc: 0.6386 - val_loss: 1.1032 - val_acc: 0.3594\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8518 - acc: 0.6191 - val_loss: 1.3226 - val_acc: 0.2500\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8952 - acc: 0.5932 - val_loss: 1.2915 - val_acc: 0.2344\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8341 - acc: 0.6500 - val_loss: 1.1731 - val_acc: 0.3125\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9248 - acc: 0.5948 - val_loss: 1.1735 - val_acc: 0.3438\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8298 - acc: 0.6353 - val_loss: 1.0861 - val_acc: 0.3594\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8888 - acc: 0.6029 - val_loss: 1.1578 - val_acc: 0.3594\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8566 - acc: 0.6224 - val_loss: 1.2170 - val_acc: 0.3438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "510b95fa-f229-418a-b792-3b2a353716ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.8106707334518433,\n",
              "  1.906511902809143,\n",
              "  1.8534843921661377,\n",
              "  1.6960035562515259,\n",
              "  1.9395077228546143,\n",
              "  1.932572603225708,\n",
              "  1.8295506238937378,\n",
              "  1.6671348810195923,\n",
              "  1.8022947311401367,\n",
              "  1.7544934749603271,\n",
              "  1.8592404127120972,\n",
              "  1.8094488382339478,\n",
              "  1.8198823928833008,\n",
              "  1.842084527015686,\n",
              "  1.7764116525650024,\n",
              "  1.7156013250350952,\n",
              "  1.6702616214752197,\n",
              "  1.7450884580612183,\n",
              "  1.7419685125350952,\n",
              "  1.7310529947280884,\n",
              "  1.705755352973938,\n",
              "  1.7343710660934448,\n",
              "  1.7397440671920776,\n",
              "  1.819420576095581,\n",
              "  1.7344691753387451,\n",
              "  1.6948877573013306,\n",
              "  1.774296760559082,\n",
              "  1.7057478427886963,\n",
              "  1.800163984298706,\n",
              "  1.6756813526153564,\n",
              "  1.671582579612732,\n",
              "  1.7976206541061401,\n",
              "  1.6182806491851807,\n",
              "  1.6972264051437378,\n",
              "  1.7887866497039795,\n",
              "  1.7266372442245483,\n",
              "  1.799641489982605,\n",
              "  1.634304165840149,\n",
              "  1.7071400880813599,\n",
              "  1.801697015762329,\n",
              "  1.711086630821228,\n",
              "  1.796598196029663,\n",
              "  1.6705067157745361,\n",
              "  1.594327449798584,\n",
              "  1.712172031402588,\n",
              "  1.674332618713379,\n",
              "  1.6831302642822266,\n",
              "  1.6104604005813599,\n",
              "  1.7418982982635498,\n",
              "  1.7354894876480103,\n",
              "  1.6410001516342163,\n",
              "  1.694575309753418,\n",
              "  1.6590538024902344,\n",
              "  1.7404212951660156,\n",
              "  1.549788475036621,\n",
              "  1.655760645866394,\n",
              "  1.6160411834716797,\n",
              "  1.6545149087905884,\n",
              "  1.6192222833633423,\n",
              "  1.692644715309143,\n",
              "  1.5351667404174805,\n",
              "  1.6657825708389282,\n",
              "  1.7400585412979126,\n",
              "  1.6753169298171997,\n",
              "  1.5197219848632812,\n",
              "  1.684711217880249,\n",
              "  1.661784291267395,\n",
              "  1.580353021621704,\n",
              "  1.5430777072906494,\n",
              "  1.5570106506347656,\n",
              "  1.580622673034668,\n",
              "  1.4915060997009277,\n",
              "  1.640795350074768,\n",
              "  1.4873853921890259,\n",
              "  1.6057814359664917,\n",
              "  1.7051854133605957,\n",
              "  1.6226024627685547,\n",
              "  1.494592547416687,\n",
              "  1.531173825263977,\n",
              "  1.5915040969848633,\n",
              "  1.5298185348510742,\n",
              "  1.5371816158294678,\n",
              "  1.5952911376953125,\n",
              "  1.627720594406128,\n",
              "  1.570191740989685,\n",
              "  1.5265988111495972,\n",
              "  1.5865037441253662,\n",
              "  1.5565167665481567,\n",
              "  1.592598557472229,\n",
              "  1.737337350845337,\n",
              "  1.5689095258712769,\n",
              "  1.5823928117752075,\n",
              "  1.5351206064224243,\n",
              "  1.4108444452285767,\n",
              "  1.3845487833023071,\n",
              "  1.4978381395339966,\n",
              "  1.5884064435958862,\n",
              "  1.5067472457885742,\n",
              "  1.513298749923706,\n",
              "  1.5960310697555542,\n",
              "  1.4994971752166748,\n",
              "  1.563474178314209,\n",
              "  1.5208882093429565,\n",
              "  1.4523414373397827,\n",
              "  1.4987019300460815,\n",
              "  1.4792629480361938,\n",
              "  1.5095736980438232,\n",
              "  1.5443412065505981,\n",
              "  1.5724117755889893,\n",
              "  1.481042504310608,\n",
              "  1.6391407251358032,\n",
              "  1.6214362382888794,\n",
              "  1.5631794929504395,\n",
              "  1.4999852180480957,\n",
              "  1.5037765502929688,\n",
              "  1.5212379693984985,\n",
              "  1.4328778982162476,\n",
              "  1.5541651248931885,\n",
              "  1.5822865962982178,\n",
              "  1.525572657585144,\n",
              "  1.5171420574188232,\n",
              "  1.453991174697876,\n",
              "  1.415615439414978,\n",
              "  1.4744811058044434,\n",
              "  1.4459521770477295,\n",
              "  1.5230847597122192,\n",
              "  1.457838535308838,\n",
              "  1.4613724946975708,\n",
              "  1.4705979824066162,\n",
              "  1.431416392326355,\n",
              "  1.3920865058898926,\n",
              "  1.3837811946868896,\n",
              "  1.4924973249435425,\n",
              "  1.405231237411499,\n",
              "  1.5660125017166138,\n",
              "  1.3392446041107178,\n",
              "  1.5111370086669922,\n",
              "  1.484479308128357,\n",
              "  1.4383373260498047,\n",
              "  1.4027944803237915,\n",
              "  1.3877805471420288,\n",
              "  1.5271196365356445,\n",
              "  1.3962403535842896,\n",
              "  1.5060815811157227,\n",
              "  1.3994165658950806,\n",
              "  1.3765743970870972,\n",
              "  1.3417800664901733,\n",
              "  1.5201395750045776,\n",
              "  1.4874929189682007,\n",
              "  1.508436918258667,\n",
              "  1.3980916738510132,\n",
              "  1.4788767099380493,\n",
              "  1.4749979972839355,\n",
              "  1.5239750146865845,\n",
              "  1.468242883682251,\n",
              "  1.454007625579834,\n",
              "  1.4848823547363281,\n",
              "  1.5183602571487427,\n",
              "  1.5626846551895142,\n",
              "  1.386420488357544,\n",
              "  1.3911813497543335,\n",
              "  1.3789891004562378,\n",
              "  1.3473596572875977,\n",
              "  1.3434972763061523,\n",
              "  1.409345030784607,\n",
              "  1.3427510261535645,\n",
              "  1.4738820791244507,\n",
              "  1.4160257577896118,\n",
              "  1.4458484649658203,\n",
              "  1.3915624618530273,\n",
              "  1.3851276636123657,\n",
              "  1.3722413778305054,\n",
              "  1.3665778636932373,\n",
              "  1.4272595643997192,\n",
              "  1.3650522232055664,\n",
              "  1.4587072134017944,\n",
              "  1.4449058771133423,\n",
              "  1.3725576400756836,\n",
              "  1.2719995975494385,\n",
              "  1.376565933227539,\n",
              "  1.4518578052520752,\n",
              "  1.356544017791748,\n",
              "  1.3993545770645142,\n",
              "  1.3919250965118408,\n",
              "  1.3587788343429565,\n",
              "  1.4251329898834229,\n",
              "  1.404272198677063,\n",
              "  1.403531551361084,\n",
              "  1.4095155000686646,\n",
              "  1.3560253381729126,\n",
              "  1.4452177286148071,\n",
              "  1.3903061151504517,\n",
              "  1.385520577430725,\n",
              "  1.4298697710037231,\n",
              "  1.3616009950637817,\n",
              "  1.4434300661087036,\n",
              "  1.389110803604126,\n",
              "  1.3189400434494019,\n",
              "  1.2687840461730957,\n",
              "  1.2567840814590454,\n",
              "  1.4264062643051147,\n",
              "  1.3067783117294312,\n",
              "  1.3216595649719238,\n",
              "  1.3175500631332397,\n",
              "  1.3322522640228271,\n",
              "  1.4470072984695435,\n",
              "  1.4849311113357544,\n",
              "  1.3905463218688965,\n",
              "  1.3946648836135864,\n",
              "  1.3859469890594482,\n",
              "  1.4253038167953491,\n",
              "  1.360870361328125,\n",
              "  1.3775383234024048,\n",
              "  1.3401286602020264,\n",
              "  1.372411847114563,\n",
              "  1.282999873161316,\n",
              "  1.4190257787704468,\n",
              "  1.298272728919983,\n",
              "  1.4425139427185059,\n",
              "  1.3786258697509766,\n",
              "  1.4561758041381836,\n",
              "  1.3199388980865479,\n",
              "  1.2677961587905884,\n",
              "  1.3074703216552734,\n",
              "  1.3464199304580688,\n",
              "  1.330415964126587,\n",
              "  1.2819238901138306,\n",
              "  1.2406238317489624,\n",
              "  1.3110873699188232,\n",
              "  1.3549505472183228,\n",
              "  1.273078203201294,\n",
              "  1.3988561630249023,\n",
              "  1.3749072551727295,\n",
              "  1.3751304149627686,\n",
              "  1.363821268081665,\n",
              "  1.3706591129302979,\n",
              "  1.3251830339431763,\n",
              "  1.3757076263427734,\n",
              "  1.3782354593276978,\n",
              "  1.3285129070281982,\n",
              "  1.277795672416687,\n",
              "  1.2848902940750122,\n",
              "  1.3050405979156494,\n",
              "  1.3302620649337769,\n",
              "  1.2736774682998657,\n",
              "  1.3233386278152466,\n",
              "  1.3744802474975586,\n",
              "  1.2865493297576904,\n",
              "  1.3526787757873535,\n",
              "  1.2799642086029053,\n",
              "  1.3149759769439697,\n",
              "  1.3413828611373901,\n",
              "  1.404284954071045,\n",
              "  1.300398826599121,\n",
              "  1.3340092897415161,\n",
              "  1.2628509998321533,\n",
              "  1.3053900003433228,\n",
              "  1.2986781597137451,\n",
              "  1.2077385187149048,\n",
              "  1.3161165714263916,\n",
              "  1.3661510944366455,\n",
              "  1.238739252090454,\n",
              "  1.3283072710037231,\n",
              "  1.376188039779663,\n",
              "  1.2531265020370483,\n",
              "  1.2688095569610596,\n",
              "  1.1820842027664185,\n",
              "  1.2612789869308472,\n",
              "  1.2740567922592163,\n",
              "  1.2787599563598633,\n",
              "  1.2898603677749634,\n",
              "  1.3503934144973755,\n",
              "  1.308464765548706,\n",
              "  1.2081613540649414,\n",
              "  1.2583720684051514,\n",
              "  1.2984142303466797,\n",
              "  1.264330267906189,\n",
              "  1.2984063625335693,\n",
              "  1.307252287864685,\n",
              "  1.3100172281265259,\n",
              "  1.2740226984024048,\n",
              "  1.2600189447402954,\n",
              "  1.326110601425171,\n",
              "  1.2174426317214966,\n",
              "  1.29964280128479,\n",
              "  1.2790170907974243,\n",
              "  1.3615416288375854,\n",
              "  1.30497145652771,\n",
              "  1.1688228845596313,\n",
              "  1.2906044721603394,\n",
              "  1.282422423362732,\n",
              "  1.2746195793151855,\n",
              "  1.2477173805236816,\n",
              "  1.2048746347427368,\n",
              "  1.3189570903778076,\n",
              "  1.2393816709518433,\n",
              "  1.2353862524032593,\n",
              "  1.3102960586547852,\n",
              "  1.241837739944458,\n",
              "  1.275052785873413,\n",
              "  1.2479792833328247,\n",
              "  1.3318455219268799,\n",
              "  1.2031313180923462,\n",
              "  1.1544495820999146,\n",
              "  1.2607723474502563,\n",
              "  1.2375999689102173,\n",
              "  1.262468934059143,\n",
              "  1.271445393562317,\n",
              "  1.215856671333313,\n",
              "  1.1491048336029053,\n",
              "  1.2246556282043457,\n",
              "  1.1964722871780396,\n",
              "  1.2894065380096436,\n",
              "  1.2626898288726807,\n",
              "  1.2117091417312622,\n",
              "  1.1150139570236206,\n",
              "  1.2274541854858398,\n",
              "  1.1718451976776123,\n",
              "  1.2576828002929688,\n",
              "  1.2539622783660889,\n",
              "  1.157797932624817,\n",
              "  1.2253378629684448,\n",
              "  1.208017110824585,\n",
              "  1.2497613430023193,\n",
              "  1.1980524063110352,\n",
              "  1.210747241973877,\n",
              "  1.2547506093978882,\n",
              "  1.2006776332855225,\n",
              "  1.1868847608566284,\n",
              "  1.238918662071228,\n",
              "  1.229274034500122,\n",
              "  1.2792959213256836,\n",
              "  1.2381576299667358,\n",
              "  1.291911005973816,\n",
              "  1.241613507270813,\n",
              "  1.2176302671432495,\n",
              "  1.246321678161621,\n",
              "  1.1604374647140503,\n",
              "  1.2890902757644653,\n",
              "  1.2718955278396606,\n",
              "  1.2502504587173462,\n",
              "  1.1464271545410156,\n",
              "  1.2534688711166382,\n",
              "  1.1544971466064453,\n",
              "  1.2503553628921509,\n",
              "  1.2165359258651733,\n",
              "  1.1867905855178833,\n",
              "  1.1409521102905273,\n",
              "  1.164238452911377,\n",
              "  1.163715124130249,\n",
              "  1.0922741889953613,\n",
              "  1.1969338655471802,\n",
              "  1.1528286933898926,\n",
              "  1.1971783638000488,\n",
              "  1.1688368320465088,\n",
              "  1.1788759231567383,\n",
              "  1.1868969202041626,\n",
              "  1.2173136472702026,\n",
              "  1.213855504989624,\n",
              "  1.188794732093811,\n",
              "  1.1702263355255127,\n",
              "  1.2407838106155396,\n",
              "  1.168221354484558,\n",
              "  1.1588208675384521,\n",
              "  1.197428822517395,\n",
              "  1.1530611515045166,\n",
              "  1.1656874418258667,\n",
              "  1.192582368850708,\n",
              "  1.2189589738845825,\n",
              "  1.1750884056091309,\n",
              "  1.1672356128692627,\n",
              "  1.1356110572814941,\n",
              "  1.1544291973114014,\n",
              "  1.2073134183883667,\n",
              "  1.1609232425689697,\n",
              "  1.1585543155670166,\n",
              "  1.1642569303512573,\n",
              "  1.1020115613937378,\n",
              "  1.1333144903182983,\n",
              "  1.2190649509429932,\n",
              "  1.1103092432022095,\n",
              "  1.2090480327606201,\n",
              "  1.1236662864685059,\n",
              "  1.1367180347442627,\n",
              "  1.2632908821105957,\n",
              "  1.1435267925262451,\n",
              "  1.1984457969665527,\n",
              "  1.215777039527893,\n",
              "  1.1407579183578491,\n",
              "  1.202584981918335,\n",
              "  1.1089826822280884,\n",
              "  1.1542073488235474,\n",
              "  1.1509851217269897,\n",
              "  1.1256542205810547,\n",
              "  1.164800763130188,\n",
              "  1.0931615829467773,\n",
              "  1.2156208753585815,\n",
              "  1.1335850954055786,\n",
              "  1.1663554906845093,\n",
              "  1.1490885019302368,\n",
              "  1.1845462322235107,\n",
              "  1.1427228450775146,\n",
              "  1.1322157382965088,\n",
              "  1.1549129486083984,\n",
              "  1.077134370803833,\n",
              "  1.1875319480895996,\n",
              "  1.1826497316360474,\n",
              "  1.1595693826675415,\n",
              "  1.200282335281372,\n",
              "  1.2555538415908813,\n",
              "  1.150246500968933,\n",
              "  1.0776236057281494,\n",
              "  1.0879607200622559,\n",
              "  1.134171962738037,\n",
              "  1.2249804735183716,\n",
              "  1.167444109916687,\n",
              "  1.1037942171096802,\n",
              "  1.1360790729522705,\n",
              "  1.1091686487197876,\n",
              "  1.1121418476104736,\n",
              "  1.1324161291122437,\n",
              "  1.1041663885116577,\n",
              "  1.1424429416656494,\n",
              "  1.0858423709869385,\n",
              "  1.0487903356552124,\n",
              "  1.1432355642318726,\n",
              "  1.1824204921722412,\n",
              "  1.1158746480941772,\n",
              "  1.1473827362060547,\n",
              "  1.1479480266571045,\n",
              "  1.1687079668045044,\n",
              "  1.1075913906097412,\n",
              "  1.1194919347763062,\n",
              "  1.1449995040893555,\n",
              "  1.2134835720062256,\n",
              "  1.1800427436828613,\n",
              "  1.0974280834197998,\n",
              "  1.0246636867523193,\n",
              "  1.1628801822662354,\n",
              "  1.085062861442566,\n",
              "  1.1574522256851196,\n",
              "  1.1135483980178833,\n",
              "  1.1372334957122803,\n",
              "  1.134782314300537,\n",
              "  1.1108516454696655,\n",
              "  1.1052592992782593,\n",
              "  1.1002013683319092,\n",
              "  1.0700581073760986,\n",
              "  1.150964617729187,\n",
              "  1.0829869508743286,\n",
              "  1.122017502784729,\n",
              "  1.1167048215866089,\n",
              "  1.1325042247772217,\n",
              "  1.214504599571228,\n",
              "  1.1634974479675293,\n",
              "  1.0442967414855957,\n",
              "  1.1451607942581177,\n",
              "  1.1675183773040771,\n",
              "  1.1129987239837646,\n",
              "  1.1571413278579712,\n",
              "  1.124790072441101,\n",
              "  1.0950552225112915,\n",
              "  1.0982961654663086,\n",
              "  1.1077481508255005,\n",
              "  1.0734823942184448,\n",
              "  1.0544192790985107,\n",
              "  1.1580561399459839,\n",
              "  1.0495373010635376,\n",
              "  1.0600210428237915,\n",
              "  1.1666454076766968,\n",
              "  1.018791913986206,\n",
              "  1.0668047666549683,\n",
              "  1.0541971921920776,\n",
              "  1.0820964574813843,\n",
              "  1.060996651649475,\n",
              "  1.0867995023727417,\n",
              "  1.1021662950515747,\n",
              "  1.1456496715545654,\n",
              "  1.0973767042160034,\n",
              "  1.1468955278396606,\n",
              "  1.070886492729187,\n",
              "  1.113450288772583,\n",
              "  1.1422011852264404,\n",
              "  1.0828702449798584,\n",
              "  1.1172606945037842,\n",
              "  1.0582808256149292,\n",
              "  1.1286004781723022,\n",
              "  1.036600947380066,\n",
              "  1.0644705295562744,\n",
              "  1.0787594318389893,\n",
              "  1.119592547416687,\n",
              "  1.1344386339187622,\n",
              "  1.1175563335418701,\n",
              "  1.0649933815002441,\n",
              "  1.132002353668213,\n",
              "  1.1184723377227783,\n",
              "  1.11459481716156,\n",
              "  1.1488128900527954,\n",
              "  1.0605276823043823,\n",
              "  1.066219687461853,\n",
              "  1.1025817394256592,\n",
              "  1.0718822479248047,\n",
              "  1.0639228820800781,\n",
              "  1.1090229749679565,\n",
              "  1.0394914150238037,\n",
              "  1.0714930295944214,\n",
              "  1.050848364830017,\n",
              "  1.0038294792175293,\n",
              "  1.0889309644699097,\n",
              "  1.0483475923538208,\n",
              "  1.1179345846176147,\n",
              "  1.048924446105957,\n",
              "  1.0510185956954956,\n",
              "  1.1252340078353882,\n",
              "  1.071692943572998,\n",
              "  1.1132129430770874,\n",
              "  1.0290340185165405,\n",
              "  1.0374034643173218,\n",
              "  1.0336840152740479,\n",
              "  1.1014506816864014,\n",
              "  1.0835487842559814,\n",
              "  1.1515988111495972,\n",
              "  1.103305459022522,\n",
              "  1.090122938156128,\n",
              "  1.1100736856460571,\n",
              "  1.1041697263717651,\n",
              "  1.0609376430511475,\n",
              "  1.0736130475997925,\n",
              "  1.0146063566207886,\n",
              "  1.120141625404358,\n",
              "  1.020668625831604,\n",
              "  1.0326063632965088,\n",
              "  1.1018394231796265,\n",
              "  1.1356408596038818,\n",
              "  1.2043222188949585,\n",
              "  1.0861656665802002,\n",
              "  1.0472204685211182,\n",
              "  1.083592414855957,\n",
              "  1.1073417663574219,\n",
              "  1.0202662944793701,\n",
              "  1.0605535507202148,\n",
              "  1.0531150102615356,\n",
              "  1.0817246437072754,\n",
              "  1.02189302444458,\n",
              "  1.0762585401535034,\n",
              "  1.0269196033477783,\n",
              "  1.0949946641921997,\n",
              "  1.0516701936721802,\n",
              "  1.0501415729522705,\n",
              "  1.1283252239227295,\n",
              "  1.0411535501480103,\n",
              "  1.0823622941970825,\n",
              "  1.0166367292404175,\n",
              "  1.0130400657653809,\n",
              "  1.0448698997497559,\n",
              "  1.0973674058914185,\n",
              "  1.0365633964538574,\n",
              "  1.051421880722046,\n",
              "  1.0045613050460815,\n",
              "  1.0662155151367188,\n",
              "  1.0433893203735352,\n",
              "  0.977687418460846,\n",
              "  0.9550962448120117,\n",
              "  1.0469725131988525,\n",
              "  1.094411849975586,\n",
              "  1.0993777513504028,\n",
              "  1.049799919128418,\n",
              "  1.00179123878479,\n",
              "  1.0225642919540405,\n",
              "  1.0660991668701172,\n",
              "  1.06937837600708,\n",
              "  1.0594878196716309,\n",
              "  1.0573623180389404,\n",
              "  1.018166184425354,\n",
              "  1.0678579807281494,\n",
              "  1.063957691192627,\n",
              "  1.031368613243103,\n",
              "  1.0145140886306763,\n",
              "  1.0159860849380493,\n",
              "  1.079552412033081,\n",
              "  1.0083643198013306,\n",
              "  1.0056614875793457,\n",
              "  1.095693588256836,\n",
              "  0.9898741245269775,\n",
              "  1.0470142364501953,\n",
              "  1.0191044807434082,\n",
              "  1.0700429677963257,\n",
              "  1.0522475242614746,\n",
              "  1.0152068138122559,\n",
              "  1.0757862329483032,\n",
              "  1.0224473476409912,\n",
              "  0.9922239184379578,\n",
              "  1.0147145986557007,\n",
              "  1.0699108839035034,\n",
              "  1.0959144830703735,\n",
              "  1.0135865211486816,\n",
              "  0.9830009937286377,\n",
              "  1.048777461051941,\n",
              "  1.0381357669830322,\n",
              "  1.0471047163009644,\n",
              "  0.961096465587616,\n",
              "  1.0117682218551636,\n",
              "  1.0826787948608398,\n",
              "  1.094123363494873,\n",
              "  1.0066969394683838,\n",
              "  1.042536735534668,\n",
              "  1.0748841762542725,\n",
              "  1.1014870405197144,\n",
              "  1.0199023485183716,\n",
              "  0.9709064960479736,\n",
              "  1.0413808822631836,\n",
              "  0.9728303551673889,\n",
              "  1.0452193021774292,\n",
              "  1.046518325805664,\n",
              "  1.0504393577575684,\n",
              "  0.9468397498130798,\n",
              "  0.9754893779754639,\n",
              "  0.9971975088119507,\n",
              "  1.041669249534607,\n",
              "  1.0032856464385986,\n",
              "  1.0528823137283325,\n",
              "  1.0424878597259521,\n",
              "  1.0234602689743042,\n",
              "  1.0397210121154785,\n",
              "  0.9676957130432129,\n",
              "  1.039908528327942,\n",
              "  0.9936268329620361,\n",
              "  1.0101029872894287,\n",
              "  1.059899091720581,\n",
              "  1.031955361366272,\n",
              "  1.0175262689590454,\n",
              "  1.0425418615341187,\n",
              "  0.9815062284469604,\n",
              "  0.9687888622283936,\n",
              "  1.0143884420394897,\n",
              "  1.0442525148391724,\n",
              "  1.0251623392105103,\n",
              "  1.0223532915115356,\n",
              "  0.9984827041625977,\n",
              "  1.0114524364471436,\n",
              "  1.0196471214294434,\n",
              "  1.0503166913986206,\n",
              "  1.0481228828430176,\n",
              "  0.9463174343109131,\n",
              "  1.0043574571609497,\n",
              "  0.9817519783973694,\n",
              "  1.0055248737335205,\n",
              "  0.958962619304657,\n",
              "  0.9275550246238708,\n",
              "  1.0318012237548828,\n",
              "  0.9865475296974182,\n",
              "  0.9126821160316467,\n",
              "  1.0585201978683472,\n",
              "  1.0255286693572998,\n",
              "  1.0180526971817017,\n",
              "  0.9877411127090454,\n",
              "  0.9309743046760559,\n",
              "  0.9777036905288696,\n",
              "  0.9477270245552063,\n",
              "  1.0213309526443481,\n",
              "  0.9702925682067871,\n",
              "  1.0132135152816772,\n",
              "  1.0530849695205688,\n",
              "  1.001207709312439,\n",
              "  1.017804503440857,\n",
              "  0.9803410172462463,\n",
              "  0.9782730937004089,\n",
              "  1.0207524299621582,\n",
              "  0.9811182022094727,\n",
              "  1.018484115600586,\n",
              "  0.9714950919151306,\n",
              "  0.9558125138282776,\n",
              "  1.022367238998413,\n",
              "  0.9425647258758545,\n",
              "  0.9261318445205688,\n",
              "  0.9765064120292664,\n",
              "  0.9679645299911499,\n",
              "  1.0042433738708496,\n",
              "  1.0022212266921997,\n",
              "  0.9637131690979004,\n",
              "  0.9871913194656372,\n",
              "  0.998439371585846,\n",
              "  1.0265483856201172,\n",
              "  1.03489089012146,\n",
              "  0.9630364179611206,\n",
              "  1.046783447265625,\n",
              "  1.0308572053909302,\n",
              "  0.9947592616081238,\n",
              "  1.0059492588043213,\n",
              "  0.9826405048370361,\n",
              "  0.9994685649871826,\n",
              "  0.9423415660858154,\n",
              "  0.9842872619628906,\n",
              "  0.9935066103935242,\n",
              "  0.9922102689743042,\n",
              "  0.9505598545074463,\n",
              "  0.9257707595825195,\n",
              "  0.9951656460762024,\n",
              "  0.9130443930625916,\n",
              "  0.9789464473724365,\n",
              "  1.006967544555664,\n",
              "  0.9785938858985901,\n",
              "  0.9540995955467224,\n",
              "  0.9105798602104187,\n",
              "  1.0356005430221558,\n",
              "  0.9590052366256714,\n",
              "  0.9745792150497437,\n",
              "  0.928600549697876,\n",
              "  0.9893834590911865,\n",
              "  0.9890455007553101,\n",
              "  0.9198359251022339,\n",
              "  1.005827784538269,\n",
              "  1.0002152919769287,\n",
              "  0.9536728262901306,\n",
              "  0.9384844899177551,\n",
              "  0.9591628313064575,\n",
              "  0.9182060360908508,\n",
              "  0.9743178486824036,\n",
              "  0.9594938158988953,\n",
              "  0.9497524499893188,\n",
              "  0.955801248550415,\n",
              "  0.9804867506027222,\n",
              "  0.978843092918396,\n",
              "  0.9447895884513855,\n",
              "  0.9366949796676636,\n",
              "  0.9302849173545837,\n",
              "  0.9382408261299133,\n",
              "  0.9752570986747742,\n",
              "  1.006195068359375,\n",
              "  0.9647625088691711,\n",
              "  1.001115083694458,\n",
              "  1.0227280855178833,\n",
              "  0.908866822719574,\n",
              "  0.9966718554496765,\n",
              "  0.9133806228637695,\n",
              "  1.011173129081726,\n",
              "  0.8750945925712585,\n",
              "  0.9666267037391663,\n",
              "  0.9677631258964539,\n",
              "  1.0013655424118042,\n",
              "  0.9606636762619019,\n",
              "  0.8950355648994446,\n",
              "  0.9384675621986389,\n",
              "  0.912888765335083,\n",
              "  0.9287646412849426,\n",
              "  0.9844550490379333,\n",
              "  0.9074318408966064,\n",
              "  0.9579597115516663,\n",
              "  1.0012168884277344,\n",
              "  0.8841075897216797,\n",
              "  0.9610681533813477,\n",
              "  0.9189308881759644,\n",
              "  1.0073890686035156,\n",
              "  0.9677689075469971,\n",
              "  0.9582244753837585,\n",
              "  0.970731794834137,\n",
              "  0.9120326042175293,\n",
              "  0.9361375570297241,\n",
              "  0.8776882290840149,\n",
              "  0.965070903301239,\n",
              "  1.0139389038085938,\n",
              "  0.9670897126197815,\n",
              "  0.9299782514572144,\n",
              "  0.9506149291992188,\n",
              "  0.93854820728302,\n",
              "  0.8737829327583313,\n",
              "  0.942616879940033,\n",
              "  1.0019969940185547,\n",
              "  0.9598695039749146,\n",
              "  0.9205276370048523,\n",
              "  0.938332736492157,\n",
              "  0.9925652742385864,\n",
              "  0.958709180355072,\n",
              "  0.9084678292274475,\n",
              "  0.9131019711494446,\n",
              "  0.9506540894508362,\n",
              "  0.9222564697265625,\n",
              "  0.965386152267456,\n",
              "  0.9872189164161682,\n",
              "  0.9741111993789673,\n",
              "  0.9377985596656799,\n",
              "  0.9935697317123413,\n",
              "  0.9957840442657471,\n",
              "  0.9848901629447937,\n",
              "  0.9197870492935181,\n",
              "  0.9696769118309021,\n",
              "  0.9923528432846069,\n",
              "  0.9803081750869751,\n",
              "  0.9205023646354675,\n",
              "  0.9792897701263428,\n",
              "  0.9159138798713684,\n",
              "  0.9427580237388611,\n",
              "  0.9439026713371277,\n",
              "  0.9395984411239624,\n",
              "  0.9232762455940247,\n",
              "  0.9537050724029541,\n",
              "  0.9136949777603149,\n",
              "  0.9692274928092957,\n",
              "  0.915920078754425,\n",
              "  0.9798449277877808,\n",
              "  0.9308964610099792,\n",
              "  0.9129592180252075,\n",
              "  0.9759770035743713,\n",
              "  0.9917731285095215,\n",
              "  0.9511426091194153,\n",
              "  0.9164091348648071,\n",
              "  0.9510283470153809,\n",
              "  0.9898413419723511,\n",
              "  0.9485504031181335,\n",
              "  0.9429328441619873,\n",
              "  0.9345204830169678,\n",
              "  1.0241179466247559,\n",
              "  0.9029536843299866,\n",
              "  0.885453462600708,\n",
              "  0.9012892842292786,\n",
              "  0.964599609375,\n",
              "  0.9391252994537354,\n",
              "  0.9794962406158447,\n",
              "  0.9522349834442139,\n",
              "  0.9775298833847046,\n",
              "  0.9009215235710144,\n",
              "  0.893460214138031,\n",
              "  0.9220524430274963,\n",
              "  0.963517963886261,\n",
              "  0.9276060461997986,\n",
              "  0.9538627862930298,\n",
              "  0.9155128002166748,\n",
              "  0.9715970158576965,\n",
              "  0.9326562881469727,\n",
              "  0.895962655544281,\n",
              "  0.8825487494468689,\n",
              "  0.9518653154373169,\n",
              "  0.9441991448402405,\n",
              "  0.8920823335647583,\n",
              "  0.912182629108429,\n",
              "  0.9503946304321289,\n",
              "  0.9528487324714661,\n",
              "  0.8965729475021362,\n",
              "  0.980237603187561,\n",
              "  0.9423893690109253,\n",
              "  0.9612809419631958,\n",
              "  0.9716270565986633,\n",
              "  0.9330468773841858,\n",
              "  0.8618990778923035,\n",
              "  0.9274305105209351,\n",
              "  0.9543035626411438,\n",
              "  0.870014488697052,\n",
              "  0.937373697757721,\n",
              "  0.9088767170906067,\n",
              "  0.9705935120582581,\n",
              "  0.9438121318817139,\n",
              "  0.9176743626594543,\n",
              "  0.951276957988739,\n",
              "  0.9951118230819702,\n",
              "  0.8968333601951599,\n",
              "  0.9040854573249817,\n",
              "  0.971494197845459,\n",
              "  0.9685938954353333,\n",
              "  0.9525017738342285,\n",
              "  0.8643597960472107,\n",
              "  0.9022671580314636,\n",
              "  0.8806873559951782,\n",
              "  1.0088412761688232,\n",
              "  0.9107730388641357,\n",
              "  0.8946419358253479,\n",
              "  0.8770204186439514,\n",
              "  0.8895158767700195,\n",
              "  0.9610384106636047,\n",
              "  0.9396777749061584,\n",
              "  0.9068648219108582,\n",
              "  0.9534569978713989,\n",
              "  0.8895077705383301,\n",
              "  0.9271921515464783,\n",
              "  0.9229525327682495,\n",
              "  0.9479133486747742,\n",
              "  0.9119877219200134,\n",
              "  0.8957452774047852,\n",
              "  0.8954293727874756,\n",
              "  0.8777756690979004,\n",
              "  0.912391722202301,\n",
              "  0.8571942448616028,\n",
              "  0.8907314538955688,\n",
              "  0.9106935262680054,\n",
              "  0.9331913590431213,\n",
              "  0.9616941809654236,\n",
              "  0.954867422580719,\n",
              "  0.893639087677002,\n",
              "  0.869831383228302,\n",
              "  0.8801824450492859,\n",
              "  0.8691580891609192,\n",
              "  0.8865711092948914,\n",
              "  0.9080638289451599,\n",
              "  0.9332361221313477,\n",
              "  0.9256525635719299,\n",
              "  0.9137477874755859,\n",
              "  0.8674831986427307,\n",
              "  0.9562139511108398,\n",
              "  0.9259639978408813,\n",
              "  0.9152167439460754,\n",
              "  0.9047178626060486,\n",
              "  0.8991308212280273,\n",
              "  0.9587087035179138,\n",
              "  0.9267053604125977,\n",
              "  0.9056054353713989,\n",
              "  0.9087036848068237,\n",
              "  0.9374144077301025,\n",
              "  0.9211956262588501,\n",
              "  0.9109452366828918,\n",
              "  0.9292628169059753,\n",
              "  0.9018352031707764,\n",
              "  0.8901158571243286,\n",
              "  0.9070777893066406,\n",
              "  0.9192793965339661,\n",
              "  0.871202290058136,\n",
              "  0.874397873878479,\n",
              "  0.9412257075309753,\n",
              "  0.8923542499542236,\n",
              "  0.8709374666213989,\n",
              "  0.8527665734291077,\n",
              "  0.9151816964149475,\n",
              "  0.910520076751709,\n",
              "  0.9065758585929871,\n",
              "  0.8885235786437988,\n",
              "  0.8783798813819885,\n",
              "  0.8813961148262024,\n",
              "  0.8674954175949097,\n",
              "  0.8831672072410583,\n",
              "  0.8987041711807251,\n",
              "  0.8791008591651917,\n",
              "  0.8896262645721436,\n",
              "  0.9248006343841553,\n",
              "  0.9285743236541748,\n",
              "  0.8925517201423645,\n",
              "  0.871934711933136,\n",
              "  0.918071448802948,\n",
              "  0.8933660387992859,\n",
              "  0.8587632775306702,\n",
              "  0.9315301179885864,\n",
              "  0.865373432636261,\n",
              "  0.9283928275108337,\n",
              "  0.8579091429710388,\n",
              "  0.8994957804679871,\n",
              "  0.874687135219574,\n",
              "  0.8674312233924866,\n",
              "  0.8460212349891663,\n",
              "  0.9131514430046082,\n",
              "  0.9166301488876343,\n",
              "  0.861589252948761,\n",
              "  0.9067668914794922,\n",
              "  0.8894259333610535,\n",
              "  0.9379443526268005,\n",
              "  0.9007633924484253,\n",
              "  0.8564628958702087,\n",
              "  0.9077022671699524,\n",
              "  0.8838555216789246,\n",
              "  0.8726171255111694,\n",
              "  0.8988835215568542,\n",
              "  0.9062756299972534,\n",
              "  0.9073715806007385,\n",
              "  0.8880781531333923,\n",
              "  0.9181278944015503,\n",
              "  0.8884851932525635,\n",
              "  0.9404179453849792,\n",
              "  0.8886224031448364,\n",
              "  0.8338173627853394,\n",
              "  0.8350131511688232,\n",
              "  0.9328771829605103,\n",
              "  0.9146485328674316,\n",
              "  0.8711807131767273,\n",
              "  0.8740055561065674,\n",
              "  0.8477898836135864,\n",
              "  0.946269690990448,\n",
              "  0.8832457661628723,\n",
              "  0.9059470295906067,\n",
              "  0.8618236184120178,\n",
              "  0.8784229159355164,\n",
              "  0.9023014307022095,\n",
              "  0.9312242865562439,\n",
              "  0.9785916209220886,\n",
              "  0.865494430065155,\n",
              "  0.8866701126098633,\n",
              "  0.8783004283905029,\n",
              "  0.8581233620643616,\n",
              "  0.9213449954986572,\n",
              "  0.9029771685600281,\n",
              "  0.9090705513954163,\n",
              "  0.8711754679679871,\n",
              "  0.9321314692497253,\n",
              "  0.9015213251113892,\n",
              "  0.907368540763855,\n",
              "  0.893544614315033,\n",
              "  0.8675891160964966,\n",
              "  0.8685514330863953,\n",
              "  0.8517907857894897,\n",
              "  0.8952156901359558,\n",
              "  0.8340689539909363,\n",
              "  0.9247923493385315,\n",
              "  0.8297845125198364,\n",
              "  0.888820230960846,\n",
              "  0.8565896153450012],\n",
              " 'acc': [0.398703396320343,\n",
              "  0.38087520003318787,\n",
              "  0.3873581886291504,\n",
              "  0.4035656452178955,\n",
              "  0.35494327545166016,\n",
              "  0.3687500059604645,\n",
              "  0.3662884831428528,\n",
              "  0.41166937351226807,\n",
              "  0.4132901132106781,\n",
              "  0.3727714717388153,\n",
              "  0.37925446033477783,\n",
              "  0.3776337206363678,\n",
              "  0.41004863381385803,\n",
              "  0.3581847548484802,\n",
              "  0.3922204077243805,\n",
              "  0.39546191692352295,\n",
              "  0.41166937351226807,\n",
              "  0.3938411772251129,\n",
              "  0.38087520003318787,\n",
              "  0.4035656452178955,\n",
              "  0.40518638491630554,\n",
              "  0.397082656621933,\n",
              "  0.3824959397315979,\n",
              "  0.3630470037460327,\n",
              "  0.38573744893074036,\n",
              "  0.40312498807907104,\n",
              "  0.4068071246147156,\n",
              "  0.4181523621082306,\n",
              "  0.398703396320343,\n",
              "  0.41166937351226807,\n",
              "  0.4197731018066406,\n",
              "  0.398703396320343,\n",
              "  0.4078125059604645,\n",
              "  0.37925446033477783,\n",
              "  0.3727714717388153,\n",
              "  0.39546191692352295,\n",
              "  0.39059966802597046,\n",
              "  0.3938411772251129,\n",
              "  0.40032413601875305,\n",
              "  0.3776337206363678,\n",
              "  0.3873581886291504,\n",
              "  0.3711507320404053,\n",
              "  0.42139384150505066,\n",
              "  0.4294975697994232,\n",
              "  0.4068071246147156,\n",
              "  0.3938411772251129,\n",
              "  0.421875,\n",
              "  0.44084277749061584,\n",
              "  0.38573744893074036,\n",
              "  0.39546191692352295,\n",
              "  0.38749998807907104,\n",
              "  0.3760129511356354,\n",
              "  0.4019449055194855,\n",
              "  0.397082656621933,\n",
              "  0.4197731018066406,\n",
              "  0.38087520003318787,\n",
              "  0.3968749940395355,\n",
              "  0.41166937351226807,\n",
              "  0.4197731018066406,\n",
              "  0.4019449055194855,\n",
              "  0.4538087546825409,\n",
              "  0.4132901132106781,\n",
              "  0.39546191692352295,\n",
              "  0.41004863381385803,\n",
              "  0.42625609040260315,\n",
              "  0.4181523621082306,\n",
              "  0.397082656621933,\n",
              "  0.38749998807907104,\n",
              "  0.4181523621082306,\n",
              "  0.43598055839538574,\n",
              "  0.40032413601875305,\n",
              "  0.4343598186969757,\n",
              "  0.39375001192092896,\n",
              "  0.4278768301010132,\n",
              "  0.39546191692352295,\n",
              "  0.397082656621933,\n",
              "  0.4181523621082306,\n",
              "  0.44246354699134827,\n",
              "  0.4440842866897583,\n",
              "  0.4084278643131256,\n",
              "  0.43598055839538574,\n",
              "  0.4181523621082306,\n",
              "  0.44084277749061584,\n",
              "  0.4124999940395355,\n",
              "  0.44570502638816833,\n",
              "  0.4327390491962433,\n",
              "  0.42625609040260315,\n",
              "  0.4327390491962433,\n",
              "  0.42139384150505066,\n",
              "  0.38411667943000793,\n",
              "  0.4505672752857208,\n",
              "  0.39531248807907104,\n",
              "  0.4392220377922058,\n",
              "  0.47163695096969604,\n",
              "  0.4635332226753235,\n",
              "  0.4489465057849884,\n",
              "  0.4278768301010132,\n",
              "  0.4278768301010132,\n",
              "  0.4376012980937958,\n",
              "  0.3873581886291504,\n",
              "  0.4181523621082306,\n",
              "  0.468395471572876,\n",
              "  0.41491085290908813,\n",
              "  0.44246354699134827,\n",
              "  0.46191248297691345,\n",
              "  0.4246353209018707,\n",
              "  0.42625609040260315,\n",
              "  0.43111830949783325,\n",
              "  0.41874998807907104,\n",
              "  0.4440842866897583,\n",
              "  0.4197731018066406,\n",
              "  0.37439221143722534,\n",
              "  0.42139384150505066,\n",
              "  0.4294975697994232,\n",
              "  0.4278768301010132,\n",
              "  0.44732576608657837,\n",
              "  0.44084277749061584,\n",
              "  0.41166937351226807,\n",
              "  0.4132901132106781,\n",
              "  0.4392220377922058,\n",
              "  0.4124999940395355,\n",
              "  0.4554294943809509,\n",
              "  0.4602917432785034,\n",
              "  0.44218748807907104,\n",
              "  0.47811993956565857,\n",
              "  0.44246354699134827,\n",
              "  0.4651539623737335,\n",
              "  0.46677470207214355,\n",
              "  0.4440842866897583,\n",
              "  0.4376012980937958,\n",
              "  0.4748784303665161,\n",
              "  0.458670973777771,\n",
              "  0.44246354699134827,\n",
              "  0.468395471572876,\n",
              "  0.3938411772251129,\n",
              "  0.47343748807907104,\n",
              "  0.4440842866897583,\n",
              "  0.4392220377922058,\n",
              "  0.45705023407936096,\n",
              "  0.45705023407936096,\n",
              "  0.48784440755844116,\n",
              "  0.4359374940395355,\n",
              "  0.47811993956565857,\n",
              "  0.4505672752857208,\n",
              "  0.4554294943809509,\n",
              "  0.47649919986724854,\n",
              "  0.4943273961544037,\n",
              "  0.4278768301010132,\n",
              "  0.4437499940395355,\n",
              "  0.4376012980937958,\n",
              "  0.46875,\n",
              "  0.44246354699134827,\n",
              "  0.4635332226753235,\n",
              "  0.4538087546825409,\n",
              "  0.4392220377922058,\n",
              "  0.4392220377922058,\n",
              "  0.4327390491962433,\n",
              "  0.4538087546825409,\n",
              "  0.4392220377922058,\n",
              "  0.458670973777771,\n",
              "  0.4991896152496338,\n",
              "  0.47649919986724854,\n",
              "  0.48622366786003113,\n",
              "  0.4797406792640686,\n",
              "  0.468395471572876,\n",
              "  0.4489465057849884,\n",
              "  0.43281251192092896,\n",
              "  0.4440842866897583,\n",
              "  0.46677470207214355,\n",
              "  0.470016211271286,\n",
              "  0.4846029281616211,\n",
              "  0.45705023407936096,\n",
              "  0.48136141896247864,\n",
              "  0.44084277749061584,\n",
              "  0.4538087546825409,\n",
              "  0.44732576608657837,\n",
              "  0.4440842866897583,\n",
              "  0.4651539623737335,\n",
              "  0.48906248807907104,\n",
              "  0.468395471572876,\n",
              "  0.4635332226753235,\n",
              "  0.4732576906681061,\n",
              "  0.470016211271286,\n",
              "  0.468395471572876,\n",
              "  0.48622366786003113,\n",
              "  0.458670973777771,\n",
              "  0.468395471572876,\n",
              "  0.4797406792640686,\n",
              "  0.47163695096969604,\n",
              "  0.4797406792640686,\n",
              "  0.4505672752857208,\n",
              "  0.47163695096969604,\n",
              "  0.46677470207214355,\n",
              "  0.4343598186969757,\n",
              "  0.45705023407936096,\n",
              "  0.468395471572876,\n",
              "  0.468395471572876,\n",
              "  0.48784440755844116,\n",
              "  0.46677470207214355,\n",
              "  0.49270665645599365,\n",
              "  0.4554294943809509,\n",
              "  0.4651539623737335,\n",
              "  0.49108588695526123,\n",
              "  0.4651539623737335,\n",
              "  0.4797406792640686,\n",
              "  0.4181523621082306,\n",
              "  0.45218801498413086,\n",
              "  0.468395471572876,\n",
              "  0.4797406792640686,\n",
              "  0.46677470207214355,\n",
              "  0.4651539623737335,\n",
              "  0.458670973777771,\n",
              "  0.48136141896247864,\n",
              "  0.47163695096969604,\n",
              "  0.470016211271286,\n",
              "  0.48136141896247864,\n",
              "  0.4505672752857208,\n",
              "  0.503125011920929,\n",
              "  0.4602917432785034,\n",
              "  0.47649919986724854,\n",
              "  0.4554294943809509,\n",
              "  0.4894651472568512,\n",
              "  0.5121555924415588,\n",
              "  0.4732576906681061,\n",
              "  0.4651539623737335,\n",
              "  0.47968751192092896,\n",
              "  0.4943273961544037,\n",
              "  0.5105348229408264,\n",
              "  0.48298215866088867,\n",
              "  0.4602917432785034,\n",
              "  0.4846029281616211,\n",
              "  0.47163695096969604,\n",
              "  0.47163695096969604,\n",
              "  0.4921875,\n",
              "  0.470016211271286,\n",
              "  0.4846029281616211,\n",
              "  0.47163695096969604,\n",
              "  0.4846029281616211,\n",
              "  0.47187501192092896,\n",
              "  0.468395471572876,\n",
              "  0.5072933435440063,\n",
              "  0.48298215866088867,\n",
              "  0.4797406792640686,\n",
              "  0.4943273961544037,\n",
              "  0.5089141130447388,\n",
              "  0.4846029281616211,\n",
              "  0.47811993956565857,\n",
              "  0.4651539623737335,\n",
              "  0.4732576906681061,\n",
              "  0.48622366786003113,\n",
              "  0.47811993956565857,\n",
              "  0.49108588695526123,\n",
              "  0.46677470207214355,\n",
              "  0.47163695096969604,\n",
              "  0.47811993956565857,\n",
              "  0.4828124940395355,\n",
              "  0.46677470207214355,\n",
              "  0.49756887555122375,\n",
              "  0.5170178413391113,\n",
              "  0.5008103847503662,\n",
              "  0.47187501192092896,\n",
              "  0.5137763619422913,\n",
              "  0.4748784303665161,\n",
              "  0.44732576608657837,\n",
              "  0.4959481358528137,\n",
              "  0.49270665645599365,\n",
              "  0.5234375,\n",
              "  0.5056726336479187,\n",
              "  0.46677470207214355,\n",
              "  0.468395471572876,\n",
              "  0.49270665645599365,\n",
              "  0.48136141896247864,\n",
              "  0.4635332226753235,\n",
              "  0.5235008001327515,\n",
              "  0.5105348229408264,\n",
              "  0.46677470207214355,\n",
              "  0.48136141896247864,\n",
              "  0.4921875,\n",
              "  0.5024310946464539,\n",
              "  0.4943273961544037,\n",
              "  0.48784440755844116,\n",
              "  0.4959481358528137,\n",
              "  0.47649919986724854,\n",
              "  0.5137763619422913,\n",
              "  0.4846029281616211,\n",
              "  0.4991896152496338,\n",
              "  0.48136141896247864,\n",
              "  0.47649919986724854,\n",
              "  0.5510534644126892,\n",
              "  0.49687498807907104,\n",
              "  0.48622366786003113,\n",
              "  0.4703125059604645,\n",
              "  0.510937511920929,\n",
              "  0.48622366786003113,\n",
              "  0.4538087546825409,\n",
              "  0.518638551235199,\n",
              "  0.4894651472568512,\n",
              "  0.4894651472568512,\n",
              "  0.49270665645599365,\n",
              "  0.470016211271286,\n",
              "  0.48784440755844116,\n",
              "  0.49108588695526123,\n",
              "  0.5024310946464539,\n",
              "  0.5380875468254089,\n",
              "  0.5089141130447388,\n",
              "  0.5056726336479187,\n",
              "  0.5040518641471863,\n",
              "  0.49756887555122375,\n",
              "  0.5299838185310364,\n",
              "  0.5380875468254089,\n",
              "  0.5267422795295715,\n",
              "  0.528363049030304,\n",
              "  0.48298215866088867,\n",
              "  0.49270665645599365,\n",
              "  0.48622366786003113,\n",
              "  0.5153970718383789,\n",
              "  0.5267422795295715,\n",
              "  0.5364667773246765,\n",
              "  0.5218800902366638,\n",
              "  0.5008103847503662,\n",
              "  0.5153970718383789,\n",
              "  0.5089141130447388,\n",
              "  0.5202593207359314,\n",
              "  0.4943273961544037,\n",
              "  0.49756887555122375,\n",
              "  0.5024310946464539,\n",
              "  0.4991896152496338,\n",
              "  0.5089141130447388,\n",
              "  0.5170178413391113,\n",
              "  0.48298215866088867,\n",
              "  0.468395471572876,\n",
              "  0.4921875,\n",
              "  0.4991896152496338,\n",
              "  0.4959481358528137,\n",
              "  0.5202593207359314,\n",
              "  0.5,\n",
              "  0.5235008001327515,\n",
              "  0.5170178413391113,\n",
              "  0.4991896152496338,\n",
              "  0.4959481358528137,\n",
              "  0.5348460078239441,\n",
              "  0.518638551235199,\n",
              "  0.47649919986724854,\n",
              "  0.5153970718383789,\n",
              "  0.4906249940395355,\n",
              "  0.518638551235199,\n",
              "  0.5105348229408264,\n",
              "  0.5267422795295715,\n",
              "  0.5348460078239441,\n",
              "  0.531604528427124,\n",
              "  0.5510534644126892,\n",
              "  0.5153970718383789,\n",
              "  0.5332252979278564,\n",
              "  0.5170178413391113,\n",
              "  0.520312488079071,\n",
              "  0.528363049030304,\n",
              "  0.5170178413391113,\n",
              "  0.5137763619422913,\n",
              "  0.5040518641471863,\n",
              "  0.49756887555122375,\n",
              "  0.48622366786003113,\n",
              "  0.526562511920929,\n",
              "  0.5251215696334839,\n",
              "  0.5072933435440063,\n",
              "  0.541329026222229,\n",
              "  0.554295003414154,\n",
              "  0.5121555924415588,\n",
              "  0.5008103847503662,\n",
              "  0.4894651472568512,\n",
              "  0.5040518641471863,\n",
              "  0.5137763619422913,\n",
              "  0.5202593207359314,\n",
              "  0.5170178413391113,\n",
              "  0.5089141130447388,\n",
              "  0.518638551235199,\n",
              "  0.5105348229408264,\n",
              "  0.5170178413391113,\n",
              "  0.5364667773246765,\n",
              "  0.5267422795295715,\n",
              "  0.5056726336479187,\n",
              "  0.5299838185310364,\n",
              "  0.48622366786003113,\n",
              "  0.5218800902366638,\n",
              "  0.5526742339134216,\n",
              "  0.49756887555122375,\n",
              "  0.5299838185310364,\n",
              "  0.5121555924415588,\n",
              "  0.5251215696334839,\n",
              "  0.5299838185310364,\n",
              "  0.5170178413391113,\n",
              "  0.528363049030304,\n",
              "  0.5296875238418579,\n",
              "  0.531604528427124,\n",
              "  0.5332252979278564,\n",
              "  0.5249999761581421,\n",
              "  0.5562499761581421,\n",
              "  0.5105348229408264,\n",
              "  0.5267422795295715,\n",
              "  0.5170178413391113,\n",
              "  0.518750011920929,\n",
              "  0.5105348229408264,\n",
              "  0.5332252979278564,\n",
              "  0.518638551235199,\n",
              "  0.5202593207359314,\n",
              "  0.5546875,\n",
              "  0.520312488079071,\n",
              "  0.5235008001327515,\n",
              "  0.528124988079071,\n",
              "  0.5380875468254089,\n",
              "  0.5024310946464539,\n",
              "  0.518750011920929,\n",
              "  0.5834683775901794,\n",
              "  0.5484374761581421,\n",
              "  0.541329026222229,\n",
              "  0.49108588695526123,\n",
              "  0.5040518641471863,\n",
              "  0.541329026222229,\n",
              "  0.5494327545166016,\n",
              "  0.531604528427124,\n",
              "  0.5332252979278564,\n",
              "  0.528363049030304,\n",
              "  0.5380875468254089,\n",
              "  0.5024310946464539,\n",
              "  0.5348460078239441,\n",
              "  0.5640194416046143,\n",
              "  0.4943273961544037,\n",
              "  0.531604528427124,\n",
              "  0.5397082567214966,\n",
              "  0.518638551235199,\n",
              "  0.5445705056190491,\n",
              "  0.5218800902366638,\n",
              "  0.518638551235199,\n",
              "  0.5461912751197815,\n",
              "  0.5656402111053467,\n",
              "  0.5015624761581421,\n",
              "  0.5251215696334839,\n",
              "  0.5607779622077942,\n",
              "  0.541329026222229,\n",
              "  0.5089141130447388,\n",
              "  0.5364667773246765,\n",
              "  0.5218800902366638,\n",
              "  0.5429497361183167,\n",
              "  0.5607779622077942,\n",
              "  0.5218750238418579,\n",
              "  0.5494327545166016,\n",
              "  0.5429497361183167,\n",
              "  0.5494327545166016,\n",
              "  0.5445705056190491,\n",
              "  0.528363049030304,\n",
              "  0.5218800902366638,\n",
              "  0.5332252979278564,\n",
              "  0.5364667773246765,\n",
              "  0.5299838185310364,\n",
              "  0.47649919986724854,\n",
              "  0.5153970718383789,\n",
              "  0.5380875468254089,\n",
              "  0.5364667773246765,\n",
              "  0.5202593207359314,\n",
              "  0.49756887555122375,\n",
              "  0.5332252979278564,\n",
              "  0.5202593207359314,\n",
              "  0.5494327545166016,\n",
              "  0.5429497361183167,\n",
              "  0.5299838185310364,\n",
              "  0.546875,\n",
              "  0.5478119850158691,\n",
              "  0.541329026222229,\n",
              "  0.5591571927070618,\n",
              "  0.5623987317085266,\n",
              "  0.5332252979278564,\n",
              "  0.5640624761581421,\n",
              "  0.5591571927070618,\n",
              "  0.5591571927070618,\n",
              "  0.5559157133102417,\n",
              "  0.5834683775901794,\n",
              "  0.5332252979278564,\n",
              "  0.5170178413391113,\n",
              "  0.5397082567214966,\n",
              "  0.5397082567214966,\n",
              "  0.526562511920929,\n",
              "  0.5429497361183167,\n",
              "  0.528124988079071,\n",
              "  0.5089141130447388,\n",
              "  0.5332252979278564,\n",
              "  0.5591571927070618,\n",
              "  0.5494327545166016,\n",
              "  0.5105348229408264,\n",
              "  0.5786061882972717,\n",
              "  0.5153970718383789,\n",
              "  0.5559157133102417,\n",
              "  0.5348460078239441,\n",
              "  0.5332252979278564,\n",
              "  0.5380875468254089,\n",
              "  0.541329026222229,\n",
              "  0.5202593207359314,\n",
              "  0.5328124761581421,\n",
              "  0.528363049030304,\n",
              "  0.528363049030304,\n",
              "  0.5202593207359314,\n",
              "  0.5640194416046143,\n",
              "  0.5299838185310364,\n",
              "  0.5299838185310364,\n",
              "  0.5769854187965393,\n",
              "  0.554295003414154,\n",
              "  0.5397082567214966,\n",
              "  0.5296875238418579,\n",
              "  0.5688816905021667,\n",
              "  0.5802268981933594,\n",
              "  0.5332252979278564,\n",
              "  0.531604528427124,\n",
              "  0.5380875468254089,\n",
              "  0.5656402111053467,\n",
              "  0.5510534644126892,\n",
              "  0.531604528427124,\n",
              "  0.5445705056190491,\n",
              "  0.5494327545166016,\n",
              "  0.5575364828109741,\n",
              "  0.5688816905021667,\n",
              "  0.5607779622077942,\n",
              "  0.5559157133102417,\n",
              "  0.5406249761581421,\n",
              "  0.528363049030304,\n",
              "  0.5105348229408264,\n",
              "  0.5429497361183167,\n",
              "  0.5494327545166016,\n",
              "  0.5559157133102417,\n",
              "  0.5296875238418579,\n",
              "  0.5478119850158691,\n",
              "  0.5575364828109741,\n",
              "  0.5348460078239441,\n",
              "  0.5575364828109741,\n",
              "  0.5688816905021667,\n",
              "  0.5348460078239441,\n",
              "  0.5251215696334839,\n",
              "  0.5218800902366638,\n",
              "  0.5364667773246765,\n",
              "  0.5510534644126892,\n",
              "  0.5364667773246765,\n",
              "  0.5267422795295715,\n",
              "  0.5769854187965393,\n",
              "  0.5429497361183167,\n",
              "  0.5607779622077942,\n",
              "  0.5591571927070618,\n",
              "  0.5705024600028992,\n",
              "  0.5332252979278564,\n",
              "  0.5737439393997192,\n",
              "  0.5348460078239441,\n",
              "  0.5623987317085266,\n",
              "  0.5445705056190491,\n",
              "  0.5153970718383789,\n",
              "  0.5656402111053467,\n",
              "  0.5397082567214966,\n",
              "  0.5672609210014343,\n",
              "  0.5753646492958069,\n",
              "  0.5656402111053467,\n",
              "  0.5429497361183167,\n",
              "  0.5218800902366638,\n",
              "  0.554295003414154,\n",
              "  0.5559157133102417,\n",
              "  0.5461912751197815,\n",
              "  0.5526742339134216,\n",
              "  0.5818476676940918,\n",
              "  0.5850891470909119,\n",
              "  0.5559157133102417,\n",
              "  0.5380875468254089,\n",
              "  0.5348460078239441,\n",
              "  0.5688816905021667,\n",
              "  0.5623987317085266,\n",
              "  0.5484374761581421,\n",
              "  0.5380875468254089,\n",
              "  0.5251215696334839,\n",
              "  0.5575364828109741,\n",
              "  0.5656402111053467,\n",
              "  0.5688816905021667,\n",
              "  0.5526742339134216,\n",
              "  0.5429497361183167,\n",
              "  0.5705024600028992,\n",
              "  0.5494327545166016,\n",
              "  0.5575364828109741,\n",
              "  0.528363049030304,\n",
              "  0.5672609210014343,\n",
              "  0.5818476676940918,\n",
              "  0.5656402111053467,\n",
              "  0.5640194416046143,\n",
              "  0.5364667773246765,\n",
              "  0.5510534644126892,\n",
              "  0.5461912751197815,\n",
              "  0.5640194416046143,\n",
              "  0.5705024600028992,\n",
              "  0.5796874761581421,\n",
              "  0.5397082567214966,\n",
              "  0.541329026222229,\n",
              "  0.5531250238418579,\n",
              "  0.5397082567214966,\n",
              "  0.5380875468254089,\n",
              "  0.5607779622077942,\n",
              "  0.5786061882972717,\n",
              "  0.5559157133102417,\n",
              "  0.5786061882972717,\n",
              "  0.5591571927070618,\n",
              "  0.5818476676940918,\n",
              "  0.5753646492958069,\n",
              "  0.5397082567214966,\n",
              "  0.541329026222229,\n",
              "  0.5753646492958069,\n",
              "  0.5461912751197815,\n",
              "  0.5575364828109741,\n",
              "  0.5235008001327515,\n",
              "  0.5575364828109741,\n",
              "  0.5850891470909119,\n",
              "  0.5429497361183167,\n",
              "  0.5737439393997192,\n",
              "  0.5640194416046143,\n",
              "  0.5461912751197815,\n",
              "  0.5575364828109741,\n",
              "  0.5867098569869995,\n",
              "  0.5656402111053467,\n",
              "  0.5526742339134216,\n",
              "  0.518638551235199,\n",
              "  0.5461912751197815,\n",
              "  0.5364667773246765,\n",
              "  0.5494327545166016,\n",
              "  0.5546875,\n",
              "  0.5802268981933594,\n",
              "  0.5818476676940918,\n",
              "  0.5380875468254089,\n",
              "  0.5656402111053467,\n",
              "  0.5445705056190491,\n",
              "  0.5429497361183167,\n",
              "  0.5640194416046143,\n",
              "  0.5850891470909119,\n",
              "  0.541329026222229,\n",
              "  0.5623987317085266,\n",
              "  0.6077795624732971,\n",
              "  0.5802268981933594,\n",
              "  0.5348460078239441,\n",
              "  0.5591571927070618,\n",
              "  0.5640194416046143,\n",
              "  0.5818476676940918,\n",
              "  0.5526742339134216,\n",
              "  0.5526742339134216,\n",
              "  0.5575364828109741,\n",
              "  0.5445705056190491,\n",
              "  0.5786061882972717,\n",
              "  0.5607779622077942,\n",
              "  0.5607779622077942,\n",
              "  0.5921875238418579,\n",
              "  0.591572105884552,\n",
              "  0.601296603679657,\n",
              "  0.5656402111053467,\n",
              "  0.5769854187965393,\n",
              "  0.6288492679595947,\n",
              "  0.554295003414154,\n",
              "  0.5526742339134216,\n",
              "  0.5640624761581421,\n",
              "  0.5834683775901794,\n",
              "  0.5640624761581421,\n",
              "  0.591572105884552,\n",
              "  0.5786061882972717,\n",
              "  0.5623987317085266,\n",
              "  0.5948135852813721,\n",
              "  0.5445705056190491,\n",
              "  0.5656402111053467,\n",
              "  0.5980551242828369,\n",
              "  0.5656402111053467,\n",
              "  0.5818476676940918,\n",
              "  0.5672609210014343,\n",
              "  0.5640194416046143,\n",
              "  0.6029173135757446,\n",
              "  0.5478119850158691,\n",
              "  0.591572105884552,\n",
              "  0.5834683775901794,\n",
              "  0.5850891470909119,\n",
              "  0.5948135852813721,\n",
              "  0.6256077885627747,\n",
              "  0.5802268981933594,\n",
              "  0.5899513959884644,\n",
              "  0.5705024600028992,\n",
              "  0.5948135852813721,\n",
              "  0.6077795624732971,\n",
              "  0.554295003414154,\n",
              "  0.5737439393997192,\n",
              "  0.5640194416046143,\n",
              "  0.5672609210014343,\n",
              "  0.5623987317085266,\n",
              "  0.5461912751197815,\n",
              "  0.5802268981933594,\n",
              "  0.5445705056190491,\n",
              "  0.5818476676940918,\n",
              "  0.5818476676940918,\n",
              "  0.5769854187965393,\n",
              "  0.6061588525772095,\n",
              "  0.554295003414154,\n",
              "  0.5623987317085266,\n",
              "  0.5850891470909119,\n",
              "  0.5899513959884644,\n",
              "  0.5948135852813721,\n",
              "  0.5899513959884644,\n",
              "  0.6175040602684021,\n",
              "  0.5980551242828369,\n",
              "  0.5705024600028992,\n",
              "  0.5948135852813721,\n",
              "  0.5899513959884644,\n",
              "  0.6061588525772095,\n",
              "  0.5688816905021667,\n",
              "  0.5802268981933594,\n",
              "  0.5948135852813721,\n",
              "  0.5818476676940918,\n",
              "  0.5623987317085266,\n",
              "  0.5672609210014343,\n",
              "  0.5996758341789246,\n",
              "  0.5656402111053467,\n",
              "  0.5769854187965393,\n",
              "  0.6175040602684021,\n",
              "  0.5656402111053467,\n",
              "  0.5688816905021667,\n",
              "  0.5850891470909119,\n",
              "  0.5526742339134216,\n",
              "  0.5867098569869995,\n",
              "  0.5834683775901794,\n",
              "  0.5867098569869995,\n",
              "  0.5931928753852844,\n",
              "  0.578125,\n",
              "  0.5834683775901794,\n",
              "  0.601296603679657,\n",
              "  0.5996758341789246,\n",
              "  0.601296603679657,\n",
              "  0.5769854187965393,\n",
              "  0.5559157133102417,\n",
              "  0.582812488079071,\n",
              "  0.5623987317085266,\n",
              "  0.5786061882972717,\n",
              "  0.5850891470909119,\n",
              "  0.5562499761581421,\n",
              "  0.6110210418701172,\n",
              "  0.5607779622077942,\n",
              "  0.601296603679657,\n",
              "  0.5623987317085266,\n",
              "  0.5899513959884644,\n",
              "  0.5850891470909119,\n",
              "  0.5721231698989868,\n",
              "  0.6158832907676697,\n",
              "  0.5818476676940918,\n",
              "  0.5964343547821045,\n",
              "  0.5964343547821045,\n",
              "  0.5705024600028992,\n",
              "  0.5899513959884644,\n",
              "  0.591572105884552,\n",
              "  0.5607779622077942,\n",
              "  0.5964343547821045,\n",
              "  0.5899513959884644,\n",
              "  0.5980551242828369,\n",
              "  0.5802268981933594,\n",
              "  0.5899513959884644,\n",
              "  0.5802268981933594,\n",
              "  0.5850891470909119,\n",
              "  0.5996758341789246,\n",
              "  0.5980551242828369,\n",
              "  0.625,\n",
              "  0.5931928753852844,\n",
              "  0.5526742339134216,\n",
              "  0.591572105884552,\n",
              "  0.591572105884552,\n",
              "  0.5672609210014343,\n",
              "  0.612500011920929,\n",
              "  0.5964343547821045,\n",
              "  0.5964343547821045,\n",
              "  0.5672609210014343,\n",
              "  0.590624988079071,\n",
              "  0.5703125,\n",
              "  0.5737439393997192,\n",
              "  0.5850891470909119,\n",
              "  0.5964343547821045,\n",
              "  0.6029173135757446,\n",
              "  0.5818476676940918,\n",
              "  0.5786061882972717,\n",
              "  0.5818476676940918,\n",
              "  0.5948135852813721,\n",
              "  0.5867098569869995,\n",
              "  0.5672609210014343,\n",
              "  0.5769854187965393,\n",
              "  0.5721231698989868,\n",
              "  0.5672609210014343,\n",
              "  0.5688816905021667,\n",
              "  0.598437488079071,\n",
              "  0.5737439393997192,\n",
              "  0.5765625238418579,\n",
              "  0.5718749761581421,\n",
              "  0.614262580871582,\n",
              "  0.5786061882972717,\n",
              "  0.5843750238418579,\n",
              "  0.5834683775901794,\n",
              "  0.5883306264877319,\n",
              "  0.614062488079071,\n",
              "  0.5850891470909119,\n",
              "  0.5931928753852844,\n",
              "  0.6061588525772095,\n",
              "  0.5786061882972717,\n",
              "  0.614262580871582,\n",
              "  0.5656402111053467,\n",
              "  0.5996758341789246,\n",
              "  0.604538083076477,\n",
              "  0.5948135852813721,\n",
              "  0.554295003414154,\n",
              "  0.5688816905021667,\n",
              "  0.5850891470909119,\n",
              "  0.5867098569869995,\n",
              "  0.5607779622077942,\n",
              "  0.5818476676940918,\n",
              "  0.591572105884552,\n",
              "  0.6061588525772095,\n",
              "  0.5559157133102417,\n",
              "  0.6223663091659546,\n",
              "  0.614262580871582,\n",
              "  0.598437488079071,\n",
              "  0.5640194416046143,\n",
              "  0.5769854187965393,\n",
              "  0.5607779622077942,\n",
              "  0.5996758341789246,\n",
              "  0.5753646492958069,\n",
              "  0.601296603679657,\n",
              "  0.6191247701644897,\n",
              "  0.6029173135757446,\n",
              "  0.5850891470909119,\n",
              "  0.6029173135757446,\n",
              "  0.5834683775901794,\n",
              "  0.6029173135757446,\n",
              "  0.5721231698989868,\n",
              "  0.6094003319740295,\n",
              "  0.6158832907676697,\n",
              "  0.6029173135757446,\n",
              "  0.5687500238418579,\n",
              "  0.6094003319740295,\n",
              "  0.6158832907676697,\n",
              "  0.6000000238418579,\n",
              "  0.5802268981933594,\n",
              "  0.5867098569869995,\n",
              "  0.620312511920929,\n",
              "  0.574999988079071,\n",
              "  0.5834683775901794,\n",
              "  0.5850891470909119,\n",
              "  0.6029173135757446,\n",
              "  0.604538083076477,\n",
              "  0.6077795624732971,\n",
              "  0.5980551242828369,\n",
              "  0.5834683775901794,\n",
              "  0.6256077885627747,\n",
              "  0.5964343547821045,\n",
              "  0.5931928753852844,\n",
              "  0.5737439393997192,\n",
              "  0.5883306264877319,\n",
              "  0.5996758341789246,\n",
              "  0.5996758341789246,\n",
              "  0.5607779622077942,\n",
              "  0.6126418113708496,\n",
              "  0.5899513959884644,\n",
              "  0.5753646492958069,\n",
              "  0.5834683775901794,\n",
              "  0.5834683775901794,\n",
              "  0.5883306264877319,\n",
              "  0.6158832907676697,\n",
              "  0.6126418113708496,\n",
              "  0.5609375238418579,\n",
              "  0.6029173135757446,\n",
              "  0.6029173135757446,\n",
              "  0.6175040602684021,\n",
              "  0.601296603679657,\n",
              "  0.5818476676940918,\n",
              "  0.5996758341789246,\n",
              "  0.5931928753852844,\n",
              "  0.6061588525772095,\n",
              "  0.6077795624732971,\n",
              "  0.6061588525772095,\n",
              "  0.5948135852813721,\n",
              "  0.5737439393997192,\n",
              "  0.6061588525772095,\n",
              "  0.6109374761581421,\n",
              "  0.6223663091659546,\n",
              "  0.596875011920929,\n",
              "  0.604538083076477,\n",
              "  0.6191247701644897,\n",
              "  0.5948135852813721,\n",
              "  0.5769854187965393,\n",
              "  0.591572105884552,\n",
              "  0.5834683775901794,\n",
              "  0.5575364828109741,\n",
              "  0.5980551242828369,\n",
              "  0.6320907473564148,\n",
              "  0.6239870190620422,\n",
              "  0.6312500238418579,\n",
              "  0.5953124761581421,\n",
              "  0.5996758341789246,\n",
              "  0.5769854187965393,\n",
              "  0.5996758341789246,\n",
              "  0.5953124761581421,\n",
              "  0.6272284984588623,\n",
              "  0.5575364828109741,\n",
              "  0.596875011920929,\n",
              "  0.6077795624732971,\n",
              "  0.6077795624732971,\n",
              "  0.5996758341789246,\n",
              "  0.5607779622077942,\n",
              "  0.5834683775901794,\n",
              "  0.6239870190620422,\n",
              "  0.6304700374603271,\n",
              "  0.5818476676940918,\n",
              "  0.5843750238418579,\n",
              "  0.6094003319740295,\n",
              "  0.5931928753852844,\n",
              "  0.6126418113708496,\n",
              "  0.5953124761581421,\n",
              "  0.6126418113708496,\n",
              "  0.5964343547821045,\n",
              "  0.6191247701644897,\n",
              "  0.601296603679657,\n",
              "  0.5834683775901794,\n",
              "  0.6077795624732971,\n",
              "  0.6288492679595947,\n",
              "  0.6223663091659546,\n",
              "  0.5672609210014343,\n",
              "  0.5964343547821045,\n",
              "  0.5850891470909119,\n",
              "  0.6061588525772095,\n",
              "  0.5867098569869995,\n",
              "  0.6239870190620422,\n",
              "  0.6110210418701172,\n",
              "  0.6207455396652222,\n",
              "  0.604538083076477,\n",
              "  0.614262580871582,\n",
              "  0.5980551242828369,\n",
              "  0.601296603679657,\n",
              "  0.5931928753852844,\n",
              "  0.5867098569869995,\n",
              "  0.6234375238418579,\n",
              "  0.5980551242828369,\n",
              "  0.591572105884552,\n",
              "  0.6077795624732971,\n",
              "  0.5883306264877319,\n",
              "  0.635937511920929,\n",
              "  0.6110210418701172,\n",
              "  0.6207455396652222,\n",
              "  0.6207455396652222,\n",
              "  0.6223663091659546,\n",
              "  0.6288492679595947,\n",
              "  0.6353322267532349,\n",
              "  0.5818476676940918,\n",
              "  0.5948135852813721,\n",
              "  0.6239870190620422,\n",
              "  0.5753646492958069,\n",
              "  0.5964343547821045,\n",
              "  0.601296603679657,\n",
              "  0.604538083076477,\n",
              "  0.6337115168571472,\n",
              "  0.5931928753852844,\n",
              "  0.6094003319740295,\n",
              "  0.614062488079071,\n",
              "  0.5818476676940918,\n",
              "  0.6031249761581421,\n",
              "  0.6077795624732971,\n",
              "  0.614262580871582,\n",
              "  0.6094003319740295,\n",
              "  0.6207455396652222,\n",
              "  0.5818476676940918,\n",
              "  0.6175040602684021,\n",
              "  0.6265624761581421,\n",
              "  0.6337115168571472,\n",
              "  0.5769854187965393,\n",
              "  0.5948135852813721,\n",
              "  0.6077795624732971,\n",
              "  0.5996758341789246,\n",
              "  0.6450567245483398,\n",
              "  0.5705024600028992,\n",
              "  0.591572105884552,\n",
              "  0.591572105884552,\n",
              "  0.6126418113708496,\n",
              "  0.5931928753852844,\n",
              "  0.6094003319740295,\n",
              "  0.5899513959884644,\n",
              "  0.5769854187965393,\n",
              "  0.6158832907676697,\n",
              "  0.6187499761581421,\n",
              "  0.6304700374603271,\n",
              "  0.6256077885627747,\n",
              "  0.5980551242828369,\n",
              "  0.620312511920929,\n",
              "  0.614262580871582,\n",
              "  0.614262580871582,\n",
              "  0.591572105884552,\n",
              "  0.601296603679657,\n",
              "  0.6061588525772095,\n",
              "  0.6029173135757446,\n",
              "  0.6234375238418579,\n",
              "  0.6385737657546997,\n",
              "  0.6191247701644897,\n",
              "  0.5931928753852844,\n",
              "  0.6499999761581421,\n",
              "  0.5948135852813721,\n",
              "  0.6353322267532349,\n",
              "  0.6029173135757446,\n",
              "  0.6223663091659546],\n",
              " 'val_loss': [1.6325204372406006,\n",
              "  1.6087241172790527,\n",
              "  1.5798711776733398,\n",
              "  1.521826982498169,\n",
              "  1.4040868282318115,\n",
              "  1.4675639867782593,\n",
              "  1.310889720916748,\n",
              "  1.4387247562408447,\n",
              "  1.3658573627471924,\n",
              "  1.409034013748169,\n",
              "  1.3335940837860107,\n",
              "  1.3629472255706787,\n",
              "  1.394972801208496,\n",
              "  1.4393787384033203,\n",
              "  1.28960120677948,\n",
              "  1.3008627891540527,\n",
              "  1.3660002946853638,\n",
              "  1.2833762168884277,\n",
              "  1.2660877704620361,\n",
              "  1.2923153638839722,\n",
              "  1.394352674484253,\n",
              "  1.208425521850586,\n",
              "  1.2702054977416992,\n",
              "  1.333726406097412,\n",
              "  1.3586490154266357,\n",
              "  1.2214305400848389,\n",
              "  1.3560956716537476,\n",
              "  1.3636705875396729,\n",
              "  1.322807788848877,\n",
              "  1.2607369422912598,\n",
              "  1.2402675151824951,\n",
              "  1.3383935689926147,\n",
              "  1.2288165092468262,\n",
              "  1.3233505487442017,\n",
              "  1.2825706005096436,\n",
              "  1.2633692026138306,\n",
              "  1.2742761373519897,\n",
              "  1.2240533828735352,\n",
              "  1.1729727983474731,\n",
              "  1.273693323135376,\n",
              "  1.2457590103149414,\n",
              "  1.1418819427490234,\n",
              "  1.2219884395599365,\n",
              "  1.0972185134887695,\n",
              "  1.1496002674102783,\n",
              "  1.255232572555542,\n",
              "  1.1554467678070068,\n",
              "  1.1844624280929565,\n",
              "  1.2278168201446533,\n",
              "  1.3002092838287354,\n",
              "  1.2364320755004883,\n",
              "  1.178680181503296,\n",
              "  1.1980960369110107,\n",
              "  1.164825201034546,\n",
              "  1.0929384231567383,\n",
              "  1.154789924621582,\n",
              "  1.0961251258850098,\n",
              "  1.284916639328003,\n",
              "  1.1500980854034424,\n",
              "  1.2350592613220215,\n",
              "  1.0811097621917725,\n",
              "  1.1588184833526611,\n",
              "  1.2128645181655884,\n",
              "  1.1575305461883545,\n",
              "  1.1717875003814697,\n",
              "  1.2068957090377808,\n",
              "  1.1796770095825195,\n",
              "  1.2713768482208252,\n",
              "  1.0770127773284912,\n",
              "  1.1371889114379883,\n",
              "  1.209845781326294,\n",
              "  1.2153465747833252,\n",
              "  1.2502081394195557,\n",
              "  1.162100076675415,\n",
              "  1.1442463397979736,\n",
              "  1.1853480339050293,\n",
              "  1.1606144905090332,\n",
              "  1.0578551292419434,\n",
              "  1.0869131088256836,\n",
              "  1.1111695766448975,\n",
              "  1.1036803722381592,\n",
              "  1.1093976497650146,\n",
              "  1.0881856679916382,\n",
              "  1.2266390323638916,\n",
              "  1.1090128421783447,\n",
              "  1.2156000137329102,\n",
              "  1.1380594968795776,\n",
              "  1.2001019716262817,\n",
              "  1.1458563804626465,\n",
              "  1.1587833166122437,\n",
              "  1.0952482223510742,\n",
              "  1.1323199272155762,\n",
              "  1.1254196166992188,\n",
              "  1.2069426774978638,\n",
              "  1.1504583358764648,\n",
              "  1.1796776056289673,\n",
              "  1.1664845943450928,\n",
              "  1.028354287147522,\n",
              "  1.1280560493469238,\n",
              "  1.1271744966506958,\n",
              "  1.0771920680999756,\n",
              "  1.1604340076446533,\n",
              "  1.1936697959899902,\n",
              "  1.2193950414657593,\n",
              "  1.2139304876327515,\n",
              "  1.0905742645263672,\n",
              "  1.231576681137085,\n",
              "  1.1875829696655273,\n",
              "  1.1244564056396484,\n",
              "  1.1529440879821777,\n",
              "  1.2164709568023682,\n",
              "  1.1483148336410522,\n",
              "  1.1318023204803467,\n",
              "  1.0869228839874268,\n",
              "  1.1793570518493652,\n",
              "  1.114702582359314,\n",
              "  1.1292235851287842,\n",
              "  1.1113920211791992,\n",
              "  1.194945216178894,\n",
              "  1.1486035585403442,\n",
              "  1.2529622316360474,\n",
              "  1.191691279411316,\n",
              "  1.0861576795578003,\n",
              "  1.0926012992858887,\n",
              "  1.1747814416885376,\n",
              "  1.1068134307861328,\n",
              "  1.117558479309082,\n",
              "  1.0324268341064453,\n",
              "  1.1178133487701416,\n",
              "  1.0575110912322998,\n",
              "  1.103590488433838,\n",
              "  1.1598215103149414,\n",
              "  1.1600992679595947,\n",
              "  1.1511869430541992,\n",
              "  1.227391242980957,\n",
              "  1.2227330207824707,\n",
              "  1.2446200847625732,\n",
              "  1.1469128131866455,\n",
              "  1.0735399723052979,\n",
              "  1.2221815586090088,\n",
              "  1.1039212942123413,\n",
              "  1.2505104541778564,\n",
              "  1.088963508605957,\n",
              "  1.0766814947128296,\n",
              "  1.0512609481811523,\n",
              "  1.1317639350891113,\n",
              "  1.2688865661621094,\n",
              "  1.1203564405441284,\n",
              "  1.2358057498931885,\n",
              "  1.1724854707717896,\n",
              "  1.0631279945373535,\n",
              "  1.2175350189208984,\n",
              "  1.168916940689087,\n",
              "  1.1156814098358154,\n",
              "  1.2018128633499146,\n",
              "  1.181386113166809,\n",
              "  1.191680908203125,\n",
              "  1.1709859371185303,\n",
              "  1.1554118394851685,\n",
              "  1.1577880382537842,\n",
              "  1.1786061525344849,\n",
              "  1.1160547733306885,\n",
              "  1.0005416870117188,\n",
              "  1.0995380878448486,\n",
              "  1.0704034566879272,\n",
              "  1.2806801795959473,\n",
              "  1.1634299755096436,\n",
              "  1.0232988595962524,\n",
              "  1.0697543621063232,\n",
              "  1.1150500774383545,\n",
              "  1.1247472763061523,\n",
              "  1.0089341402053833,\n",
              "  1.212121605873108,\n",
              "  1.2164770364761353,\n",
              "  1.1253429651260376,\n",
              "  1.109155535697937,\n",
              "  1.1530663967132568,\n",
              "  1.0889445543289185,\n",
              "  1.1071438789367676,\n",
              "  1.090088129043579,\n",
              "  1.1621301174163818,\n",
              "  1.1427066326141357,\n",
              "  1.1846519708633423,\n",
              "  1.2842351198196411,\n",
              "  1.1320767402648926,\n",
              "  1.1246635913848877,\n",
              "  1.1843087673187256,\n",
              "  1.2004845142364502,\n",
              "  1.18723726272583,\n",
              "  1.0690488815307617,\n",
              "  1.169749140739441,\n",
              "  1.077831745147705,\n",
              "  1.1978940963745117,\n",
              "  1.1795849800109863,\n",
              "  1.263822317123413,\n",
              "  1.1712944507598877,\n",
              "  1.1015942096710205,\n",
              "  1.1235923767089844,\n",
              "  1.0949127674102783,\n",
              "  1.1213312149047852,\n",
              "  1.0419278144836426,\n",
              "  1.1418323516845703,\n",
              "  0.9664820432662964,\n",
              "  1.1678378582000732,\n",
              "  1.0712380409240723,\n",
              "  1.0738892555236816,\n",
              "  1.159124493598938,\n",
              "  1.2086820602416992,\n",
              "  1.0558888912200928,\n",
              "  1.1349546909332275,\n",
              "  1.0897196531295776,\n",
              "  1.0997650623321533,\n",
              "  1.0671809911727905,\n",
              "  1.1483910083770752,\n",
              "  1.0445165634155273,\n",
              "  1.1886234283447266,\n",
              "  1.220855712890625,\n",
              "  1.2229578495025635,\n",
              "  1.1775822639465332,\n",
              "  1.2277264595031738,\n",
              "  1.1058943271636963,\n",
              "  1.2064285278320312,\n",
              "  1.2066220045089722,\n",
              "  1.2709110975265503,\n",
              "  1.105290412902832,\n",
              "  1.1872254610061646,\n",
              "  1.2469549179077148,\n",
              "  1.2427713871002197,\n",
              "  1.1575158834457397,\n",
              "  1.1740586757659912,\n",
              "  1.1386232376098633,\n",
              "  1.1958394050598145,\n",
              "  1.115733027458191,\n",
              "  1.1174712181091309,\n",
              "  1.1857109069824219,\n",
              "  1.1100671291351318,\n",
              "  1.0897904634475708,\n",
              "  1.1807814836502075,\n",
              "  1.20029616355896,\n",
              "  1.2138302326202393,\n",
              "  0.978384256362915,\n",
              "  1.2593661546707153,\n",
              "  1.2765527963638306,\n",
              "  1.1459416151046753,\n",
              "  1.2440762519836426,\n",
              "  1.064118504524231,\n",
              "  1.1977967023849487,\n",
              "  1.1477861404418945,\n",
              "  1.1868021488189697,\n",
              "  1.1816198825836182,\n",
              "  1.1014277935028076,\n",
              "  1.1141682863235474,\n",
              "  1.1791353225708008,\n",
              "  1.181648850440979,\n",
              "  1.1506773233413696,\n",
              "  1.0858030319213867,\n",
              "  1.1534837484359741,\n",
              "  1.1943743228912354,\n",
              "  1.0120530128479004,\n",
              "  1.2118817567825317,\n",
              "  1.1884173154830933,\n",
              "  1.1162054538726807,\n",
              "  1.139355182647705,\n",
              "  1.0946249961853027,\n",
              "  1.1143232583999634,\n",
              "  1.1755285263061523,\n",
              "  1.1033389568328857,\n",
              "  1.0242769718170166,\n",
              "  1.144618272781372,\n",
              "  1.1901276111602783,\n",
              "  1.1387972831726074,\n",
              "  1.0657341480255127,\n",
              "  1.1897971630096436,\n",
              "  1.120583176612854,\n",
              "  1.2280066013336182,\n",
              "  1.0661842823028564,\n",
              "  1.2728641033172607,\n",
              "  1.1437184810638428,\n",
              "  1.1820688247680664,\n",
              "  1.1660842895507812,\n",
              "  1.2159523963928223,\n",
              "  1.2003587484359741,\n",
              "  1.1414178609848022,\n",
              "  1.2541663646697998,\n",
              "  1.1462905406951904,\n",
              "  1.1877881288528442,\n",
              "  1.1556752920150757,\n",
              "  1.2928519248962402,\n",
              "  1.1356868743896484,\n",
              "  1.078505516052246,\n",
              "  1.2212636470794678,\n",
              "  1.0610994100570679,\n",
              "  0.9461043477058411,\n",
              "  1.1675889492034912,\n",
              "  1.1801700592041016,\n",
              "  1.2011756896972656,\n",
              "  1.1782907247543335,\n",
              "  1.1044988632202148,\n",
              "  1.1707375049591064,\n",
              "  1.2160863876342773,\n",
              "  1.2481929063796997,\n",
              "  1.2965879440307617,\n",
              "  1.2552475929260254,\n",
              "  1.1532881259918213,\n",
              "  1.272829294204712,\n",
              "  1.1402440071105957,\n",
              "  1.3323670625686646,\n",
              "  1.0981475114822388,\n",
              "  1.254148006439209,\n",
              "  1.2574241161346436,\n",
              "  1.2469384670257568,\n",
              "  1.356033205986023,\n",
              "  1.1777100563049316,\n",
              "  1.1462582349777222,\n",
              "  1.1280348300933838,\n",
              "  1.122166633605957,\n",
              "  1.298861026763916,\n",
              "  1.2813212871551514,\n",
              "  1.3068840503692627,\n",
              "  1.1961371898651123,\n",
              "  1.284938931465149,\n",
              "  1.2234691381454468,\n",
              "  1.195345163345337,\n",
              "  1.07570481300354,\n",
              "  1.2258083820343018,\n",
              "  1.1925606727600098,\n",
              "  1.1440625190734863,\n",
              "  1.1289873123168945,\n",
              "  1.196740984916687,\n",
              "  1.1435191631317139,\n",
              "  1.3124001026153564,\n",
              "  1.1841411590576172,\n",
              "  1.1482598781585693,\n",
              "  1.1878836154937744,\n",
              "  1.1985316276550293,\n",
              "  1.2397737503051758,\n",
              "  1.2602992057800293,\n",
              "  1.1587469577789307,\n",
              "  1.125319242477417,\n",
              "  1.2380008697509766,\n",
              "  1.201220154762268,\n",
              "  1.2617754936218262,\n",
              "  1.2460541725158691,\n",
              "  1.1980690956115723,\n",
              "  1.2075544595718384,\n",
              "  1.3091799020767212,\n",
              "  1.2574763298034668,\n",
              "  1.241562843322754,\n",
              "  1.143426537513733,\n",
              "  1.1520311832427979,\n",
              "  1.2221087217330933,\n",
              "  1.2047131061553955,\n",
              "  1.2403764724731445,\n",
              "  1.1919912099838257,\n",
              "  1.0907049179077148,\n",
              "  1.2211195230484009,\n",
              "  1.244215726852417,\n",
              "  1.1928751468658447,\n",
              "  1.1828292608261108,\n",
              "  1.160766363143921,\n",
              "  1.1746797561645508,\n",
              "  1.2075361013412476,\n",
              "  1.2496337890625,\n",
              "  1.1590384244918823,\n",
              "  1.082794427871704,\n",
              "  1.1786961555480957,\n",
              "  1.251916527748108,\n",
              "  1.269935965538025,\n",
              "  1.22501802444458,\n",
              "  1.2302231788635254,\n",
              "  1.274135708808899,\n",
              "  1.1569582223892212,\n",
              "  1.2142767906188965,\n",
              "  1.188753604888916,\n",
              "  1.2754924297332764,\n",
              "  1.0533778667449951,\n",
              "  1.1576595306396484,\n",
              "  1.2028439044952393,\n",
              "  1.1857630014419556,\n",
              "  1.2616806030273438,\n",
              "  1.1851670742034912,\n",
              "  1.1489425897598267,\n",
              "  1.1812142133712769,\n",
              "  1.3508381843566895,\n",
              "  1.2283456325531006,\n",
              "  1.2426599264144897,\n",
              "  1.1645429134368896,\n",
              "  1.2240787744522095,\n",
              "  1.1453890800476074,\n",
              "  1.159329891204834,\n",
              "  1.036453366279602,\n",
              "  1.2488110065460205,\n",
              "  1.214404582977295,\n",
              "  1.1907165050506592,\n",
              "  1.236446738243103,\n",
              "  1.085815191268921,\n",
              "  1.221161961555481,\n",
              "  1.266408085823059,\n",
              "  1.2508591413497925,\n",
              "  1.135697603225708,\n",
              "  1.3178787231445312,\n",
              "  1.2492525577545166,\n",
              "  1.1464300155639648,\n",
              "  1.224003791809082,\n",
              "  1.2047392129898071,\n",
              "  1.231034278869629,\n",
              "  1.1995539665222168,\n",
              "  1.3338212966918945,\n",
              "  1.2225439548492432,\n",
              "  1.2385053634643555,\n",
              "  1.2260067462921143,\n",
              "  1.2216589450836182,\n",
              "  0.9934354424476624,\n",
              "  1.180972695350647,\n",
              "  1.0008937120437622,\n",
              "  1.135359287261963,\n",
              "  1.1400399208068848,\n",
              "  1.1331337690353394,\n",
              "  1.1489417552947998,\n",
              "  1.2718926668167114,\n",
              "  1.10199773311615,\n",
              "  1.130591869354248,\n",
              "  1.2821040153503418,\n",
              "  1.1510273218154907,\n",
              "  1.1016559600830078,\n",
              "  1.122802972793579,\n",
              "  1.298040509223938,\n",
              "  1.2667343616485596,\n",
              "  1.160443663597107,\n",
              "  1.1209251880645752,\n",
              "  1.13478422164917,\n",
              "  1.2901161909103394,\n",
              "  1.265904426574707,\n",
              "  1.2768633365631104,\n",
              "  1.2171311378479004,\n",
              "  1.1765509843826294,\n",
              "  1.0044300556182861,\n",
              "  1.1508524417877197,\n",
              "  1.0838414430618286,\n",
              "  1.2549946308135986,\n",
              "  1.2871618270874023,\n",
              "  1.0860564708709717,\n",
              "  1.246825933456421,\n",
              "  1.118370771408081,\n",
              "  1.230759859085083,\n",
              "  1.2597806453704834,\n",
              "  1.1457099914550781,\n",
              "  1.2698060274124146,\n",
              "  1.1732003688812256,\n",
              "  1.145777702331543,\n",
              "  1.203683853149414,\n",
              "  1.1977097988128662,\n",
              "  1.241729736328125,\n",
              "  1.3020068407058716,\n",
              "  1.2600150108337402,\n",
              "  1.3166813850402832,\n",
              "  1.1933097839355469,\n",
              "  1.1706907749176025,\n",
              "  1.2930971384048462,\n",
              "  1.2779338359832764,\n",
              "  1.1124639511108398,\n",
              "  1.2453007698059082,\n",
              "  1.2092225551605225,\n",
              "  1.280341625213623,\n",
              "  1.1423377990722656,\n",
              "  1.1729910373687744,\n",
              "  1.1591918468475342,\n",
              "  1.1829346418380737,\n",
              "  1.3237433433532715,\n",
              "  1.1143245697021484,\n",
              "  1.1786869764328003,\n",
              "  1.231344223022461,\n",
              "  1.1294207572937012,\n",
              "  1.2249681949615479,\n",
              "  1.346004605293274,\n",
              "  1.2638307809829712,\n",
              "  1.2109971046447754,\n",
              "  1.2668814659118652,\n",
              "  1.1657788753509521,\n",
              "  1.2810375690460205,\n",
              "  1.121020793914795,\n",
              "  1.2652647495269775,\n",
              "  1.3326536417007446,\n",
              "  1.2343173027038574,\n",
              "  1.175898551940918,\n",
              "  1.2413711547851562,\n",
              "  1.1773571968078613,\n",
              "  1.2714377641677856,\n",
              "  1.2789539098739624,\n",
              "  1.198411464691162,\n",
              "  1.326050043106079,\n",
              "  1.1436333656311035,\n",
              "  1.3463960886001587,\n",
              "  1.3377509117126465,\n",
              "  1.1488951444625854,\n",
              "  1.3262455463409424,\n",
              "  1.2490376234054565,\n",
              "  1.2528562545776367,\n",
              "  1.3445475101470947,\n",
              "  1.1629668474197388,\n",
              "  1.2176227569580078,\n",
              "  1.3134872913360596,\n",
              "  1.1551505327224731,\n",
              "  1.2053515911102295,\n",
              "  1.1097544431686401,\n",
              "  1.252784252166748,\n",
              "  1.1095870733261108,\n",
              "  1.182323694229126,\n",
              "  1.218571424484253,\n",
              "  1.2337908744812012,\n",
              "  1.201799988746643,\n",
              "  1.1605254411697388,\n",
              "  1.0951573848724365,\n",
              "  1.1122676134109497,\n",
              "  1.248828649520874,\n",
              "  1.1814512014389038,\n",
              "  1.2865636348724365,\n",
              "  1.3183585405349731,\n",
              "  1.2315077781677246,\n",
              "  1.2250781059265137,\n",
              "  1.355636477470398,\n",
              "  1.327629804611206,\n",
              "  1.2170767784118652,\n",
              "  1.1110026836395264,\n",
              "  1.222045660018921,\n",
              "  1.3609524965286255,\n",
              "  1.2318859100341797,\n",
              "  1.2074909210205078,\n",
              "  1.1959387063980103,\n",
              "  1.2129143476486206,\n",
              "  1.2758511304855347,\n",
              "  1.2116608619689941,\n",
              "  1.2191054821014404,\n",
              "  1.0865098237991333,\n",
              "  1.1365373134613037,\n",
              "  1.2266108989715576,\n",
              "  1.240497350692749,\n",
              "  1.1684645414352417,\n",
              "  1.2961390018463135,\n",
              "  1.1993085145950317,\n",
              "  1.2237539291381836,\n",
              "  1.165233850479126,\n",
              "  1.2190666198730469,\n",
              "  1.2354508638381958,\n",
              "  1.2022998332977295,\n",
              "  1.157515287399292,\n",
              "  1.3394925594329834,\n",
              "  1.1292786598205566,\n",
              "  1.1243162155151367,\n",
              "  1.1875205039978027,\n",
              "  1.0881643295288086,\n",
              "  1.1968755722045898,\n",
              "  1.1772243976593018,\n",
              "  1.1448979377746582,\n",
              "  1.2310776710510254,\n",
              "  1.2592153549194336,\n",
              "  1.1939442157745361,\n",
              "  1.1587997674942017,\n",
              "  1.124343991279602,\n",
              "  1.1931037902832031,\n",
              "  1.0756659507751465,\n",
              "  1.2148863077163696,\n",
              "  1.2551641464233398,\n",
              "  1.1601823568344116,\n",
              "  1.080439805984497,\n",
              "  1.2018176317214966,\n",
              "  1.1755518913269043,\n",
              "  1.158367395401001,\n",
              "  1.2609381675720215,\n",
              "  1.2021958827972412,\n",
              "  1.2165956497192383,\n",
              "  1.26114022731781,\n",
              "  1.1757524013519287,\n",
              "  1.147573471069336,\n",
              "  1.2762460708618164,\n",
              "  1.2372186183929443,\n",
              "  1.2392618656158447,\n",
              "  1.3264960050582886,\n",
              "  1.2525835037231445,\n",
              "  1.1442193984985352,\n",
              "  1.1680413484573364,\n",
              "  1.1407179832458496,\n",
              "  1.2257611751556396,\n",
              "  1.2481355667114258,\n",
              "  1.111863613128662,\n",
              "  1.148728847503662,\n",
              "  1.0916370153427124,\n",
              "  1.0465762615203857,\n",
              "  1.2199417352676392,\n",
              "  1.1067209243774414,\n",
              "  1.3141390085220337,\n",
              "  1.203553318977356,\n",
              "  1.2800815105438232,\n",
              "  1.1097416877746582,\n",
              "  1.160630226135254,\n",
              "  1.156801700592041,\n",
              "  1.2435566186904907,\n",
              "  1.1840623617172241,\n",
              "  1.2681443691253662,\n",
              "  1.1290478706359863,\n",
              "  1.1699285507202148,\n",
              "  1.184741497039795,\n",
              "  1.080984354019165,\n",
              "  1.1521992683410645,\n",
              "  1.242370843887329,\n",
              "  1.1557672023773193,\n",
              "  1.1928480863571167,\n",
              "  1.1363334655761719,\n",
              "  1.1737772226333618,\n",
              "  1.1393309831619263,\n",
              "  1.121148705482483,\n",
              "  1.1541861295700073,\n",
              "  1.183851718902588,\n",
              "  1.182790756225586,\n",
              "  1.1066155433654785,\n",
              "  1.120330572128296,\n",
              "  1.2244282960891724,\n",
              "  1.3065199851989746,\n",
              "  1.238025188446045,\n",
              "  1.1768479347229004,\n",
              "  1.0624910593032837,\n",
              "  1.0736362934112549,\n",
              "  1.0742249488830566,\n",
              "  1.163170337677002,\n",
              "  1.1136503219604492,\n",
              "  1.2035696506500244,\n",
              "  1.1794605255126953,\n",
              "  1.1545064449310303,\n",
              "  1.2262479066848755,\n",
              "  1.192568302154541,\n",
              "  1.1567862033843994,\n",
              "  1.2251269817352295,\n",
              "  1.188969612121582,\n",
              "  1.1953942775726318,\n",
              "  1.1453464031219482,\n",
              "  1.216354250907898,\n",
              "  1.2301830053329468,\n",
              "  1.151867389678955,\n",
              "  1.2226548194885254,\n",
              "  1.1596581935882568,\n",
              "  1.266800880432129,\n",
              "  1.2209306955337524,\n",
              "  1.2233939170837402,\n",
              "  1.2284729480743408,\n",
              "  1.1623642444610596,\n",
              "  1.1978628635406494,\n",
              "  1.1941380500793457,\n",
              "  1.1645134687423706,\n",
              "  1.1812421083450317,\n",
              "  1.1655383110046387,\n",
              "  1.2544785737991333,\n",
              "  1.1817998886108398,\n",
              "  1.0993192195892334,\n",
              "  1.0862503051757812,\n",
              "  1.3056762218475342,\n",
              "  1.1076829433441162,\n",
              "  1.2585139274597168,\n",
              "  1.0538649559020996,\n",
              "  1.1266542673110962,\n",
              "  1.2581729888916016,\n",
              "  1.175553798675537,\n",
              "  1.1963868141174316,\n",
              "  1.2468878030776978,\n",
              "  1.1363568305969238,\n",
              "  1.211369514465332,\n",
              "  1.3311136960983276,\n",
              "  1.2344722747802734,\n",
              "  1.2769908905029297,\n",
              "  1.2211155891418457,\n",
              "  1.0980578660964966,\n",
              "  1.2372041940689087,\n",
              "  1.143326759338379,\n",
              "  1.2423725128173828,\n",
              "  1.2465014457702637,\n",
              "  1.326690435409546,\n",
              "  1.1516811847686768,\n",
              "  1.1908721923828125,\n",
              "  1.212672233581543,\n",
              "  1.151289939880371,\n",
              "  1.2129452228546143,\n",
              "  1.244788408279419,\n",
              "  1.1101353168487549,\n",
              "  1.1726560592651367,\n",
              "  1.2052483558654785,\n",
              "  1.231819987297058,\n",
              "  1.1906200647354126,\n",
              "  1.2178113460540771,\n",
              "  1.283623456954956,\n",
              "  1.2516111135482788,\n",
              "  1.1106637716293335,\n",
              "  1.1677887439727783,\n",
              "  1.2222882509231567,\n",
              "  1.1904101371765137,\n",
              "  1.319172739982605,\n",
              "  1.1872150897979736,\n",
              "  1.2624614238739014,\n",
              "  1.2123968601226807,\n",
              "  1.236659288406372,\n",
              "  1.3121542930603027,\n",
              "  1.2362432479858398,\n",
              "  1.2104823589324951,\n",
              "  1.2227476835250854,\n",
              "  1.2486047744750977,\n",
              "  1.1977343559265137,\n",
              "  1.1991040706634521,\n",
              "  1.2429779767990112,\n",
              "  1.216717004776001,\n",
              "  1.3179471492767334,\n",
              "  1.2147681713104248,\n",
              "  1.2434086799621582,\n",
              "  1.264941930770874,\n",
              "  1.2965816259384155,\n",
              "  1.166552186012268,\n",
              "  1.0599782466888428,\n",
              "  1.322340488433838,\n",
              "  1.2562000751495361,\n",
              "  1.226548671722412,\n",
              "  1.2459795475006104,\n",
              "  1.1730648279190063,\n",
              "  1.1970946788787842,\n",
              "  1.1483447551727295,\n",
              "  1.1744632720947266,\n",
              "  1.255940318107605,\n",
              "  1.1896586418151855,\n",
              "  1.2004696130752563,\n",
              "  1.1988227367401123,\n",
              "  1.1645631790161133,\n",
              "  1.1130766868591309,\n",
              "  1.1959607601165771,\n",
              "  1.1493775844573975,\n",
              "  1.261810064315796,\n",
              "  1.1005048751831055,\n",
              "  1.1953738927841187,\n",
              "  1.270801305770874,\n",
              "  1.2712171077728271,\n",
              "  1.1583881378173828,\n",
              "  1.2569266557693481,\n",
              "  1.2548832893371582,\n",
              "  1.0437663793563843,\n",
              "  1.2084364891052246,\n",
              "  1.110478401184082,\n",
              "  1.1984443664550781,\n",
              "  1.136730670928955,\n",
              "  1.1375095844268799,\n",
              "  1.1357526779174805,\n",
              "  1.2257485389709473,\n",
              "  1.110709547996521,\n",
              "  1.1519970893859863,\n",
              "  1.1875762939453125,\n",
              "  1.1356163024902344,\n",
              "  1.1478673219680786,\n",
              "  1.1617510318756104,\n",
              "  1.1304090023040771,\n",
              "  1.1613491773605347,\n",
              "  1.2437866926193237,\n",
              "  1.1266944408416748,\n",
              "  1.2776339054107666,\n",
              "  1.2442302703857422,\n",
              "  1.1900525093078613,\n",
              "  1.140621304512024,\n",
              "  1.2637193202972412,\n",
              "  1.3200263977050781,\n",
              "  1.1525839567184448,\n",
              "  1.2427716255187988,\n",
              "  1.2759487628936768,\n",
              "  1.1613845825195312,\n",
              "  1.097611427307129,\n",
              "  1.265368938446045,\n",
              "  1.2659082412719727,\n",
              "  1.239793062210083,\n",
              "  1.0894620418548584,\n",
              "  1.1686043739318848,\n",
              "  1.098442792892456,\n",
              "  1.060977578163147,\n",
              "  1.2316925525665283,\n",
              "  1.100890874862671,\n",
              "  1.12260103225708,\n",
              "  1.0450408458709717,\n",
              "  1.2489017248153687,\n",
              "  1.168905258178711,\n",
              "  1.184877634048462,\n",
              "  1.1198670864105225,\n",
              "  1.1679664850234985,\n",
              "  1.1447876691818237,\n",
              "  1.2742795944213867,\n",
              "  1.0984339714050293,\n",
              "  1.1043157577514648,\n",
              "  1.167963981628418,\n",
              "  1.1617190837860107,\n",
              "  1.2268650531768799,\n",
              "  1.0913925170898438,\n",
              "  1.2257490158081055,\n",
              "  1.1276886463165283,\n",
              "  1.2594858407974243,\n",
              "  1.2207858562469482,\n",
              "  1.140754222869873,\n",
              "  1.2173539400100708,\n",
              "  1.286893606185913,\n",
              "  1.2652394771575928,\n",
              "  1.049170732498169,\n",
              "  1.1371574401855469,\n",
              "  1.2102844715118408,\n",
              "  1.3291383981704712,\n",
              "  1.2311601638793945,\n",
              "  1.270768165588379,\n",
              "  1.2101411819458008,\n",
              "  1.3132665157318115,\n",
              "  1.1984679698944092,\n",
              "  1.1446313858032227,\n",
              "  1.2431190013885498,\n",
              "  1.2043530941009521,\n",
              "  1.177553653717041,\n",
              "  1.2444336414337158,\n",
              "  1.2084413766860962,\n",
              "  1.2547545433044434,\n",
              "  1.2280964851379395,\n",
              "  1.125075101852417,\n",
              "  1.2649726867675781,\n",
              "  1.2012066841125488,\n",
              "  1.2202534675598145,\n",
              "  1.224992036819458,\n",
              "  1.1981933116912842,\n",
              "  1.2545149326324463,\n",
              "  1.2523250579833984,\n",
              "  1.248199462890625,\n",
              "  1.1559746265411377,\n",
              "  1.115034818649292,\n",
              "  1.2021698951721191,\n",
              "  1.1224822998046875,\n",
              "  1.0914676189422607,\n",
              "  1.1511904001235962,\n",
              "  1.3537061214447021,\n",
              "  1.2984752655029297,\n",
              "  1.1277145147323608,\n",
              "  1.1987202167510986,\n",
              "  1.336834192276001,\n",
              "  1.1345804929733276,\n",
              "  1.0993337631225586,\n",
              "  1.2375763654708862,\n",
              "  1.3174878358840942,\n",
              "  1.1866942644119263,\n",
              "  1.270164132118225,\n",
              "  1.2319097518920898,\n",
              "  1.2209796905517578,\n",
              "  1.1144351959228516,\n",
              "  1.2286250591278076,\n",
              "  1.2152084112167358,\n",
              "  1.2870242595672607,\n",
              "  1.18236243724823,\n",
              "  1.177685022354126,\n",
              "  1.1560451984405518,\n",
              "  1.1772959232330322,\n",
              "  1.1955715417861938,\n",
              "  1.1156847476959229,\n",
              "  1.2301068305969238,\n",
              "  1.223961353302002,\n",
              "  1.2167069911956787,\n",
              "  1.0899603366851807,\n",
              "  1.2840923070907593,\n",
              "  1.1443780660629272,\n",
              "  1.17617928981781,\n",
              "  1.206574559211731,\n",
              "  1.2067451477050781,\n",
              "  1.0881999731063843,\n",
              "  1.1177468299865723,\n",
              "  1.164945125579834,\n",
              "  1.1966137886047363,\n",
              "  1.1104295253753662,\n",
              "  1.2272499799728394,\n",
              "  1.157393455505371,\n",
              "  1.1927846670150757,\n",
              "  1.1682379245758057,\n",
              "  1.1205275058746338,\n",
              "  1.0920521020889282,\n",
              "  1.2219431400299072,\n",
              "  1.1123018264770508,\n",
              "  1.157222032546997,\n",
              "  1.110127329826355,\n",
              "  1.1784175634384155,\n",
              "  1.0401638746261597,\n",
              "  1.1385245323181152,\n",
              "  1.2385990619659424,\n",
              "  1.0951018333435059,\n",
              "  1.1489105224609375,\n",
              "  1.2309534549713135,\n",
              "  1.2840070724487305,\n",
              "  1.0665894746780396,\n",
              "  1.1945574283599854,\n",
              "  1.2212488651275635,\n",
              "  1.1013398170471191,\n",
              "  1.2101738452911377,\n",
              "  1.1418492794036865,\n",
              "  1.30429208278656,\n",
              "  1.2646396160125732,\n",
              "  1.0988186597824097,\n",
              "  1.1658787727355957,\n",
              "  1.2045049667358398,\n",
              "  1.2832074165344238,\n",
              "  1.1221144199371338,\n",
              "  1.2293274402618408,\n",
              "  1.21586275100708,\n",
              "  1.1421866416931152,\n",
              "  1.1497950553894043,\n",
              "  1.1211775541305542,\n",
              "  1.3163131475448608,\n",
              "  1.0640021562576294,\n",
              "  1.2517807483673096,\n",
              "  1.2236318588256836,\n",
              "  1.1353540420532227,\n",
              "  1.2160453796386719,\n",
              "  1.0405919551849365,\n",
              "  1.219304084777832,\n",
              "  1.1669058799743652,\n",
              "  1.2009092569351196,\n",
              "  1.2369633913040161,\n",
              "  1.3980172872543335,\n",
              "  1.1480252742767334,\n",
              "  1.1954059600830078,\n",
              "  1.1514629125595093,\n",
              "  1.0839588642120361,\n",
              "  1.15716552734375,\n",
              "  1.1624386310577393,\n",
              "  1.0414396524429321,\n",
              "  1.2704511880874634,\n",
              "  1.185438871383667,\n",
              "  1.218839406967163,\n",
              "  1.1538894176483154,\n",
              "  1.204911470413208,\n",
              "  1.1980661153793335,\n",
              "  1.2148836851119995,\n",
              "  1.1749744415283203,\n",
              "  1.084082841873169,\n",
              "  1.159299373626709,\n",
              "  1.3376048803329468,\n",
              "  1.1962770223617554,\n",
              "  1.1374940872192383,\n",
              "  1.1850217580795288,\n",
              "  1.1133103370666504,\n",
              "  1.2020516395568848,\n",
              "  1.0818649530410767,\n",
              "  1.2925742864608765,\n",
              "  1.0394628047943115,\n",
              "  1.223430871963501,\n",
              "  1.0841963291168213,\n",
              "  1.2051138877868652,\n",
              "  1.1756305694580078,\n",
              "  1.1408860683441162,\n",
              "  1.139790415763855,\n",
              "  1.1216603517532349,\n",
              "  1.180086612701416,\n",
              "  1.1531349420547485,\n",
              "  1.2568085193634033,\n",
              "  1.1306270360946655,\n",
              "  1.0179786682128906,\n",
              "  1.2671407461166382,\n",
              "  1.0957951545715332,\n",
              "  1.222447395324707,\n",
              "  1.1024483442306519,\n",
              "  1.124382734298706,\n",
              "  1.2788071632385254,\n",
              "  1.159638524055481,\n",
              "  1.1944090127944946,\n",
              "  1.174804925918579,\n",
              "  1.1305172443389893,\n",
              "  1.2754149436950684,\n",
              "  1.1995042562484741,\n",
              "  1.11502206325531,\n",
              "  1.3074297904968262,\n",
              "  1.1639963388442993,\n",
              "  1.1392849683761597,\n",
              "  1.2625973224639893,\n",
              "  1.0965737104415894,\n",
              "  1.1570537090301514,\n",
              "  1.215471863746643,\n",
              "  1.3659253120422363,\n",
              "  1.229142427444458,\n",
              "  1.095989465713501,\n",
              "  1.2377946376800537,\n",
              "  1.1614006757736206,\n",
              "  1.1573021411895752,\n",
              "  1.1310087442398071,\n",
              "  1.1719300746917725,\n",
              "  1.172074794769287,\n",
              "  1.1516401767730713,\n",
              "  1.2161033153533936,\n",
              "  1.2915539741516113,\n",
              "  1.1885442733764648,\n",
              "  1.179725170135498,\n",
              "  1.1742680072784424,\n",
              "  1.147942304611206,\n",
              "  1.1927359104156494,\n",
              "  1.1167526245117188,\n",
              "  1.1031670570373535,\n",
              "  1.3226349353790283,\n",
              "  1.2914717197418213,\n",
              "  1.173132061958313,\n",
              "  1.173537015914917,\n",
              "  1.0860648155212402,\n",
              "  1.1578415632247925,\n",
              "  1.2170133590698242],\n",
              " 'val_acc': [0.1875,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.4375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.453125,\n",
              "  0.359375,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.40625,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.421875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.453125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.453125,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.453125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.234375,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.4375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.421875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.453125,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.453125,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.40625,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.21875,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.421875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.203125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.4375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.234375,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.421875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.34375]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "396e5788-2f21-423b-eabe-a5379612e630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "09d8398f-10ae-4ab5-fcef-214ce878b430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_667ba478-87f0-46af-bedc-e8a0ef861993\", \"newdata_SEM1.h5\", 16615536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}