{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPO1tnX21MRefspkHRIB6M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/2Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "87daeb48-264a-47ce-c901-088696b39d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "ab93f26e-f476-4a2d-d8ee-6a94d4464d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-105454f0-360c-4b14-8aab-e96acc24d0e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-105454f0-360c-4b14-8aab-e96acc24d0e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-105454f0-360c-4b14-8aab-e96acc24d0e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-105454f0-360c-4b14-8aab-e96acc24d0e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "bb0032f5-7e12-4c52-c4a6-c592c36a0987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb80lEQVR4nO3df7AV5Z3n8fcn/mTAGUDMDVEimmASHFfUW8REJ3tnLX9AEtGplMF1lRgzZGZ1R6uwUpipim5cqzQjuhsrZRZLRzJD/DExBjKaicTxTOJkNYKLAiIRDa7cIIyK4CWOycXv/tHP1eZ4Lveee371bT6vqq7T/XT36e/p+9zv6fP0092KCMzMrFze1+kAzMys+ZzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzcC0TSJknbJI3NlX1ZUqWDYZk1Varnb0rqk7Rd0gOSpqR5d0r6XZo3MDwl6U9y07skRdUyH+r05yoaJ/fi2Q+4vNNBmLXY5yJiHDAZ2Arckpv3zYgYlxuOj4ifD0wDx6blxueW+X/t/gBF5+RePH8DXClpfPUMSZ+S9ISkHen1Ux2Iz6xpIuLfge8D0zsdS9k4uRfPSqACXJkvlDQReAD4FnAocBPwgKRD2x2gWbNI+gPgC8BjnY6lbJzci+nrwH+TdFiu7DPAcxHxdxHRHxF3Ac8Cn+tIhGaN+aGk14EdwOlkv1gHXCnp9dywpDMhjm5O7gUUEWuBfwQW5oo/CLxYteiLwOHtisusic6JiPHAwcBlwL9I+kCad2NEjM8N8zoX5ujl5F5cVwN/zrvJ+zfAkVXLfAjobWdQZs0UEbsj4gfAbuDUTsdTJk7uBRURG4F7gL9KRQ8Cx0j6z5L2l/QFspNQ/9ipGM0apcwcYAKwvtPxlImTe7F9AxgLEBGvAp8FFgCvAl8FPhsRr3QuPLMR+5GkPmAncB0wLyLWpXlfrerD7jo+AvLDOszMysdH7mZmJeTkbmZWQk7uZmYl5ORuZlZC+3c6AIBJkybF1KlTa87btWsXY8eOrTmvKBxj8zQS56pVq16JiMOGXrLzXOfbYzTE2bI6HxEdH0466aQYzCOPPDLovKJwjM3TSJzAymhCfQSmAI8AzwDrgMtT+URgBfBcep2QykV2z5+NwNPAiUNtw3W+PUZDnK2q826WMXuvfmBBREwHTgYulTSd7HYQD0fENOBh3r09xCxgWhrmA7e2P2SzPTm5m1WJiC0R8WQaf4PsysnDgTnAwE2slgDnpPE5wHfTwdRjwHhJk9scttkeCtHmblZUkqYCJwCPA10RsSXNehnoSuOHAy/lVtucyrbkypA0n+zInq6uLiqVSs1t9vX1DTqvKEZDjDA64mxVjIVP7mt6d/DFhQ90Ooy9WnBcv2NskqHi3HT9Z9oWi6RxwH3AFRGxU9I78yIiJNV1eXdELAYWA3R3d0dPT0/N5W5ZuoxFj+6qK9Z27heASqXCYPEXyWiIs1UxulnGrAZJB5Al9qWR3bUQYOtAc0t63ZbKe8lOwg44At+t0zpsxMld0kclrc4NOyVdIekaSb258tnNDNis1ZQdot8OrI+Im3KzlgMD9xafByzLlV+U7nB4MrAj13xj1hEjbpaJiA3ADABJ+5EdqdwPXAzcHBE3NiVCs/Y7BbgQWCNpdSr7GnA9cK+kS8gelHJemvcgMJusK+Rvyf4HzDqqWW3upwHPR8SL+XZJs9EoIh4l67tey2k1lg/g0pYGZVanZiX3ucBduenLJF1E9rDnBRGxvXqF4fYc6BqTnWQrMsfYPEPFWfSeD2ZF0XByl3QgcDZwVSq6FbgWiPS6CPhS9Xp19RxYU+xOPQuO63eMTTJUnJsu6GlfMGajWDN6y8wCnoyIrQARsTWy5yK+DdwGzGzCNszMrA7NSO7nk2uSqboy71xgbRO2YWZmdWjod7qkscDpwFdyxd+UNIOsWWZT1TwzM2uDhpJ7ROwCDq0qu7ChiMzMrGG+QtXMrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshIp/m0AzG9LUET4ft93PXrX28ZG7mVkJObmbmZWQk7uZWQk5uZuZlZCTu5lZCTm5m5mVkJO7mVkJObmbmZWQk7uZWQk1+oDsTcAbwG6gPyK6JU0E7gGmkj0g+7yI2N5YmGZmVo9mHLn/aUTMiIjuNL0QeDgipgEPp2kzM2ujVjTLzAGWpPElwDkt2IaZme1Fo8k9gIckrZI0P5V1RcSWNP4y0NXgNszMrE6N3hXy1IjolfR+YIWkZ/MzIyIkRa0V05fBfICuri4qlUrNDXSNgQXH9TcYZms5xuYZKs7B6omZ7amh5B4Rvel1m6T7gZnAVkmTI2KLpMnAtkHWXQwsBuju7o6enp6a27hl6TIWrSn2nYkXHNfvGJtkqDg3XdDTvmDMRrERN8tIGivpkIFx4AxgLbAcmJcWmwcsazRIMzOrTyOHcl3A/ZIG3ud7EfFPkp4A7pV0CfAicF7jYZqZWT1GnNwj4gXg+BrlrwKnNRKUmZk1pviNsGbWMiN5PN9IHs3Xru3Yu3z7ATOzEnJyN6tB0h2StklamyubKGmFpOfS64RULknfkrRR0tOSTuxc5GYZJ3ez2u4EzqoqG+zWGrOAaWmYD9zaphjNBuXkblZDRPwMeK2qeLBba8wBvhuZx4Dx6RoPs47xCVWz4Rvs1hqHAy/lltucyrbkykpzVXalUqGvr6+uq4VH8nmacTVyvXF2QqtidHI3G4G93VpjL+uU4qrsTRf0UKlUGCz+Wr44kt4yTbgaud44O6FVMbpZxmz4tg40t1TdWqMXmJJb7ohUZtYxxT08MCuegVtrXM+et9ZYDlwm6W7gE8COXPONjZD7xjfGyd2sBkl3AT3AJEmbgavJknqtW2s8CMwGNgK/BS5ue8BmVZzczWqIiPMHmfWeW2tERACXtjYis/q4zd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshEac3CVNkfSIpGckrZN0eSq/RlKvpNVpmN28cM3MbDgauf1AP7AgIp6UdAiwStKKNO/miLix8fDMzGwkRpzc013vtqTxNyStJ3tAgZmZdVhTbhwmaSpwAvA4cArZ7U8vAlaSHd1vr7FOKZ5KA46xmYaKs+hP1TErioaTu6RxwH3AFRGxU9KtwLVApNdFwJeq1yvLU2kgS0aOsTmGirMZT+cx2xc01FtG0gFkiX1pRPwAICK2RsTuiHgbuA2Y2XiYZmZWj0Z6ywi4HVgfETflyvNPfT8XWDvy8MzMbCQa+Z1+CnAhsEbS6lT2NeB8STPImmU2AV9pKEIzs2GqfjTfguP6h/Vw7jI+nq+R3jKPAqox68GRh2NmZs3gK1TNzErIyd3MrISK3zfOzApl6sIHht2WbZ3jI3czsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3Myshd4U0MxuB6lsdDEc7b3PgI3czsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3MyshJ3czsxJycjczKyEndzOzEmpZcpd0lqQNkjZKWtiq7ZgVheu8FUlLbj8gaT/g28DpwGbgCUnLI+KZVmzPrNNc5204at2yYKinWo30lgWturfMTGBjRLwAIOluYA7gim5l5To/io3kPjFFp4ho/ptKnwfOiogvp+kLgU9ExGW5ZeYD89PkR4ENg7zdJOCVpgfZXI6xeRqJ88iIOKyZwQyX63xhjYY4W1LnO3ZXyIhYDCweajlJKyOiuw0hjZhjbJ7REudIuM6332iIs1UxtuqEai8wJTd9RCozKyvXeSuUViX3J4Bpko6SdCAwF1jeom2ZFYHrvBVKS5plIqJf0mXAT4D9gDsiYt0I327In7EF4BibZ7TEuQfX+cIaDXG2JMaWnFA1M7PO8hWqZmYl5ORuZlZChU3uRbmUW9IUSY9IekbSOkmXp/JrJPVKWp2G2bl1rkpxb5B0Zhtj3SRpTYpnZSqbKGmFpOfS64RULknfSnE+LenENsT30dz+Wi1pp6QrirgvO6WT9V7SHZK2SVqbK6u7/kial5Z/TtK8Jsc42P9jYeKUdLCkX0p6KsX431P5UZIeT7Hck068I+mgNL0xzZ+ae6+R1/+IKNxAdkLqeeBo4EDgKWB6h2KZDJyYxg8BfgVMB64Brqyx/PQU70HAUelz7NemWDcBk6rKvgksTOMLgRvS+Gzgx4CAk4HHO/A3fhk4soj7skN1raP1Hvg0cCKwdqT1B5gIvJBeJ6TxCU2McbD/x8LEmbY1Lo0fADyetn0vMDeVfwf4yzT+X4HvpPG5wD1pvKH6X9Qj93cu5Y6I3wEDl3K3XURsiYgn0/gbwHrg8L2sMge4OyLeiohfAxvJPk+nzAGWpPElwDm58u9G5jFgvKTJbYzrNOD5iHhxL8sUbV+2WkfrfUT8DHitqrje+nMmsCIiXouI7cAK4KwmxjjY/2Nh4kzb6kuTB6QhgP8EfH+QGAdi/z5wmiTRYP0vanI/HHgpN72ZvSfUtkg/l04g+yYGuCz91Ltj4GcgnY09gIckrVJ2qTtAV0RsSeMvA11pvNP7eC5wV266aPuyE4r4eeutP237DFX/j4WKU9J+klYD28i+OJ4HXo+I/hrbeyeWNH8HcGijMRY1uReOpHHAfcAVEbETuBX4MDAD2AIs6mB4A06NiBOBWcClkj6dnxnZb72O931NbY1nA/+Qioq4L61KUeoP1Px/fEcR4oyI3RExg+xK5ZnAx9odQ1GTe6Eu5ZZ0AFlFWhoRPwCIiK3pD/g2cBtwuqSHaDB2SYdJelbSmHrjjIheSX3AOOB+skq1daC5Jb1uS4sPGmc6GXRsvduvwyzgyYjYmuKu3pcDPz0LVQ/aoIift9760/LPUOv/sYhxAkTE68AjwCfJmoQGLhzNb++dWNL8PwJebTTGoib3wlzKndq+bgfWR8RNkk6V9IvU0+M1Sf8K/BXwrxFxRopzbjoDfhQwDfhlHZtcCNwZEW/WGedYSYdExDhgK3AGsDbFM9ATYB6wLI0vBy5KvQlOBnbkftbeCHyjnu3X6XxyTTJVbf3nprgHYmxkX442han3OfXWn58AZ0iakJrXzkhlTVH9/1jEONMB2vg0PobsHv/ryZL85weJcSD2zwP/nH59NFb/m3F2uBUD2VnuX5G1Vf11B+M4lewn3tNp2A3cAPw9WRJ6AagAk3Pr/HWKewMwq45tHUR2688jRhDn0WRn1p8C1g3sM7K2u4eB54CfAhPj3TP6305xrgG6c+91MNmJtQ+0YH+OJTsq+aNc2d+lGJ5OFbrhfTlah07We7Iv3C3A78nady8ZYf35EtnJv43AxU2OMf//uDoNs4sUJ/AfgP+bYlwLfD2VH02WnDeSNUkelMoPTtMb0/yjc+814vrf8co8mgagm+ykSK15XwQeTeNfBfpyw+/JjsYh+8l1e/on6gX+B6l7E1lXtI1V71tJy/wivdePUkVeCuwkO9qbmls+gI+k8TFk7dcvkp2keRQYk+adTfYl8HraxsertrsCmNfpfe7Bg4eRDUVtlimqXwG7JS2RNCvXq2MPEfHNiBgXWRPJx4F/A+5Js+8E+oGPkJ3pPwP4cpp3HLUf4DAXuJDsTPmHgf8D/C1ZH931wNWDxHsjcBLwqbTsV4G3JR1DdpR2BXAY8CDwo4GLKpL1wPGD7gkzKzQn9zpEdlZ+4GfhbcC/SVouqavW8qm97YfA/4qIH6flZpOd4d8VEduAm8mSN8B44I0ab/W3EfF8ROwguyDj+Yj4aWTdpv6B7EuietvvI/vZeXlE9EZ2wvIXEfEW8AXggYhYERG/J/sSGEP2JTDgjRSPmY1CHXsS02gVEevJmmCQ9DGytvf/Se2TMbcDGyLihjR9JNkFDVuy80JA9gU70Jd1O9lVd9W25sbfrDE9rsY6k8ja8p6vMe+DZE01A5/pbUkvsWcf2kPImmzMbBTykXsDIuJZsmaWP66ep+y+IMeQnZQa8BLwFtktAsan4Q8jYqDb4dNpnWZ4Bfh3smacar8h+6IZiFVkXa7y3aw+TnZy1sxGISf3Okj6mKQFko5I01PIuvU9VrXcLLLukedGrktjZF2wHgIWSfpDSe+T9GFJ/zEt8kuyvrANXykXWZ/xO4CbJH0wXTH3SUkHkd3j4jOSTkt9hheQfen8IsV/MFlb/YpG4zCzznByr88bwCeAxyXtIkvqa8mSY94XyE5UrpfUl4bvpHkXkd0U6hmyZpjvk90MicjuJ3In8F+aFO+VZN2/niDr2ngD8L6I2JC2cQvZEf7ngM+l7ZOmKxHxmybFYWZt5icxFYykw4CfAydEnRcyNTGGx4FLImLtkAubWSE5uZuZlZCbZczMSsjJ3cyshJzczcxKqBAXMU2aNCmmTp1ac96uXbsYO3ZsewMqIO+HzN72w6pVq16JiMPaHJJZIRUiuU+dOpWVK1fWnFepVOjp6WlvQAXk/ZDZ236QtLdH9pntU9wsY2ZWQk7uZmYl5ORuZlZChWhzt6Gt6d3BFxc+UNc6m67/TIuiMbOi85G7mVkJDZncJX1U0urcsFPSFZKukdSbK5+dW+cqSRslbZB0Zms/gpmZVRuyWSbdQXAGgKT9yO75fT9wMXBzRNyYX17SdLInCx1L9lCIn0o6JiJ2Nzl2MzMbRL3NMqeRPeJtb/2J5wB3R8RbEfFrsid6zxxpgGZmVr96T6jOJXuw8oDLJF0ErAQWRMR2ske15R9esZk9H98GgKT5wHyArq4uKpVKzQ329fUNOm9f0jUGFhzXX9c6Zdxvrg9mwzPs5C7pQOBs4KpUdCtwLdnDoq8FFpE9kHlYImIxsBigu7s7Brvq0FdmZm5ZuoxFa+r7Lt50QU9rgukg1wez4amnWWYW8GREbAWIiK0RsTs9zu023m166SV7HueAI9jz2ZxmZtZi9ST388k1yUianJt3Ltnj5gCWA3MlHSTpKGAa2bNBzcysTYb1O1/SWOB04Cu54m9KmkHWLLNpYF5ErJN0L9kzQvuBS91TxsysvYaV3CNiF3BoVdmFe1n+OuC6xkIzM7OR8hWqZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYlNKzkLmmTpDWSVktamcomSloh6bn0OiGVS9K3JG2U9LSkE1v5AczM7L3qOXL/04iYERHdaXoh8HBETAMeTtMAs4BpaZgP3NqsYM3MbHgaaZaZAyxJ40uAc3Ll343MY8B4SZMb2I6ZmdVpuMk9gIckrZI0P5V1RcSWNP4y0JXGDwdeyq27OZWZmVmb7D/M5U6NiF5J7wdWSHo2PzMiQlLUs+H0JTEfoKuri0qlUnO5vr6+QeftS7rGwILj+utap4z7zfXBbHiGldwjoje9bpN0PzAT2CppckRsSc0u29LivcCU3OpHpLLq91wMLAbo7u6Onp6emtuuVCoMNm9fcsvSZSxaM9zv4symC3paE0wHuT6YDc+QzTKSxko6ZGAcOANYCywH5qXF5gHL0vhy4KLUa+ZkYEeu+cbMzNpgOIeCXcD9kgaW/15E/JOkJ4B7JV0CvAicl5Z/EJgNbAR+C1zc9KjNzGyvhkzuEfECcHyN8leB02qUB3BpU6IzM7MR8RWqZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCQyZ3SVMkPSLpGUnrJF2eyq+R1CtpdRpm59a5StJGSRskndnKD2BmZu+1/zCW6QcWRMSTkg4BVklakebdHBE35heWNB2YCxwLfBD4qaRjImJ3MwM3M7PBDXnkHhFbIuLJNP4GsB44fC+rzAHujoi3IuLXwEZgZjOCNTOz4RnOkfs7JE0FTgAeB04BLpN0EbCS7Oh+O1nifyy32mZqfBlImg/MB+jq6qJSqdTcZl9f36Dz9iVdY2DBcf11rVPG/eb6YDY8w07uksYB9wFXRMROSbcC1wKRXhcBXxru+0XEYmAxQHd3d/T09NRcrlKpMNi8fcktS5exaE1d38VsuqCnNcF0kOuD2fAMq7eMpAPIEvvSiPgBQERsjYjdEfE2cBvvNr30AlNyqx+RyszMrE2G01tGwO3A+oi4KVc+ObfYucDaNL4cmCvpIElHAdOAXzYvZDMzG8pwfuefAlwIrJG0OpV9DThf0gyyZplNwFcAImKdpHuBZ8h62lzqnjJmZu01ZHKPiEcB1Zj14F7WuQ64roG4zMysAb5C1cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshFqW3CWdJWmDpI2SFrZqO2Zm9l4tSe6S9gO+DcwCpgPnS5reim2Zmdl7terIfSawMSJeiIjfAXcDc1q0LTMzq7J/i973cOCl3PRm4BP5BSTNB+anyT5JGwZ5r0nAK02PcPSpez/ohhZF0ll72w9HtjMQsyJrVXIfUkQsBhYPtZyklRHR3YaQCs37IeP9YDY8rWqW6QWm5KaPSGVmZtYGrUruTwDTJB0l6UBgLrC8RdsyM7MqLWmWiYh+SZcBPwH2A+6IiHUjfLshm272Ed4PGe8Hs2FQRHQ6BjMzazJfoWpmVkJO7mZmJVSI5C7pcklrJa2TdEWN+T2SdkhanYavdyLOVpB0h6RtktbmyiZKWiHpufQ6YZB156VlnpM0r31RN1+D+2F3rm74xL0ZBUjukv4Y+HOyq1qPBz4r6SM1Fv15RMxIwzfaGmRr3QmcVVW2EHg4IqYBD6fpPUiaCFxNdnHYTODqwZLfKHEnI9gPyZu5unF2C2M0GzU6ntyBjwOPR8RvI6If+BfgzzocU9tExM+A16qK5wBL0vgS4Jwaq54JrIiI1yJiO7CC9ybHUaOB/WBmNRQhua8F/kTSoZL+AJjNnhdADfikpKck/VjSse0Nse26ImJLGn8Z6KqxTK1bPBze6sDabDj7AeBgSSslPSbJXwBmdPD2AwMiYr2kG4CHgF3AamB31WJPAkdGRJ+k2cAPgWntjbQzIiIk7fP9VYfYD0dGRK+ko4F/lrQmIp5vZ3xmRVOEI3ci4vaIOCkiPg1sB35VNX9nRPSl8QeBAyRN6kCo7bJV0mSA9LqtxjL7wi0ehrMfiIje9PoCUAFOaFeAZkVViOQu6f3p9UNk7e3fq5r/AUlK4zPJ4n613XG20XJgoPfLPGBZjWV+ApwhaUI6kXpGKiuTIfdD+vwHpfFJwCnAM22L0KygOt4sk9wn6VDg98ClEfG6pL8AiIjvAJ8H/lJSP/AmMDdKcmmtpLuAHmCSpM1kPWCuB+6VdAnwInBeWrYb+IuI+HJEvCbpWrL7+AB8IyKqT0iOGiPdD2Qn5P+3pLfJvvSvjwgnd9vn+fYDZmYlVIhmGTMzay4ndzOzEnJyNzMrISd3M7MScnI3MyshJ3czsxJycjczK6H/DxgwhIJdffxAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "fef8338d-cf3a-417a-909c-389244cbf3dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "aca07bb6-c36f-4b76-c3f5-167e4c6184ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "8e296ea1-504b-4a2f-9f1a-1aca48a8f0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-800')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '801-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-800')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '801-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-800')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '801-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-800']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='801-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-800']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='801-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-800']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='801-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "cf9d84e5-336a-4c2d-ec64-0bbbf7868c09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "643339c8-e116-4477-b4cc-155cece81a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_195 (Conv2D)            (None, 75, 75, 32)   864         ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_147 (Batch  (None, 75, 75, 32)  128         ['conv2d_195[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_147 (Swish)              (None, 75, 75, 32)   0           ['batch_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_48 (Depthwise  (None, 75, 75, 32)  288         ['swish_147[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_148 (Batch  (None, 75, 75, 32)  128         ['depthwise_conv2d_48[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_148 (Swish)              (None, 75, 75, 32)   0           ['batch_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " lambda_48 (Lambda)             (None, 1, 1, 32)     0           ['swish_148[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_196 (Conv2D)            (None, 1, 1, 8)      264         ['lambda_48[0][0]']              \n",
            "                                                                                                  \n",
            " swish_149 (Swish)              (None, 1, 1, 8)      0           ['conv2d_196[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_197 (Conv2D)            (None, 1, 1, 32)     288         ['swish_149[0][0]']              \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 1, 1, 32)     0           ['conv2d_197[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_48 (Multiply)         (None, 75, 75, 32)   0           ['activation_48[0][0]',          \n",
            "                                                                  'swish_148[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_198 (Conv2D)            (None, 75, 75, 16)   512         ['multiply_48[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_149 (Batch  (None, 75, 75, 16)  64          ['conv2d_198[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_199 (Conv2D)            (None, 75, 75, 96)   1536        ['batch_normalization_149[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_150 (Batch  (None, 75, 75, 96)  384         ['conv2d_199[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_150 (Swish)              (None, 75, 75, 96)   0           ['batch_normalization_150[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_49 (Depthwise  (None, 38, 38, 96)  864         ['swish_150[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_151 (Batch  (None, 38, 38, 96)  384         ['depthwise_conv2d_49[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_151 (Swish)              (None, 38, 38, 96)   0           ['batch_normalization_151[0][0]']\n",
            "                                                                                                  \n",
            " lambda_49 (Lambda)             (None, 1, 1, 96)     0           ['swish_151[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)            (None, 1, 1, 4)      388         ['lambda_49[0][0]']              \n",
            "                                                                                                  \n",
            " swish_152 (Swish)              (None, 1, 1, 4)      0           ['conv2d_200[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)            (None, 1, 1, 96)     480         ['swish_152[0][0]']              \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 1, 1, 96)     0           ['conv2d_201[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_49 (Multiply)         (None, 38, 38, 96)   0           ['activation_49[0][0]',          \n",
            "                                                                  'swish_151[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)            (None, 38, 38, 24)   2304        ['multiply_49[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_152 (Batch  (None, 38, 38, 24)  96          ['conv2d_202[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_203 (Conv2D)            (None, 38, 38, 144)  3456        ['batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_153 (Batch  (None, 38, 38, 144)  576        ['conv2d_203[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_153 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_153[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_50 (Depthwise  (None, 38, 38, 144)  1296       ['swish_153[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_154 (Batch  (None, 38, 38, 144)  576        ['depthwise_conv2d_50[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_154 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_154[0][0]']\n",
            "                                                                                                  \n",
            " lambda_50 (Lambda)             (None, 1, 1, 144)    0           ['swish_154[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_50[0][0]']              \n",
            "                                                                                                  \n",
            " swish_155 (Swish)              (None, 1, 1, 6)      0           ['conv2d_204[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_155[0][0]']              \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 1, 1, 144)    0           ['conv2d_205[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_50 (Multiply)         (None, 38, 38, 144)  0           ['activation_50[0][0]',          \n",
            "                                                                  'swish_154[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)            (None, 38, 38, 24)   3456        ['multiply_50[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_155 (Batch  (None, 38, 38, 24)  96          ['conv2d_206[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_27 (DropConnect)  (None, 38, 38, 24)   0           ['batch_normalization_155[0][0]']\n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 38, 38, 24)   0           ['drop_connect_27[0][0]',        \n",
            "                                                                  'batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)            (None, 38, 38, 144)  3456        ['add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_156 (Batch  (None, 38, 38, 144)  576        ['conv2d_207[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_156 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_156[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_51 (Depthwise  (None, 19, 19, 144)  3600       ['swish_156[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_157 (Batch  (None, 19, 19, 144)  576        ['depthwise_conv2d_51[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_157 (Swish)              (None, 19, 19, 144)  0           ['batch_normalization_157[0][0]']\n",
            "                                                                                                  \n",
            " lambda_51 (Lambda)             (None, 1, 1, 144)    0           ['swish_157[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_51[0][0]']              \n",
            "                                                                                                  \n",
            " swish_158 (Swish)              (None, 1, 1, 6)      0           ['conv2d_208[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_158[0][0]']              \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 1, 1, 144)    0           ['conv2d_209[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_51 (Multiply)         (None, 19, 19, 144)  0           ['activation_51[0][0]',          \n",
            "                                                                  'swish_157[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, 19, 19, 40)   5760        ['multiply_51[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_158 (Batch  (None, 19, 19, 40)  160         ['conv2d_210[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)            (None, 19, 19, 240)  9600        ['batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_159 (Batch  (None, 19, 19, 240)  960        ['conv2d_211[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_159 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_159[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_52 (Depthwise  (None, 19, 19, 240)  6000       ['swish_159[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_160 (Batch  (None, 19, 19, 240)  960        ['depthwise_conv2d_52[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_160 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_160[0][0]']\n",
            "                                                                                                  \n",
            " lambda_52 (Lambda)             (None, 1, 1, 240)    0           ['swish_160[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_52[0][0]']              \n",
            "                                                                                                  \n",
            " swish_161 (Swish)              (None, 1, 1, 10)     0           ['conv2d_212[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_161[0][0]']              \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 1, 1, 240)    0           ['conv2d_213[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_52 (Multiply)         (None, 19, 19, 240)  0           ['activation_52[0][0]',          \n",
            "                                                                  'swish_160[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)            (None, 19, 19, 40)   9600        ['multiply_52[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_161 (Batch  (None, 19, 19, 40)  160         ['conv2d_214[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_28 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_161[0][0]']\n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_28[0][0]',        \n",
            "                                                                  'batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)            (None, 19, 19, 240)  9600        ['add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_162 (Batch  (None, 19, 19, 240)  960        ['conv2d_215[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_162 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_162[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_53 (Depthwise  (None, 10, 10, 240)  2160       ['swish_162[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_163 (Batch  (None, 10, 10, 240)  960        ['depthwise_conv2d_53[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_163 (Swish)              (None, 10, 10, 240)  0           ['batch_normalization_163[0][0]']\n",
            "                                                                                                  \n",
            " lambda_53 (Lambda)             (None, 1, 1, 240)    0           ['swish_163[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_53[0][0]']              \n",
            "                                                                                                  \n",
            " swish_164 (Swish)              (None, 1, 1, 10)     0           ['conv2d_216[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_164[0][0]']              \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 1, 1, 240)    0           ['conv2d_217[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_53 (Multiply)         (None, 10, 10, 240)  0           ['activation_53[0][0]',          \n",
            "                                                                  'swish_163[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_218 (Conv2D)            (None, 10, 10, 80)   19200       ['multiply_53[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_164 (Batch  (None, 10, 10, 80)  320         ['conv2d_218[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)            (None, 10, 10, 480)  38400       ['batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_165 (Batch  (None, 10, 10, 480)  1920       ['conv2d_219[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_165 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_165[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_54 (Depthwise  (None, 10, 10, 480)  4320       ['swish_165[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_166 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_54[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_166 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_166[0][0]']\n",
            "                                                                                                  \n",
            " lambda_54 (Lambda)             (None, 1, 1, 480)    0           ['swish_166[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_54[0][0]']              \n",
            "                                                                                                  \n",
            " swish_167 (Swish)              (None, 1, 1, 20)     0           ['conv2d_220[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_167[0][0]']              \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 1, 1, 480)    0           ['conv2d_221[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_54 (Multiply)         (None, 10, 10, 480)  0           ['activation_54[0][0]',          \n",
            "                                                                  'swish_166[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_54[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_167 (Batch  (None, 10, 10, 80)  320         ['conv2d_222[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_29 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_29[0][0]',        \n",
            "                                                                  'batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_223 (Conv2D)            (None, 10, 10, 480)  38400       ['add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_168 (Batch  (None, 10, 10, 480)  1920       ['conv2d_223[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_168 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_168[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_55 (Depthwise  (None, 10, 10, 480)  4320       ['swish_168[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_169 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_55[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_169 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_169[0][0]']\n",
            "                                                                                                  \n",
            " lambda_55 (Lambda)             (None, 1, 1, 480)    0           ['swish_169[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_55[0][0]']              \n",
            "                                                                                                  \n",
            " swish_170 (Swish)              (None, 1, 1, 20)     0           ['conv2d_224[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_170[0][0]']              \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 1, 1, 480)    0           ['conv2d_225[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_55 (Multiply)         (None, 10, 10, 480)  0           ['activation_55[0][0]',          \n",
            "                                                                  'swish_169[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_55[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_170 (Batch  (None, 10, 10, 80)  320         ['conv2d_226[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_30 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_170[0][0]']\n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_30[0][0]',        \n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)            (None, 10, 10, 480)  38400       ['add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_171 (Batch  (None, 10, 10, 480)  1920       ['conv2d_227[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_171 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_171[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_56 (Depthwise  (None, 10, 10, 480)  12000      ['swish_171[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_172 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_56[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_172 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_172[0][0]']\n",
            "                                                                                                  \n",
            " lambda_56 (Lambda)             (None, 1, 1, 480)    0           ['swish_172[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_228 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_56[0][0]']              \n",
            "                                                                                                  \n",
            " swish_173 (Swish)              (None, 1, 1, 20)     0           ['conv2d_228[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_229 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_173[0][0]']              \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 1, 1, 480)    0           ['conv2d_229[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_56 (Multiply)         (None, 10, 10, 480)  0           ['activation_56[0][0]',          \n",
            "                                                                  'swish_172[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_230 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_56[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_173 (Batch  (None, 10, 10, 112)  448        ['conv2d_230[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_231 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_174 (Batch  (None, 10, 10, 672)  2688       ['conv2d_231[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_174 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_174[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_57 (Depthwise  (None, 10, 10, 672)  16800      ['swish_174[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_175 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_57[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_175 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_175[0][0]']\n",
            "                                                                                                  \n",
            " lambda_57 (Lambda)             (None, 1, 1, 672)    0           ['swish_175[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_232 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_57[0][0]']              \n",
            "                                                                                                  \n",
            " swish_176 (Swish)              (None, 1, 1, 28)     0           ['conv2d_232[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_233 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_176[0][0]']              \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 1, 1, 672)    0           ['conv2d_233[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_57 (Multiply)         (None, 10, 10, 672)  0           ['activation_57[0][0]',          \n",
            "                                                                  'swish_175[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_234 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_57[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_176 (Batch  (None, 10, 10, 112)  448        ['conv2d_234[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_31 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_176[0][0]']\n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_31[0][0]',        \n",
            "                                                                  'batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_235 (Conv2D)            (None, 10, 10, 672)  75264       ['add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_177 (Batch  (None, 10, 10, 672)  2688       ['conv2d_235[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_177 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_177[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_58 (Depthwise  (None, 10, 10, 672)  16800      ['swish_177[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_178 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_58[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_178 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_178[0][0]']\n",
            "                                                                                                  \n",
            " lambda_58 (Lambda)             (None, 1, 1, 672)    0           ['swish_178[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_236 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_58[0][0]']              \n",
            "                                                                                                  \n",
            " swish_179 (Swish)              (None, 1, 1, 28)     0           ['conv2d_236[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_237 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_179[0][0]']              \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 1, 1, 672)    0           ['conv2d_237[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_58 (Multiply)         (None, 10, 10, 672)  0           ['activation_58[0][0]',          \n",
            "                                                                  'swish_178[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_238 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_58[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_179 (Batch  (None, 10, 10, 112)  448        ['conv2d_238[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_32 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_179[0][0]']\n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_32[0][0]',        \n",
            "                                                                  'add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_239 (Conv2D)            (None, 10, 10, 672)  75264       ['add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_180 (Batch  (None, 10, 10, 672)  2688       ['conv2d_239[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_180 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_180[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_59 (Depthwise  (None, 5, 5, 672)   16800       ['swish_180[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_181 (Batch  (None, 5, 5, 672)   2688        ['depthwise_conv2d_59[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_181 (Swish)              (None, 5, 5, 672)    0           ['batch_normalization_181[0][0]']\n",
            "                                                                                                  \n",
            " lambda_59 (Lambda)             (None, 1, 1, 672)    0           ['swish_181[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_240 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_59[0][0]']              \n",
            "                                                                                                  \n",
            " swish_182 (Swish)              (None, 1, 1, 28)     0           ['conv2d_240[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_241 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_182[0][0]']              \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 1, 1, 672)    0           ['conv2d_241[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_59 (Multiply)         (None, 5, 5, 672)    0           ['activation_59[0][0]',          \n",
            "                                                                  'swish_181[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_242 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_59[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_182 (Batch  (None, 5, 5, 192)   768         ['conv2d_242[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_243 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_183 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_243[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_183 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_183[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_60 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_183[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_184 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_60[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_184 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_184[0][0]']\n",
            "                                                                                                  \n",
            " lambda_60 (Lambda)             (None, 1, 1, 1152)   0           ['swish_184[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_244 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_60[0][0]']              \n",
            "                                                                                                  \n",
            " swish_185 (Swish)              (None, 1, 1, 48)     0           ['conv2d_244[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_245 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_185[0][0]']              \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_245[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_60 (Multiply)         (None, 5, 5, 1152)   0           ['activation_60[0][0]',          \n",
            "                                                                  'swish_184[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_246 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_60[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_185 (Batch  (None, 5, 5, 192)   768         ['conv2d_246[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_33 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_185[0][0]']\n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_33[0][0]',        \n",
            "                                                                  'batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_247 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_186 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_247[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_186 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_61 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_186[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_187 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_61[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_187 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_187[0][0]']\n",
            "                                                                                                  \n",
            " lambda_61 (Lambda)             (None, 1, 1, 1152)   0           ['swish_187[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_248 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_61[0][0]']              \n",
            "                                                                                                  \n",
            " swish_188 (Swish)              (None, 1, 1, 48)     0           ['conv2d_248[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_249 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_188[0][0]']              \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_249[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_61 (Multiply)         (None, 5, 5, 1152)   0           ['activation_61[0][0]',          \n",
            "                                                                  'swish_187[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_250 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_61[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_188 (Batch  (None, 5, 5, 192)   768         ['conv2d_250[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_34 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_188[0][0]']\n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_34[0][0]',        \n",
            "                                                                  'add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_251 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_189 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_251[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_189 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_62 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_189[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_190 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_62[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_190 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_190[0][0]']\n",
            "                                                                                                  \n",
            " lambda_62 (Lambda)             (None, 1, 1, 1152)   0           ['swish_190[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_252 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_62[0][0]']              \n",
            "                                                                                                  \n",
            " swish_191 (Swish)              (None, 1, 1, 48)     0           ['conv2d_252[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_253 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_191[0][0]']              \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_253[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_62 (Multiply)         (None, 5, 5, 1152)   0           ['activation_62[0][0]',          \n",
            "                                                                  'swish_190[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_254 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_62[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_191 (Batch  (None, 5, 5, 192)   768         ['conv2d_254[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_35 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_191[0][0]']\n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_35[0][0]',        \n",
            "                                                                  'add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_255 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_192 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_255[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_192 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_192[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_63 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_192[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_193 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_63[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_193 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_193[0][0]']\n",
            "                                                                                                  \n",
            " lambda_63 (Lambda)             (None, 1, 1, 1152)   0           ['swish_193[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_256 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_63[0][0]']              \n",
            "                                                                                                  \n",
            " swish_194 (Swish)              (None, 1, 1, 48)     0           ['conv2d_256[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_257 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_194[0][0]']              \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_257[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_63 (Multiply)         (None, 5, 5, 1152)   0           ['activation_63[0][0]',          \n",
            "                                                                  'swish_193[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_258 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_63[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_194 (Batch  (None, 5, 5, 320)   1280        ['conv2d_258[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_259 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_194[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_195 (Batch  (None, 5, 5, 1280)  5120        ['conv2d_259[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_195 (Swish)              (None, 5, 5, 1280)   0           ['batch_normalization_195[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "591d9e56-8f7a-4cb2-a9e0-5b0297e45a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "211b65b1-f141-4698-ba3d-23ade1664d0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "5cf53076-e419-43c4-9053-948acfe3959e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea8112d-1065-4fb4-b08f-5c6271b96d0d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-111-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 10s 98ms/step - loss: 1.1789 - acc: 0.5120 - val_loss: 0.8420 - val_acc: 0.3854\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.2321 - acc: 0.5223 - val_loss: 0.8383 - val_acc: 0.3958\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1755 - acc: 0.5189 - val_loss: 0.8077 - val_acc: 0.4896\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1889 - acc: 0.4983 - val_loss: 0.8194 - val_acc: 0.4896\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1760 - acc: 0.5103 - val_loss: 0.8372 - val_acc: 0.4583\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.1646 - acc: 0.5103 - val_loss: 0.8145 - val_acc: 0.4583\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1294 - acc: 0.5412 - val_loss: 0.8307 - val_acc: 0.4583\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1666 - acc: 0.5395 - val_loss: 0.8259 - val_acc: 0.5208\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1897 - acc: 0.5120 - val_loss: 0.8281 - val_acc: 0.5208\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1.1200 - acc: 0.5172 - val_loss: 0.8555 - val_acc: 0.4792\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1160 - acc: 0.5344 - val_loss: 0.8318 - val_acc: 0.4688\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1916 - acc: 0.5017 - val_loss: 0.8382 - val_acc: 0.4896\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1141 - acc: 0.5326 - val_loss: 0.8509 - val_acc: 0.5000\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1973 - acc: 0.4983 - val_loss: 0.8294 - val_acc: 0.4583\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1926 - acc: 0.4897 - val_loss: 0.8294 - val_acc: 0.4583\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1972 - acc: 0.5052 - val_loss: 0.8572 - val_acc: 0.3958\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.0842 - acc: 0.5601 - val_loss: 0.8523 - val_acc: 0.4375\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1029 - acc: 0.5464 - val_loss: 0.8613 - val_acc: 0.4375\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1324 - acc: 0.5155 - val_loss: 0.8366 - val_acc: 0.4688\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1335 - acc: 0.5223 - val_loss: 0.8355 - val_acc: 0.4583\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.0929 - acc: 0.5258 - val_loss: 0.8418 - val_acc: 0.4479\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1254 - acc: 0.5275 - val_loss: 0.8437 - val_acc: 0.4271\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.0955 - acc: 0.5103 - val_loss: 0.8664 - val_acc: 0.4167\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1170 - acc: 0.5412 - val_loss: 0.8448 - val_acc: 0.4271\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.1205 - acc: 0.5430 - val_loss: 0.8489 - val_acc: 0.4583\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0589 - acc: 0.5584 - val_loss: 0.8527 - val_acc: 0.4792\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.0652 - acc: 0.5395 - val_loss: 0.8452 - val_acc: 0.4271\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1231 - acc: 0.5103 - val_loss: 0.8559 - val_acc: 0.4062\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0682 - acc: 0.5430 - val_loss: 0.8504 - val_acc: 0.4375\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0929 - acc: 0.5292 - val_loss: 0.8539 - val_acc: 0.4479\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0757 - acc: 0.5430 - val_loss: 0.8594 - val_acc: 0.4479\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.0664 - acc: 0.5258 - val_loss: 0.8672 - val_acc: 0.4375\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0613 - acc: 0.5206 - val_loss: 0.8613 - val_acc: 0.3750\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0461 - acc: 0.5825 - val_loss: 0.8668 - val_acc: 0.4375\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1.0458 - acc: 0.5206 - val_loss: 0.8608 - val_acc: 0.4271\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1036 - acc: 0.5206 - val_loss: 0.8705 - val_acc: 0.4271\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1351 - acc: 0.5172 - val_loss: 0.8558 - val_acc: 0.4062\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.9812 - acc: 0.5653 - val_loss: 0.8706 - val_acc: 0.3646\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0261 - acc: 0.5619 - val_loss: 0.8723 - val_acc: 0.3854\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0727 - acc: 0.5430 - val_loss: 0.8822 - val_acc: 0.4062\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1.0101 - acc: 0.5241 - val_loss: 0.8634 - val_acc: 0.4062\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9884 - acc: 0.5326 - val_loss: 0.8766 - val_acc: 0.4167\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.0279 - acc: 0.5344 - val_loss: 0.8673 - val_acc: 0.4271\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0644 - acc: 0.5120 - val_loss: 0.8740 - val_acc: 0.4167\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0286 - acc: 0.5326 - val_loss: 0.8873 - val_acc: 0.4271\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1.0963 - acc: 0.5584 - val_loss: 0.8789 - val_acc: 0.4583\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9945 - acc: 0.5481 - val_loss: 0.9017 - val_acc: 0.4271\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9980 - acc: 0.5344 - val_loss: 0.8863 - val_acc: 0.4479\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0465 - acc: 0.5412 - val_loss: 0.8795 - val_acc: 0.3646\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0526 - acc: 0.5172 - val_loss: 0.8592 - val_acc: 0.4167\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9414 - acc: 0.5739 - val_loss: 0.8706 - val_acc: 0.4375\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0457 - acc: 0.5412 - val_loss: 0.8746 - val_acc: 0.3958\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1.0113 - acc: 0.5481 - val_loss: 0.8597 - val_acc: 0.3854\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0189 - acc: 0.5584 - val_loss: 0.8576 - val_acc: 0.3750\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1.0392 - acc: 0.5412 - val_loss: 0.8748 - val_acc: 0.4167\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0522 - acc: 0.5361 - val_loss: 0.8694 - val_acc: 0.3854\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9742 - acc: 0.5430 - val_loss: 0.8754 - val_acc: 0.4271\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9998 - acc: 0.5498 - val_loss: 0.8795 - val_acc: 0.4062\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1.0128 - acc: 0.5601 - val_loss: 0.8880 - val_acc: 0.4375\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.0170 - acc: 0.5321 - val_loss: 0.8767 - val_acc: 0.4167\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9571 - acc: 0.5653 - val_loss: 0.8787 - val_acc: 0.3750\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9987 - acc: 0.5584 - val_loss: 0.8809 - val_acc: 0.3854\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8893 - acc: 0.5895 - val_loss: 0.8825 - val_acc: 0.3750\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9449 - acc: 0.5464 - val_loss: 0.8783 - val_acc: 0.3750\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9303 - acc: 0.5739 - val_loss: 0.8722 - val_acc: 0.3854\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9840 - acc: 0.5155 - val_loss: 0.9037 - val_acc: 0.4375\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9661 - acc: 0.5619 - val_loss: 0.8864 - val_acc: 0.3854\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9590 - acc: 0.5653 - val_loss: 0.8769 - val_acc: 0.4375\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9638 - acc: 0.5687 - val_loss: 0.9074 - val_acc: 0.4583\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9190 - acc: 0.5790 - val_loss: 0.8777 - val_acc: 0.4479\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9212 - acc: 0.5670 - val_loss: 0.8836 - val_acc: 0.3646\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8980 - acc: 0.5653 - val_loss: 0.8780 - val_acc: 0.3854\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9291 - acc: 0.5773 - val_loss: 0.8957 - val_acc: 0.3750\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.8677 - acc: 0.5893 - val_loss: 0.8779 - val_acc: 0.3646\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9037 - acc: 0.5722 - val_loss: 0.8805 - val_acc: 0.4062\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9560 - acc: 0.5790 - val_loss: 0.8715 - val_acc: 0.3854\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0397 - acc: 0.5430 - val_loss: 0.8820 - val_acc: 0.4167\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0418 - acc: 0.5447 - val_loss: 0.9021 - val_acc: 0.4479\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9357 - acc: 0.5498 - val_loss: 0.8902 - val_acc: 0.4688\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8621 - acc: 0.5962 - val_loss: 0.9013 - val_acc: 0.4375\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0503 - acc: 0.5515 - val_loss: 0.8868 - val_acc: 0.4688\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9417 - acc: 0.5825 - val_loss: 0.9069 - val_acc: 0.4479\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9728 - acc: 0.5430 - val_loss: 0.8997 - val_acc: 0.4479\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9456 - acc: 0.5653 - val_loss: 0.9125 - val_acc: 0.4167\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8909 - acc: 0.5997 - val_loss: 0.9074 - val_acc: 0.4583\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9216 - acc: 0.5773 - val_loss: 0.8971 - val_acc: 0.4583\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8849 - acc: 0.5790 - val_loss: 0.8911 - val_acc: 0.4479\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.8836 - acc: 0.5859 - val_loss: 0.9198 - val_acc: 0.4271\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.9714 - acc: 0.5567 - val_loss: 0.9099 - val_acc: 0.4062\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8937 - acc: 0.5584 - val_loss: 0.9097 - val_acc: 0.4583\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9469 - acc: 0.5911 - val_loss: 0.9097 - val_acc: 0.3958\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9099 - acc: 0.5533 - val_loss: 0.9049 - val_acc: 0.4167\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9522 - acc: 0.5945 - val_loss: 0.9026 - val_acc: 0.3438\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9438 - acc: 0.5619 - val_loss: 0.8994 - val_acc: 0.3646\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9140 - acc: 0.5928 - val_loss: 0.9030 - val_acc: 0.3542\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9120 - acc: 0.5808 - val_loss: 0.8706 - val_acc: 0.3854\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9196 - acc: 0.5756 - val_loss: 0.9072 - val_acc: 0.3958\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8723 - acc: 0.6048 - val_loss: 0.8776 - val_acc: 0.4271\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9153 - acc: 0.5704 - val_loss: 0.8957 - val_acc: 0.4479\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.8979 - acc: 0.6134 - val_loss: 0.8907 - val_acc: 0.4167\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8947 - acc: 0.5722 - val_loss: 0.9021 - val_acc: 0.3854\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9116 - acc: 0.5653 - val_loss: 0.8932 - val_acc: 0.3958\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8505 - acc: 0.5876 - val_loss: 0.9095 - val_acc: 0.3750\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8774 - acc: 0.6065 - val_loss: 0.8992 - val_acc: 0.3958\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9429 - acc: 0.5601 - val_loss: 0.8932 - val_acc: 0.4375\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9110 - acc: 0.5859 - val_loss: 0.9030 - val_acc: 0.4688\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9109 - acc: 0.5842 - val_loss: 0.8974 - val_acc: 0.4583\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8937 - acc: 0.5653 - val_loss: 0.9044 - val_acc: 0.3854\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9041 - acc: 0.5704 - val_loss: 0.8899 - val_acc: 0.3958\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8670 - acc: 0.5928 - val_loss: 0.8942 - val_acc: 0.4688\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8736 - acc: 0.5739 - val_loss: 0.8967 - val_acc: 0.3750\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9043 - acc: 0.5670 - val_loss: 0.9026 - val_acc: 0.3750\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8834 - acc: 0.5842 - val_loss: 0.8981 - val_acc: 0.3750\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8571 - acc: 0.5945 - val_loss: 0.9052 - val_acc: 0.3854\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8100 - acc: 0.6323 - val_loss: 0.9035 - val_acc: 0.3750\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8922 - acc: 0.5878 - val_loss: 0.8992 - val_acc: 0.4167\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8692 - acc: 0.5876 - val_loss: 0.9069 - val_acc: 0.3542\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8948 - acc: 0.5704 - val_loss: 0.9110 - val_acc: 0.3542\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8738 - acc: 0.6100 - val_loss: 0.8837 - val_acc: 0.4062\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8582 - acc: 0.6065 - val_loss: 0.8875 - val_acc: 0.3646\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9639 - acc: 0.5515 - val_loss: 0.9135 - val_acc: 0.3542\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9048 - acc: 0.5876 - val_loss: 0.9010 - val_acc: 0.3646\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.8452 - acc: 0.5911 - val_loss: 0.9136 - val_acc: 0.3333\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8685 - acc: 0.5928 - val_loss: 0.9077 - val_acc: 0.3646\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8875 - acc: 0.5704 - val_loss: 0.9041 - val_acc: 0.3542\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8668 - acc: 0.6014 - val_loss: 0.8873 - val_acc: 0.3542\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8237 - acc: 0.6048 - val_loss: 0.8936 - val_acc: 0.3646\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9570 - acc: 0.5533 - val_loss: 0.9045 - val_acc: 0.3438\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8381 - acc: 0.5979 - val_loss: 0.8898 - val_acc: 0.3542\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8493 - acc: 0.5893 - val_loss: 0.9039 - val_acc: 0.3750\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8103 - acc: 0.6048 - val_loss: 0.8918 - val_acc: 0.3542\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8451 - acc: 0.6100 - val_loss: 0.9097 - val_acc: 0.3542\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9105 - acc: 0.5515 - val_loss: 0.8901 - val_acc: 0.3542\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8963 - acc: 0.5808 - val_loss: 0.8941 - val_acc: 0.3646\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8748 - acc: 0.5790 - val_loss: 0.9046 - val_acc: 0.3438\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8050 - acc: 0.5928 - val_loss: 0.8950 - val_acc: 0.3750\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8356 - acc: 0.6082 - val_loss: 0.8923 - val_acc: 0.3646\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8471 - acc: 0.5893 - val_loss: 0.8953 - val_acc: 0.3333\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8436 - acc: 0.5893 - val_loss: 0.9051 - val_acc: 0.3646\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8478 - acc: 0.5790 - val_loss: 0.8979 - val_acc: 0.3542\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8738 - acc: 0.6014 - val_loss: 0.9089 - val_acc: 0.3542\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8750 - acc: 0.6048 - val_loss: 0.9111 - val_acc: 0.3542\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8063 - acc: 0.6048 - val_loss: 0.9112 - val_acc: 0.3438\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8627 - acc: 0.6031 - val_loss: 0.9074 - val_acc: 0.3542\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8387 - acc: 0.6082 - val_loss: 0.9061 - val_acc: 0.3438\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8036 - acc: 0.6271 - val_loss: 0.8711 - val_acc: 0.3646\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8234 - acc: 0.6134 - val_loss: 0.8882 - val_acc: 0.3646\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8466 - acc: 0.5911 - val_loss: 0.9175 - val_acc: 0.3438\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8257 - acc: 0.6168 - val_loss: 0.8876 - val_acc: 0.3958\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8182 - acc: 0.5997 - val_loss: 0.9196 - val_acc: 0.3750\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9057 - acc: 0.5945 - val_loss: 0.9058 - val_acc: 0.3958\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8458 - acc: 0.6237 - val_loss: 0.8918 - val_acc: 0.3646\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8966 - acc: 0.5739 - val_loss: 0.8999 - val_acc: 0.3854\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8384 - acc: 0.6031 - val_loss: 0.8737 - val_acc: 0.3854\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8766 - acc: 0.6014 - val_loss: 0.9090 - val_acc: 0.3750\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8561 - acc: 0.5962 - val_loss: 0.8945 - val_acc: 0.3542\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8600 - acc: 0.5739 - val_loss: 0.8953 - val_acc: 0.4688\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8310 - acc: 0.5962 - val_loss: 0.8719 - val_acc: 0.4688\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7820 - acc: 0.6340 - val_loss: 0.9122 - val_acc: 0.3958\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8234 - acc: 0.5979 - val_loss: 0.8887 - val_acc: 0.4688\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.8301 - acc: 0.6254 - val_loss: 0.8901 - val_acc: 0.4375\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8392 - acc: 0.6048 - val_loss: 0.8742 - val_acc: 0.3958\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7639 - acc: 0.6168 - val_loss: 0.8883 - val_acc: 0.3750\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7511 - acc: 0.6478 - val_loss: 0.8757 - val_acc: 0.4271\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7948 - acc: 0.6082 - val_loss: 0.8933 - val_acc: 0.4375\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8309 - acc: 0.6014 - val_loss: 0.8829 - val_acc: 0.3958\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7914 - acc: 0.6014 - val_loss: 0.8921 - val_acc: 0.4062\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7634 - acc: 0.6564 - val_loss: 0.8793 - val_acc: 0.3750\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7661 - acc: 0.6323 - val_loss: 0.8985 - val_acc: 0.3542\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7898 - acc: 0.5911 - val_loss: 0.8867 - val_acc: 0.3646\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8419 - acc: 0.5876 - val_loss: 0.8796 - val_acc: 0.3750\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8582 - acc: 0.5808 - val_loss: 0.8966 - val_acc: 0.3542\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7577 - acc: 0.6254 - val_loss: 0.8626 - val_acc: 0.3750\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8015 - acc: 0.6100 - val_loss: 0.8767 - val_acc: 0.4062\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8422 - acc: 0.5670 - val_loss: 0.9127 - val_acc: 0.3750\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7645 - acc: 0.6271 - val_loss: 0.8946 - val_acc: 0.3750\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7831 - acc: 0.6186 - val_loss: 0.8836 - val_acc: 0.3958\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.8198 - acc: 0.6203 - val_loss: 0.8978 - val_acc: 0.3646\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8435 - acc: 0.5911 - val_loss: 0.8664 - val_acc: 0.3750\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8343 - acc: 0.6048 - val_loss: 0.8796 - val_acc: 0.3958\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7910 - acc: 0.6443 - val_loss: 0.8944 - val_acc: 0.3958\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8518 - acc: 0.5893 - val_loss: 0.8800 - val_acc: 0.4062\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7953 - acc: 0.6082 - val_loss: 0.8828 - val_acc: 0.3750\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7950 - acc: 0.5808 - val_loss: 0.8876 - val_acc: 0.4062\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8381 - acc: 0.5739 - val_loss: 0.8686 - val_acc: 0.4062\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.7930 - acc: 0.6134 - val_loss: 0.8770 - val_acc: 0.4062\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.8327 - acc: 0.6117 - val_loss: 0.8826 - val_acc: 0.3750\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7673 - acc: 0.6409 - val_loss: 0.8798 - val_acc: 0.3646\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7704 - acc: 0.6237 - val_loss: 0.8836 - val_acc: 0.3646\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7806 - acc: 0.6340 - val_loss: 0.8829 - val_acc: 0.4062\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7303 - acc: 0.6323 - val_loss: 0.8759 - val_acc: 0.3542\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8366 - acc: 0.5704 - val_loss: 0.8837 - val_acc: 0.3646\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8038 - acc: 0.6203 - val_loss: 0.8725 - val_acc: 0.3438\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.8172 - acc: 0.5979 - val_loss: 0.8562 - val_acc: 0.3646\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.8097 - acc: 0.5876 - val_loss: 0.8481 - val_acc: 0.3750\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8082 - acc: 0.6117 - val_loss: 0.8774 - val_acc: 0.3854\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.8708 - acc: 0.5842 - val_loss: 0.8381 - val_acc: 0.4062\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8076 - acc: 0.5911 - val_loss: 0.8835 - val_acc: 0.3646\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7503 - acc: 0.6117 - val_loss: 0.8727 - val_acc: 0.3958\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7899 - acc: 0.6254 - val_loss: 0.8786 - val_acc: 0.3854\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8216 - acc: 0.6186 - val_loss: 0.8851 - val_acc: 0.3958\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8002 - acc: 0.6082 - val_loss: 0.8908 - val_acc: 0.3750\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7326 - acc: 0.6306 - val_loss: 0.8853 - val_acc: 0.3750\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7909 - acc: 0.5979 - val_loss: 0.8689 - val_acc: 0.3646\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7980 - acc: 0.6478 - val_loss: 0.8655 - val_acc: 0.3542\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7320 - acc: 0.6426 - val_loss: 0.8736 - val_acc: 0.3958\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7829 - acc: 0.6220 - val_loss: 0.8792 - val_acc: 0.3854\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8027 - acc: 0.5979 - val_loss: 0.8591 - val_acc: 0.3854\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7891 - acc: 0.6203 - val_loss: 0.8642 - val_acc: 0.3750\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8268 - acc: 0.6100 - val_loss: 0.8800 - val_acc: 0.3750\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8001 - acc: 0.5911 - val_loss: 0.8814 - val_acc: 0.3854\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8409 - acc: 0.6100 - val_loss: 0.8646 - val_acc: 0.4062\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7669 - acc: 0.6254 - val_loss: 0.8688 - val_acc: 0.3958\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8153 - acc: 0.5790 - val_loss: 0.8767 - val_acc: 0.3750\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7453 - acc: 0.6168 - val_loss: 0.8529 - val_acc: 0.3750\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7630 - acc: 0.6323 - val_loss: 0.8779 - val_acc: 0.4062\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7867 - acc: 0.6254 - val_loss: 0.8729 - val_acc: 0.3750\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8016 - acc: 0.6082 - val_loss: 0.8647 - val_acc: 0.3750\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8017 - acc: 0.5859 - val_loss: 0.8497 - val_acc: 0.3854\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7259 - acc: 0.6460 - val_loss: 0.8642 - val_acc: 0.3750\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7679 - acc: 0.6048 - val_loss: 0.8447 - val_acc: 0.3750\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8115 - acc: 0.5928 - val_loss: 0.8498 - val_acc: 0.3854\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8167 - acc: 0.5928 - val_loss: 0.8620 - val_acc: 0.3750\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7469 - acc: 0.6117 - val_loss: 0.8524 - val_acc: 0.3854\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7737 - acc: 0.6065 - val_loss: 0.8750 - val_acc: 0.3438\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7410 - acc: 0.6443 - val_loss: 0.8573 - val_acc: 0.3854\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7793 - acc: 0.6082 - val_loss: 0.8785 - val_acc: 0.3646\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7259 - acc: 0.6357 - val_loss: 0.8627 - val_acc: 0.3750\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7679 - acc: 0.6203 - val_loss: 0.8785 - val_acc: 0.3646\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6986 - acc: 0.6520 - val_loss: 0.8654 - val_acc: 0.3854\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7808 - acc: 0.6031 - val_loss: 0.8538 - val_acc: 0.3854\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7170 - acc: 0.6323 - val_loss: 0.8652 - val_acc: 0.3750\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8367 - acc: 0.6014 - val_loss: 0.8804 - val_acc: 0.3542\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7757 - acc: 0.6065 - val_loss: 0.8641 - val_acc: 0.4062\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7412 - acc: 0.6134 - val_loss: 0.8635 - val_acc: 0.3854\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7911 - acc: 0.6254 - val_loss: 0.8612 - val_acc: 0.3854\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.7975 - acc: 0.6216 - val_loss: 0.8695 - val_acc: 0.3750\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6944 - acc: 0.6460 - val_loss: 0.8485 - val_acc: 0.3958\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7591 - acc: 0.6351 - val_loss: 0.8549 - val_acc: 0.3750\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7875 - acc: 0.6254 - val_loss: 0.8517 - val_acc: 0.3750\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7321 - acc: 0.6392 - val_loss: 0.8685 - val_acc: 0.3854\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6892 - acc: 0.6375 - val_loss: 0.8638 - val_acc: 0.3750\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8167 - acc: 0.5997 - val_loss: 0.8469 - val_acc: 0.4062\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7474 - acc: 0.6340 - val_loss: 0.8529 - val_acc: 0.3854\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7527 - acc: 0.6254 - val_loss: 0.8569 - val_acc: 0.3958\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7620 - acc: 0.6375 - val_loss: 0.8542 - val_acc: 0.3854\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7938 - acc: 0.6100 - val_loss: 0.8356 - val_acc: 0.3854\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7895 - acc: 0.6134 - val_loss: 0.8527 - val_acc: 0.3854\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7357 - acc: 0.6546 - val_loss: 0.8279 - val_acc: 0.3854\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.8101 - acc: 0.6134 - val_loss: 0.8458 - val_acc: 0.4271\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.8012 - acc: 0.6117 - val_loss: 0.8246 - val_acc: 0.4271\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7220 - acc: 0.6254 - val_loss: 0.8279 - val_acc: 0.4062\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7841 - acc: 0.6166 - val_loss: 0.8597 - val_acc: 0.4896\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7642 - acc: 0.6168 - val_loss: 0.8346 - val_acc: 0.4062\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7139 - acc: 0.6512 - val_loss: 0.8566 - val_acc: 0.3646\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7191 - acc: 0.6581 - val_loss: 0.8444 - val_acc: 0.3854\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7554 - acc: 0.6203 - val_loss: 0.8398 - val_acc: 0.3750\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7673 - acc: 0.6203 - val_loss: 0.8338 - val_acc: 0.3854\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7199 - acc: 0.6306 - val_loss: 0.8495 - val_acc: 0.3958\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7494 - acc: 0.6271 - val_loss: 0.8375 - val_acc: 0.4167\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7457 - acc: 0.6357 - val_loss: 0.8489 - val_acc: 0.3958\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6741 - acc: 0.6460 - val_loss: 0.8431 - val_acc: 0.3854\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7002 - acc: 0.6375 - val_loss: 0.8363 - val_acc: 0.3854\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.7088 - acc: 0.6478 - val_loss: 0.8391 - val_acc: 0.3958\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7487 - acc: 0.5979 - val_loss: 0.8338 - val_acc: 0.3958\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6836 - acc: 0.6564 - val_loss: 0.8387 - val_acc: 0.4271\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7538 - acc: 0.6082 - val_loss: 0.8428 - val_acc: 0.3854\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7255 - acc: 0.6426 - val_loss: 0.8380 - val_acc: 0.3854\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7155 - acc: 0.6495 - val_loss: 0.8440 - val_acc: 0.3750\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7244 - acc: 0.6392 - val_loss: 0.8371 - val_acc: 0.4062\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7812 - acc: 0.5979 - val_loss: 0.8473 - val_acc: 0.3854\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6842 - acc: 0.6495 - val_loss: 0.8585 - val_acc: 0.3854\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7091 - acc: 0.6392 - val_loss: 0.8269 - val_acc: 0.3854\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7890 - acc: 0.6117 - val_loss: 0.8555 - val_acc: 0.3958\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7330 - acc: 0.6392 - val_loss: 0.8353 - val_acc: 0.4062\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7410 - acc: 0.6186 - val_loss: 0.8461 - val_acc: 0.4062\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7271 - acc: 0.6529 - val_loss: 0.8348 - val_acc: 0.4062\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7157 - acc: 0.6357 - val_loss: 0.8306 - val_acc: 0.4271\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6798 - acc: 0.6649 - val_loss: 0.8398 - val_acc: 0.3958\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7373 - acc: 0.6392 - val_loss: 0.8425 - val_acc: 0.4167\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.7316 - acc: 0.6375 - val_loss: 0.8498 - val_acc: 0.4167\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.6860 - acc: 0.6546 - val_loss: 0.8283 - val_acc: 0.4271\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7398 - acc: 0.6357 - val_loss: 0.8476 - val_acc: 0.3958\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7372 - acc: 0.6134 - val_loss: 0.8514 - val_acc: 0.3854\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6672 - acc: 0.6632 - val_loss: 0.8517 - val_acc: 0.3958\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.7184 - acc: 0.6512 - val_loss: 0.8359 - val_acc: 0.4062\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7259 - acc: 0.6470 - val_loss: 0.8241 - val_acc: 0.4167\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7464 - acc: 0.6409 - val_loss: 0.8222 - val_acc: 0.4167\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7127 - acc: 0.6701 - val_loss: 0.8477 - val_acc: 0.4062\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6963 - acc: 0.6529 - val_loss: 0.8251 - val_acc: 0.4167\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7476 - acc: 0.6220 - val_loss: 0.8335 - val_acc: 0.4167\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7517 - acc: 0.6220 - val_loss: 0.8279 - val_acc: 0.3854\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7212 - acc: 0.6100 - val_loss: 0.8225 - val_acc: 0.4375\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.8107 - acc: 0.6014 - val_loss: 0.8391 - val_acc: 0.3854\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6935 - acc: 0.6598 - val_loss: 0.8362 - val_acc: 0.4062\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7250 - acc: 0.6351 - val_loss: 0.8219 - val_acc: 0.3958\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7196 - acc: 0.6323 - val_loss: 0.8385 - val_acc: 0.4062\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6889 - acc: 0.6598 - val_loss: 0.8338 - val_acc: 0.4062\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7527 - acc: 0.6220 - val_loss: 0.8327 - val_acc: 0.4062\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7082 - acc: 0.6254 - val_loss: 0.8259 - val_acc: 0.4167\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6561 - acc: 0.6770 - val_loss: 0.8222 - val_acc: 0.4062\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7482 - acc: 0.6117 - val_loss: 0.8469 - val_acc: 0.3854\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.7002 - acc: 0.6375 - val_loss: 0.8208 - val_acc: 0.4167\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6983 - acc: 0.6460 - val_loss: 0.8309 - val_acc: 0.4062\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6997 - acc: 0.6529 - val_loss: 0.8391 - val_acc: 0.3854\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6771 - acc: 0.6443 - val_loss: 0.8517 - val_acc: 0.3646\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6931 - acc: 0.6546 - val_loss: 0.8346 - val_acc: 0.4062\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.7120 - acc: 0.6340 - val_loss: 0.8432 - val_acc: 0.3958\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7470 - acc: 0.6168 - val_loss: 0.8396 - val_acc: 0.4062\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7190 - acc: 0.6392 - val_loss: 0.8191 - val_acc: 0.4271\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6818 - acc: 0.6564 - val_loss: 0.8337 - val_acc: 0.3958\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7235 - acc: 0.6237 - val_loss: 0.8538 - val_acc: 0.3542\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.6657 - acc: 0.6632 - val_loss: 0.8478 - val_acc: 0.3854\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7235 - acc: 0.6392 - val_loss: 0.8279 - val_acc: 0.4062\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7123 - acc: 0.6203 - val_loss: 0.8235 - val_acc: 0.4271\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6934 - acc: 0.6495 - val_loss: 0.8098 - val_acc: 0.4167\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7358 - acc: 0.6100 - val_loss: 0.8205 - val_acc: 0.4167\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6643 - acc: 0.6701 - val_loss: 0.8364 - val_acc: 0.3958\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6817 - acc: 0.6581 - val_loss: 0.8347 - val_acc: 0.4167\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6865 - acc: 0.6684 - val_loss: 0.8197 - val_acc: 0.4167\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6628 - acc: 0.6615 - val_loss: 0.8300 - val_acc: 0.4167\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6297 - acc: 0.6873 - val_loss: 0.8153 - val_acc: 0.4062\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6812 - acc: 0.6375 - val_loss: 0.8252 - val_acc: 0.4062\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7149 - acc: 0.6334 - val_loss: 0.8204 - val_acc: 0.4167\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7039 - acc: 0.6529 - val_loss: 0.8175 - val_acc: 0.4167\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7250 - acc: 0.6667 - val_loss: 0.8214 - val_acc: 0.4062\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6678 - acc: 0.6718 - val_loss: 0.8197 - val_acc: 0.4167\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7132 - acc: 0.6546 - val_loss: 0.8178 - val_acc: 0.4062\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7200 - acc: 0.6289 - val_loss: 0.8155 - val_acc: 0.4062\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6999 - acc: 0.6392 - val_loss: 0.8134 - val_acc: 0.4167\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6815 - acc: 0.6375 - val_loss: 0.8138 - val_acc: 0.4271\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7470 - acc: 0.6460 - val_loss: 0.8406 - val_acc: 0.3750\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7120 - acc: 0.6426 - val_loss: 0.8367 - val_acc: 0.4062\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6617 - acc: 0.6821 - val_loss: 0.8300 - val_acc: 0.3958\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6635 - acc: 0.6426 - val_loss: 0.8215 - val_acc: 0.3958\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6650 - acc: 0.6495 - val_loss: 0.8085 - val_acc: 0.4167\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6581 - acc: 0.6443 - val_loss: 0.8144 - val_acc: 0.4062\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6479 - acc: 0.6718 - val_loss: 0.8095 - val_acc: 0.4271\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6696 - acc: 0.6598 - val_loss: 0.8122 - val_acc: 0.4062\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6948 - acc: 0.6478 - val_loss: 0.8110 - val_acc: 0.3958\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6917 - acc: 0.6478 - val_loss: 0.8220 - val_acc: 0.3854\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6730 - acc: 0.6546 - val_loss: 0.8292 - val_acc: 0.3750\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6620 - acc: 0.6684 - val_loss: 0.8052 - val_acc: 0.3958\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6751 - acc: 0.6581 - val_loss: 0.8133 - val_acc: 0.4062\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7171 - acc: 0.6684 - val_loss: 0.8026 - val_acc: 0.4271\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6531 - acc: 0.6787 - val_loss: 0.8024 - val_acc: 0.4062\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6661 - acc: 0.6667 - val_loss: 0.8213 - val_acc: 0.3854\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.6296 - acc: 0.6718 - val_loss: 0.8098 - val_acc: 0.3958\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6775 - acc: 0.6426 - val_loss: 0.8172 - val_acc: 0.4271\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6964 - acc: 0.6254 - val_loss: 0.8194 - val_acc: 0.3854\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6864 - acc: 0.6478 - val_loss: 0.8151 - val_acc: 0.3958\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6891 - acc: 0.6478 - val_loss: 0.8153 - val_acc: 0.3958\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6707 - acc: 0.6684 - val_loss: 0.8047 - val_acc: 0.4375\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6833 - acc: 0.6470 - val_loss: 0.8005 - val_acc: 0.3958\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6330 - acc: 0.6942 - val_loss: 0.8021 - val_acc: 0.4167\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6377 - acc: 0.6667 - val_loss: 0.8081 - val_acc: 0.3958\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6922 - acc: 0.6615 - val_loss: 0.7945 - val_acc: 0.4167\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7155 - acc: 0.6340 - val_loss: 0.7959 - val_acc: 0.4167\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6687 - acc: 0.6649 - val_loss: 0.8055 - val_acc: 0.4062\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6957 - acc: 0.6271 - val_loss: 0.8117 - val_acc: 0.3958\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6792 - acc: 0.6443 - val_loss: 0.7960 - val_acc: 0.4271\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6840 - acc: 0.6409 - val_loss: 0.7987 - val_acc: 0.4271\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.6498 - acc: 0.6478 - val_loss: 0.8211 - val_acc: 0.3854\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7197 - acc: 0.6375 - val_loss: 0.8068 - val_acc: 0.3958\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7063 - acc: 0.6220 - val_loss: 0.8063 - val_acc: 0.4271\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7023 - acc: 0.6306 - val_loss: 0.8120 - val_acc: 0.4271\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7143 - acc: 0.6323 - val_loss: 0.8207 - val_acc: 0.4062\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6739 - acc: 0.6632 - val_loss: 0.7959 - val_acc: 0.4167\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6220 - acc: 0.6993 - val_loss: 0.7875 - val_acc: 0.4271\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6543 - acc: 0.6718 - val_loss: 0.8056 - val_acc: 0.4271\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6946 - acc: 0.6581 - val_loss: 0.7822 - val_acc: 0.4271\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6696 - acc: 0.6753 - val_loss: 0.8098 - val_acc: 0.4167\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6222 - acc: 0.6770 - val_loss: 0.8012 - val_acc: 0.4271\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6574 - acc: 0.6564 - val_loss: 0.8180 - val_acc: 0.3958\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.6702 - acc: 0.6718 - val_loss: 0.7993 - val_acc: 0.4271\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6843 - acc: 0.6529 - val_loss: 0.8033 - val_acc: 0.4167\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6259 - acc: 0.6615 - val_loss: 0.8110 - val_acc: 0.3958\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6968 - acc: 0.6340 - val_loss: 0.8028 - val_acc: 0.4271\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6753 - acc: 0.6564 - val_loss: 0.8008 - val_acc: 0.4167\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6706 - acc: 0.6632 - val_loss: 0.7821 - val_acc: 0.4167\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6582 - acc: 0.6632 - val_loss: 0.7978 - val_acc: 0.4167\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6959 - acc: 0.6426 - val_loss: 0.8036 - val_acc: 0.4271\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7023 - acc: 0.6375 - val_loss: 0.7977 - val_acc: 0.4271\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6926 - acc: 0.6512 - val_loss: 0.8031 - val_acc: 0.3958\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6769 - acc: 0.6512 - val_loss: 0.7842 - val_acc: 0.4167\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6767 - acc: 0.6529 - val_loss: 0.8021 - val_acc: 0.4271\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7032 - acc: 0.6375 - val_loss: 0.7889 - val_acc: 0.4271\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6625 - acc: 0.6684 - val_loss: 0.7839 - val_acc: 0.4271\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6865 - acc: 0.6392 - val_loss: 0.7918 - val_acc: 0.4271\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6198 - acc: 0.6684 - val_loss: 0.7759 - val_acc: 0.4375\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6668 - acc: 0.6546 - val_loss: 0.7924 - val_acc: 0.4167\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6820 - acc: 0.6357 - val_loss: 0.8052 - val_acc: 0.4062\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6935 - acc: 0.6186 - val_loss: 0.8023 - val_acc: 0.4062\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6693 - acc: 0.6478 - val_loss: 0.7888 - val_acc: 0.4271\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6491 - acc: 0.6684 - val_loss: 0.7964 - val_acc: 0.4062\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6577 - acc: 0.6753 - val_loss: 0.7881 - val_acc: 0.4271\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6632 - acc: 0.6460 - val_loss: 0.7736 - val_acc: 0.4375\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6218 - acc: 0.6838 - val_loss: 0.7890 - val_acc: 0.4167\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6583 - acc: 0.6460 - val_loss: 0.7851 - val_acc: 0.4375\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6449 - acc: 0.6890 - val_loss: 0.7886 - val_acc: 0.4271\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6957 - acc: 0.6478 - val_loss: 0.7965 - val_acc: 0.4062\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6374 - acc: 0.6718 - val_loss: 0.7921 - val_acc: 0.4271\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6662 - acc: 0.6375 - val_loss: 0.7904 - val_acc: 0.4167\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6337 - acc: 0.6581 - val_loss: 0.7790 - val_acc: 0.4479\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6442 - acc: 0.6581 - val_loss: 0.7832 - val_acc: 0.4271\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6945 - acc: 0.6392 - val_loss: 0.7756 - val_acc: 0.4479\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6545 - acc: 0.6605 - val_loss: 0.7938 - val_acc: 0.4271\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6059 - acc: 0.6993 - val_loss: 0.7704 - val_acc: 0.4375\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6994 - acc: 0.6082 - val_loss: 0.7888 - val_acc: 0.4167\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6927 - acc: 0.6357 - val_loss: 0.7870 - val_acc: 0.4271\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6555 - acc: 0.6632 - val_loss: 0.7881 - val_acc: 0.4167\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6404 - acc: 0.6649 - val_loss: 0.7941 - val_acc: 0.4271\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7009 - acc: 0.6186 - val_loss: 0.7938 - val_acc: 0.4062\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6427 - acc: 0.6684 - val_loss: 0.7899 - val_acc: 0.4271\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6248 - acc: 0.6821 - val_loss: 0.7900 - val_acc: 0.4271\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6989 - acc: 0.6186 - val_loss: 0.7815 - val_acc: 0.4271\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6234 - acc: 0.6649 - val_loss: 0.7738 - val_acc: 0.4271\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6499 - acc: 0.6564 - val_loss: 0.7840 - val_acc: 0.4271\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6474 - acc: 0.6667 - val_loss: 0.7893 - val_acc: 0.4271\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6398 - acc: 0.6976 - val_loss: 0.7701 - val_acc: 0.4271\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6612 - acc: 0.6443 - val_loss: 0.7752 - val_acc: 0.4479\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6633 - acc: 0.6409 - val_loss: 0.7862 - val_acc: 0.4271\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6332 - acc: 0.6649 - val_loss: 0.7589 - val_acc: 0.4479\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6097 - acc: 0.6873 - val_loss: 0.7777 - val_acc: 0.4271\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6319 - acc: 0.6890 - val_loss: 0.7901 - val_acc: 0.4167\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6305 - acc: 0.6924 - val_loss: 0.7729 - val_acc: 0.4271\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6180 - acc: 0.6753 - val_loss: 0.7682 - val_acc: 0.4479\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6410 - acc: 0.6856 - val_loss: 0.7691 - val_acc: 0.4271\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6315 - acc: 0.6838 - val_loss: 0.7728 - val_acc: 0.4375\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6743 - acc: 0.6684 - val_loss: 0.7723 - val_acc: 0.4688\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6481 - acc: 0.6581 - val_loss: 0.7745 - val_acc: 0.4375\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6242 - acc: 0.6701 - val_loss: 0.7810 - val_acc: 0.4167\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6192 - acc: 0.6753 - val_loss: 0.7761 - val_acc: 0.4583\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6382 - acc: 0.6615 - val_loss: 0.7922 - val_acc: 0.4375\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6821 - acc: 0.6529 - val_loss: 0.7672 - val_acc: 0.4479\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.6165 - acc: 0.6856 - val_loss: 0.7815 - val_acc: 0.4375\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6827 - acc: 0.6667 - val_loss: 0.7680 - val_acc: 0.4688\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6651 - acc: 0.6615 - val_loss: 0.7593 - val_acc: 0.4375\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6452 - acc: 0.6615 - val_loss: 0.7717 - val_acc: 0.4375\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6581 - acc: 0.6649 - val_loss: 0.7628 - val_acc: 0.4583\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6443 - acc: 0.6942 - val_loss: 0.7557 - val_acc: 0.4688\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6432 - acc: 0.6684 - val_loss: 0.7731 - val_acc: 0.4167\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6475 - acc: 0.6770 - val_loss: 0.7638 - val_acc: 0.4479\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6493 - acc: 0.6598 - val_loss: 0.7727 - val_acc: 0.4271\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6892 - acc: 0.6821 - val_loss: 0.7469 - val_acc: 0.4479\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6253 - acc: 0.6856 - val_loss: 0.7771 - val_acc: 0.4688\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5779 - acc: 0.7131 - val_loss: 0.7662 - val_acc: 0.4479\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6114 - acc: 0.6770 - val_loss: 0.7692 - val_acc: 0.4792\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6497 - acc: 0.6478 - val_loss: 0.7690 - val_acc: 0.4375\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6450 - acc: 0.6649 - val_loss: 0.7767 - val_acc: 0.4479\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6207 - acc: 0.6890 - val_loss: 0.7725 - val_acc: 0.4583\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6577 - acc: 0.6873 - val_loss: 0.7729 - val_acc: 0.4479\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6441 - acc: 0.6907 - val_loss: 0.7765 - val_acc: 0.4375\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6370 - acc: 0.6564 - val_loss: 0.7720 - val_acc: 0.4479\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6330 - acc: 0.6649 - val_loss: 0.7623 - val_acc: 0.4375\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6273 - acc: 0.6667 - val_loss: 0.7655 - val_acc: 0.4479\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6249 - acc: 0.6632 - val_loss: 0.7601 - val_acc: 0.4688\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6404 - acc: 0.6564 - val_loss: 0.7794 - val_acc: 0.4271\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6493 - acc: 0.6581 - val_loss: 0.7683 - val_acc: 0.4583\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6367 - acc: 0.6615 - val_loss: 0.7665 - val_acc: 0.4583\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6248 - acc: 0.6735 - val_loss: 0.7804 - val_acc: 0.4375\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6453 - acc: 0.6667 - val_loss: 0.7461 - val_acc: 0.4583\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6458 - acc: 0.6787 - val_loss: 0.7666 - val_acc: 0.4479\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6080 - acc: 0.6787 - val_loss: 0.7615 - val_acc: 0.4479\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6276 - acc: 0.6787 - val_loss: 0.7547 - val_acc: 0.4688\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6519 - acc: 0.6701 - val_loss: 0.7660 - val_acc: 0.4375\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6881 - acc: 0.6289 - val_loss: 0.7611 - val_acc: 0.4479\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6350 - acc: 0.6529 - val_loss: 0.7634 - val_acc: 0.4583\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6072 - acc: 0.6701 - val_loss: 0.7862 - val_acc: 0.4167\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6499 - acc: 0.6701 - val_loss: 0.7742 - val_acc: 0.4479\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.6184 - acc: 0.6942 - val_loss: 0.7594 - val_acc: 0.4479\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6202 - acc: 0.6770 - val_loss: 0.7641 - val_acc: 0.4479\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6257 - acc: 0.7062 - val_loss: 0.7634 - val_acc: 0.4479\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6194 - acc: 0.6718 - val_loss: 0.7556 - val_acc: 0.4688\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6029 - acc: 0.6890 - val_loss: 0.7663 - val_acc: 0.4375\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6599 - acc: 0.6684 - val_loss: 0.7712 - val_acc: 0.4375\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6399 - acc: 0.6564 - val_loss: 0.7596 - val_acc: 0.4583\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6072 - acc: 0.6873 - val_loss: 0.7829 - val_acc: 0.4271\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6483 - acc: 0.6838 - val_loss: 0.7639 - val_acc: 0.4479\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6185 - acc: 0.6718 - val_loss: 0.7758 - val_acc: 0.4375\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.6069 - acc: 0.6907 - val_loss: 0.7711 - val_acc: 0.4583\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6261 - acc: 0.6804 - val_loss: 0.7748 - val_acc: 0.4479\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6191 - acc: 0.6856 - val_loss: 0.7657 - val_acc: 0.4479\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6026 - acc: 0.6959 - val_loss: 0.7654 - val_acc: 0.4583\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6705 - acc: 0.6546 - val_loss: 0.7797 - val_acc: 0.4479\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6342 - acc: 0.6460 - val_loss: 0.7652 - val_acc: 0.4479\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6452 - acc: 0.6649 - val_loss: 0.7732 - val_acc: 0.4479\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6093 - acc: 0.6718 - val_loss: 0.7585 - val_acc: 0.4583\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6293 - acc: 0.6804 - val_loss: 0.7588 - val_acc: 0.4583\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6694 - acc: 0.6564 - val_loss: 0.7612 - val_acc: 0.4375\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6090 - acc: 0.6615 - val_loss: 0.7617 - val_acc: 0.4479\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6504 - acc: 0.6426 - val_loss: 0.7781 - val_acc: 0.4271\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6401 - acc: 0.6684 - val_loss: 0.7815 - val_acc: 0.4479\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6347 - acc: 0.6770 - val_loss: 0.7418 - val_acc: 0.4792\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6437 - acc: 0.6787 - val_loss: 0.7719 - val_acc: 0.4479\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6187 - acc: 0.6718 - val_loss: 0.7578 - val_acc: 0.4479\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6255 - acc: 0.6942 - val_loss: 0.7725 - val_acc: 0.4375\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.6721 - acc: 0.6237 - val_loss: 0.7787 - val_acc: 0.4062\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6316 - acc: 0.6841 - val_loss: 0.7582 - val_acc: 0.4375\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6232 - acc: 0.6512 - val_loss: 0.7549 - val_acc: 0.4479\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6250 - acc: 0.6581 - val_loss: 0.7524 - val_acc: 0.4583\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6167 - acc: 0.6804 - val_loss: 0.7569 - val_acc: 0.4583\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6053 - acc: 0.6770 - val_loss: 0.7709 - val_acc: 0.4375\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5969 - acc: 0.6873 - val_loss: 0.7548 - val_acc: 0.4688\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6193 - acc: 0.6774 - val_loss: 0.7617 - val_acc: 0.4375\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6334 - acc: 0.6718 - val_loss: 0.7672 - val_acc: 0.4375\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.6083 - acc: 0.6924 - val_loss: 0.7714 - val_acc: 0.4167\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5912 - acc: 0.6770 - val_loss: 0.7546 - val_acc: 0.4479\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6595 - acc: 0.6460 - val_loss: 0.7606 - val_acc: 0.4688\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5638 - acc: 0.7045 - val_loss: 0.7666 - val_acc: 0.4479\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6296 - acc: 0.6718 - val_loss: 0.7673 - val_acc: 0.4375\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6579 - acc: 0.6564 - val_loss: 0.7488 - val_acc: 0.4688\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5833 - acc: 0.6993 - val_loss: 0.7420 - val_acc: 0.4583\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6422 - acc: 0.6684 - val_loss: 0.7450 - val_acc: 0.4479\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6276 - acc: 0.6581 - val_loss: 0.7554 - val_acc: 0.4688\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.6418 - acc: 0.6529 - val_loss: 0.7569 - val_acc: 0.4896\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6228 - acc: 0.6753 - val_loss: 0.7394 - val_acc: 0.4688\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6055 - acc: 0.6753 - val_loss: 0.7417 - val_acc: 0.4688\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6159 - acc: 0.6684 - val_loss: 0.7611 - val_acc: 0.4479\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6089 - acc: 0.6735 - val_loss: 0.7499 - val_acc: 0.4583\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5749 - acc: 0.6993 - val_loss: 0.7634 - val_acc: 0.4479\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6564 - acc: 0.6478 - val_loss: 0.7573 - val_acc: 0.4688\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5919 - acc: 0.7010 - val_loss: 0.7607 - val_acc: 0.4688\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6339 - acc: 0.6615 - val_loss: 0.7510 - val_acc: 0.5000\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5888 - acc: 0.7131 - val_loss: 0.7540 - val_acc: 0.4688\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6227 - acc: 0.6856 - val_loss: 0.7609 - val_acc: 0.4479\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6176 - acc: 0.6667 - val_loss: 0.7666 - val_acc: 0.4583\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6469 - acc: 0.6649 - val_loss: 0.7610 - val_acc: 0.4688\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6204 - acc: 0.6838 - val_loss: 0.7598 - val_acc: 0.4479\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5936 - acc: 0.6873 - val_loss: 0.7714 - val_acc: 0.4271\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6145 - acc: 0.6684 - val_loss: 0.7728 - val_acc: 0.4167\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6057 - acc: 0.7027 - val_loss: 0.7597 - val_acc: 0.4583\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6004 - acc: 0.6856 - val_loss: 0.7445 - val_acc: 0.4479\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5816 - acc: 0.7079 - val_loss: 0.7589 - val_acc: 0.4688\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6482 - acc: 0.6787 - val_loss: 0.7782 - val_acc: 0.4167\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5830 - acc: 0.6632 - val_loss: 0.7560 - val_acc: 0.4479\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5996 - acc: 0.6770 - val_loss: 0.7619 - val_acc: 0.4688\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5875 - acc: 0.7045 - val_loss: 0.7804 - val_acc: 0.4375\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6561 - acc: 0.6495 - val_loss: 0.7607 - val_acc: 0.4688\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.6153 - acc: 0.6856 - val_loss: 0.7388 - val_acc: 0.4792\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6005 - acc: 0.6942 - val_loss: 0.7499 - val_acc: 0.4688\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6681 - acc: 0.6478 - val_loss: 0.7622 - val_acc: 0.4375\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6668 - acc: 0.6684 - val_loss: 0.7617 - val_acc: 0.4375\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6011 - acc: 0.6770 - val_loss: 0.7550 - val_acc: 0.4583\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6045 - acc: 0.6907 - val_loss: 0.7559 - val_acc: 0.4792\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6065 - acc: 0.7027 - val_loss: 0.7600 - val_acc: 0.4688\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6007 - acc: 0.6701 - val_loss: 0.7419 - val_acc: 0.4792\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5631 - acc: 0.7045 - val_loss: 0.7585 - val_acc: 0.4479\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6061 - acc: 0.6907 - val_loss: 0.7650 - val_acc: 0.4271\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5907 - acc: 0.7027 - val_loss: 0.7531 - val_acc: 0.4479\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5840 - acc: 0.7148 - val_loss: 0.7498 - val_acc: 0.4479\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6397 - acc: 0.6856 - val_loss: 0.7483 - val_acc: 0.4583\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.6016 - acc: 0.7010 - val_loss: 0.7615 - val_acc: 0.4375\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5929 - acc: 0.7045 - val_loss: 0.7629 - val_acc: 0.4375\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6348 - acc: 0.6581 - val_loss: 0.7659 - val_acc: 0.4792\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5971 - acc: 0.6856 - val_loss: 0.7652 - val_acc: 0.4479\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6287 - acc: 0.6821 - val_loss: 0.7684 - val_acc: 0.4583\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5940 - acc: 0.6873 - val_loss: 0.7625 - val_acc: 0.4688\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.6001 - acc: 0.6856 - val_loss: 0.7553 - val_acc: 0.4792\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6370 - acc: 0.6753 - val_loss: 0.7497 - val_acc: 0.4688\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6175 - acc: 0.6856 - val_loss: 0.7461 - val_acc: 0.4479\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6054 - acc: 0.7062 - val_loss: 0.7463 - val_acc: 0.4583\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6217 - acc: 0.6856 - val_loss: 0.7615 - val_acc: 0.4583\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6050 - acc: 0.6959 - val_loss: 0.7523 - val_acc: 0.4688\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6222 - acc: 0.6632 - val_loss: 0.7558 - val_acc: 0.4688\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5768 - acc: 0.7216 - val_loss: 0.7571 - val_acc: 0.4896\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5548 - acc: 0.7131 - val_loss: 0.7574 - val_acc: 0.4688\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6157 - acc: 0.6976 - val_loss: 0.7495 - val_acc: 0.4688\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5996 - acc: 0.6838 - val_loss: 0.7566 - val_acc: 0.4688\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6268 - acc: 0.6718 - val_loss: 0.7524 - val_acc: 0.4792\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5525 - acc: 0.7302 - val_loss: 0.7533 - val_acc: 0.4583\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6589 - acc: 0.6718 - val_loss: 0.7491 - val_acc: 0.4583\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6204 - acc: 0.6735 - val_loss: 0.7537 - val_acc: 0.4479\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6061 - acc: 0.6856 - val_loss: 0.7553 - val_acc: 0.4375\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5394 - acc: 0.7423 - val_loss: 0.7603 - val_acc: 0.4375\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5819 - acc: 0.6942 - val_loss: 0.7569 - val_acc: 0.4583\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6443 - acc: 0.6770 - val_loss: 0.7485 - val_acc: 0.4792\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6043 - acc: 0.6753 - val_loss: 0.7475 - val_acc: 0.4792\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5662 - acc: 0.6976 - val_loss: 0.7443 - val_acc: 0.4792\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5966 - acc: 0.6924 - val_loss: 0.7430 - val_acc: 0.4792\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6042 - acc: 0.6873 - val_loss: 0.7514 - val_acc: 0.4583\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6111 - acc: 0.6976 - val_loss: 0.7396 - val_acc: 0.4896\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6055 - acc: 0.6890 - val_loss: 0.7529 - val_acc: 0.4688\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6119 - acc: 0.6838 - val_loss: 0.7641 - val_acc: 0.4479\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5877 - acc: 0.6907 - val_loss: 0.7565 - val_acc: 0.4688\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5680 - acc: 0.7045 - val_loss: 0.7431 - val_acc: 0.4688\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5875 - acc: 0.7113 - val_loss: 0.7451 - val_acc: 0.4896\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6031 - acc: 0.6993 - val_loss: 0.7444 - val_acc: 0.4896\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6036 - acc: 0.6753 - val_loss: 0.7297 - val_acc: 0.5000\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5786 - acc: 0.6907 - val_loss: 0.7359 - val_acc: 0.4792\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.6113 - acc: 0.6856 - val_loss: 0.7422 - val_acc: 0.4688\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5723 - acc: 0.6907 - val_loss: 0.7582 - val_acc: 0.4375\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5683 - acc: 0.7268 - val_loss: 0.7275 - val_acc: 0.5000\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5650 - acc: 0.6976 - val_loss: 0.7509 - val_acc: 0.4479\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5344 - acc: 0.7371 - val_loss: 0.7327 - val_acc: 0.4792\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5790 - acc: 0.6873 - val_loss: 0.7431 - val_acc: 0.4896\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5920 - acc: 0.6993 - val_loss: 0.7511 - val_acc: 0.4688\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5909 - acc: 0.7010 - val_loss: 0.7316 - val_acc: 0.5000\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5628 - acc: 0.6993 - val_loss: 0.7356 - val_acc: 0.4792\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5917 - acc: 0.6838 - val_loss: 0.7438 - val_acc: 0.4896\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5704 - acc: 0.6976 - val_loss: 0.7544 - val_acc: 0.4583\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5657 - acc: 0.6993 - val_loss: 0.7363 - val_acc: 0.5000\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6217 - acc: 0.6770 - val_loss: 0.7500 - val_acc: 0.4792\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6126 - acc: 0.6924 - val_loss: 0.7428 - val_acc: 0.4792\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5920 - acc: 0.6735 - val_loss: 0.7253 - val_acc: 0.5104\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6190 - acc: 0.6735 - val_loss: 0.7228 - val_acc: 0.5000\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5862 - acc: 0.6804 - val_loss: 0.7310 - val_acc: 0.5000\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5511 - acc: 0.7251 - val_loss: 0.7265 - val_acc: 0.5104\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5787 - acc: 0.6926 - val_loss: 0.7455 - val_acc: 0.4896\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5419 - acc: 0.7285 - val_loss: 0.7289 - val_acc: 0.5208\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5793 - acc: 0.7045 - val_loss: 0.7277 - val_acc: 0.4792\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5896 - acc: 0.6907 - val_loss: 0.7229 - val_acc: 0.5104\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5906 - acc: 0.6924 - val_loss: 0.7328 - val_acc: 0.5104\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5901 - acc: 0.6924 - val_loss: 0.7325 - val_acc: 0.5000\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6213 - acc: 0.6701 - val_loss: 0.7322 - val_acc: 0.5000\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5951 - acc: 0.6856 - val_loss: 0.7206 - val_acc: 0.5104\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5765 - acc: 0.6959 - val_loss: 0.7355 - val_acc: 0.5104\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5944 - acc: 0.6890 - val_loss: 0.7340 - val_acc: 0.5104\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.6017 - acc: 0.6667 - val_loss: 0.7380 - val_acc: 0.4896\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 0.6132 - acc: 0.6890 - val_loss: 0.7176 - val_acc: 0.5208\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5866 - acc: 0.7251 - val_loss: 0.7268 - val_acc: 0.5104\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5922 - acc: 0.6924 - val_loss: 0.7424 - val_acc: 0.4792\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5894 - acc: 0.6907 - val_loss: 0.7375 - val_acc: 0.4792\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5849 - acc: 0.6959 - val_loss: 0.7375 - val_acc: 0.5000\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5991 - acc: 0.6753 - val_loss: 0.7535 - val_acc: 0.4688\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5832 - acc: 0.6942 - val_loss: 0.7441 - val_acc: 0.5104\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5920 - acc: 0.7096 - val_loss: 0.7405 - val_acc: 0.5000\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5841 - acc: 0.7045 - val_loss: 0.7382 - val_acc: 0.4896\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6051 - acc: 0.6735 - val_loss: 0.7108 - val_acc: 0.5208\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5596 - acc: 0.7234 - val_loss: 0.7211 - val_acc: 0.5000\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5794 - acc: 0.7062 - val_loss: 0.7302 - val_acc: 0.5104\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5894 - acc: 0.6942 - val_loss: 0.7382 - val_acc: 0.5104\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5981 - acc: 0.6684 - val_loss: 0.7285 - val_acc: 0.5208\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5736 - acc: 0.6821 - val_loss: 0.7437 - val_acc: 0.4896\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5666 - acc: 0.6976 - val_loss: 0.7445 - val_acc: 0.4896\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5979 - acc: 0.7010 - val_loss: 0.7332 - val_acc: 0.5000\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5796 - acc: 0.6976 - val_loss: 0.7372 - val_acc: 0.4792\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5934 - acc: 0.6924 - val_loss: 0.7363 - val_acc: 0.4896\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5967 - acc: 0.7062 - val_loss: 0.7265 - val_acc: 0.5000\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5732 - acc: 0.7182 - val_loss: 0.7353 - val_acc: 0.5208\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5661 - acc: 0.7131 - val_loss: 0.7393 - val_acc: 0.4896\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5860 - acc: 0.6770 - val_loss: 0.7259 - val_acc: 0.5000\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5840 - acc: 0.6907 - val_loss: 0.7316 - val_acc: 0.5000\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5735 - acc: 0.6993 - val_loss: 0.7226 - val_acc: 0.5104\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5851 - acc: 0.6959 - val_loss: 0.7368 - val_acc: 0.5000\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6112 - acc: 0.6718 - val_loss: 0.7270 - val_acc: 0.5104\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5454 - acc: 0.7320 - val_loss: 0.7400 - val_acc: 0.5000\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5589 - acc: 0.7182 - val_loss: 0.7265 - val_acc: 0.5000\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5824 - acc: 0.6873 - val_loss: 0.7385 - val_acc: 0.5000\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6239 - acc: 0.6735 - val_loss: 0.7408 - val_acc: 0.4896\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5470 - acc: 0.7045 - val_loss: 0.7315 - val_acc: 0.5000\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5531 - acc: 0.7165 - val_loss: 0.7275 - val_acc: 0.5000\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5982 - acc: 0.7062 - val_loss: 0.7392 - val_acc: 0.5104\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5612 - acc: 0.7113 - val_loss: 0.7353 - val_acc: 0.4896\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5720 - acc: 0.6907 - val_loss: 0.7450 - val_acc: 0.4792\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5896 - acc: 0.7010 - val_loss: 0.7249 - val_acc: 0.5104\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5810 - acc: 0.7027 - val_loss: 0.7258 - val_acc: 0.5208\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5679 - acc: 0.7216 - val_loss: 0.7329 - val_acc: 0.5000\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5963 - acc: 0.7062 - val_loss: 0.7209 - val_acc: 0.5208\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5739 - acc: 0.7010 - val_loss: 0.7189 - val_acc: 0.5104\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.5575 - acc: 0.7010 - val_loss: 0.7241 - val_acc: 0.5000\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5805 - acc: 0.7113 - val_loss: 0.7363 - val_acc: 0.5000\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5994 - acc: 0.6993 - val_loss: 0.7360 - val_acc: 0.4792\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5689 - acc: 0.7199 - val_loss: 0.7282 - val_acc: 0.5000\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6074 - acc: 0.6701 - val_loss: 0.7277 - val_acc: 0.5000\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5918 - acc: 0.7010 - val_loss: 0.7136 - val_acc: 0.5104\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6184 - acc: 0.6649 - val_loss: 0.7216 - val_acc: 0.5104\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5605 - acc: 0.7096 - val_loss: 0.7134 - val_acc: 0.4792\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5992 - acc: 0.6942 - val_loss: 0.7144 - val_acc: 0.5208\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5704 - acc: 0.6787 - val_loss: 0.7074 - val_acc: 0.5312\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5223 - acc: 0.7234 - val_loss: 0.7146 - val_acc: 0.5312\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5778 - acc: 0.6959 - val_loss: 0.7236 - val_acc: 0.5104\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6075 - acc: 0.6774 - val_loss: 0.7361 - val_acc: 0.4792\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5994 - acc: 0.6841 - val_loss: 0.7343 - val_acc: 0.4688\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5835 - acc: 0.6890 - val_loss: 0.7232 - val_acc: 0.5104\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5755 - acc: 0.6890 - val_loss: 0.7249 - val_acc: 0.5104\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5757 - acc: 0.7079 - val_loss: 0.7332 - val_acc: 0.5000\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5574 - acc: 0.7062 - val_loss: 0.7350 - val_acc: 0.4896\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5737 - acc: 0.7010 - val_loss: 0.7202 - val_acc: 0.5208\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5378 - acc: 0.7423 - val_loss: 0.7291 - val_acc: 0.5104\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5729 - acc: 0.7010 - val_loss: 0.7212 - val_acc: 0.5208\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5911 - acc: 0.6804 - val_loss: 0.7239 - val_acc: 0.5104\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6037 - acc: 0.7079 - val_loss: 0.7343 - val_acc: 0.4896\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5616 - acc: 0.7320 - val_loss: 0.7232 - val_acc: 0.5104\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5955 - acc: 0.6993 - val_loss: 0.7243 - val_acc: 0.5104\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6326 - acc: 0.6684 - val_loss: 0.7251 - val_acc: 0.4896\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5777 - acc: 0.6907 - val_loss: 0.7244 - val_acc: 0.5208\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5636 - acc: 0.6907 - val_loss: 0.7154 - val_acc: 0.5104\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6047 - acc: 0.6924 - val_loss: 0.7157 - val_acc: 0.5208\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5615 - acc: 0.7182 - val_loss: 0.7312 - val_acc: 0.4896\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5870 - acc: 0.7027 - val_loss: 0.7174 - val_acc: 0.5208\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5440 - acc: 0.7577 - val_loss: 0.7214 - val_acc: 0.5104\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5921 - acc: 0.6804 - val_loss: 0.7248 - val_acc: 0.5208\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5747 - acc: 0.7199 - val_loss: 0.7249 - val_acc: 0.5104\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5778 - acc: 0.6993 - val_loss: 0.7362 - val_acc: 0.4896\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5629 - acc: 0.7131 - val_loss: 0.7200 - val_acc: 0.5104\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5615 - acc: 0.7113 - val_loss: 0.7207 - val_acc: 0.5312\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5891 - acc: 0.6838 - val_loss: 0.7340 - val_acc: 0.4896\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5764 - acc: 0.6993 - val_loss: 0.7171 - val_acc: 0.5312\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5628 - acc: 0.7165 - val_loss: 0.7282 - val_acc: 0.5208\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5758 - acc: 0.6890 - val_loss: 0.7201 - val_acc: 0.5104\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5760 - acc: 0.7045 - val_loss: 0.7079 - val_acc: 0.5208\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5978 - acc: 0.6942 - val_loss: 0.7092 - val_acc: 0.5000\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5660 - acc: 0.7027 - val_loss: 0.7120 - val_acc: 0.5208\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5688 - acc: 0.7010 - val_loss: 0.7131 - val_acc: 0.5312\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5715 - acc: 0.7162 - val_loss: 0.7191 - val_acc: 0.5208\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6017 - acc: 0.6907 - val_loss: 0.7222 - val_acc: 0.5104\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5998 - acc: 0.6718 - val_loss: 0.7258 - val_acc: 0.5104\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5786 - acc: 0.7045 - val_loss: 0.7182 - val_acc: 0.5000\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 0.5707 - acc: 0.6959 - val_loss: 0.7100 - val_acc: 0.5104\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5884 - acc: 0.6838 - val_loss: 0.7228 - val_acc: 0.5000\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5955 - acc: 0.6838 - val_loss: 0.7131 - val_acc: 0.5104\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5566 - acc: 0.7234 - val_loss: 0.7227 - val_acc: 0.5208\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5614 - acc: 0.6907 - val_loss: 0.7221 - val_acc: 0.5104\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5822 - acc: 0.7096 - val_loss: 0.7207 - val_acc: 0.5000\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5733 - acc: 0.6959 - val_loss: 0.7126 - val_acc: 0.5104\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5523 - acc: 0.7079 - val_loss: 0.7105 - val_acc: 0.5312\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5907 - acc: 0.6873 - val_loss: 0.7187 - val_acc: 0.5312\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5559 - acc: 0.7010 - val_loss: 0.7337 - val_acc: 0.5104\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5403 - acc: 0.7354 - val_loss: 0.7078 - val_acc: 0.5312\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5610 - acc: 0.6993 - val_loss: 0.7168 - val_acc: 0.5104\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5747 - acc: 0.6993 - val_loss: 0.7266 - val_acc: 0.5000\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5980 - acc: 0.6735 - val_loss: 0.7213 - val_acc: 0.5000\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5660 - acc: 0.7045 - val_loss: 0.7238 - val_acc: 0.5000\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6071 - acc: 0.6856 - val_loss: 0.7214 - val_acc: 0.5000\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5308 - acc: 0.7216 - val_loss: 0.7109 - val_acc: 0.5208\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5373 - acc: 0.7337 - val_loss: 0.7314 - val_acc: 0.4688\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5486 - acc: 0.7062 - val_loss: 0.7172 - val_acc: 0.5208\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5758 - acc: 0.6976 - val_loss: 0.7283 - val_acc: 0.4792\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5567 - acc: 0.7096 - val_loss: 0.6987 - val_acc: 0.5312\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5957 - acc: 0.6976 - val_loss: 0.7239 - val_acc: 0.4792\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6010 - acc: 0.6787 - val_loss: 0.7129 - val_acc: 0.5000\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5323 - acc: 0.7491 - val_loss: 0.7061 - val_acc: 0.5312\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5754 - acc: 0.7010 - val_loss: 0.7076 - val_acc: 0.5208\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5646 - acc: 0.7251 - val_loss: 0.7226 - val_acc: 0.4896\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5833 - acc: 0.6787 - val_loss: 0.7092 - val_acc: 0.5208\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5542 - acc: 0.7128 - val_loss: 0.7126 - val_acc: 0.5208\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5498 - acc: 0.7079 - val_loss: 0.7112 - val_acc: 0.5000\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5875 - acc: 0.6959 - val_loss: 0.7164 - val_acc: 0.5000\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5535 - acc: 0.7131 - val_loss: 0.7082 - val_acc: 0.5208\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.5520 - acc: 0.7268 - val_loss: 0.7079 - val_acc: 0.5208\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5681 - acc: 0.6976 - val_loss: 0.7364 - val_acc: 0.5000\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5556 - acc: 0.7079 - val_loss: 0.7337 - val_acc: 0.4896\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5613 - acc: 0.7079 - val_loss: 0.7050 - val_acc: 0.5521\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5618 - acc: 0.6959 - val_loss: 0.7232 - val_acc: 0.5208\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5850 - acc: 0.6735 - val_loss: 0.7193 - val_acc: 0.5208\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5489 - acc: 0.7234 - val_loss: 0.7096 - val_acc: 0.5208\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5545 - acc: 0.7113 - val_loss: 0.7092 - val_acc: 0.5312\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5473 - acc: 0.6976 - val_loss: 0.7292 - val_acc: 0.5104\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5813 - acc: 0.6804 - val_loss: 0.7324 - val_acc: 0.5104\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5634 - acc: 0.7216 - val_loss: 0.7235 - val_acc: 0.5312\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5622 - acc: 0.7027 - val_loss: 0.7236 - val_acc: 0.5104\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5316 - acc: 0.7199 - val_loss: 0.7071 - val_acc: 0.5312\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5379 - acc: 0.7251 - val_loss: 0.7308 - val_acc: 0.5208\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5745 - acc: 0.6976 - val_loss: 0.7061 - val_acc: 0.5312\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5489 - acc: 0.7405 - val_loss: 0.7224 - val_acc: 0.5104\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5369 - acc: 0.7457 - val_loss: 0.7133 - val_acc: 0.5312\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5805 - acc: 0.7062 - val_loss: 0.7309 - val_acc: 0.5312\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5391 - acc: 0.7371 - val_loss: 0.7268 - val_acc: 0.5000\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5440 - acc: 0.6993 - val_loss: 0.7015 - val_acc: 0.5312\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5625 - acc: 0.6856 - val_loss: 0.7200 - val_acc: 0.5208\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5482 - acc: 0.7062 - val_loss: 0.7229 - val_acc: 0.5104\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5742 - acc: 0.6907 - val_loss: 0.7200 - val_acc: 0.5208\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5484 - acc: 0.7148 - val_loss: 0.7111 - val_acc: 0.5208\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5811 - acc: 0.7199 - val_loss: 0.7260 - val_acc: 0.5312\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5599 - acc: 0.7268 - val_loss: 0.7407 - val_acc: 0.5000\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5635 - acc: 0.7234 - val_loss: 0.7135 - val_acc: 0.5000\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5851 - acc: 0.6873 - val_loss: 0.7333 - val_acc: 0.5104\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5500 - acc: 0.7216 - val_loss: 0.7226 - val_acc: 0.5208\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5754 - acc: 0.6770 - val_loss: 0.7148 - val_acc: 0.5208\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5544 - acc: 0.7268 - val_loss: 0.7149 - val_acc: 0.5312\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5378 - acc: 0.7251 - val_loss: 0.7253 - val_acc: 0.4792\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5603 - acc: 0.7285 - val_loss: 0.7153 - val_acc: 0.5000\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5748 - acc: 0.7079 - val_loss: 0.7248 - val_acc: 0.4792\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5639 - acc: 0.7199 - val_loss: 0.7321 - val_acc: 0.4792\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5728 - acc: 0.7062 - val_loss: 0.7277 - val_acc: 0.5208\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5345 - acc: 0.7440 - val_loss: 0.7314 - val_acc: 0.5000\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5725 - acc: 0.7079 - val_loss: 0.7266 - val_acc: 0.5208\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5682 - acc: 0.7045 - val_loss: 0.7319 - val_acc: 0.5104\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.4934 - acc: 0.7732 - val_loss: 0.7061 - val_acc: 0.5312\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5600 - acc: 0.7061 - val_loss: 0.7194 - val_acc: 0.5208\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5351 - acc: 0.7354 - val_loss: 0.7149 - val_acc: 0.5312\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5462 - acc: 0.7079 - val_loss: 0.7260 - val_acc: 0.4792\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5703 - acc: 0.7062 - val_loss: 0.7096 - val_acc: 0.5312\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5718 - acc: 0.7079 - val_loss: 0.7025 - val_acc: 0.4896\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5475 - acc: 0.7165 - val_loss: 0.7092 - val_acc: 0.4792\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5490 - acc: 0.7045 - val_loss: 0.6957 - val_acc: 0.5104\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5999 - acc: 0.6890 - val_loss: 0.7065 - val_acc: 0.5000\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5189 - acc: 0.7354 - val_loss: 0.7145 - val_acc: 0.4896\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5173 - acc: 0.7646 - val_loss: 0.7028 - val_acc: 0.5417\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5606 - acc: 0.7010 - val_loss: 0.7144 - val_acc: 0.5312\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5324 - acc: 0.7337 - val_loss: 0.7233 - val_acc: 0.5104\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5454 - acc: 0.7079 - val_loss: 0.7243 - val_acc: 0.4896\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5463 - acc: 0.7285 - val_loss: 0.7196 - val_acc: 0.5208\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5754 - acc: 0.7027 - val_loss: 0.7160 - val_acc: 0.5104\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5651 - acc: 0.7268 - val_loss: 0.7026 - val_acc: 0.5417\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5595 - acc: 0.7079 - val_loss: 0.7139 - val_acc: 0.5417\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5581 - acc: 0.7199 - val_loss: 0.7039 - val_acc: 0.5417\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5640 - acc: 0.7045 - val_loss: 0.7271 - val_acc: 0.5208\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5384 - acc: 0.7285 - val_loss: 0.7161 - val_acc: 0.5521\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5659 - acc: 0.7113 - val_loss: 0.7074 - val_acc: 0.5417\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5403 - acc: 0.6993 - val_loss: 0.7163 - val_acc: 0.5208\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5770 - acc: 0.6942 - val_loss: 0.7173 - val_acc: 0.5208\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5418 - acc: 0.7234 - val_loss: 0.7186 - val_acc: 0.5208\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5426 - acc: 0.7302 - val_loss: 0.7124 - val_acc: 0.5417\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5636 - acc: 0.6993 - val_loss: 0.7052 - val_acc: 0.5521\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5017 - acc: 0.7474 - val_loss: 0.7156 - val_acc: 0.5312\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5338 - acc: 0.7285 - val_loss: 0.7174 - val_acc: 0.5312\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5242 - acc: 0.7320 - val_loss: 0.7198 - val_acc: 0.5312\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5298 - acc: 0.7302 - val_loss: 0.7238 - val_acc: 0.5312\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5703 - acc: 0.6890 - val_loss: 0.7237 - val_acc: 0.5312\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5629 - acc: 0.7045 - val_loss: 0.7143 - val_acc: 0.5521\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5702 - acc: 0.7251 - val_loss: 0.7263 - val_acc: 0.5312\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5720 - acc: 0.7062 - val_loss: 0.7200 - val_acc: 0.5312\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5439 - acc: 0.7234 - val_loss: 0.7088 - val_acc: 0.5312\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5647 - acc: 0.6976 - val_loss: 0.7074 - val_acc: 0.5000\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5531 - acc: 0.7285 - val_loss: 0.7199 - val_acc: 0.5104\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5180 - acc: 0.7216 - val_loss: 0.7148 - val_acc: 0.5312\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5484 - acc: 0.7320 - val_loss: 0.7052 - val_acc: 0.5417\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5574 - acc: 0.7320 - val_loss: 0.7146 - val_acc: 0.5208\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5651 - acc: 0.7285 - val_loss: 0.7212 - val_acc: 0.5208\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5519 - acc: 0.7216 - val_loss: 0.7074 - val_acc: 0.5521\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5603 - acc: 0.7320 - val_loss: 0.7225 - val_acc: 0.5208\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5637 - acc: 0.7096 - val_loss: 0.7118 - val_acc: 0.5417\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5313 - acc: 0.7165 - val_loss: 0.7095 - val_acc: 0.5417\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5597 - acc: 0.7079 - val_loss: 0.6994 - val_acc: 0.5521\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5280 - acc: 0.7285 - val_loss: 0.7052 - val_acc: 0.5625\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5433 - acc: 0.7268 - val_loss: 0.7249 - val_acc: 0.5312\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5472 - acc: 0.7199 - val_loss: 0.7162 - val_acc: 0.5521\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5428 - acc: 0.7457 - val_loss: 0.7211 - val_acc: 0.5521\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5741 - acc: 0.6959 - val_loss: 0.7048 - val_acc: 0.5521\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5359 - acc: 0.7320 - val_loss: 0.7133 - val_acc: 0.5521\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5349 - acc: 0.7268 - val_loss: 0.7152 - val_acc: 0.5521\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5720 - acc: 0.6942 - val_loss: 0.7037 - val_acc: 0.5417\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5452 - acc: 0.7148 - val_loss: 0.7096 - val_acc: 0.5417\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5151 - acc: 0.7337 - val_loss: 0.6947 - val_acc: 0.5729\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5795 - acc: 0.6924 - val_loss: 0.6972 - val_acc: 0.5521\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5439 - acc: 0.7096 - val_loss: 0.6914 - val_acc: 0.5625\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5636 - acc: 0.7027 - val_loss: 0.7084 - val_acc: 0.5625\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5152 - acc: 0.7440 - val_loss: 0.7106 - val_acc: 0.5312\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5779 - acc: 0.7045 - val_loss: 0.6952 - val_acc: 0.5729\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5265 - acc: 0.7302 - val_loss: 0.7127 - val_acc: 0.5521\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5854 - acc: 0.6993 - val_loss: 0.7034 - val_acc: 0.5833\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5159 - acc: 0.7337 - val_loss: 0.6978 - val_acc: 0.5833\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.5549 - acc: 0.7216 - val_loss: 0.6865 - val_acc: 0.5521\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5487 - acc: 0.7234 - val_loss: 0.6948 - val_acc: 0.5625\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5246 - acc: 0.7543 - val_loss: 0.6955 - val_acc: 0.5625\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5488 - acc: 0.7131 - val_loss: 0.7089 - val_acc: 0.5312\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5790 - acc: 0.6824 - val_loss: 0.6998 - val_acc: 0.5417\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5743 - acc: 0.6993 - val_loss: 0.7092 - val_acc: 0.5833\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5485 - acc: 0.7251 - val_loss: 0.7097 - val_acc: 0.5417\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5704 - acc: 0.7182 - val_loss: 0.6879 - val_acc: 0.5521\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5959 - acc: 0.6735 - val_loss: 0.7021 - val_acc: 0.5625\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5341 - acc: 0.7251 - val_loss: 0.6967 - val_acc: 0.5625\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5349 - acc: 0.7354 - val_loss: 0.7070 - val_acc: 0.5625\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5680 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.5104\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5595 - acc: 0.7216 - val_loss: 0.7053 - val_acc: 0.5729\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5338 - acc: 0.7371 - val_loss: 0.6976 - val_acc: 0.5625\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5335 - acc: 0.7234 - val_loss: 0.6960 - val_acc: 0.5729\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5545 - acc: 0.7216 - val_loss: 0.6979 - val_acc: 0.5521\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5345 - acc: 0.7216 - val_loss: 0.7078 - val_acc: 0.5000\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5393 - acc: 0.7457 - val_loss: 0.7276 - val_acc: 0.5208\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5444 - acc: 0.7096 - val_loss: 0.7235 - val_acc: 0.5417\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5461 - acc: 0.6959 - val_loss: 0.7186 - val_acc: 0.5729\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5339 - acc: 0.7216 - val_loss: 0.7092 - val_acc: 0.5521\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5791 - acc: 0.7182 - val_loss: 0.7244 - val_acc: 0.5312\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5212 - acc: 0.7382 - val_loss: 0.7151 - val_acc: 0.5625\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5469 - acc: 0.7216 - val_loss: 0.7283 - val_acc: 0.5521\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5194 - acc: 0.7423 - val_loss: 0.7140 - val_acc: 0.5312\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5411 - acc: 0.7251 - val_loss: 0.7098 - val_acc: 0.5625\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5684 - acc: 0.7010 - val_loss: 0.7134 - val_acc: 0.5625\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5460 - acc: 0.7199 - val_loss: 0.7348 - val_acc: 0.5417\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4944 - acc: 0.7612 - val_loss: 0.7331 - val_acc: 0.5625\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5704 - acc: 0.6993 - val_loss: 0.6995 - val_acc: 0.5833\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5132 - acc: 0.7509 - val_loss: 0.7245 - val_acc: 0.5417\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5461 - acc: 0.7216 - val_loss: 0.7265 - val_acc: 0.5625\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5760 - acc: 0.6804 - val_loss: 0.7216 - val_acc: 0.5625\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5425 - acc: 0.7165 - val_loss: 0.7197 - val_acc: 0.5729\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5674 - acc: 0.7096 - val_loss: 0.7004 - val_acc: 0.5938\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5680 - acc: 0.7113 - val_loss: 0.7114 - val_acc: 0.5625\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5563 - acc: 0.7079 - val_loss: 0.6940 - val_acc: 0.5938\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5440 - acc: 0.7199 - val_loss: 0.7320 - val_acc: 0.5521\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5054 - acc: 0.7577 - val_loss: 0.7303 - val_acc: 0.5521\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5789 - acc: 0.7268 - val_loss: 0.7242 - val_acc: 0.5417\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5648 - acc: 0.7268 - val_loss: 0.7418 - val_acc: 0.5417\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5618 - acc: 0.7526 - val_loss: 0.7280 - val_acc: 0.5625\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5434 - acc: 0.7199 - val_loss: 0.7193 - val_acc: 0.5625\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5749 - acc: 0.6942 - val_loss: 0.7137 - val_acc: 0.5833\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5701 - acc: 0.6924 - val_loss: 0.7214 - val_acc: 0.5521\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5112 - acc: 0.7285 - val_loss: 0.7217 - val_acc: 0.5833\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5757 - acc: 0.7079 - val_loss: 0.7268 - val_acc: 0.5729\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5434 - acc: 0.7234 - val_loss: 0.7282 - val_acc: 0.5417\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5546 - acc: 0.6976 - val_loss: 0.7024 - val_acc: 0.5521\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5748 - acc: 0.7062 - val_loss: 0.7249 - val_acc: 0.5312\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5476 - acc: 0.7165 - val_loss: 0.7003 - val_acc: 0.5417\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5353 - acc: 0.7440 - val_loss: 0.7058 - val_acc: 0.5833\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5472 - acc: 0.7320 - val_loss: 0.7200 - val_acc: 0.5417\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5972 - acc: 0.6856 - val_loss: 0.7039 - val_acc: 0.5729\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5272 - acc: 0.7234 - val_loss: 0.7067 - val_acc: 0.5104\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5623 - acc: 0.7148 - val_loss: 0.7085 - val_acc: 0.5417\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5582 - acc: 0.7079 - val_loss: 0.7079 - val_acc: 0.5729\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5272 - acc: 0.7337 - val_loss: 0.6987 - val_acc: 0.5833\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5461 - acc: 0.7285 - val_loss: 0.7047 - val_acc: 0.5833\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5343 - acc: 0.7302 - val_loss: 0.7110 - val_acc: 0.5625\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5126 - acc: 0.7612 - val_loss: 0.6947 - val_acc: 0.5417\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5764 - acc: 0.6976 - val_loss: 0.7071 - val_acc: 0.5312\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5045 - acc: 0.7715 - val_loss: 0.7009 - val_acc: 0.5521\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5180 - acc: 0.7405 - val_loss: 0.7098 - val_acc: 0.5312\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5620 - acc: 0.7148 - val_loss: 0.7082 - val_acc: 0.5312\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5419 - acc: 0.7113 - val_loss: 0.7080 - val_acc: 0.5417\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5234 - acc: 0.7405 - val_loss: 0.6968 - val_acc: 0.5833\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5581 - acc: 0.7148 - val_loss: 0.6948 - val_acc: 0.5417\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5479 - acc: 0.7320 - val_loss: 0.6979 - val_acc: 0.5417\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5423 - acc: 0.7234 - val_loss: 0.7019 - val_acc: 0.5208\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5326 - acc: 0.7388 - val_loss: 0.6931 - val_acc: 0.5417\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5319 - acc: 0.7405 - val_loss: 0.6900 - val_acc: 0.5521\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5289 - acc: 0.7371 - val_loss: 0.6866 - val_acc: 0.5417\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5333 - acc: 0.7474 - val_loss: 0.7083 - val_acc: 0.5521\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5588 - acc: 0.7320 - val_loss: 0.6921 - val_acc: 0.5521\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5225 - acc: 0.7285 - val_loss: 0.7098 - val_acc: 0.5104\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5151 - acc: 0.7371 - val_loss: 0.7145 - val_acc: 0.5104\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5292 - acc: 0.7337 - val_loss: 0.6892 - val_acc: 0.5833\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5676 - acc: 0.7062 - val_loss: 0.6856 - val_acc: 0.5417\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5273 - acc: 0.7285 - val_loss: 0.6900 - val_acc: 0.5417\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5298 - acc: 0.7320 - val_loss: 0.6973 - val_acc: 0.5833\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5358 - acc: 0.7199 - val_loss: 0.7041 - val_acc: 0.5312\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5483 - acc: 0.7268 - val_loss: 0.7007 - val_acc: 0.5521\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5250 - acc: 0.7491 - val_loss: 0.7069 - val_acc: 0.5417\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5416 - acc: 0.7302 - val_loss: 0.7072 - val_acc: 0.5208\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5328 - acc: 0.7216 - val_loss: 0.7028 - val_acc: 0.5312\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5539 - acc: 0.7165 - val_loss: 0.7120 - val_acc: 0.5625\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5473 - acc: 0.7079 - val_loss: 0.6996 - val_acc: 0.5625\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5620 - acc: 0.6993 - val_loss: 0.7099 - val_acc: 0.5729\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5810 - acc: 0.7045 - val_loss: 0.6980 - val_acc: 0.6042\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5504 - acc: 0.7062 - val_loss: 0.7111 - val_acc: 0.5729\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5416 - acc: 0.7354 - val_loss: 0.7107 - val_acc: 0.5521\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5350 - acc: 0.7199 - val_loss: 0.7060 - val_acc: 0.5521\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5473 - acc: 0.7113 - val_loss: 0.7041 - val_acc: 0.5625\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5814 - acc: 0.7062 - val_loss: 0.6878 - val_acc: 0.5521\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5250 - acc: 0.7302 - val_loss: 0.6948 - val_acc: 0.5833\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5794 - acc: 0.7165 - val_loss: 0.6969 - val_acc: 0.5938\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5451 - acc: 0.7251 - val_loss: 0.6940 - val_acc: 0.5729\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5238 - acc: 0.7491 - val_loss: 0.7091 - val_acc: 0.5312\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5285 - acc: 0.7388 - val_loss: 0.7205 - val_acc: 0.5729\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5372 - acc: 0.7165 - val_loss: 0.7175 - val_acc: 0.5729\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5719 - acc: 0.6873 - val_loss: 0.7251 - val_acc: 0.5625\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5700 - acc: 0.7182 - val_loss: 0.7018 - val_acc: 0.5521\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5406 - acc: 0.7491 - val_loss: 0.7056 - val_acc: 0.5729\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5165 - acc: 0.7388 - val_loss: 0.7106 - val_acc: 0.5833\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5562 - acc: 0.7199 - val_loss: 0.7158 - val_acc: 0.5417\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5447 - acc: 0.7268 - val_loss: 0.7242 - val_acc: 0.5833\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5674 - acc: 0.7113 - val_loss: 0.7206 - val_acc: 0.5312\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5121 - acc: 0.7457 - val_loss: 0.7067 - val_acc: 0.5312\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5296 - acc: 0.7302 - val_loss: 0.6977 - val_acc: 0.5521\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5070 - acc: 0.7491 - val_loss: 0.7130 - val_acc: 0.5729\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5418 - acc: 0.7062 - val_loss: 0.7087 - val_acc: 0.5938\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5712 - acc: 0.7113 - val_loss: 0.7076 - val_acc: 0.5625\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5348 - acc: 0.7405 - val_loss: 0.7158 - val_acc: 0.5208\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5612 - acc: 0.7096 - val_loss: 0.6989 - val_acc: 0.5521\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5305 - acc: 0.7440 - val_loss: 0.7117 - val_acc: 0.5208\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5461 - acc: 0.7251 - val_loss: 0.7045 - val_acc: 0.5729\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.5464 - acc: 0.7320 - val_loss: 0.7052 - val_acc: 0.5729\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5332 - acc: 0.7165 - val_loss: 0.7107 - val_acc: 0.5208\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5394 - acc: 0.7268 - val_loss: 0.7060 - val_acc: 0.5312\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5719 - acc: 0.7027 - val_loss: 0.7007 - val_acc: 0.5729\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5727 - acc: 0.6873 - val_loss: 0.6779 - val_acc: 0.5833\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5219 - acc: 0.7405 - val_loss: 0.6934 - val_acc: 0.5938\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5420 - acc: 0.7268 - val_loss: 0.6973 - val_acc: 0.5417\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5246 - acc: 0.7199 - val_loss: 0.6854 - val_acc: 0.5521\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5236 - acc: 0.7405 - val_loss: 0.7029 - val_acc: 0.5208\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5984 - acc: 0.6581 - val_loss: 0.6874 - val_acc: 0.5417\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5601 - acc: 0.6856 - val_loss: 0.7025 - val_acc: 0.5208\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.5226 - acc: 0.7491 - val_loss: 0.6999 - val_acc: 0.5208\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5148 - acc: 0.7491 - val_loss: 0.6777 - val_acc: 0.5521\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5302 - acc: 0.7302 - val_loss: 0.7042 - val_acc: 0.5938\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5326 - acc: 0.7285 - val_loss: 0.6863 - val_acc: 0.5729\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5429 - acc: 0.7268 - val_loss: 0.6823 - val_acc: 0.5938\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5242 - acc: 0.7354 - val_loss: 0.7021 - val_acc: 0.5208\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5418 - acc: 0.7165 - val_loss: 0.7047 - val_acc: 0.5938\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.5174 - acc: 0.7285 - val_loss: 0.7023 - val_acc: 0.5938\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5223 - acc: 0.7371 - val_loss: 0.7121 - val_acc: 0.5833\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5310 - acc: 0.7405 - val_loss: 0.7027 - val_acc: 0.5625\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5360 - acc: 0.7423 - val_loss: 0.6984 - val_acc: 0.6042\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5047 - acc: 0.7526 - val_loss: 0.6755 - val_acc: 0.6250\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5400 - acc: 0.7285 - val_loss: 0.6938 - val_acc: 0.5938\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5332 - acc: 0.7466 - val_loss: 0.6983 - val_acc: 0.5729\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5187 - acc: 0.7302 - val_loss: 0.7166 - val_acc: 0.5729\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5322 - acc: 0.7285 - val_loss: 0.7064 - val_acc: 0.5417\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5313 - acc: 0.7388 - val_loss: 0.7141 - val_acc: 0.5521\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.5464 - acc: 0.7216 - val_loss: 0.7137 - val_acc: 0.5938\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5546 - acc: 0.7131 - val_loss: 0.7275 - val_acc: 0.5833\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5770 - acc: 0.6976 - val_loss: 0.7023 - val_acc: 0.5417\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5560 - acc: 0.7182 - val_loss: 0.7139 - val_acc: 0.5729\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5576 - acc: 0.7148 - val_loss: 0.6974 - val_acc: 0.5729\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5229 - acc: 0.7268 - val_loss: 0.7062 - val_acc: 0.5208\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5271 - acc: 0.7302 - val_loss: 0.6969 - val_acc: 0.6042\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5536 - acc: 0.7234 - val_loss: 0.7122 - val_acc: 0.5729\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.5362 - acc: 0.7337 - val_loss: 0.7084 - val_acc: 0.5833\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.5070 - acc: 0.7784 - val_loss: 0.7160 - val_acc: 0.5833\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5411 - acc: 0.7182 - val_loss: 0.6989 - val_acc: 0.5833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "7060bf95-fdaf-4559-b035-6fb6fdaf8340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.178944706916809,\n",
              "  1.2320854663848877,\n",
              "  1.1754788160324097,\n",
              "  1.1889152526855469,\n",
              "  1.1760047674179077,\n",
              "  1.1646207571029663,\n",
              "  1.1294032335281372,\n",
              "  1.1666449308395386,\n",
              "  1.1897238492965698,\n",
              "  1.1199513673782349,\n",
              "  1.116047978401184,\n",
              "  1.1915650367736816,\n",
              "  1.1140577793121338,\n",
              "  1.1973053216934204,\n",
              "  1.1926442384719849,\n",
              "  1.1971718072891235,\n",
              "  1.0841631889343262,\n",
              "  1.1028772592544556,\n",
              "  1.132351279258728,\n",
              "  1.1334848403930664,\n",
              "  1.0929280519485474,\n",
              "  1.1253751516342163,\n",
              "  1.0954968929290771,\n",
              "  1.1169525384902954,\n",
              "  1.1205434799194336,\n",
              "  1.058948040008545,\n",
              "  1.0652258396148682,\n",
              "  1.1231030225753784,\n",
              "  1.0681521892547607,\n",
              "  1.0929499864578247,\n",
              "  1.0756961107254028,\n",
              "  1.0664385557174683,\n",
              "  1.061303973197937,\n",
              "  1.0461056232452393,\n",
              "  1.0458370447158813,\n",
              "  1.1036102771759033,\n",
              "  1.1351468563079834,\n",
              "  0.9811999201774597,\n",
              "  1.026098608970642,\n",
              "  1.0727280378341675,\n",
              "  1.0101032257080078,\n",
              "  0.9883517026901245,\n",
              "  1.0279065370559692,\n",
              "  1.064376950263977,\n",
              "  1.0285804271697998,\n",
              "  1.0963445901870728,\n",
              "  0.9945490956306458,\n",
              "  0.9980013370513916,\n",
              "  1.0465030670166016,\n",
              "  1.0526392459869385,\n",
              "  0.9413725733757019,\n",
              "  1.0457412004470825,\n",
              "  1.0112780332565308,\n",
              "  1.0189259052276611,\n",
              "  1.0392136573791504,\n",
              "  1.0522419214248657,\n",
              "  0.9742031097412109,\n",
              "  0.9998390078544617,\n",
              "  1.0127925872802734,\n",
              "  1.0170131921768188,\n",
              "  0.9570894241333008,\n",
              "  0.9986631870269775,\n",
              "  0.8893280029296875,\n",
              "  0.9449116587638855,\n",
              "  0.9302890300750732,\n",
              "  0.9840246438980103,\n",
              "  0.9660699963569641,\n",
              "  0.959018886089325,\n",
              "  0.9638208150863647,\n",
              "  0.9189679622650146,\n",
              "  0.9211859703063965,\n",
              "  0.897983729839325,\n",
              "  0.9291338920593262,\n",
              "  0.8676605820655823,\n",
              "  0.9037308692932129,\n",
              "  0.9560405015945435,\n",
              "  1.0396846532821655,\n",
              "  1.0417603254318237,\n",
              "  0.9357007145881653,\n",
              "  0.8621138334274292,\n",
              "  1.0502535104751587,\n",
              "  0.9416854977607727,\n",
              "  0.9728135466575623,\n",
              "  0.9455987811088562,\n",
              "  0.8909173607826233,\n",
              "  0.9215894341468811,\n",
              "  0.8848759531974792,\n",
              "  0.883571445941925,\n",
              "  0.9714261889457703,\n",
              "  0.8937318921089172,\n",
              "  0.946855366230011,\n",
              "  0.9098830819129944,\n",
              "  0.9522233009338379,\n",
              "  0.9437660574913025,\n",
              "  0.9139788150787354,\n",
              "  0.9119624495506287,\n",
              "  0.9195728898048401,\n",
              "  0.8722603917121887,\n",
              "  0.9153243899345398,\n",
              "  0.897881805896759,\n",
              "  0.8947380781173706,\n",
              "  0.9115864038467407,\n",
              "  0.8505288362503052,\n",
              "  0.877430260181427,\n",
              "  0.942905068397522,\n",
              "  0.9110177755355835,\n",
              "  0.9108788967132568,\n",
              "  0.8937255144119263,\n",
              "  0.9041280150413513,\n",
              "  0.866998016834259,\n",
              "  0.8735769987106323,\n",
              "  0.9042644500732422,\n",
              "  0.883362889289856,\n",
              "  0.8571007251739502,\n",
              "  0.809998631477356,\n",
              "  0.8921529650688171,\n",
              "  0.869171142578125,\n",
              "  0.8948278427124023,\n",
              "  0.8737986087799072,\n",
              "  0.8581722378730774,\n",
              "  0.9639119505882263,\n",
              "  0.9047555923461914,\n",
              "  0.8452231884002686,\n",
              "  0.868534505367279,\n",
              "  0.8874545097351074,\n",
              "  0.8668307662010193,\n",
              "  0.8236753344535828,\n",
              "  0.9570384621620178,\n",
              "  0.8381105661392212,\n",
              "  0.8493133187294006,\n",
              "  0.8102632761001587,\n",
              "  0.8450787663459778,\n",
              "  0.9104876518249512,\n",
              "  0.8962939381599426,\n",
              "  0.8747872114181519,\n",
              "  0.804973304271698,\n",
              "  0.8356207609176636,\n",
              "  0.8471468091011047,\n",
              "  0.8436124324798584,\n",
              "  0.8478036522865295,\n",
              "  0.8738024830818176,\n",
              "  0.8750345706939697,\n",
              "  0.8062824606895447,\n",
              "  0.8626645803451538,\n",
              "  0.8387064337730408,\n",
              "  0.803638219833374,\n",
              "  0.8233814835548401,\n",
              "  0.8465936183929443,\n",
              "  0.8257355690002441,\n",
              "  0.8182017803192139,\n",
              "  0.905676543712616,\n",
              "  0.8458050489425659,\n",
              "  0.8966211080551147,\n",
              "  0.8384284377098083,\n",
              "  0.8765657544136047,\n",
              "  0.8560517430305481,\n",
              "  0.8599586486816406,\n",
              "  0.8309622406959534,\n",
              "  0.7819542288780212,\n",
              "  0.823351263999939,\n",
              "  0.8301102519035339,\n",
              "  0.8391831517219543,\n",
              "  0.7638645172119141,\n",
              "  0.75113844871521,\n",
              "  0.7947530746459961,\n",
              "  0.8308990001678467,\n",
              "  0.7913993000984192,\n",
              "  0.7633530497550964,\n",
              "  0.7660568356513977,\n",
              "  0.7897772192955017,\n",
              "  0.8418912887573242,\n",
              "  0.858182430267334,\n",
              "  0.7576754689216614,\n",
              "  0.8014786243438721,\n",
              "  0.842197835445404,\n",
              "  0.7644610404968262,\n",
              "  0.7830727100372314,\n",
              "  0.8197919726371765,\n",
              "  0.8435112237930298,\n",
              "  0.8343234658241272,\n",
              "  0.7909783124923706,\n",
              "  0.8518425822257996,\n",
              "  0.7952952980995178,\n",
              "  0.7950237393379211,\n",
              "  0.8381197452545166,\n",
              "  0.7929702401161194,\n",
              "  0.832694411277771,\n",
              "  0.7672864198684692,\n",
              "  0.7703511714935303,\n",
              "  0.7805864810943604,\n",
              "  0.7303462028503418,\n",
              "  0.8366407155990601,\n",
              "  0.8038077354431152,\n",
              "  0.8171761631965637,\n",
              "  0.8097466826438904,\n",
              "  0.808181643486023,\n",
              "  0.8707640171051025,\n",
              "  0.8075501918792725,\n",
              "  0.7503372430801392,\n",
              "  0.7898728251457214,\n",
              "  0.8216397762298584,\n",
              "  0.8002404570579529,\n",
              "  0.7325674295425415,\n",
              "  0.7909049987792969,\n",
              "  0.798007607460022,\n",
              "  0.732022225856781,\n",
              "  0.7829351425170898,\n",
              "  0.8026798963546753,\n",
              "  0.7890661358833313,\n",
              "  0.82680344581604,\n",
              "  0.8001028895378113,\n",
              "  0.8408636450767517,\n",
              "  0.766867995262146,\n",
              "  0.8152695894241333,\n",
              "  0.745269775390625,\n",
              "  0.7630071043968201,\n",
              "  0.7867477536201477,\n",
              "  0.8016387820243835,\n",
              "  0.801726222038269,\n",
              "  0.7259436249732971,\n",
              "  0.767911434173584,\n",
              "  0.8115130662918091,\n",
              "  0.8166627287864685,\n",
              "  0.7469053864479065,\n",
              "  0.7736976742744446,\n",
              "  0.7409777045249939,\n",
              "  0.7792734503746033,\n",
              "  0.7259402871131897,\n",
              "  0.7678702473640442,\n",
              "  0.6986145973205566,\n",
              "  0.7808108925819397,\n",
              "  0.7169749736785889,\n",
              "  0.83671635389328,\n",
              "  0.7756620049476624,\n",
              "  0.7411701679229736,\n",
              "  0.7910559773445129,\n",
              "  0.7975286245346069,\n",
              "  0.6944268345832825,\n",
              "  0.7590753436088562,\n",
              "  0.7874864935874939,\n",
              "  0.7320868968963623,\n",
              "  0.6891669034957886,\n",
              "  0.8167047500610352,\n",
              "  0.7474280595779419,\n",
              "  0.7526527643203735,\n",
              "  0.762005090713501,\n",
              "  0.7938244938850403,\n",
              "  0.7894547581672668,\n",
              "  0.7356550097465515,\n",
              "  0.8101096153259277,\n",
              "  0.8012414574623108,\n",
              "  0.7219560146331787,\n",
              "  0.7841016054153442,\n",
              "  0.764159083366394,\n",
              "  0.7139275670051575,\n",
              "  0.719084620475769,\n",
              "  0.7553811073303223,\n",
              "  0.7672764658927917,\n",
              "  0.71992427110672,\n",
              "  0.7494086623191833,\n",
              "  0.7456665635108948,\n",
              "  0.6741169691085815,\n",
              "  0.7001785039901733,\n",
              "  0.7088314294815063,\n",
              "  0.7487245798110962,\n",
              "  0.6836100220680237,\n",
              "  0.7537912130355835,\n",
              "  0.7254741191864014,\n",
              "  0.7154580950737,\n",
              "  0.7244067788124084,\n",
              "  0.7811504006385803,\n",
              "  0.6842123866081238,\n",
              "  0.7090517282485962,\n",
              "  0.7889979481697083,\n",
              "  0.7329649925231934,\n",
              "  0.7409791350364685,\n",
              "  0.7270864844322205,\n",
              "  0.7157068252563477,\n",
              "  0.6797586679458618,\n",
              "  0.7372512221336365,\n",
              "  0.731584906578064,\n",
              "  0.6860496997833252,\n",
              "  0.7397968769073486,\n",
              "  0.7372227311134338,\n",
              "  0.6672061085700989,\n",
              "  0.7183892726898193,\n",
              "  0.7259464263916016,\n",
              "  0.7463843822479248,\n",
              "  0.7127066850662231,\n",
              "  0.696254312992096,\n",
              "  0.7476351261138916,\n",
              "  0.7516766786575317,\n",
              "  0.7212358713150024,\n",
              "  0.8106677532196045,\n",
              "  0.693450391292572,\n",
              "  0.7249506711959839,\n",
              "  0.719607412815094,\n",
              "  0.6889297366142273,\n",
              "  0.7526935935020447,\n",
              "  0.708247184753418,\n",
              "  0.6560792922973633,\n",
              "  0.7482060790061951,\n",
              "  0.700207531452179,\n",
              "  0.6982942223548889,\n",
              "  0.6997157335281372,\n",
              "  0.677088737487793,\n",
              "  0.6931241154670715,\n",
              "  0.7119855284690857,\n",
              "  0.7470492720603943,\n",
              "  0.7189748287200928,\n",
              "  0.68183833360672,\n",
              "  0.7235374450683594,\n",
              "  0.6657322645187378,\n",
              "  0.7234755754470825,\n",
              "  0.7122522592544556,\n",
              "  0.6934285163879395,\n",
              "  0.7358071208000183,\n",
              "  0.6642571091651917,\n",
              "  0.6817160844802856,\n",
              "  0.6864778995513916,\n",
              "  0.6628386378288269,\n",
              "  0.6296772360801697,\n",
              "  0.6812430024147034,\n",
              "  0.7148540616035461,\n",
              "  0.7039010524749756,\n",
              "  0.7250202298164368,\n",
              "  0.6678381562232971,\n",
              "  0.7132426500320435,\n",
              "  0.7200342416763306,\n",
              "  0.6999277472496033,\n",
              "  0.681459903717041,\n",
              "  0.7469893097877502,\n",
              "  0.7120038866996765,\n",
              "  0.6616815328598022,\n",
              "  0.6635404229164124,\n",
              "  0.6649656891822815,\n",
              "  0.6580801010131836,\n",
              "  0.6479232311248779,\n",
              "  0.6696117520332336,\n",
              "  0.6948162913322449,\n",
              "  0.6916683316230774,\n",
              "  0.6729729175567627,\n",
              "  0.6620399355888367,\n",
              "  0.6751349568367004,\n",
              "  0.7171380519866943,\n",
              "  0.6531485319137573,\n",
              "  0.6661463975906372,\n",
              "  0.6296178102493286,\n",
              "  0.6775100827217102,\n",
              "  0.6963953375816345,\n",
              "  0.6864325404167175,\n",
              "  0.6890583038330078,\n",
              "  0.670716404914856,\n",
              "  0.6833471655845642,\n",
              "  0.6329955458641052,\n",
              "  0.6376764178276062,\n",
              "  0.692166268825531,\n",
              "  0.7155278325080872,\n",
              "  0.6686935424804688,\n",
              "  0.6956840753555298,\n",
              "  0.6791765093803406,\n",
              "  0.684032142162323,\n",
              "  0.6497646570205688,\n",
              "  0.7197036743164062,\n",
              "  0.7063201069831848,\n",
              "  0.7022948265075684,\n",
              "  0.7143480181694031,\n",
              "  0.6739150881767273,\n",
              "  0.622042179107666,\n",
              "  0.6542661190032959,\n",
              "  0.6945692896842957,\n",
              "  0.6695763468742371,\n",
              "  0.6222161054611206,\n",
              "  0.6574460864067078,\n",
              "  0.6702026724815369,\n",
              "  0.684326708316803,\n",
              "  0.6259142756462097,\n",
              "  0.6967911124229431,\n",
              "  0.6752935647964478,\n",
              "  0.670627236366272,\n",
              "  0.6581634879112244,\n",
              "  0.6959497332572937,\n",
              "  0.7023390531539917,\n",
              "  0.692622721195221,\n",
              "  0.6768573522567749,\n",
              "  0.6767467856407166,\n",
              "  0.7031823396682739,\n",
              "  0.6625000238418579,\n",
              "  0.686459481716156,\n",
              "  0.6197934746742249,\n",
              "  0.6667917966842651,\n",
              "  0.6819841861724854,\n",
              "  0.6934968829154968,\n",
              "  0.6692516803741455,\n",
              "  0.6490690112113953,\n",
              "  0.6577394008636475,\n",
              "  0.6631567478179932,\n",
              "  0.6217510104179382,\n",
              "  0.6583258509635925,\n",
              "  0.6449153423309326,\n",
              "  0.6956671476364136,\n",
              "  0.6373961567878723,\n",
              "  0.6662150621414185,\n",
              "  0.6337354779243469,\n",
              "  0.6442251801490784,\n",
              "  0.6944901347160339,\n",
              "  0.6544546484947205,\n",
              "  0.605880081653595,\n",
              "  0.6993957161903381,\n",
              "  0.6927046179771423,\n",
              "  0.6555234789848328,\n",
              "  0.6403582692146301,\n",
              "  0.7009215354919434,\n",
              "  0.642667293548584,\n",
              "  0.6247839331626892,\n",
              "  0.6989089250564575,\n",
              "  0.6233809590339661,\n",
              "  0.6499145030975342,\n",
              "  0.6473888754844666,\n",
              "  0.6398381590843201,\n",
              "  0.6612372994422913,\n",
              "  0.6632786989212036,\n",
              "  0.6332201361656189,\n",
              "  0.6096869707107544,\n",
              "  0.6318654417991638,\n",
              "  0.6304903030395508,\n",
              "  0.6180342435836792,\n",
              "  0.6409726142883301,\n",
              "  0.6314697265625,\n",
              "  0.6742969155311584,\n",
              "  0.6480863094329834,\n",
              "  0.6242079138755798,\n",
              "  0.6192206144332886,\n",
              "  0.6381964087486267,\n",
              "  0.682081937789917,\n",
              "  0.6165274381637573,\n",
              "  0.682655930519104,\n",
              "  0.665084183216095,\n",
              "  0.6451581716537476,\n",
              "  0.658065676689148,\n",
              "  0.6443352103233337,\n",
              "  0.6432228684425354,\n",
              "  0.6475139260292053,\n",
              "  0.6492909789085388,\n",
              "  0.6891593337059021,\n",
              "  0.6252918243408203,\n",
              "  0.5778551697731018,\n",
              "  0.6113510131835938,\n",
              "  0.6497204303741455,\n",
              "  0.6449927091598511,\n",
              "  0.6207277178764343,\n",
              "  0.6576700210571289,\n",
              "  0.644088089466095,\n",
              "  0.6370046138763428,\n",
              "  0.6329569220542908,\n",
              "  0.6272940635681152,\n",
              "  0.6248971223831177,\n",
              "  0.6404426693916321,\n",
              "  0.6493209600448608,\n",
              "  0.6367399096488953,\n",
              "  0.6248278617858887,\n",
              "  0.6453158855438232,\n",
              "  0.6458256244659424,\n",
              "  0.6080341935157776,\n",
              "  0.6276330351829529,\n",
              "  0.6518504023551941,\n",
              "  0.6880887746810913,\n",
              "  0.6349683403968811,\n",
              "  0.6072137951850891,\n",
              "  0.6498584747314453,\n",
              "  0.6183705925941467,\n",
              "  0.6202449202537537,\n",
              "  0.625704288482666,\n",
              "  0.61942458152771,\n",
              "  0.6028602719306946,\n",
              "  0.6598616242408752,\n",
              "  0.6398907899856567,\n",
              "  0.6072202920913696,\n",
              "  0.6482928991317749,\n",
              "  0.618455171585083,\n",
              "  0.6068580746650696,\n",
              "  0.6261119246482849,\n",
              "  0.6191473007202148,\n",
              "  0.602597713470459,\n",
              "  0.6704944968223572,\n",
              "  0.6341624855995178,\n",
              "  0.6451705694198608,\n",
              "  0.6092581748962402,\n",
              "  0.6293143630027771,\n",
              "  0.669397234916687,\n",
              "  0.6089763641357422,\n",
              "  0.6504167914390564,\n",
              "  0.640116810798645,\n",
              "  0.6346778869628906,\n",
              "  0.6436904668807983,\n",
              "  0.6187108159065247,\n",
              "  0.6254923939704895,\n",
              "  0.672110378742218,\n",
              "  0.6316264867782593,\n",
              "  0.6231589317321777,\n",
              "  0.624981701374054,\n",
              "  0.6167254447937012,\n",
              "  0.6052860021591187,\n",
              "  0.5968502759933472,\n",
              "  0.6193092465400696,\n",
              "  0.6333873867988586,\n",
              "  0.6083192229270935,\n",
              "  0.5911867022514343,\n",
              "  0.6595175266265869,\n",
              "  0.5637832283973694,\n",
              "  0.6295990347862244,\n",
              "  0.6579281091690063,\n",
              "  0.5832952857017517,\n",
              "  0.6422478556632996,\n",
              "  0.6276055574417114,\n",
              "  0.6417814493179321,\n",
              "  0.6228089332580566,\n",
              "  0.6055368185043335,\n",
              "  0.6158929467201233,\n",
              "  0.6088877320289612,\n",
              "  0.5749450325965881,\n",
              "  0.6564337611198425,\n",
              "  0.5918917059898376,\n",
              "  0.6339199542999268,\n",
              "  0.5887607336044312,\n",
              "  0.6226854920387268,\n",
              "  0.6175832152366638,\n",
              "  0.6468531489372253,\n",
              "  0.6204328536987305,\n",
              "  0.5935856103897095,\n",
              "  0.6144667863845825,\n",
              "  0.6057394742965698,\n",
              "  0.6003715991973877,\n",
              "  0.5816475749015808,\n",
              "  0.6482362747192383,\n",
              "  0.5829693078994751,\n",
              "  0.5995944738388062,\n",
              "  0.5874951481819153,\n",
              "  0.6561235785484314,\n",
              "  0.6152974367141724,\n",
              "  0.6004986763000488,\n",
              "  0.6681020855903625,\n",
              "  0.6668069958686829,\n",
              "  0.6010725498199463,\n",
              "  0.604461669921875,\n",
              "  0.606478750705719,\n",
              "  0.600736141204834,\n",
              "  0.5631402134895325,\n",
              "  0.6060736179351807,\n",
              "  0.5906844139099121,\n",
              "  0.583977997303009,\n",
              "  0.6396695971488953,\n",
              "  0.6015809774398804,\n",
              "  0.5928860306739807,\n",
              "  0.6348289847373962,\n",
              "  0.5970789194107056,\n",
              "  0.6286998987197876,\n",
              "  0.5939785242080688,\n",
              "  0.6001310348510742,\n",
              "  0.6370280385017395,\n",
              "  0.6175318956375122,\n",
              "  0.6053773164749146,\n",
              "  0.6217119693756104,\n",
              "  0.6049949526786804,\n",
              "  0.6222371459007263,\n",
              "  0.5768056511878967,\n",
              "  0.5547553896903992,\n",
              "  0.6156593561172485,\n",
              "  0.5996365547180176,\n",
              "  0.6268342733383179,\n",
              "  0.552527666091919,\n",
              "  0.6588820815086365,\n",
              "  0.6204120516777039,\n",
              "  0.6060982346534729,\n",
              "  0.5393617749214172,\n",
              "  0.5818853974342346,\n",
              "  0.6443485021591187,\n",
              "  0.6042625904083252,\n",
              "  0.5661547780036926,\n",
              "  0.5965578556060791,\n",
              "  0.6041673421859741,\n",
              "  0.6110857129096985,\n",
              "  0.6055291295051575,\n",
              "  0.6118559241294861,\n",
              "  0.5876520872116089,\n",
              "  0.5679989457130432,\n",
              "  0.587470293045044,\n",
              "  0.6031228303909302,\n",
              "  0.6036007404327393,\n",
              "  0.5786239504814148,\n",
              "  0.6112637519836426,\n",
              "  0.5722714066505432,\n",
              "  0.5683433413505554,\n",
              "  0.5649829506874084,\n",
              "  0.5343795418739319,\n",
              "  0.5790113806724548,\n",
              "  0.5919833183288574,\n",
              "  0.5909266471862793,\n",
              "  0.562847912311554,\n",
              "  0.5917347073554993,\n",
              "  0.5703539252281189,\n",
              "  0.5657058954238892,\n",
              "  0.6217078566551208,\n",
              "  0.6126026511192322,\n",
              "  0.5919614434242249,\n",
              "  0.6189616322517395,\n",
              "  0.5861698389053345,\n",
              "  0.5510854721069336,\n",
              "  0.5786791443824768,\n",
              "  0.5418685078620911,\n",
              "  0.5793045163154602,\n",
              "  0.5895931124687195,\n",
              "  0.5906127691268921,\n",
              "  0.5900773406028748,\n",
              "  0.6212509274482727,\n",
              "  0.5950822830200195,\n",
              "  0.5765252113342285,\n",
              "  0.5943779349327087,\n",
              "  0.60166996717453,\n",
              "  0.6132277250289917,\n",
              "  0.5866410732269287,\n",
              "  0.5921735167503357,\n",
              "  0.5893576741218567,\n",
              "  0.5848518013954163,\n",
              "  0.5991082191467285,\n",
              "  0.5831640958786011,\n",
              "  0.59201979637146,\n",
              "  0.5840926766395569,\n",
              "  0.605070948600769,\n",
              "  0.559554398059845,\n",
              "  0.5794313549995422,\n",
              "  0.5894489884376526,\n",
              "  0.598055362701416,\n",
              "  0.5736224055290222,\n",
              "  0.5666159391403198,\n",
              "  0.5978695750236511,\n",
              "  0.5795567035675049,\n",
              "  0.5934242010116577,\n",
              "  0.5967037081718445,\n",
              "  0.5732231140136719,\n",
              "  0.5661135911941528,\n",
              "  0.5860201716423035,\n",
              "  0.5840404629707336,\n",
              "  0.5734878778457642,\n",
              "  0.585117757320404,\n",
              "  0.6112315654754639,\n",
              "  0.5454369783401489,\n",
              "  0.5589377284049988,\n",
              "  0.5823561549186707,\n",
              "  0.6238933801651001,\n",
              "  0.546984076499939,\n",
              "  0.55311518907547,\n",
              "  0.5982018709182739,\n",
              "  0.5611976981163025,\n",
              "  0.5719952583312988,\n",
              "  0.5895979404449463,\n",
              "  0.580989420413971,\n",
              "  0.5679221749305725,\n",
              "  0.5963420867919922,\n",
              "  0.573863685131073,\n",
              "  0.5574942231178284,\n",
              "  0.5804945826530457,\n",
              "  0.5994463562965393,\n",
              "  0.568915843963623,\n",
              "  0.6073935627937317,\n",
              "  0.5917877554893494,\n",
              "  0.6183988451957703,\n",
              "  0.5604677200317383,\n",
              "  0.5992147326469421,\n",
              "  0.570378303527832,\n",
              "  0.5223202705383301,\n",
              "  0.5778253078460693,\n",
              "  0.6074910163879395,\n",
              "  0.5994482636451721,\n",
              "  0.5835350751876831,\n",
              "  0.5755127668380737,\n",
              "  0.5756934881210327,\n",
              "  0.55738765001297,\n",
              "  0.5736675262451172,\n",
              "  0.5377867221832275,\n",
              "  0.572862982749939,\n",
              "  0.5911093354225159,\n",
              "  0.6037049293518066,\n",
              "  0.561577320098877,\n",
              "  0.5954800844192505,\n",
              "  0.632555365562439,\n",
              "  0.5777085423469543,\n",
              "  0.5635517835617065,\n",
              "  0.6046962738037109,\n",
              "  0.5614801049232483,\n",
              "  0.5870031714439392,\n",
              "  0.5440366268157959,\n",
              "  0.5920583009719849,\n",
              "  0.5747394561767578,\n",
              "  0.5778246521949768,\n",
              "  0.5628500580787659,\n",
              "  0.5614765882492065,\n",
              "  0.5891146063804626,\n",
              "  0.5764235854148865,\n",
              "  0.5627606511116028,\n",
              "  0.5758360624313354,\n",
              "  0.5759877562522888,\n",
              "  0.5977782011032104,\n",
              "  0.5660073161125183,\n",
              "  0.5687794089317322,\n",
              "  0.5715376138687134,\n",
              "  0.6016755700111389,\n",
              "  0.5998100638389587,\n",
              "  0.5786027908325195,\n",
              "  0.5706556439399719,\n",
              "  0.5884340405464172,\n",
              "  0.5954751372337341,\n",
              "  0.556636393070221,\n",
              "  0.5614005923271179,\n",
              "  0.5821794867515564,\n",
              "  0.5732508897781372,\n",
              "  0.5523337125778198,\n",
              "  0.5907285213470459,\n",
              "  0.5558892488479614,\n",
              "  0.5403007864952087,\n",
              "  0.560951828956604,\n",
              "  0.5747056007385254,\n",
              "  0.5980373620986938,\n",
              "  0.5659916400909424,\n",
              "  0.6071307063102722,\n",
              "  0.5308211445808411,\n",
              "  0.5372646450996399,\n",
              "  0.5486149787902832,\n",
              "  0.5757927298545837,\n",
              "  0.5567033290863037,\n",
              "  0.5957464575767517,\n",
              "  0.600951611995697,\n",
              "  0.5323379039764404,\n",
              "  0.5753986835479736,\n",
              "  0.5645866394042969,\n",
              "  0.583250105381012,\n",
              "  0.5541979670524597,\n",
              "  0.5498284101486206,\n",
              "  0.5874570608139038,\n",
              "  0.5535027980804443,\n",
              "  0.5519936680793762,\n",
              "  0.5681175589561462,\n",
              "  0.5555593371391296,\n",
              "  0.5612753033638,\n",
              "  0.5617517828941345,\n",
              "  0.5849773287773132,\n",
              "  0.5489481687545776,\n",
              "  0.5545355081558228,\n",
              "  0.5472500920295715,\n",
              "  0.5813417434692383,\n",
              "  0.5633949637413025,\n",
              "  0.5621700286865234,\n",
              "  0.5315583348274231,\n",
              "  0.5378767848014832,\n",
              "  0.5745260715484619,\n",
              "  0.5488510131835938,\n",
              "  0.5368945598602295,\n",
              "  0.5804799199104309,\n",
              "  0.5391107201576233,\n",
              "  0.5439817309379578,\n",
              "  0.562510073184967,\n",
              "  0.5481966733932495,\n",
              "  0.5742311477661133,\n",
              "  0.5484154224395752,\n",
              "  0.5811373591423035,\n",
              "  0.5599185824394226,\n",
              "  0.5634928345680237,\n",
              "  0.5850630402565002,\n",
              "  0.5499523282051086,\n",
              "  0.5754071474075317,\n",
              "  0.5543590784072876,\n",
              "  0.5377722978591919,\n",
              "  0.5602979063987732,\n",
              "  0.5747568607330322,\n",
              "  0.5638790726661682,\n",
              "  0.5728164911270142,\n",
              "  0.534456729888916,\n",
              "  0.5725136399269104,\n",
              "  0.568181037902832,\n",
              "  0.4934275448322296,\n",
              "  0.5600180625915527,\n",
              "  0.5350713729858398,\n",
              "  0.5461502075195312,\n",
              "  0.5702518224716187,\n",
              "  0.5717989206314087,\n",
              "  0.5474628806114197,\n",
              "  0.54902583360672,\n",
              "  0.5999403595924377,\n",
              "  0.5189293026924133,\n",
              "  0.517297625541687,\n",
              "  0.5605865716934204,\n",
              "  0.5324418544769287,\n",
              "  0.5454259514808655,\n",
              "  0.5462580323219299,\n",
              "  0.5754008889198303,\n",
              "  0.5651096105575562,\n",
              "  0.5595006942749023,\n",
              "  0.5581367015838623,\n",
              "  0.5639532804489136,\n",
              "  0.5383955836296082,\n",
              "  0.5659220814704895,\n",
              "  0.5403457880020142,\n",
              "  0.5769899487495422,\n",
              "  0.5417743921279907,\n",
              "  0.5426008701324463,\n",
              "  0.5635587573051453,\n",
              "  0.5016852021217346,\n",
              "  0.5338156223297119,\n",
              "  0.5242348909378052,\n",
              "  0.529830813407898,\n",
              "  0.570289134979248,\n",
              "  0.5628980994224548,\n",
              "  0.5702002644538879,\n",
              "  0.5720410943031311,\n",
              "  0.5438891649246216,\n",
              "  0.5646935105323792,\n",
              "  0.5531286597251892,\n",
              "  0.5179685354232788,\n",
              "  0.5484045743942261,\n",
              "  0.5573544502258301,\n",
              "  0.5650882720947266,\n",
              "  0.5519038438796997,\n",
              "  0.5603182315826416,\n",
              "  0.5636729598045349,\n",
              "  0.5313185453414917,\n",
              "  0.5597440004348755,\n",
              "  0.5280343294143677,\n",
              "  0.5432812571525574,\n",
              "  0.5471746325492859,\n",
              "  0.5427566170692444,\n",
              "  0.5740969181060791,\n",
              "  0.5359220504760742,\n",
              "  0.5349436402320862,\n",
              "  0.5719923973083496,\n",
              "  0.5451536178588867,\n",
              "  0.5151285529136658,\n",
              "  0.579526424407959,\n",
              "  0.5439012050628662,\n",
              "  0.5635628700256348,\n",
              "  0.5152357220649719,\n",
              "  0.577854335308075,\n",
              "  0.5265317559242249,\n",
              "  0.585445761680603,\n",
              "  0.5159314870834351,\n",
              "  0.5549126863479614,\n",
              "  0.5486682057380676,\n",
              "  0.5246355533599854,\n",
              "  0.5488290190696716,\n",
              "  0.5789949297904968,\n",
              "  0.5743351578712463,\n",
              "  0.5485069751739502,\n",
              "  0.5703903436660767,\n",
              "  0.595936119556427,\n",
              "  0.5340917706489563,\n",
              "  0.5348560810089111,\n",
              "  0.5680183172225952,\n",
              "  0.5594572424888611,\n",
              "  0.5338347554206848,\n",
              "  0.5334727168083191,\n",
              "  0.5544867515563965,\n",
              "  0.5345138907432556,\n",
              "  0.5393463969230652,\n",
              "  0.5443912148475647,\n",
              "  0.5460715889930725,\n",
              "  0.5339208841323853,\n",
              "  0.5790594220161438,\n",
              "  0.521240234375,\n",
              "  0.5468912720680237,\n",
              "  0.5193845629692078,\n",
              "  0.5410904884338379,\n",
              "  0.5683853626251221,\n",
              "  0.5460102558135986,\n",
              "  0.4944450557231903,\n",
              "  0.5703679323196411,\n",
              "  0.5132418274879456,\n",
              "  0.5461451411247253,\n",
              "  0.5760377049446106,\n",
              "  0.5424512028694153,\n",
              "  0.5674418210983276,\n",
              "  0.5680325031280518,\n",
              "  0.5562818050384521,\n",
              "  0.5439585447311401,\n",
              "  0.5054396986961365,\n",
              "  0.5788682103157043,\n",
              "  0.5647512674331665,\n",
              "  0.5618155002593994,\n",
              "  0.5433801412582397,\n",
              "  0.5749170184135437,\n",
              "  0.570143461227417,\n",
              "  0.5112349987030029,\n",
              "  0.5757219195365906,\n",
              "  0.5434340238571167,\n",
              "  0.554570734500885,\n",
              "  0.5747963190078735,\n",
              "  0.547552764415741,\n",
              "  0.5352662801742554,\n",
              "  0.5472472310066223,\n",
              "  0.5972417593002319,\n",
              "  0.5271634459495544,\n",
              "  0.5622981786727905,\n",
              "  0.5581515431404114,\n",
              "  0.527172863483429,\n",
              "  0.5461163520812988,\n",
              "  0.5343130826950073,\n",
              "  0.512600302696228,\n",
              "  0.5764148831367493,\n",
              "  0.5044947862625122,\n",
              "  0.5180090665817261,\n",
              "  0.562006950378418,\n",
              "  0.5419185757637024,\n",
              "  0.523410975933075,\n",
              "  0.5581409931182861,\n",
              "  0.5479123592376709,\n",
              "  0.5422697067260742,\n",
              "  0.5325610041618347,\n",
              "  0.5318737626075745,\n",
              "  0.5289453268051147,\n",
              "  0.5332807898521423,\n",
              "  0.5588338375091553,\n",
              "  0.5225266814231873,\n",
              "  0.5151491165161133,\n",
              "  0.5291878581047058,\n",
              "  0.567562460899353,\n",
              "  0.5272856950759888,\n",
              "  0.5298386216163635,\n",
              "  0.5357925891876221,\n",
              "  0.5483089089393616,\n",
              "  0.5250203013420105,\n",
              "  0.5415990948677063,\n",
              "  0.5327870845794678,\n",
              "  0.553886890411377,\n",
              "  0.5472714900970459,\n",
              "  0.5619746446609497,\n",
              "  0.5809884667396545,\n",
              "  0.5503937005996704,\n",
              "  0.5416016578674316,\n",
              "  0.5349505543708801,\n",
              "  0.5472798943519592,\n",
              "  0.5813955068588257,\n",
              "  0.5250458121299744,\n",
              "  0.5794416069984436,\n",
              "  0.5450986623764038,\n",
              "  0.523750901222229,\n",
              "  0.5285114645957947,\n",
              "  0.537246823310852,\n",
              "  0.571850597858429,\n",
              "  0.570026695728302,\n",
              "  0.5406448245048523,\n",
              "  0.5165391564369202,\n",
              "  0.5561710000038147,\n",
              "  0.5446640253067017,\n",
              "  0.567406952381134,\n",
              "  0.5121394991874695,\n",
              "  0.5296445488929749,\n",
              "  0.5069945454597473,\n",
              "  0.5418114066123962,\n",
              "  0.5712249279022217,\n",
              "  0.5347620844841003,\n",
              "  0.5612075328826904,\n",
              "  0.5304946303367615,\n",
              "  0.5461176633834839,\n",
              "  0.5463628768920898,\n",
              "  0.5331737995147705,\n",
              "  0.5393978357315063,\n",
              "  0.5719372034072876,\n",
              "  0.5726832747459412,\n",
              "  0.5218836665153503,\n",
              "  0.5419842004776001,\n",
              "  0.5245558023452759,\n",
              "  0.523618221282959,\n",
              "  0.5983691811561584,\n",
              "  0.5601205825805664,\n",
              "  0.5226035714149475,\n",
              "  0.5148043036460876,\n",
              "  0.5301758646965027,\n",
              "  0.532565176486969,\n",
              "  0.5429177284240723,\n",
              "  0.5242403149604797,\n",
              "  0.5418386459350586,\n",
              "  0.5174305438995361,\n",
              "  0.5222803950309753,\n",
              "  0.5309740900993347,\n",
              "  0.5359601974487305,\n",
              "  0.5046641826629639,\n",
              "  0.5400487184524536,\n",
              "  0.5331531763076782,\n",
              "  0.5186787247657776,\n",
              "  0.5321627259254456,\n",
              "  0.5313486456871033,\n",
              "  0.5464285016059875,\n",
              "  0.554645299911499,\n",
              "  0.5770197510719299,\n",
              "  0.5559802055358887,\n",
              "  0.5575535297393799,\n",
              "  0.522850751876831,\n",
              "  0.5270516276359558,\n",
              "  0.5535560250282288,\n",
              "  0.5362467169761658,\n",
              "  0.5070454478263855,\n",
              "  0.5411044955253601],\n",
              " 'acc': [0.5120275020599365,\n",
              "  0.5223367810249329,\n",
              "  0.5189003348350525,\n",
              "  0.4982817769050598,\n",
              "  0.5103092789649963,\n",
              "  0.5103092789649963,\n",
              "  0.5412371158599854,\n",
              "  0.5395188927650452,\n",
              "  0.5120275020599365,\n",
              "  0.5171821117401123,\n",
              "  0.5343642830848694,\n",
              "  0.5017182230949402,\n",
              "  0.5326460599899292,\n",
              "  0.4982817769050598,\n",
              "  0.48969072103500366,\n",
              "  0.5051546096801758,\n",
              "  0.5601374506950378,\n",
              "  0.5463917255401611,\n",
              "  0.5154638886451721,\n",
              "  0.5223367810249329,\n",
              "  0.5257731676101685,\n",
              "  0.5274913907051086,\n",
              "  0.5103092789649963,\n",
              "  0.5412371158599854,\n",
              "  0.5429553389549255,\n",
              "  0.5584192276000977,\n",
              "  0.5395188927650452,\n",
              "  0.5103092789649963,\n",
              "  0.5429553389549255,\n",
              "  0.5292096138000488,\n",
              "  0.5429553389549255,\n",
              "  0.5257731676101685,\n",
              "  0.5206185579299927,\n",
              "  0.5824742317199707,\n",
              "  0.5206185579299927,\n",
              "  0.5206185579299927,\n",
              "  0.5171821117401123,\n",
              "  0.5652921199798584,\n",
              "  0.561855673789978,\n",
              "  0.5429553389549255,\n",
              "  0.524055004119873,\n",
              "  0.5326460599899292,\n",
              "  0.5343642830848694,\n",
              "  0.5120275020599365,\n",
              "  0.5326460599899292,\n",
              "  0.5584192276000977,\n",
              "  0.5481099486351013,\n",
              "  0.5343642830848694,\n",
              "  0.5412371158599854,\n",
              "  0.5171821117401123,\n",
              "  0.5738831758499146,\n",
              "  0.5412371158599854,\n",
              "  0.5481099486351013,\n",
              "  0.5584192276000977,\n",
              "  0.5412371158599854,\n",
              "  0.5360824465751648,\n",
              "  0.5429553389549255,\n",
              "  0.5498281717300415,\n",
              "  0.5601374506950378,\n",
              "  0.5320945978164673,\n",
              "  0.5652921199798584,\n",
              "  0.5584192276000977,\n",
              "  0.5895270109176636,\n",
              "  0.5463917255401611,\n",
              "  0.5738831758499146,\n",
              "  0.5154638886451721,\n",
              "  0.561855673789978,\n",
              "  0.5652921199798584,\n",
              "  0.568728506565094,\n",
              "  0.5790377855300903,\n",
              "  0.5670102834701538,\n",
              "  0.5652921199798584,\n",
              "  0.5773195624351501,\n",
              "  0.5893470644950867,\n",
              "  0.5721649527549744,\n",
              "  0.5790377855300903,\n",
              "  0.5429553389549255,\n",
              "  0.5446735620498657,\n",
              "  0.5498281717300415,\n",
              "  0.5962199568748474,\n",
              "  0.5515463948249817,\n",
              "  0.5824742317199707,\n",
              "  0.5429553389549255,\n",
              "  0.5652921199798584,\n",
              "  0.599656343460083,\n",
              "  0.5773195624351501,\n",
              "  0.5790377855300903,\n",
              "  0.5859106779098511,\n",
              "  0.5567010045051575,\n",
              "  0.5584192276000977,\n",
              "  0.5910652875900269,\n",
              "  0.5532646179199219,\n",
              "  0.5945017337799072,\n",
              "  0.561855673789978,\n",
              "  0.592783510684967,\n",
              "  0.5807560086250305,\n",
              "  0.5756013989448547,\n",
              "  0.6048110127449036,\n",
              "  0.5704467296600342,\n",
              "  0.6134020686149597,\n",
              "  0.5721649527549744,\n",
              "  0.5652921199798584,\n",
              "  0.5876288414001465,\n",
              "  0.6065292358398438,\n",
              "  0.5601374506950378,\n",
              "  0.5859106779098511,\n",
              "  0.5841924548149109,\n",
              "  0.5652921199798584,\n",
              "  0.5704467296600342,\n",
              "  0.592783510684967,\n",
              "  0.5738831758499146,\n",
              "  0.5670102834701538,\n",
              "  0.5841924548149109,\n",
              "  0.5945017337799072,\n",
              "  0.6323024034500122,\n",
              "  0.587837815284729,\n",
              "  0.5876288414001465,\n",
              "  0.5704467296600342,\n",
              "  0.6099656224250793,\n",
              "  0.6065292358398438,\n",
              "  0.5515463948249817,\n",
              "  0.5876288414001465,\n",
              "  0.5910652875900269,\n",
              "  0.592783510684967,\n",
              "  0.5704467296600342,\n",
              "  0.6013745665550232,\n",
              "  0.6048110127449036,\n",
              "  0.5532646179199219,\n",
              "  0.5979381203651428,\n",
              "  0.5893470644950867,\n",
              "  0.6048110127449036,\n",
              "  0.6099656224250793,\n",
              "  0.5515463948249817,\n",
              "  0.5807560086250305,\n",
              "  0.5790377855300903,\n",
              "  0.592783510684967,\n",
              "  0.6082473993301392,\n",
              "  0.5893470644950867,\n",
              "  0.5893470644950867,\n",
              "  0.5790377855300903,\n",
              "  0.6013745665550232,\n",
              "  0.6048110127449036,\n",
              "  0.6048110127449036,\n",
              "  0.6030927896499634,\n",
              "  0.6082473993301392,\n",
              "  0.6271477937698364,\n",
              "  0.6134020686149597,\n",
              "  0.5910652875900269,\n",
              "  0.6168385148048401,\n",
              "  0.599656343460083,\n",
              "  0.5945017337799072,\n",
              "  0.623711347579956,\n",
              "  0.5738831758499146,\n",
              "  0.6030927896499634,\n",
              "  0.6013745665550232,\n",
              "  0.5962199568748474,\n",
              "  0.5738831758499146,\n",
              "  0.5962199568748474,\n",
              "  0.6340206265449524,\n",
              "  0.5979381203651428,\n",
              "  0.6254295706748962,\n",
              "  0.6048110127449036,\n",
              "  0.6168385148048401,\n",
              "  0.6477663516998291,\n",
              "  0.6082473993301392,\n",
              "  0.6013745665550232,\n",
              "  0.6013745665550232,\n",
              "  0.6563574075698853,\n",
              "  0.6323024034500122,\n",
              "  0.5910652875900269,\n",
              "  0.5876288414001465,\n",
              "  0.5807560086250305,\n",
              "  0.6254295706748962,\n",
              "  0.6099656224250793,\n",
              "  0.5670102834701538,\n",
              "  0.6271477937698364,\n",
              "  0.6185566782951355,\n",
              "  0.6202749013900757,\n",
              "  0.5910652875900269,\n",
              "  0.6048110127449036,\n",
              "  0.6443299055099487,\n",
              "  0.5893470644950867,\n",
              "  0.6082473993301392,\n",
              "  0.5807560086250305,\n",
              "  0.5738831758499146,\n",
              "  0.6134020686149597,\n",
              "  0.6116838455200195,\n",
              "  0.6408934593200684,\n",
              "  0.623711347579956,\n",
              "  0.6340206265449524,\n",
              "  0.6323024034500122,\n",
              "  0.5704467296600342,\n",
              "  0.6202749013900757,\n",
              "  0.5979381203651428,\n",
              "  0.5876288414001465,\n",
              "  0.6116838455200195,\n",
              "  0.5841924548149109,\n",
              "  0.5910652875900269,\n",
              "  0.6116838455200195,\n",
              "  0.6254295706748962,\n",
              "  0.6185566782951355,\n",
              "  0.6082473993301392,\n",
              "  0.630584180355072,\n",
              "  0.5979381203651428,\n",
              "  0.6477663516998291,\n",
              "  0.6426116824150085,\n",
              "  0.6219931244850159,\n",
              "  0.5979381203651428,\n",
              "  0.6202749013900757,\n",
              "  0.6099656224250793,\n",
              "  0.5910652875900269,\n",
              "  0.6099656224250793,\n",
              "  0.6254295706748962,\n",
              "  0.5790377855300903,\n",
              "  0.6168385148048401,\n",
              "  0.6323024034500122,\n",
              "  0.6254295706748962,\n",
              "  0.6082473993301392,\n",
              "  0.5859106779098511,\n",
              "  0.6460481286048889,\n",
              "  0.6048110127449036,\n",
              "  0.592783510684967,\n",
              "  0.592783510684967,\n",
              "  0.6116838455200195,\n",
              "  0.6065292358398438,\n",
              "  0.6443299055099487,\n",
              "  0.6082473993301392,\n",
              "  0.6357388496398926,\n",
              "  0.6202749013900757,\n",
              "  0.6520270109176636,\n",
              "  0.6030927896499634,\n",
              "  0.6323024034500122,\n",
              "  0.6013745665550232,\n",
              "  0.6065292358398438,\n",
              "  0.6134020686149597,\n",
              "  0.6254295706748962,\n",
              "  0.6216216087341309,\n",
              "  0.6460481286048889,\n",
              "  0.6351351141929626,\n",
              "  0.6254295706748962,\n",
              "  0.6391752362251282,\n",
              "  0.6374570727348328,\n",
              "  0.599656343460083,\n",
              "  0.6340206265449524,\n",
              "  0.6254295706748962,\n",
              "  0.6374570727348328,\n",
              "  0.6099656224250793,\n",
              "  0.6134020686149597,\n",
              "  0.6546391844749451,\n",
              "  0.6134020686149597,\n",
              "  0.6116838455200195,\n",
              "  0.6254295706748962,\n",
              "  0.6165540814399719,\n",
              "  0.6168385148048401,\n",
              "  0.6512027382850647,\n",
              "  0.6580756306648254,\n",
              "  0.6202749013900757,\n",
              "  0.6202749013900757,\n",
              "  0.630584180355072,\n",
              "  0.6271477937698364,\n",
              "  0.6357388496398926,\n",
              "  0.6460481286048889,\n",
              "  0.6374570727348328,\n",
              "  0.6477663516998291,\n",
              "  0.5979381203651428,\n",
              "  0.6563574075698853,\n",
              "  0.6082473993301392,\n",
              "  0.6426116824150085,\n",
              "  0.6494845151901245,\n",
              "  0.6391752362251282,\n",
              "  0.5979381203651428,\n",
              "  0.6494845151901245,\n",
              "  0.6391752362251282,\n",
              "  0.6116838455200195,\n",
              "  0.6391752362251282,\n",
              "  0.6185566782951355,\n",
              "  0.6529209613800049,\n",
              "  0.6357388496398926,\n",
              "  0.6649484634399414,\n",
              "  0.6391752362251282,\n",
              "  0.6374570727348328,\n",
              "  0.6546391844749451,\n",
              "  0.6357388496398926,\n",
              "  0.6134020686149597,\n",
              "  0.6632302403450012,\n",
              "  0.6512027382850647,\n",
              "  0.6469594836235046,\n",
              "  0.6408934593200684,\n",
              "  0.6701030731201172,\n",
              "  0.6529209613800049,\n",
              "  0.6219931244850159,\n",
              "  0.6219931244850159,\n",
              "  0.6099656224250793,\n",
              "  0.6013745665550232,\n",
              "  0.6597937941551208,\n",
              "  0.6351351141929626,\n",
              "  0.6323024034500122,\n",
              "  0.6597937941551208,\n",
              "  0.6219931244850159,\n",
              "  0.6254295706748962,\n",
              "  0.6769759654998779,\n",
              "  0.6116838455200195,\n",
              "  0.6374570727348328,\n",
              "  0.6460481286048889,\n",
              "  0.6529209613800049,\n",
              "  0.6443299055099487,\n",
              "  0.6546391844749451,\n",
              "  0.6340206265449524,\n",
              "  0.6168385148048401,\n",
              "  0.6391752362251282,\n",
              "  0.6563574075698853,\n",
              "  0.623711347579956,\n",
              "  0.6632302403450012,\n",
              "  0.6391752362251282,\n",
              "  0.6202749013900757,\n",
              "  0.6494845151901245,\n",
              "  0.6099656224250793,\n",
              "  0.6701030731201172,\n",
              "  0.6580756306648254,\n",
              "  0.668384850025177,\n",
              "  0.661512017250061,\n",
              "  0.6872852444648743,\n",
              "  0.6374570727348328,\n",
              "  0.6334459185600281,\n",
              "  0.6529209613800049,\n",
              "  0.6666666865348816,\n",
              "  0.6718212962150574,\n",
              "  0.6546391844749451,\n",
              "  0.6288659572601318,\n",
              "  0.6391752362251282,\n",
              "  0.6374570727348328,\n",
              "  0.6460481286048889,\n",
              "  0.6426116824150085,\n",
              "  0.6821305751800537,\n",
              "  0.6426116824150085,\n",
              "  0.6494845151901245,\n",
              "  0.6443299055099487,\n",
              "  0.6718212962150574,\n",
              "  0.6597937941551208,\n",
              "  0.6477663516998291,\n",
              "  0.6477663516998291,\n",
              "  0.6546391844749451,\n",
              "  0.668384850025177,\n",
              "  0.6580756306648254,\n",
              "  0.668384850025177,\n",
              "  0.6786941289901733,\n",
              "  0.6666666865348816,\n",
              "  0.6718212962150574,\n",
              "  0.6426116824150085,\n",
              "  0.6254295706748962,\n",
              "  0.6477663516998291,\n",
              "  0.6477663516998291,\n",
              "  0.668384850025177,\n",
              "  0.6469594836235046,\n",
              "  0.6941580772399902,\n",
              "  0.6666666865348816,\n",
              "  0.661512017250061,\n",
              "  0.6340206265449524,\n",
              "  0.6649484634399414,\n",
              "  0.6271477937698364,\n",
              "  0.6443299055099487,\n",
              "  0.6408934593200684,\n",
              "  0.6477663516998291,\n",
              "  0.6374570727348328,\n",
              "  0.6219931244850159,\n",
              "  0.630584180355072,\n",
              "  0.6323024034500122,\n",
              "  0.6632302403450012,\n",
              "  0.699312686920166,\n",
              "  0.6718212962150574,\n",
              "  0.6580756306648254,\n",
              "  0.6752577424049377,\n",
              "  0.6769759654998779,\n",
              "  0.6563574075698853,\n",
              "  0.6718212962150574,\n",
              "  0.6529209613800049,\n",
              "  0.661512017250061,\n",
              "  0.6340206265449524,\n",
              "  0.6563574075698853,\n",
              "  0.6632302403450012,\n",
              "  0.6632302403450012,\n",
              "  0.6426116824150085,\n",
              "  0.6374570727348328,\n",
              "  0.6512027382850647,\n",
              "  0.6512027382850647,\n",
              "  0.6529209613800049,\n",
              "  0.6374570727348328,\n",
              "  0.668384850025177,\n",
              "  0.6391752362251282,\n",
              "  0.668384850025177,\n",
              "  0.6546391844749451,\n",
              "  0.6357388496398926,\n",
              "  0.6185566782951355,\n",
              "  0.6477663516998291,\n",
              "  0.668384850025177,\n",
              "  0.6752577424049377,\n",
              "  0.6460481286048889,\n",
              "  0.6838487982749939,\n",
              "  0.6460481286048889,\n",
              "  0.6890034079551697,\n",
              "  0.6477663516998291,\n",
              "  0.6718212962150574,\n",
              "  0.6374570727348328,\n",
              "  0.6580756306648254,\n",
              "  0.6580756306648254,\n",
              "  0.6391752362251282,\n",
              "  0.6604729890823364,\n",
              "  0.699312686920166,\n",
              "  0.6082473993301392,\n",
              "  0.6357388496398926,\n",
              "  0.6632302403450012,\n",
              "  0.6649484634399414,\n",
              "  0.6185566782951355,\n",
              "  0.668384850025177,\n",
              "  0.6821305751800537,\n",
              "  0.6185566782951355,\n",
              "  0.6649484634399414,\n",
              "  0.6563574075698853,\n",
              "  0.6666666865348816,\n",
              "  0.6975945234298706,\n",
              "  0.6443299055099487,\n",
              "  0.6408934593200684,\n",
              "  0.6649484634399414,\n",
              "  0.6872852444648743,\n",
              "  0.6890034079551697,\n",
              "  0.69243985414505,\n",
              "  0.6752577424049377,\n",
              "  0.6855670213699341,\n",
              "  0.6838487982749939,\n",
              "  0.668384850025177,\n",
              "  0.6580756306648254,\n",
              "  0.6701030731201172,\n",
              "  0.6752577424049377,\n",
              "  0.661512017250061,\n",
              "  0.6529209613800049,\n",
              "  0.6855670213699341,\n",
              "  0.6666666865348816,\n",
              "  0.661512017250061,\n",
              "  0.661512017250061,\n",
              "  0.6649484634399414,\n",
              "  0.6941580772399902,\n",
              "  0.668384850025177,\n",
              "  0.6769759654998779,\n",
              "  0.6597937941551208,\n",
              "  0.6821305751800537,\n",
              "  0.6855670213699341,\n",
              "  0.7130584120750427,\n",
              "  0.6769759654998779,\n",
              "  0.6477663516998291,\n",
              "  0.6649484634399414,\n",
              "  0.6890034079551697,\n",
              "  0.6872852444648743,\n",
              "  0.6907216310501099,\n",
              "  0.6563574075698853,\n",
              "  0.6649484634399414,\n",
              "  0.6666666865348816,\n",
              "  0.6632302403450012,\n",
              "  0.6563574075698853,\n",
              "  0.6580756306648254,\n",
              "  0.661512017250061,\n",
              "  0.6735395193099976,\n",
              "  0.6666666865348816,\n",
              "  0.6786941289901733,\n",
              "  0.6786941289901733,\n",
              "  0.6786941289901733,\n",
              "  0.6701030731201172,\n",
              "  0.6288659572601318,\n",
              "  0.6529209613800049,\n",
              "  0.6701030731201172,\n",
              "  0.6701030731201172,\n",
              "  0.6941580772399902,\n",
              "  0.6769759654998779,\n",
              "  0.7061855792999268,\n",
              "  0.6718212962150574,\n",
              "  0.6890034079551697,\n",
              "  0.668384850025177,\n",
              "  0.6563574075698853,\n",
              "  0.6872852444648743,\n",
              "  0.6838487982749939,\n",
              "  0.6718212962150574,\n",
              "  0.6907216310501099,\n",
              "  0.6804123520851135,\n",
              "  0.6855670213699341,\n",
              "  0.6958763003349304,\n",
              "  0.6546391844749451,\n",
              "  0.6460481286048889,\n",
              "  0.6649484634399414,\n",
              "  0.6718212962150574,\n",
              "  0.6804123520851135,\n",
              "  0.6563574075698853,\n",
              "  0.661512017250061,\n",
              "  0.6426116824150085,\n",
              "  0.668384850025177,\n",
              "  0.6769759654998779,\n",
              "  0.6786941289901733,\n",
              "  0.6718212962150574,\n",
              "  0.6941580772399902,\n",
              "  0.623711347579956,\n",
              "  0.6841216087341309,\n",
              "  0.6512027382850647,\n",
              "  0.6580756306648254,\n",
              "  0.6804123520851135,\n",
              "  0.6769759654998779,\n",
              "  0.6872852444648743,\n",
              "  0.6773648858070374,\n",
              "  0.6718212962150574,\n",
              "  0.69243985414505,\n",
              "  0.6769759654998779,\n",
              "  0.6460481286048889,\n",
              "  0.7044673562049866,\n",
              "  0.6718212962150574,\n",
              "  0.6563574075698853,\n",
              "  0.699312686920166,\n",
              "  0.668384850025177,\n",
              "  0.6580756306648254,\n",
              "  0.6529209613800049,\n",
              "  0.6752577424049377,\n",
              "  0.6752577424049377,\n",
              "  0.668384850025177,\n",
              "  0.6735395193099976,\n",
              "  0.699312686920166,\n",
              "  0.6477663516998291,\n",
              "  0.7010309100151062,\n",
              "  0.661512017250061,\n",
              "  0.7130584120750427,\n",
              "  0.6855670213699341,\n",
              "  0.6666666865348816,\n",
              "  0.6649484634399414,\n",
              "  0.6838487982749939,\n",
              "  0.6872852444648743,\n",
              "  0.668384850025177,\n",
              "  0.7027491331100464,\n",
              "  0.6855670213699341,\n",
              "  0.7079038023948669,\n",
              "  0.6786941289901733,\n",
              "  0.6632302403450012,\n",
              "  0.6769759654998779,\n",
              "  0.7044673562049866,\n",
              "  0.6494845151901245,\n",
              "  0.6855670213699341,\n",
              "  0.6941580772399902,\n",
              "  0.6477663516998291,\n",
              "  0.668384850025177,\n",
              "  0.6769759654998779,\n",
              "  0.6907216310501099,\n",
              "  0.7027491331100464,\n",
              "  0.6701030731201172,\n",
              "  0.7044673562049866,\n",
              "  0.6907216310501099,\n",
              "  0.7027491331100464,\n",
              "  0.7147766351699829,\n",
              "  0.6855670213699341,\n",
              "  0.7010309100151062,\n",
              "  0.7044673562049866,\n",
              "  0.6580756306648254,\n",
              "  0.6855670213699341,\n",
              "  0.6821305751800537,\n",
              "  0.6872852444648743,\n",
              "  0.6855670213699341,\n",
              "  0.6752577424049377,\n",
              "  0.6855670213699341,\n",
              "  0.7061855792999268,\n",
              "  0.6855670213699341,\n",
              "  0.6958763003349304,\n",
              "  0.6632302403450012,\n",
              "  0.7216494679450989,\n",
              "  0.7130584120750427,\n",
              "  0.6975945234298706,\n",
              "  0.6838487982749939,\n",
              "  0.6718212962150574,\n",
              "  0.730240523815155,\n",
              "  0.6718212962150574,\n",
              "  0.6735395193099976,\n",
              "  0.6855670213699341,\n",
              "  0.7422680258750916,\n",
              "  0.6941580772399902,\n",
              "  0.6769759654998779,\n",
              "  0.6752577424049377,\n",
              "  0.6975945234298706,\n",
              "  0.69243985414505,\n",
              "  0.6872852444648743,\n",
              "  0.6975945234298706,\n",
              "  0.6890034079551697,\n",
              "  0.6838487982749939,\n",
              "  0.6907216310501099,\n",
              "  0.7044673562049866,\n",
              "  0.7113401889801025,\n",
              "  0.699312686920166,\n",
              "  0.6752577424049377,\n",
              "  0.6907216310501099,\n",
              "  0.6855670213699341,\n",
              "  0.6907216310501099,\n",
              "  0.7268041372299194,\n",
              "  0.6975945234298706,\n",
              "  0.7371134161949158,\n",
              "  0.6872852444648743,\n",
              "  0.699312686920166,\n",
              "  0.7010309100151062,\n",
              "  0.699312686920166,\n",
              "  0.6838487982749939,\n",
              "  0.6975945234298706,\n",
              "  0.699312686920166,\n",
              "  0.6769759654998779,\n",
              "  0.69243985414505,\n",
              "  0.6735395193099976,\n",
              "  0.6735395193099976,\n",
              "  0.6804123520851135,\n",
              "  0.7250859141349792,\n",
              "  0.6925675868988037,\n",
              "  0.7285223603248596,\n",
              "  0.7044673562049866,\n",
              "  0.6907216310501099,\n",
              "  0.69243985414505,\n",
              "  0.69243985414505,\n",
              "  0.6701030731201172,\n",
              "  0.6855670213699341,\n",
              "  0.6958763003349304,\n",
              "  0.6890034079551697,\n",
              "  0.6666666865348816,\n",
              "  0.6890034079551697,\n",
              "  0.7250859141349792,\n",
              "  0.69243985414505,\n",
              "  0.6907216310501099,\n",
              "  0.6958763003349304,\n",
              "  0.6752577424049377,\n",
              "  0.6941580772399902,\n",
              "  0.7096219658851624,\n",
              "  0.7044673562049866,\n",
              "  0.6735395193099976,\n",
              "  0.7233676910400391,\n",
              "  0.7061855792999268,\n",
              "  0.6941580772399902,\n",
              "  0.668384850025177,\n",
              "  0.6821305751800537,\n",
              "  0.6975945234298706,\n",
              "  0.7010309100151062,\n",
              "  0.6975945234298706,\n",
              "  0.69243985414505,\n",
              "  0.7061855792999268,\n",
              "  0.7182130813598633,\n",
              "  0.7130584120750427,\n",
              "  0.6769759654998779,\n",
              "  0.6907216310501099,\n",
              "  0.699312686920166,\n",
              "  0.6958763003349304,\n",
              "  0.6718212962150574,\n",
              "  0.7319587469100952,\n",
              "  0.7182130813598633,\n",
              "  0.6872852444648743,\n",
              "  0.6735395193099976,\n",
              "  0.7044673562049866,\n",
              "  0.7164948582649231,\n",
              "  0.7061855792999268,\n",
              "  0.7113401889801025,\n",
              "  0.6907216310501099,\n",
              "  0.7010309100151062,\n",
              "  0.7027491331100464,\n",
              "  0.7216494679450989,\n",
              "  0.7061855792999268,\n",
              "  0.7010309100151062,\n",
              "  0.7010309100151062,\n",
              "  0.7113401889801025,\n",
              "  0.699312686920166,\n",
              "  0.7199312448501587,\n",
              "  0.6701030731201172,\n",
              "  0.7010309100151062,\n",
              "  0.6649484634399414,\n",
              "  0.7096219658851624,\n",
              "  0.6941580772399902,\n",
              "  0.6786941289901733,\n",
              "  0.7233676910400391,\n",
              "  0.6958763003349304,\n",
              "  0.6773648858070374,\n",
              "  0.6841216087341309,\n",
              "  0.6890034079551697,\n",
              "  0.6890034079551697,\n",
              "  0.7079038023948669,\n",
              "  0.7061855792999268,\n",
              "  0.7010309100151062,\n",
              "  0.7422680258750916,\n",
              "  0.7010309100151062,\n",
              "  0.6804123520851135,\n",
              "  0.7079038023948669,\n",
              "  0.7319587469100952,\n",
              "  0.699312686920166,\n",
              "  0.668384850025177,\n",
              "  0.6907216310501099,\n",
              "  0.6907216310501099,\n",
              "  0.69243985414505,\n",
              "  0.7182130813598633,\n",
              "  0.7027491331100464,\n",
              "  0.7577319741249084,\n",
              "  0.6804123520851135,\n",
              "  0.7199312448501587,\n",
              "  0.699312686920166,\n",
              "  0.7130584120750427,\n",
              "  0.7113401889801025,\n",
              "  0.6838487982749939,\n",
              "  0.6993243098258972,\n",
              "  0.7164948582649231,\n",
              "  0.6890034079551697,\n",
              "  0.7044673562049866,\n",
              "  0.6941580772399902,\n",
              "  0.7027491331100464,\n",
              "  0.7010309100151062,\n",
              "  0.7162162065505981,\n",
              "  0.6907216310501099,\n",
              "  0.6718212962150574,\n",
              "  0.7044673562049866,\n",
              "  0.6958763003349304,\n",
              "  0.6838487982749939,\n",
              "  0.6838487982749939,\n",
              "  0.7233676910400391,\n",
              "  0.6907216310501099,\n",
              "  0.7096219658851624,\n",
              "  0.6958763003349304,\n",
              "  0.7079038023948669,\n",
              "  0.6872852444648743,\n",
              "  0.7010309100151062,\n",
              "  0.7353951930999756,\n",
              "  0.699312686920166,\n",
              "  0.699312686920166,\n",
              "  0.6735395193099976,\n",
              "  0.7044673562049866,\n",
              "  0.6855670213699341,\n",
              "  0.7216494679450989,\n",
              "  0.7336769700050354,\n",
              "  0.7061855792999268,\n",
              "  0.6975945234298706,\n",
              "  0.7096219658851624,\n",
              "  0.6975945234298706,\n",
              "  0.6786941289901733,\n",
              "  0.7491409182548523,\n",
              "  0.7010309100151062,\n",
              "  0.7250859141349792,\n",
              "  0.6786941289901733,\n",
              "  0.712837815284729,\n",
              "  0.7079038023948669,\n",
              "  0.6958763003349304,\n",
              "  0.7130584120750427,\n",
              "  0.7268041372299194,\n",
              "  0.6975945234298706,\n",
              "  0.7079038023948669,\n",
              "  0.7079038023948669,\n",
              "  0.6958763003349304,\n",
              "  0.6735395193099976,\n",
              "  0.7233676910400391,\n",
              "  0.7113401889801025,\n",
              "  0.6975945234298706,\n",
              "  0.6804123520851135,\n",
              "  0.7216494679450989,\n",
              "  0.7027491331100464,\n",
              "  0.7199312448501587,\n",
              "  0.7250859141349792,\n",
              "  0.6975945234298706,\n",
              "  0.7405498027801514,\n",
              "  0.7457044720649719,\n",
              "  0.7061855792999268,\n",
              "  0.7371134161949158,\n",
              "  0.699312686920166,\n",
              "  0.6855670213699341,\n",
              "  0.7061855792999268,\n",
              "  0.6907216310501099,\n",
              "  0.7147766351699829,\n",
              "  0.7199312448501587,\n",
              "  0.7268041372299194,\n",
              "  0.7233676910400391,\n",
              "  0.6872852444648743,\n",
              "  0.7216494679450989,\n",
              "  0.6769759654998779,\n",
              "  0.7268041372299194,\n",
              "  0.7250859141349792,\n",
              "  0.7285223603248596,\n",
              "  0.7079038023948669,\n",
              "  0.7199312448501587,\n",
              "  0.7061855792999268,\n",
              "  0.7439862489700317,\n",
              "  0.7079038023948669,\n",
              "  0.7044673562049866,\n",
              "  0.7731958627700806,\n",
              "  0.7060810923576355,\n",
              "  0.7353951930999756,\n",
              "  0.7079038023948669,\n",
              "  0.7061855792999268,\n",
              "  0.7079038023948669,\n",
              "  0.7164948582649231,\n",
              "  0.7044673562049866,\n",
              "  0.6890034079551697,\n",
              "  0.7353951930999756,\n",
              "  0.7646048069000244,\n",
              "  0.7010309100151062,\n",
              "  0.7336769700050354,\n",
              "  0.7079038023948669,\n",
              "  0.7285223603248596,\n",
              "  0.7027491331100464,\n",
              "  0.7268041372299194,\n",
              "  0.7079038023948669,\n",
              "  0.7199312448501587,\n",
              "  0.7044673562049866,\n",
              "  0.7285223603248596,\n",
              "  0.7113401889801025,\n",
              "  0.699312686920166,\n",
              "  0.6941580772399902,\n",
              "  0.7233676910400391,\n",
              "  0.730240523815155,\n",
              "  0.699312686920166,\n",
              "  0.7474226951599121,\n",
              "  0.7285223603248596,\n",
              "  0.7319587469100952,\n",
              "  0.730240523815155,\n",
              "  0.6890034079551697,\n",
              "  0.7044673562049866,\n",
              "  0.7250859141349792,\n",
              "  0.7061855792999268,\n",
              "  0.7233676910400391,\n",
              "  0.6975945234298706,\n",
              "  0.7285223603248596,\n",
              "  0.7216494679450989,\n",
              "  0.7319587469100952,\n",
              "  0.7319587469100952,\n",
              "  0.7285223603248596,\n",
              "  0.7216494679450989,\n",
              "  0.7319587469100952,\n",
              "  0.7096219658851624,\n",
              "  0.7164948582649231,\n",
              "  0.7079038023948669,\n",
              "  0.7285223603248596,\n",
              "  0.7268041372299194,\n",
              "  0.7199312448501587,\n",
              "  0.7457044720649719,\n",
              "  0.6958763003349304,\n",
              "  0.7319587469100952,\n",
              "  0.7268041372299194,\n",
              "  0.6941580772399902,\n",
              "  0.7147766351699829,\n",
              "  0.7336769700050354,\n",
              "  0.69243985414505,\n",
              "  0.7096219658851624,\n",
              "  0.7027491331100464,\n",
              "  0.7439862489700317,\n",
              "  0.7044673562049866,\n",
              "  0.730240523815155,\n",
              "  0.699312686920166,\n",
              "  0.7336769700050354,\n",
              "  0.7216494679450989,\n",
              "  0.7233676910400391,\n",
              "  0.7542955279350281,\n",
              "  0.7130584120750427,\n",
              "  0.6824324131011963,\n",
              "  0.699312686920166,\n",
              "  0.7250859141349792,\n",
              "  0.7182130813598633,\n",
              "  0.6735395193099976,\n",
              "  0.7250859141349792,\n",
              "  0.7353951930999756,\n",
              "  0.7079038023948669,\n",
              "  0.7216494679450989,\n",
              "  0.7371134161949158,\n",
              "  0.7233676910400391,\n",
              "  0.7216494679450989,\n",
              "  0.7216494679450989,\n",
              "  0.7457044720649719,\n",
              "  0.7096219658851624,\n",
              "  0.6958763003349304,\n",
              "  0.7216494679450989,\n",
              "  0.7182130813598633,\n",
              "  0.7381756901741028,\n",
              "  0.7216494679450989,\n",
              "  0.7422680258750916,\n",
              "  0.7250859141349792,\n",
              "  0.7010309100151062,\n",
              "  0.7199312448501587,\n",
              "  0.761168360710144,\n",
              "  0.699312686920166,\n",
              "  0.7508590817451477,\n",
              "  0.7216494679450989,\n",
              "  0.6804123520851135,\n",
              "  0.7164948582649231,\n",
              "  0.7096219658851624,\n",
              "  0.7113401889801025,\n",
              "  0.7079038023948669,\n",
              "  0.7199312448501587,\n",
              "  0.7577319741249084,\n",
              "  0.7268041372299194,\n",
              "  0.7268041372299194,\n",
              "  0.7525773048400879,\n",
              "  0.7199312448501587,\n",
              "  0.6941580772399902,\n",
              "  0.69243985414505,\n",
              "  0.7285223603248596,\n",
              "  0.7079038023948669,\n",
              "  0.7233676910400391,\n",
              "  0.6975945234298706,\n",
              "  0.7061855792999268,\n",
              "  0.7164948582649231,\n",
              "  0.7439862489700317,\n",
              "  0.7319587469100952,\n",
              "  0.6855670213699341,\n",
              "  0.7233676910400391,\n",
              "  0.7147766351699829,\n",
              "  0.7079038023948669,\n",
              "  0.7336769700050354,\n",
              "  0.7285223603248596,\n",
              "  0.730240523815155,\n",
              "  0.761168360710144,\n",
              "  0.6975945234298706,\n",
              "  0.7714776396751404,\n",
              "  0.7405498027801514,\n",
              "  0.7147766351699829,\n",
              "  0.7113401889801025,\n",
              "  0.7405498027801514,\n",
              "  0.7147766351699829,\n",
              "  0.7319587469100952,\n",
              "  0.7233676910400391,\n",
              "  0.738831639289856,\n",
              "  0.7405498027801514,\n",
              "  0.7371134161949158,\n",
              "  0.7474226951599121,\n",
              "  0.7319587469100952,\n",
              "  0.7285223603248596,\n",
              "  0.7371134161949158,\n",
              "  0.7336769700050354,\n",
              "  0.7061855792999268,\n",
              "  0.7285223603248596,\n",
              "  0.7319587469100952,\n",
              "  0.7199312448501587,\n",
              "  0.7268041372299194,\n",
              "  0.7491409182548523,\n",
              "  0.730240523815155,\n",
              "  0.7216494679450989,\n",
              "  0.7164948582649231,\n",
              "  0.7079038023948669,\n",
              "  0.699312686920166,\n",
              "  0.7044673562049866,\n",
              "  0.7061855792999268,\n",
              "  0.7353951930999756,\n",
              "  0.7199312448501587,\n",
              "  0.7113401889801025,\n",
              "  0.7061855792999268,\n",
              "  0.730240523815155,\n",
              "  0.7164948582649231,\n",
              "  0.7250859141349792,\n",
              "  0.7491409182548523,\n",
              "  0.738831639289856,\n",
              "  0.7164948582649231,\n",
              "  0.6872852444648743,\n",
              "  0.7182130813598633,\n",
              "  0.7491409182548523,\n",
              "  0.738831639289856,\n",
              "  0.7199312448501587,\n",
              "  0.7268041372299194,\n",
              "  0.7113401889801025,\n",
              "  0.7457044720649719,\n",
              "  0.730240523815155,\n",
              "  0.7491409182548523,\n",
              "  0.7061855792999268,\n",
              "  0.7113401889801025,\n",
              "  0.7405498027801514,\n",
              "  0.7096219658851624,\n",
              "  0.7439862489700317,\n",
              "  0.7250859141349792,\n",
              "  0.7319587469100952,\n",
              "  0.7164948582649231,\n",
              "  0.7268041372299194,\n",
              "  0.7027027010917664,\n",
              "  0.6872852444648743,\n",
              "  0.7405498027801514,\n",
              "  0.7268041372299194,\n",
              "  0.7199312448501587,\n",
              "  0.7405498027801514,\n",
              "  0.6580756306648254,\n",
              "  0.6855670213699341,\n",
              "  0.7491409182548523,\n",
              "  0.7491409182548523,\n",
              "  0.730240523815155,\n",
              "  0.7285223603248596,\n",
              "  0.7268041372299194,\n",
              "  0.7353951930999756,\n",
              "  0.7164948582649231,\n",
              "  0.7285223603248596,\n",
              "  0.7371134161949158,\n",
              "  0.7405498027801514,\n",
              "  0.7422680258750916,\n",
              "  0.7525773048400879,\n",
              "  0.7285223603248596,\n",
              "  0.7466216087341309,\n",
              "  0.730240523815155,\n",
              "  0.7285223603248596,\n",
              "  0.738831639289856,\n",
              "  0.7216494679450989,\n",
              "  0.7130584120750427,\n",
              "  0.6975945234298706,\n",
              "  0.7182130813598633,\n",
              "  0.7147766351699829,\n",
              "  0.7268041372299194,\n",
              "  0.730240523815155,\n",
              "  0.7233676910400391,\n",
              "  0.7336769700050354,\n",
              "  0.7783505320549011,\n",
              "  0.7182130813598633],\n",
              " 'val_loss': [0.8420057892799377,\n",
              "  0.8383275866508484,\n",
              "  0.8076915740966797,\n",
              "  0.8194484710693359,\n",
              "  0.8371599316596985,\n",
              "  0.8145386576652527,\n",
              "  0.8307177424430847,\n",
              "  0.8258697390556335,\n",
              "  0.8280730247497559,\n",
              "  0.8555338978767395,\n",
              "  0.8318491578102112,\n",
              "  0.8381791710853577,\n",
              "  0.8508592247962952,\n",
              "  0.8294088244438171,\n",
              "  0.8294031023979187,\n",
              "  0.8572108745574951,\n",
              "  0.8523192405700684,\n",
              "  0.8613076210021973,\n",
              "  0.8365783095359802,\n",
              "  0.8355123996734619,\n",
              "  0.8418090343475342,\n",
              "  0.8436551690101624,\n",
              "  0.8663527965545654,\n",
              "  0.8448024392127991,\n",
              "  0.8488953709602356,\n",
              "  0.8527480959892273,\n",
              "  0.8452362418174744,\n",
              "  0.8559384346008301,\n",
              "  0.8504273295402527,\n",
              "  0.8539301753044128,\n",
              "  0.8593583106994629,\n",
              "  0.867150604724884,\n",
              "  0.8612627983093262,\n",
              "  0.8667864203453064,\n",
              "  0.8608319759368896,\n",
              "  0.87054443359375,\n",
              "  0.8557633757591248,\n",
              "  0.8706092834472656,\n",
              "  0.872321605682373,\n",
              "  0.8822341561317444,\n",
              "  0.8633531928062439,\n",
              "  0.8766140341758728,\n",
              "  0.8673226237297058,\n",
              "  0.8739838004112244,\n",
              "  0.8872968554496765,\n",
              "  0.8788678050041199,\n",
              "  0.9017243385314941,\n",
              "  0.8863406181335449,\n",
              "  0.8795347809791565,\n",
              "  0.8591618537902832,\n",
              "  0.8706029057502747,\n",
              "  0.8746075630187988,\n",
              "  0.8596950173377991,\n",
              "  0.8576486706733704,\n",
              "  0.8747640252113342,\n",
              "  0.8693841099739075,\n",
              "  0.8753741383552551,\n",
              "  0.8794586658477783,\n",
              "  0.8879663944244385,\n",
              "  0.8767326474189758,\n",
              "  0.8787181973457336,\n",
              "  0.880897581577301,\n",
              "  0.8825342059135437,\n",
              "  0.8783305287361145,\n",
              "  0.8722119927406311,\n",
              "  0.9037467837333679,\n",
              "  0.8863916397094727,\n",
              "  0.8768947720527649,\n",
              "  0.9073546528816223,\n",
              "  0.877713143825531,\n",
              "  0.8836134076118469,\n",
              "  0.8779858946800232,\n",
              "  0.895667552947998,\n",
              "  0.8778749108314514,\n",
              "  0.8804652094841003,\n",
              "  0.8714848160743713,\n",
              "  0.8820342421531677,\n",
              "  0.902147114276886,\n",
              "  0.8901646137237549,\n",
              "  0.9013164639472961,\n",
              "  0.8867649435997009,\n",
              "  0.9069415926933289,\n",
              "  0.8997446894645691,\n",
              "  0.9124873280525208,\n",
              "  0.9074248671531677,\n",
              "  0.8970541954040527,\n",
              "  0.891139566898346,\n",
              "  0.9198433756828308,\n",
              "  0.9099156260490417,\n",
              "  0.9097371101379395,\n",
              "  0.909720242023468,\n",
              "  0.9048831462860107,\n",
              "  0.9026324152946472,\n",
              "  0.8994396328926086,\n",
              "  0.902967631816864,\n",
              "  0.8705914616584778,\n",
              "  0.9071914553642273,\n",
              "  0.8776151537895203,\n",
              "  0.8956891894340515,\n",
              "  0.890651524066925,\n",
              "  0.9021224975585938,\n",
              "  0.8932416439056396,\n",
              "  0.9094612002372742,\n",
              "  0.8992131352424622,\n",
              "  0.8931546211242676,\n",
              "  0.9029805660247803,\n",
              "  0.8973788619041443,\n",
              "  0.904396116733551,\n",
              "  0.889904260635376,\n",
              "  0.8941572308540344,\n",
              "  0.8966865539550781,\n",
              "  0.9025788307189941,\n",
              "  0.8981226086616516,\n",
              "  0.9051671028137207,\n",
              "  0.9034553170204163,\n",
              "  0.8991926312446594,\n",
              "  0.9068564772605896,\n",
              "  0.9109887480735779,\n",
              "  0.8836524486541748,\n",
              "  0.8875112533569336,\n",
              "  0.913506805896759,\n",
              "  0.9010326266288757,\n",
              "  0.9135515689849854,\n",
              "  0.9076817035675049,\n",
              "  0.9040793776512146,\n",
              "  0.8873017430305481,\n",
              "  0.8935699462890625,\n",
              "  0.9044898152351379,\n",
              "  0.8898334503173828,\n",
              "  0.9039446711540222,\n",
              "  0.8917600512504578,\n",
              "  0.9097356796264648,\n",
              "  0.8900827765464783,\n",
              "  0.8940730094909668,\n",
              "  0.9046497344970703,\n",
              "  0.894995927810669,\n",
              "  0.8922564387321472,\n",
              "  0.8952921032905579,\n",
              "  0.9050589203834534,\n",
              "  0.8979339003562927,\n",
              "  0.9088818430900574,\n",
              "  0.9111387729644775,\n",
              "  0.911172091960907,\n",
              "  0.9073781371116638,\n",
              "  0.9061478972434998,\n",
              "  0.871082603931427,\n",
              "  0.8881716132164001,\n",
              "  0.9175302386283875,\n",
              "  0.8875889778137207,\n",
              "  0.9195663928985596,\n",
              "  0.9058195948600769,\n",
              "  0.8918294310569763,\n",
              "  0.8999055027961731,\n",
              "  0.8737199902534485,\n",
              "  0.9090347290039062,\n",
              "  0.8945067524909973,\n",
              "  0.8952546715736389,\n",
              "  0.8719196915626526,\n",
              "  0.9122493267059326,\n",
              "  0.88868647813797,\n",
              "  0.890148937702179,\n",
              "  0.874182403087616,\n",
              "  0.8883085250854492,\n",
              "  0.875742495059967,\n",
              "  0.8933327794075012,\n",
              "  0.8829421997070312,\n",
              "  0.892061173915863,\n",
              "  0.8792872428894043,\n",
              "  0.8985424041748047,\n",
              "  0.8867225646972656,\n",
              "  0.87960284948349,\n",
              "  0.896619975566864,\n",
              "  0.8626391291618347,\n",
              "  0.8767292499542236,\n",
              "  0.9127292633056641,\n",
              "  0.8945765495300293,\n",
              "  0.8836447596549988,\n",
              "  0.8978428840637207,\n",
              "  0.8664100170135498,\n",
              "  0.8795590996742249,\n",
              "  0.8943844437599182,\n",
              "  0.8800235390663147,\n",
              "  0.8828034400939941,\n",
              "  0.8875629901885986,\n",
              "  0.8686153292655945,\n",
              "  0.8770344853401184,\n",
              "  0.8825963139533997,\n",
              "  0.8797550201416016,\n",
              "  0.8836007714271545,\n",
              "  0.8829390406608582,\n",
              "  0.87594074010849,\n",
              "  0.8837001323699951,\n",
              "  0.8725152015686035,\n",
              "  0.8562129139900208,\n",
              "  0.8481213450431824,\n",
              "  0.8774335980415344,\n",
              "  0.8380623459815979,\n",
              "  0.8835127949714661,\n",
              "  0.8727262020111084,\n",
              "  0.8786019682884216,\n",
              "  0.8851247429847717,\n",
              "  0.8908467888832092,\n",
              "  0.8853051066398621,\n",
              "  0.8688884377479553,\n",
              "  0.8655250072479248,\n",
              "  0.873582661151886,\n",
              "  0.879216730594635,\n",
              "  0.8591299057006836,\n",
              "  0.8641505241394043,\n",
              "  0.8799509406089783,\n",
              "  0.8814011216163635,\n",
              "  0.8645989298820496,\n",
              "  0.868770182132721,\n",
              "  0.8767287731170654,\n",
              "  0.8528997302055359,\n",
              "  0.8779003620147705,\n",
              "  0.8728780746459961,\n",
              "  0.864702045917511,\n",
              "  0.8496565818786621,\n",
              "  0.8641993999481201,\n",
              "  0.8447310924530029,\n",
              "  0.8497865796089172,\n",
              "  0.8620426058769226,\n",
              "  0.8524195551872253,\n",
              "  0.8750107884407043,\n",
              "  0.8572563529014587,\n",
              "  0.8785150647163391,\n",
              "  0.8627329468727112,\n",
              "  0.878498375415802,\n",
              "  0.8654279708862305,\n",
              "  0.8537688255310059,\n",
              "  0.865215003490448,\n",
              "  0.8803866505622864,\n",
              "  0.8640699982643127,\n",
              "  0.8634929060935974,\n",
              "  0.8612416386604309,\n",
              "  0.8694977164268494,\n",
              "  0.8484954237937927,\n",
              "  0.8549283146858215,\n",
              "  0.8516852259635925,\n",
              "  0.8684986233711243,\n",
              "  0.8638433814048767,\n",
              "  0.8468589186668396,\n",
              "  0.8529074788093567,\n",
              "  0.8569192886352539,\n",
              "  0.8542037606239319,\n",
              "  0.8355786800384521,\n",
              "  0.8526921272277832,\n",
              "  0.8279402256011963,\n",
              "  0.8457669615745544,\n",
              "  0.8246373534202576,\n",
              "  0.8279485702514648,\n",
              "  0.8596987128257751,\n",
              "  0.8346400856971741,\n",
              "  0.8566456437110901,\n",
              "  0.8443813323974609,\n",
              "  0.8398339152336121,\n",
              "  0.8338410258293152,\n",
              "  0.8495122790336609,\n",
              "  0.8374798893928528,\n",
              "  0.8489473462104797,\n",
              "  0.8430710434913635,\n",
              "  0.836333692073822,\n",
              "  0.8390796184539795,\n",
              "  0.8337633013725281,\n",
              "  0.8386949896812439,\n",
              "  0.8427703976631165,\n",
              "  0.8379709720611572,\n",
              "  0.8440068364143372,\n",
              "  0.837114155292511,\n",
              "  0.8472800254821777,\n",
              "  0.8584519028663635,\n",
              "  0.8268703818321228,\n",
              "  0.8555386662483215,\n",
              "  0.8352732062339783,\n",
              "  0.8461487889289856,\n",
              "  0.8347548842430115,\n",
              "  0.8305689692497253,\n",
              "  0.8398227691650391,\n",
              "  0.8425182700157166,\n",
              "  0.849847137928009,\n",
              "  0.8282723426818848,\n",
              "  0.8476210236549377,\n",
              "  0.8514414429664612,\n",
              "  0.8516783714294434,\n",
              "  0.8359107971191406,\n",
              "  0.8241046071052551,\n",
              "  0.8221521377563477,\n",
              "  0.8477487564086914,\n",
              "  0.8250963687896729,\n",
              "  0.8335109353065491,\n",
              "  0.8278565406799316,\n",
              "  0.8225448727607727,\n",
              "  0.8390688300132751,\n",
              "  0.8362117409706116,\n",
              "  0.8219122290611267,\n",
              "  0.838469922542572,\n",
              "  0.8338160514831543,\n",
              "  0.8326570987701416,\n",
              "  0.8259416222572327,\n",
              "  0.8221922516822815,\n",
              "  0.8469471335411072,\n",
              "  0.8208116888999939,\n",
              "  0.8309233784675598,\n",
              "  0.8390576839447021,\n",
              "  0.8517282605171204,\n",
              "  0.8346223831176758,\n",
              "  0.8432021141052246,\n",
              "  0.8396008610725403,\n",
              "  0.819067656993866,\n",
              "  0.8336763978004456,\n",
              "  0.8537530899047852,\n",
              "  0.8478304743766785,\n",
              "  0.8279277682304382,\n",
              "  0.8234657645225525,\n",
              "  0.8098030090332031,\n",
              "  0.8204760551452637,\n",
              "  0.8364222049713135,\n",
              "  0.8347294330596924,\n",
              "  0.8196628093719482,\n",
              "  0.8299643397331238,\n",
              "  0.8152675032615662,\n",
              "  0.8252097964286804,\n",
              "  0.8204233050346375,\n",
              "  0.8174982070922852,\n",
              "  0.8213739395141602,\n",
              "  0.8197433352470398,\n",
              "  0.8178308010101318,\n",
              "  0.8154779076576233,\n",
              "  0.8133559226989746,\n",
              "  0.8137860894203186,\n",
              "  0.8405744433403015,\n",
              "  0.836682140827179,\n",
              "  0.8300355076789856,\n",
              "  0.8215159773826599,\n",
              "  0.8085083365440369,\n",
              "  0.814359188079834,\n",
              "  0.8094986081123352,\n",
              "  0.81222003698349,\n",
              "  0.8109631538391113,\n",
              "  0.822021484375,\n",
              "  0.8292291760444641,\n",
              "  0.8052260875701904,\n",
              "  0.8133115768432617,\n",
              "  0.8026359677314758,\n",
              "  0.8023591041564941,\n",
              "  0.8212609887123108,\n",
              "  0.809781551361084,\n",
              "  0.8172006607055664,\n",
              "  0.8193811774253845,\n",
              "  0.8150625228881836,\n",
              "  0.815314769744873,\n",
              "  0.8046849370002747,\n",
              "  0.8004676699638367,\n",
              "  0.8021175265312195,\n",
              "  0.8080625534057617,\n",
              "  0.7944526672363281,\n",
              "  0.7958521842956543,\n",
              "  0.8054800629615784,\n",
              "  0.8116788864135742,\n",
              "  0.7959573864936829,\n",
              "  0.7986733317375183,\n",
              "  0.8211198449134827,\n",
              "  0.8068118095397949,\n",
              "  0.8063383102416992,\n",
              "  0.8119826316833496,\n",
              "  0.8206543922424316,\n",
              "  0.7958775162696838,\n",
              "  0.7874980568885803,\n",
              "  0.8055779337882996,\n",
              "  0.7822281718254089,\n",
              "  0.8097670078277588,\n",
              "  0.8011626601219177,\n",
              "  0.8180327415466309,\n",
              "  0.799344003200531,\n",
              "  0.8033373355865479,\n",
              "  0.8109833598136902,\n",
              "  0.8027818202972412,\n",
              "  0.8008058071136475,\n",
              "  0.7820892333984375,\n",
              "  0.7978169322013855,\n",
              "  0.803612232208252,\n",
              "  0.7977451682090759,\n",
              "  0.8030974864959717,\n",
              "  0.7842418551445007,\n",
              "  0.8021001815795898,\n",
              "  0.7889201045036316,\n",
              "  0.7838840484619141,\n",
              "  0.7918439507484436,\n",
              "  0.7759134769439697,\n",
              "  0.7924292087554932,\n",
              "  0.8051896691322327,\n",
              "  0.8023330569267273,\n",
              "  0.7888383865356445,\n",
              "  0.7963584065437317,\n",
              "  0.7881061434745789,\n",
              "  0.773571789264679,\n",
              "  0.7889578938484192,\n",
              "  0.7851452827453613,\n",
              "  0.7885870933532715,\n",
              "  0.7965006828308105,\n",
              "  0.7920745015144348,\n",
              "  0.7903532385826111,\n",
              "  0.779036819934845,\n",
              "  0.7831804156303406,\n",
              "  0.7756116390228271,\n",
              "  0.7938222885131836,\n",
              "  0.7704102396965027,\n",
              "  0.7888109087944031,\n",
              "  0.7869675159454346,\n",
              "  0.7881197929382324,\n",
              "  0.7940900921821594,\n",
              "  0.7938156127929688,\n",
              "  0.7899372577667236,\n",
              "  0.7900233268737793,\n",
              "  0.7815092206001282,\n",
              "  0.773754358291626,\n",
              "  0.7839824557304382,\n",
              "  0.789317786693573,\n",
              "  0.7700685858726501,\n",
              "  0.7752479910850525,\n",
              "  0.7862367630004883,\n",
              "  0.7588560581207275,\n",
              "  0.7776942253112793,\n",
              "  0.7901124954223633,\n",
              "  0.7729353904724121,\n",
              "  0.768238365650177,\n",
              "  0.769140899181366,\n",
              "  0.7727811336517334,\n",
              "  0.7723355293273926,\n",
              "  0.7744849324226379,\n",
              "  0.7810206413269043,\n",
              "  0.7760891318321228,\n",
              "  0.7922045588493347,\n",
              "  0.7671825289726257,\n",
              "  0.7815489768981934,\n",
              "  0.7679856419563293,\n",
              "  0.7592573761940002,\n",
              "  0.7716773152351379,\n",
              "  0.7628161907196045,\n",
              "  0.755711019039154,\n",
              "  0.7730569243431091,\n",
              "  0.7638437151908875,\n",
              "  0.7727125287055969,\n",
              "  0.7469182014465332,\n",
              "  0.7771198153495789,\n",
              "  0.7661526799201965,\n",
              "  0.7692458033561707,\n",
              "  0.7690436244010925,\n",
              "  0.7766578197479248,\n",
              "  0.7725397944450378,\n",
              "  0.7728891372680664,\n",
              "  0.7765161395072937,\n",
              "  0.7720375061035156,\n",
              "  0.7623167634010315,\n",
              "  0.7655136585235596,\n",
              "  0.7600781321525574,\n",
              "  0.7793905138969421,\n",
              "  0.7682719230651855,\n",
              "  0.7665322422981262,\n",
              "  0.7803990840911865,\n",
              "  0.7460620999336243,\n",
              "  0.7666216492652893,\n",
              "  0.7615448832511902,\n",
              "  0.7546750903129578,\n",
              "  0.7660331130027771,\n",
              "  0.7611338496208191,\n",
              "  0.7633575797080994,\n",
              "  0.7862350344657898,\n",
              "  0.7741521000862122,\n",
              "  0.7594158053398132,\n",
              "  0.7641389966011047,\n",
              "  0.763374388217926,\n",
              "  0.7556028366088867,\n",
              "  0.7663245797157288,\n",
              "  0.7712355256080627,\n",
              "  0.7596344947814941,\n",
              "  0.7829421162605286,\n",
              "  0.7638759613037109,\n",
              "  0.7758342623710632,\n",
              "  0.7710850834846497,\n",
              "  0.7748081088066101,\n",
              "  0.7656814455986023,\n",
              "  0.7653601169586182,\n",
              "  0.7797325253486633,\n",
              "  0.7651546597480774,\n",
              "  0.773228108882904,\n",
              "  0.7585015296936035,\n",
              "  0.75882488489151,\n",
              "  0.7611605525016785,\n",
              "  0.7616685032844543,\n",
              "  0.7780908942222595,\n",
              "  0.7815432548522949,\n",
              "  0.7417591214179993,\n",
              "  0.7719466686248779,\n",
              "  0.7577584385871887,\n",
              "  0.7724615931510925,\n",
              "  0.7786723971366882,\n",
              "  0.7582079768180847,\n",
              "  0.7548783421516418,\n",
              "  0.7523901462554932,\n",
              "  0.7568579316139221,\n",
              "  0.7709436416625977,\n",
              "  0.754791259765625,\n",
              "  0.7617378234863281,\n",
              "  0.7672049403190613,\n",
              "  0.7714094519615173,\n",
              "  0.754601776599884,\n",
              "  0.7606046199798584,\n",
              "  0.7665876746177673,\n",
              "  0.7672982215881348,\n",
              "  0.7488425374031067,\n",
              "  0.7420218586921692,\n",
              "  0.7450419068336487,\n",
              "  0.7553995251655579,\n",
              "  0.7569007277488708,\n",
              "  0.7394254803657532,\n",
              "  0.7416813969612122,\n",
              "  0.7611300945281982,\n",
              "  0.7499056458473206,\n",
              "  0.7634322643280029,\n",
              "  0.7573440074920654,\n",
              "  0.7606819272041321,\n",
              "  0.7509949803352356,\n",
              "  0.753961980342865,\n",
              "  0.7608914375305176,\n",
              "  0.766640841960907,\n",
              "  0.761042594909668,\n",
              "  0.7598381042480469,\n",
              "  0.7713853716850281,\n",
              "  0.7727523446083069,\n",
              "  0.75970858335495,\n",
              "  0.7445281147956848,\n",
              "  0.7588595747947693,\n",
              "  0.7781766057014465,\n",
              "  0.7560431361198425,\n",
              "  0.7618547081947327,\n",
              "  0.780394971370697,\n",
              "  0.7607372403144836,\n",
              "  0.7387888431549072,\n",
              "  0.7499489784240723,\n",
              "  0.7621952891349792,\n",
              "  0.7616506218910217,\n",
              "  0.7550169825553894,\n",
              "  0.7558960318565369,\n",
              "  0.7600093483924866,\n",
              "  0.7418662905693054,\n",
              "  0.7585328221321106,\n",
              "  0.7649876475334167,\n",
              "  0.7530536651611328,\n",
              "  0.7497579455375671,\n",
              "  0.7482602000236511,\n",
              "  0.7614559531211853,\n",
              "  0.7629122138023376,\n",
              "  0.7659472823143005,\n",
              "  0.7652290463447571,\n",
              "  0.7683544158935547,\n",
              "  0.7624845504760742,\n",
              "  0.7552847862243652,\n",
              "  0.7497304081916809,\n",
              "  0.7460987567901611,\n",
              "  0.7463302612304688,\n",
              "  0.7615370750427246,\n",
              "  0.7522756457328796,\n",
              "  0.7558228373527527,\n",
              "  0.7571114897727966,\n",
              "  0.7573606371879578,\n",
              "  0.749483048915863,\n",
              "  0.7565914988517761,\n",
              "  0.7524173259735107,\n",
              "  0.7532598376274109,\n",
              "  0.7490596175193787,\n",
              "  0.753726065158844,\n",
              "  0.7552847266197205,\n",
              "  0.7602789402008057,\n",
              "  0.7568570971488953,\n",
              "  0.7484930157661438,\n",
              "  0.7475021481513977,\n",
              "  0.7443416714668274,\n",
              "  0.7429995536804199,\n",
              "  0.7513672709465027,\n",
              "  0.7396480441093445,\n",
              "  0.7528972625732422,\n",
              "  0.7640729546546936,\n",
              "  0.7565421462059021,\n",
              "  0.7431241869926453,\n",
              "  0.7451477646827698,\n",
              "  0.7443810105323792,\n",
              "  0.7296674251556396,\n",
              "  0.7359159588813782,\n",
              "  0.7421988844871521,\n",
              "  0.7581822872161865,\n",
              "  0.7275243401527405,\n",
              "  0.7508606314659119,\n",
              "  0.7326577305793762,\n",
              "  0.743065595626831,\n",
              "  0.7510586380958557,\n",
              "  0.7315500378608704,\n",
              "  0.7356098294258118,\n",
              "  0.7438470721244812,\n",
              "  0.754448652267456,\n",
              "  0.7362560629844666,\n",
              "  0.75002521276474,\n",
              "  0.742760419845581,\n",
              "  0.7252709269523621,\n",
              "  0.7228423953056335,\n",
              "  0.7310352921485901,\n",
              "  0.7264767289161682,\n",
              "  0.7454771995544434,\n",
              "  0.7289274334907532,\n",
              "  0.7277396321296692,\n",
              "  0.7229281067848206,\n",
              "  0.7327685952186584,\n",
              "  0.7325306534767151,\n",
              "  0.7321605086326599,\n",
              "  0.7206084132194519,\n",
              "  0.7354975342750549,\n",
              "  0.733983039855957,\n",
              "  0.7379500865936279,\n",
              "  0.7176432609558105,\n",
              "  0.7268491387367249,\n",
              "  0.7423748970031738,\n",
              "  0.7375248074531555,\n",
              "  0.7374761700630188,\n",
              "  0.7535021901130676,\n",
              "  0.7441391944885254,\n",
              "  0.740534782409668,\n",
              "  0.7381966710090637,\n",
              "  0.7108182907104492,\n",
              "  0.7210990786552429,\n",
              "  0.7301716804504395,\n",
              "  0.7381806373596191,\n",
              "  0.7284806370735168,\n",
              "  0.7437494397163391,\n",
              "  0.7444980144500732,\n",
              "  0.7331592440605164,\n",
              "  0.7371609807014465,\n",
              "  0.7363024353981018,\n",
              "  0.726505696773529,\n",
              "  0.7353419661521912,\n",
              "  0.7393191456794739,\n",
              "  0.7259325981140137,\n",
              "  0.7315657138824463,\n",
              "  0.7225813865661621,\n",
              "  0.7368285655975342,\n",
              "  0.7270095348358154,\n",
              "  0.7399657368659973,\n",
              "  0.7264847159385681,\n",
              "  0.7385178208351135,\n",
              "  0.740755558013916,\n",
              "  0.7315217852592468,\n",
              "  0.7275425791740417,\n",
              "  0.739168107509613,\n",
              "  0.735299825668335,\n",
              "  0.7450304627418518,\n",
              "  0.7249032855033875,\n",
              "  0.7258399128913879,\n",
              "  0.7329459190368652,\n",
              "  0.7208905816078186,\n",
              "  0.7188823819160461,\n",
              "  0.7240673899650574,\n",
              "  0.7362744808197021,\n",
              "  0.735992431640625,\n",
              "  0.7282434105873108,\n",
              "  0.7276576161384583,\n",
              "  0.7135714888572693,\n",
              "  0.7216105461120605,\n",
              "  0.7133752703666687,\n",
              "  0.7143974304199219,\n",
              "  0.7073871493339539,\n",
              "  0.7146258354187012,\n",
              "  0.7236225008964539,\n",
              "  0.7360879778862,\n",
              "  0.7343255877494812,\n",
              "  0.7232487201690674,\n",
              "  0.7248793244361877,\n",
              "  0.7331953048706055,\n",
              "  0.734950602054596,\n",
              "  0.7202445864677429,\n",
              "  0.7290975451469421,\n",
              "  0.7211529612541199,\n",
              "  0.7238828539848328,\n",
              "  0.7343292832374573,\n",
              "  0.7232006192207336,\n",
              "  0.7242903709411621,\n",
              "  0.7250714898109436,\n",
              "  0.7243807911872864,\n",
              "  0.7153503894805908,\n",
              "  0.7157252430915833,\n",
              "  0.7312256693840027,\n",
              "  0.7174165844917297,\n",
              "  0.7214085459709167,\n",
              "  0.7248331904411316,\n",
              "  0.7249242663383484,\n",
              "  0.7361995577812195,\n",
              "  0.7200292944908142,\n",
              "  0.7207119464874268,\n",
              "  0.7340248227119446,\n",
              "  0.7170576453208923,\n",
              "  0.7281665802001953,\n",
              "  0.7200542092323303,\n",
              "  0.7078532576560974,\n",
              "  0.7092452049255371,\n",
              "  0.7120272517204285,\n",
              "  0.7131170630455017,\n",
              "  0.7190506458282471,\n",
              "  0.7222208976745605,\n",
              "  0.7258241176605225,\n",
              "  0.7181674838066101,\n",
              "  0.7099869847297668,\n",
              "  0.7227864265441895,\n",
              "  0.7131268978118896,\n",
              "  0.7227347493171692,\n",
              "  0.7220673561096191,\n",
              "  0.7206879258155823,\n",
              "  0.7125720977783203,\n",
              "  0.7104808688163757,\n",
              "  0.7186555862426758,\n",
              "  0.7336652874946594,\n",
              "  0.7078127861022949,\n",
              "  0.7167717814445496,\n",
              "  0.7265734672546387,\n",
              "  0.7213041186332703,\n",
              "  0.7237598896026611,\n",
              "  0.7213506102561951,\n",
              "  0.7109406590461731,\n",
              "  0.7314125895500183,\n",
              "  0.7172059416770935,\n",
              "  0.7283194065093994,\n",
              "  0.6987063884735107,\n",
              "  0.7238950133323669,\n",
              "  0.71287602186203,\n",
              "  0.7060564160346985,\n",
              "  0.7075604796409607,\n",
              "  0.7226347923278809,\n",
              "  0.7091946005821228,\n",
              "  0.712609589099884,\n",
              "  0.7112314701080322,\n",
              "  0.7164053916931152,\n",
              "  0.7081608772277832,\n",
              "  0.7078813910484314,\n",
              "  0.7364034652709961,\n",
              "  0.7336783409118652,\n",
              "  0.7049694657325745,\n",
              "  0.723240077495575,\n",
              "  0.7193386554718018,\n",
              "  0.7096445560455322,\n",
              "  0.7092304229736328,\n",
              "  0.7292069792747498,\n",
              "  0.7323779463768005,\n",
              "  0.7234734892845154,\n",
              "  0.7235598564147949,\n",
              "  0.7071483731269836,\n",
              "  0.7308012843132019,\n",
              "  0.7061139941215515,\n",
              "  0.7224054932594299,\n",
              "  0.7133296132087708,\n",
              "  0.7309053540229797,\n",
              "  0.7267892360687256,\n",
              "  0.7015259265899658,\n",
              "  0.7200379371643066,\n",
              "  0.722928524017334,\n",
              "  0.7199976444244385,\n",
              "  0.7111411094665527,\n",
              "  0.7260155081748962,\n",
              "  0.7407241463661194,\n",
              "  0.7135173678398132,\n",
              "  0.7333076000213623,\n",
              "  0.7225936055183411,\n",
              "  0.7147946357727051,\n",
              "  0.7149264812469482,\n",
              "  0.7252504825592041,\n",
              "  0.7153323292732239,\n",
              "  0.7248477339744568,\n",
              "  0.7321192622184753,\n",
              "  0.7276980876922607,\n",
              "  0.7314338684082031,\n",
              "  0.726597249507904,\n",
              "  0.7318989634513855,\n",
              "  0.7060647010803223,\n",
              "  0.7193644642829895,\n",
              "  0.714904248714447,\n",
              "  0.7259647846221924,\n",
              "  0.7096068263053894,\n",
              "  0.7025415301322937,\n",
              "  0.7092108726501465,\n",
              "  0.695711612701416,\n",
              "  0.7065117955207825,\n",
              "  0.7144662737846375,\n",
              "  0.7027571797370911,\n",
              "  0.714410126209259,\n",
              "  0.7232620120048523,\n",
              "  0.7243075370788574,\n",
              "  0.7196412086486816,\n",
              "  0.7160459160804749,\n",
              "  0.7026329040527344,\n",
              "  0.713926374912262,\n",
              "  0.7039117813110352,\n",
              "  0.7271409034729004,\n",
              "  0.7161317467689514,\n",
              "  0.7074385285377502,\n",
              "  0.7162652611732483,\n",
              "  0.7173330783843994,\n",
              "  0.7186133861541748,\n",
              "  0.7123935222625732,\n",
              "  0.7052155137062073,\n",
              "  0.7155646681785583,\n",
              "  0.7173696160316467,\n",
              "  0.7197969555854797,\n",
              "  0.7237779498100281,\n",
              "  0.723696768283844,\n",
              "  0.7142634391784668,\n",
              "  0.7263055443763733,\n",
              "  0.7199618220329285,\n",
              "  0.7088280320167542,\n",
              "  0.7073586583137512,\n",
              "  0.7198514938354492,\n",
              "  0.7147969603538513,\n",
              "  0.70524662733078,\n",
              "  0.714568555355072,\n",
              "  0.7211804986000061,\n",
              "  0.7074415683746338,\n",
              "  0.7224993705749512,\n",
              "  0.7117984294891357,\n",
              "  0.7094541192054749,\n",
              "  0.6993756890296936,\n",
              "  0.705169677734375,\n",
              "  0.7248877882957458,\n",
              "  0.7162331938743591,\n",
              "  0.7210610508918762,\n",
              "  0.7048342227935791,\n",
              "  0.7132566571235657,\n",
              "  0.7151840329170227,\n",
              "  0.7036640048027039,\n",
              "  0.7096414566040039,\n",
              "  0.6946576237678528,\n",
              "  0.6971995830535889,\n",
              "  0.691353976726532,\n",
              "  0.7083635330200195,\n",
              "  0.7106416821479797,\n",
              "  0.6952186226844788,\n",
              "  0.7126836180686951,\n",
              "  0.7034306526184082,\n",
              "  0.6978104114532471,\n",
              "  0.6865243911743164,\n",
              "  0.6948328018188477,\n",
              "  0.6955001354217529,\n",
              "  0.7089384198188782,\n",
              "  0.6998353600502014,\n",
              "  0.7091882228851318,\n",
              "  0.7096698880195618,\n",
              "  0.6878733038902283,\n",
              "  0.7021265029907227,\n",
              "  0.696713924407959,\n",
              "  0.7070116996765137,\n",
              "  0.714728832244873,\n",
              "  0.705336332321167,\n",
              "  0.697614848613739,\n",
              "  0.6959509253501892,\n",
              "  0.6978781223297119,\n",
              "  0.7078163027763367,\n",
              "  0.7275578379631042,\n",
              "  0.7235491275787354,\n",
              "  0.7185947895050049,\n",
              "  0.7091875076293945,\n",
              "  0.7243897318840027,\n",
              "  0.7151097655296326,\n",
              "  0.7283299565315247,\n",
              "  0.7140483260154724,\n",
              "  0.7098185420036316,\n",
              "  0.7133991122245789,\n",
              "  0.7348281741142273,\n",
              "  0.7330858111381531,\n",
              "  0.6994978785514832,\n",
              "  0.7245003581047058,\n",
              "  0.7265415787696838,\n",
              "  0.7215949892997742,\n",
              "  0.7196846604347229,\n",
              "  0.7004345059394836,\n",
              "  0.7114092707633972,\n",
              "  0.6939792633056641,\n",
              "  0.7320437431335449,\n",
              "  0.7302663922309875,\n",
              "  0.7242493033409119,\n",
              "  0.7418293952941895,\n",
              "  0.7279767990112305,\n",
              "  0.7192915081977844,\n",
              "  0.7137237191200256,\n",
              "  0.7214474678039551,\n",
              "  0.7217118740081787,\n",
              "  0.7268124222755432,\n",
              "  0.7281808853149414,\n",
              "  0.7024189829826355,\n",
              "  0.72493577003479,\n",
              "  0.7002999186515808,\n",
              "  0.7057701945304871,\n",
              "  0.7199816703796387,\n",
              "  0.7038636207580566,\n",
              "  0.7067020535469055,\n",
              "  0.7084882855415344,\n",
              "  0.7079055309295654,\n",
              "  0.6987042427062988,\n",
              "  0.7046732902526855,\n",
              "  0.7110092639923096,\n",
              "  0.694708526134491,\n",
              "  0.7071151733398438,\n",
              "  0.7008588910102844,\n",
              "  0.7097632884979248,\n",
              "  0.7082362174987793,\n",
              "  0.7080002427101135,\n",
              "  0.6967975497245789,\n",
              "  0.6948379874229431,\n",
              "  0.6978814005851746,\n",
              "  0.7019104957580566,\n",
              "  0.6930665969848633,\n",
              "  0.6900045871734619,\n",
              "  0.6866090297698975,\n",
              "  0.7083216309547424,\n",
              "  0.6920804381370544,\n",
              "  0.7097683548927307,\n",
              "  0.7144983410835266,\n",
              "  0.689235508441925,\n",
              "  0.6856468319892883,\n",
              "  0.6900493502616882,\n",
              "  0.6973277926445007,\n",
              "  0.7040827870368958,\n",
              "  0.7007241249084473,\n",
              "  0.7068714499473572,\n",
              "  0.707247793674469,\n",
              "  0.7027670741081238,\n",
              "  0.7119626402854919,\n",
              "  0.6996133327484131,\n",
              "  0.7098793983459473,\n",
              "  0.6980032324790955,\n",
              "  0.7110967040061951,\n",
              "  0.7106900215148926,\n",
              "  0.7060449719429016,\n",
              "  0.704110324382782,\n",
              "  0.687753438949585,\n",
              "  0.6947863698005676,\n",
              "  0.6969226002693176,\n",
              "  0.6940364241600037,\n",
              "  0.7090970873832703,\n",
              "  0.7204582095146179,\n",
              "  0.7174543738365173,\n",
              "  0.725077211856842,\n",
              "  0.7018477320671082,\n",
              "  0.7056057453155518,\n",
              "  0.7106115818023682,\n",
              "  0.7158331274986267,\n",
              "  0.724189281463623,\n",
              "  0.7206199169158936,\n",
              "  0.7067155241966248,\n",
              "  0.6977077126502991,\n",
              "  0.7130298018455505,\n",
              "  0.7087042927742004,\n",
              "  0.7076310515403748,\n",
              "  0.7158252596855164,\n",
              "  0.6989238858222961,\n",
              "  0.7116876244544983,\n",
              "  0.7045156359672546,\n",
              "  0.7052083015441895,\n",
              "  0.7106764912605286,\n",
              "  0.7059953808784485,\n",
              "  0.7006763815879822,\n",
              "  0.6778607368469238,\n",
              "  0.6933798789978027,\n",
              "  0.6973319053649902,\n",
              "  0.6853579878807068,\n",
              "  0.7029407620429993,\n",
              "  0.6874053478240967,\n",
              "  0.7025178074836731,\n",
              "  0.699887752532959,\n",
              "  0.6777268052101135,\n",
              "  0.7041938900947571,\n",
              "  0.6863312721252441,\n",
              "  0.6823434829711914,\n",
              "  0.7020621299743652,\n",
              "  0.7046920657157898,\n",
              "  0.7022857666015625,\n",
              "  0.7120944857597351,\n",
              "  0.7027287483215332,\n",
              "  0.6984104514122009,\n",
              "  0.6755138039588928,\n",
              "  0.6938435435295105,\n",
              "  0.6983248591423035,\n",
              "  0.7165992856025696,\n",
              "  0.7063696980476379,\n",
              "  0.7141146659851074,\n",
              "  0.7137294411659241,\n",
              "  0.7275316715240479,\n",
              "  0.7023255825042725,\n",
              "  0.7139182090759277,\n",
              "  0.6973791718482971,\n",
              "  0.7062155604362488,\n",
              "  0.696936845779419,\n",
              "  0.7121996283531189,\n",
              "  0.7084028124809265,\n",
              "  0.7159824967384338,\n",
              "  0.6988816857337952],\n",
              " 'val_acc': [0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.4375,\n",
              "  0.3854166567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4583333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.4166666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.3541666567325592,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.3958333432674408,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.40625,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.34375,\n",
              "  0.3854166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4895833432674408,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.4375,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4375,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4791666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.4479166567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.46875,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5104166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.59375,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5208333134651184,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.5625,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184]}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "22079912-108c-4b9a-80b5-ad3102d666f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "230ccce8-2e7a-4cc9-ae5f-b08c79d335d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf2b34ae-0f2f-4e42-82bb-a6a82fc49bfc\", \"2Class.h5\", 16605968)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}