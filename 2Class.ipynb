{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPBPAjlhFtQfjyecUwAcqNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/2Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "fceec40e-ccf8-4116-fb75-998c967ebd5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "7e1c5623-42a1-4c78-d135-9153b3796dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-154edc41-584a-4506-b3d5-65e8cc777b85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-154edc41-584a-4506-b3d5-65e8cc777b85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-154edc41-584a-4506-b3d5-65e8cc777b85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-154edc41-584a-4506-b3d5-65e8cc777b85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "efcf96cc-2983-4a9d-d5d7-0a32d7650b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "0a92740d-ae22-4c33-b16b-1d31141d407e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "4bc8a704-1c83-4022-c8a5-88226df61513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "3dcb8b34-1702-4529-bfc9-486e2e00dd78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 711, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 711 (delta 179), reused 161 (delta 136), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (711/711), 12.37 MiB | 12.60 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-800')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '801-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-800')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '801-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-800')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '801-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-800']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='801-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-800']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='801-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-800']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='801-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "edc4a6d7-84db-40da-ea59-332d937a8b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "0e4fede6-6bfa-4dcf-9cbb-be76330a59c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "6f971584-cc19-421b-8af3-0df79e59ce59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "6fa78545-713e-44e5-f74e-408f9e91e45e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "e914e702-be40-4a01-ab08-8f1a9d351a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "c94c40e1-a84a-4e7a-ea7e-08422edb7647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fab0a67-d704-40aa-b018-732f869c0ee9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "9/9 [==============================] - 16s 317ms/step - loss: 1.1874 - acc: 0.4888 - val_loss: 0.5531 - val_acc: 0.6875\n",
            "Epoch 2/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 1.1527 - acc: 0.4981 - val_loss: 0.4818 - val_acc: 0.7969\n",
            "Epoch 3/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 1.0838 - acc: 0.4981 - val_loss: 0.6194 - val_acc: 0.6562\n",
            "Epoch 4/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 1.1961 - acc: 0.4813 - val_loss: 0.5663 - val_acc: 0.7031\n",
            "Epoch 5/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 1.0535 - acc: 0.5449 - val_loss: 0.5245 - val_acc: 0.7344\n",
            "Epoch 6/1000\n",
            "9/9 [==============================] - 3s 185ms/step - loss: 1.1424 - acc: 0.4963 - val_loss: 0.5733 - val_acc: 0.6875\n",
            "Epoch 7/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 1.1628 - acc: 0.4757 - val_loss: 0.5785 - val_acc: 0.6875\n",
            "Epoch 8/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 1.0695 - acc: 0.5337 - val_loss: 0.5916 - val_acc: 0.7188\n",
            "Epoch 9/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 1.0516 - acc: 0.5243 - val_loss: 0.4831 - val_acc: 0.7812\n",
            "Epoch 10/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 1.1485 - acc: 0.4931 - val_loss: 0.5897 - val_acc: 0.7188\n",
            "Epoch 11/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 1.1157 - acc: 0.5052 - val_loss: 0.5472 - val_acc: 0.7812\n",
            "Epoch 12/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 1.1685 - acc: 0.4981 - val_loss: 0.5631 - val_acc: 0.7656\n",
            "Epoch 13/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 1.1658 - acc: 0.4757 - val_loss: 0.5587 - val_acc: 0.7344\n",
            "Epoch 14/1000\n",
            "9/9 [==============================] - 3s 183ms/step - loss: 1.1086 - acc: 0.4906 - val_loss: 0.6054 - val_acc: 0.6406\n",
            "Epoch 15/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 1.1048 - acc: 0.4850 - val_loss: 0.5804 - val_acc: 0.7188\n",
            "Epoch 16/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 1.1327 - acc: 0.4719 - val_loss: 0.5559 - val_acc: 0.6875\n",
            "Epoch 17/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 1.1310 - acc: 0.4719 - val_loss: 0.6089 - val_acc: 0.6562\n",
            "Epoch 18/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 1.0339 - acc: 0.5206 - val_loss: 0.5578 - val_acc: 0.7188\n",
            "Epoch 19/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 1.1426 - acc: 0.4869 - val_loss: 0.5939 - val_acc: 0.6875\n",
            "Epoch 20/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 1.0329 - acc: 0.5225 - val_loss: 0.5831 - val_acc: 0.7344\n",
            "Epoch 21/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 1.0567 - acc: 0.5017 - val_loss: 0.5775 - val_acc: 0.7188\n",
            "Epoch 22/1000\n",
            "9/9 [==============================] - 3s 269ms/step - loss: 1.1113 - acc: 0.5000 - val_loss: 0.5821 - val_acc: 0.7656\n",
            "Epoch 23/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 1.1167 - acc: 0.4775 - val_loss: 0.6231 - val_acc: 0.6719\n",
            "Epoch 24/1000\n",
            "9/9 [==============================] - 3s 236ms/step - loss: 1.1404 - acc: 0.4896 - val_loss: 0.6137 - val_acc: 0.7031\n",
            "Epoch 25/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 1.0376 - acc: 0.5337 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 26/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 1.1090 - acc: 0.5000 - val_loss: 0.6009 - val_acc: 0.7188\n",
            "Epoch 27/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 1.1023 - acc: 0.4850 - val_loss: 0.5806 - val_acc: 0.7031\n",
            "Epoch 28/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 1.1872 - acc: 0.4569 - val_loss: 0.5952 - val_acc: 0.7031\n",
            "Epoch 29/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 1.0913 - acc: 0.5000 - val_loss: 0.5751 - val_acc: 0.7344\n",
            "Epoch 30/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 1.0394 - acc: 0.5262 - val_loss: 0.6587 - val_acc: 0.6406\n",
            "Epoch 31/1000\n",
            "9/9 [==============================] - 3s 183ms/step - loss: 1.1156 - acc: 0.4888 - val_loss: 0.6533 - val_acc: 0.6406\n",
            "Epoch 32/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 1.1721 - acc: 0.4775 - val_loss: 0.5727 - val_acc: 0.7188\n",
            "Epoch 33/1000\n",
            "9/9 [==============================] - 3s 282ms/step - loss: 1.0758 - acc: 0.5019 - val_loss: 0.6134 - val_acc: 0.7031\n",
            "Epoch 34/1000\n",
            "9/9 [==============================] - 3s 332ms/step - loss: 1.1001 - acc: 0.4719 - val_loss: 0.5562 - val_acc: 0.7500\n",
            "Epoch 35/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 1.0070 - acc: 0.5356 - val_loss: 0.6013 - val_acc: 0.7031\n",
            "Epoch 36/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 1.1016 - acc: 0.4888 - val_loss: 0.6169 - val_acc: 0.6875\n",
            "Epoch 37/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 1.0045 - acc: 0.5206 - val_loss: 0.5968 - val_acc: 0.6406\n",
            "Epoch 38/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 1.1214 - acc: 0.4682 - val_loss: 0.6582 - val_acc: 0.6406\n",
            "Epoch 39/1000\n",
            "9/9 [==============================] - 3s 178ms/step - loss: 1.0626 - acc: 0.4944 - val_loss: 0.6120 - val_acc: 0.7188\n",
            "Epoch 40/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 1.0483 - acc: 0.5019 - val_loss: 0.6184 - val_acc: 0.6562\n",
            "Epoch 41/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 1.0422 - acc: 0.5000 - val_loss: 0.6349 - val_acc: 0.6719\n",
            "Epoch 42/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 1.0328 - acc: 0.5056 - val_loss: 0.6108 - val_acc: 0.7188\n",
            "Epoch 43/1000\n",
            "9/9 [==============================] - 3s 267ms/step - loss: 1.0461 - acc: 0.5225 - val_loss: 0.6357 - val_acc: 0.6719\n",
            "Epoch 44/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.9890 - acc: 0.5150 - val_loss: 0.6315 - val_acc: 0.6719\n",
            "Epoch 45/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.9881 - acc: 0.5393 - val_loss: 0.6438 - val_acc: 0.6562\n",
            "Epoch 46/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.9725 - acc: 0.5356 - val_loss: 0.5592 - val_acc: 0.7188\n",
            "Epoch 47/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 1.0643 - acc: 0.5075 - val_loss: 0.6072 - val_acc: 0.6875\n",
            "Epoch 48/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 1.1170 - acc: 0.4948 - val_loss: 0.6120 - val_acc: 0.6406\n",
            "Epoch 49/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 1.0305 - acc: 0.5094 - val_loss: 0.6291 - val_acc: 0.6562\n",
            "Epoch 50/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.9589 - acc: 0.5469 - val_loss: 0.6658 - val_acc: 0.5938\n",
            "Epoch 51/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 1.0512 - acc: 0.5262 - val_loss: 0.6337 - val_acc: 0.6562\n",
            "Epoch 52/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 1.0024 - acc: 0.5094 - val_loss: 0.6017 - val_acc: 0.6875\n",
            "Epoch 53/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 1.0073 - acc: 0.5206 - val_loss: 0.6021 - val_acc: 0.6875\n",
            "Epoch 54/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 1.0361 - acc: 0.5037 - val_loss: 0.5979 - val_acc: 0.7031\n",
            "Epoch 55/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 1.0131 - acc: 0.5169 - val_loss: 0.6575 - val_acc: 0.6250\n",
            "Epoch 56/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.9985 - acc: 0.5281 - val_loss: 0.6717 - val_acc: 0.6094\n",
            "Epoch 57/1000\n",
            "9/9 [==============================] - 3s 233ms/step - loss: 1.0077 - acc: 0.5191 - val_loss: 0.6725 - val_acc: 0.6406\n",
            "Epoch 58/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 1.0210 - acc: 0.5037 - val_loss: 0.6429 - val_acc: 0.6250\n",
            "Epoch 59/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.9932 - acc: 0.5262 - val_loss: 0.6305 - val_acc: 0.6875\n",
            "Epoch 60/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 1.0069 - acc: 0.5243 - val_loss: 0.6529 - val_acc: 0.6562\n",
            "Epoch 61/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 1.0213 - acc: 0.4869 - val_loss: 0.6083 - val_acc: 0.6562\n",
            "Epoch 62/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 1.0082 - acc: 0.5019 - val_loss: 0.6148 - val_acc: 0.6719\n",
            "Epoch 63/1000\n",
            "9/9 [==============================] - 3s 227ms/step - loss: 1.0071 - acc: 0.4983 - val_loss: 0.6623 - val_acc: 0.6094\n",
            "Epoch 64/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 1.0066 - acc: 0.4963 - val_loss: 0.6942 - val_acc: 0.5781\n",
            "Epoch 65/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.9407 - acc: 0.5581 - val_loss: 0.6301 - val_acc: 0.6250\n",
            "Epoch 66/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.9998 - acc: 0.5243 - val_loss: 0.6030 - val_acc: 0.6562\n",
            "Epoch 67/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 1.0156 - acc: 0.5131 - val_loss: 0.6171 - val_acc: 0.6562\n",
            "Epoch 68/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.9985 - acc: 0.5019 - val_loss: 0.6028 - val_acc: 0.7031\n",
            "Epoch 69/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 1.0035 - acc: 0.5112 - val_loss: 0.6172 - val_acc: 0.6406\n",
            "Epoch 70/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.9858 - acc: 0.5337 - val_loss: 0.6433 - val_acc: 0.6719\n",
            "Epoch 71/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.9863 - acc: 0.5225 - val_loss: 0.6706 - val_acc: 0.6250\n",
            "Epoch 72/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.9806 - acc: 0.5524 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 73/1000\n",
            "9/9 [==============================] - 3s 179ms/step - loss: 1.0011 - acc: 0.5187 - val_loss: 0.6762 - val_acc: 0.6250\n",
            "Epoch 74/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 1.0219 - acc: 0.4794 - val_loss: 0.6184 - val_acc: 0.6719\n",
            "Epoch 75/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 1.0498 - acc: 0.4831 - val_loss: 0.6852 - val_acc: 0.5625\n",
            "Epoch 76/1000\n",
            "9/9 [==============================] - 3s 182ms/step - loss: 1.0751 - acc: 0.5037 - val_loss: 0.6463 - val_acc: 0.6406\n",
            "Epoch 77/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.8886 - acc: 0.5468 - val_loss: 0.6598 - val_acc: 0.6406\n",
            "Epoch 78/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.9129 - acc: 0.5449 - val_loss: 0.6441 - val_acc: 0.6406\n",
            "Epoch 79/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.9736 - acc: 0.5300 - val_loss: 0.7025 - val_acc: 0.6406\n",
            "Epoch 80/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.9796 - acc: 0.5225 - val_loss: 0.6794 - val_acc: 0.6562\n",
            "Epoch 81/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.9388 - acc: 0.5337 - val_loss: 0.6832 - val_acc: 0.5625\n",
            "Epoch 82/1000\n",
            "9/9 [==============================] - 3s 179ms/step - loss: 1.0126 - acc: 0.4925 - val_loss: 0.6792 - val_acc: 0.6094\n",
            "Epoch 83/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.9766 - acc: 0.4965 - val_loss: 0.7126 - val_acc: 0.6250\n",
            "Epoch 84/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.9010 - acc: 0.5562 - val_loss: 0.7334 - val_acc: 0.5469\n",
            "Epoch 85/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.9344 - acc: 0.5243 - val_loss: 0.6541 - val_acc: 0.6406\n",
            "Epoch 86/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.9841 - acc: 0.5318 - val_loss: 0.7049 - val_acc: 0.6250\n",
            "Epoch 87/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.9401 - acc: 0.5524 - val_loss: 0.7002 - val_acc: 0.6094\n",
            "Epoch 88/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.9528 - acc: 0.5412 - val_loss: 0.6826 - val_acc: 0.6719\n",
            "Epoch 89/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.9724 - acc: 0.5295 - val_loss: 0.6883 - val_acc: 0.6250\n",
            "Epoch 90/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.9680 - acc: 0.5112 - val_loss: 0.6839 - val_acc: 0.6406\n",
            "Epoch 91/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.9311 - acc: 0.5281 - val_loss: 0.7066 - val_acc: 0.5781\n",
            "Epoch 92/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.9428 - acc: 0.5131 - val_loss: 0.6682 - val_acc: 0.6250\n",
            "Epoch 93/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.9577 - acc: 0.5208 - val_loss: 0.6726 - val_acc: 0.5938\n",
            "Epoch 94/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.9620 - acc: 0.5449 - val_loss: 0.6905 - val_acc: 0.6094\n",
            "Epoch 95/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.8999 - acc: 0.5434 - val_loss: 0.6605 - val_acc: 0.6406\n",
            "Epoch 96/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.9652 - acc: 0.5393 - val_loss: 0.6219 - val_acc: 0.6562\n",
            "Epoch 97/1000\n",
            "9/9 [==============================] - 3s 271ms/step - loss: 0.9320 - acc: 0.5524 - val_loss: 0.7203 - val_acc: 0.5781\n",
            "Epoch 98/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.9630 - acc: 0.5356 - val_loss: 0.7395 - val_acc: 0.5469\n",
            "Epoch 99/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.9639 - acc: 0.5431 - val_loss: 0.6742 - val_acc: 0.6094\n",
            "Epoch 100/1000\n",
            "9/9 [==============================] - 3s 265ms/step - loss: 0.9665 - acc: 0.5150 - val_loss: 0.7146 - val_acc: 0.5469\n",
            "Epoch 101/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.9223 - acc: 0.5487 - val_loss: 0.6806 - val_acc: 0.5781\n",
            "Epoch 102/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.9251 - acc: 0.5150 - val_loss: 0.7133 - val_acc: 0.6250\n",
            "Epoch 103/1000\n",
            "9/9 [==============================] - 3s 234ms/step - loss: 0.9871 - acc: 0.5312 - val_loss: 0.6959 - val_acc: 0.5938\n",
            "Epoch 104/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.9319 - acc: 0.5431 - val_loss: 0.7523 - val_acc: 0.5625\n",
            "Epoch 105/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.9100 - acc: 0.5094 - val_loss: 0.7139 - val_acc: 0.5469\n",
            "Epoch 106/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.9268 - acc: 0.5562 - val_loss: 0.6849 - val_acc: 0.6562\n",
            "Epoch 107/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.9170 - acc: 0.5318 - val_loss: 0.7115 - val_acc: 0.5469\n",
            "Epoch 108/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.9084 - acc: 0.5618 - val_loss: 0.6738 - val_acc: 0.6406\n",
            "Epoch 109/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8868 - acc: 0.5543 - val_loss: 0.7573 - val_acc: 0.5781\n",
            "Epoch 110/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.8920 - acc: 0.5417 - val_loss: 0.7560 - val_acc: 0.5312\n",
            "Epoch 111/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.9041 - acc: 0.5693 - val_loss: 0.7513 - val_acc: 0.5156\n",
            "Epoch 112/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.8574 - acc: 0.5694 - val_loss: 0.6918 - val_acc: 0.5469\n",
            "Epoch 113/1000\n",
            "9/9 [==============================] - 3s 175ms/step - loss: 0.9024 - acc: 0.5543 - val_loss: 0.7657 - val_acc: 0.5156\n",
            "Epoch 114/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.9560 - acc: 0.5150 - val_loss: 0.6712 - val_acc: 0.6250\n",
            "Epoch 115/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.9499 - acc: 0.5393 - val_loss: 0.7660 - val_acc: 0.5312\n",
            "Epoch 116/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.9358 - acc: 0.5300 - val_loss: 0.7371 - val_acc: 0.5156\n",
            "Epoch 117/1000\n",
            "9/9 [==============================] - 3s 248ms/step - loss: 0.8637 - acc: 0.5637 - val_loss: 0.7395 - val_acc: 0.5156\n",
            "Epoch 118/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.8895 - acc: 0.5243 - val_loss: 0.7451 - val_acc: 0.5625\n",
            "Epoch 119/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.9006 - acc: 0.5506 - val_loss: 0.6970 - val_acc: 0.6250\n",
            "Epoch 120/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.9324 - acc: 0.5449 - val_loss: 0.7164 - val_acc: 0.5781\n",
            "Epoch 121/1000\n",
            "9/9 [==============================] - 3s 172ms/step - loss: 0.9415 - acc: 0.5262 - val_loss: 0.7489 - val_acc: 0.5938\n",
            "Epoch 122/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.8955 - acc: 0.5365 - val_loss: 0.7259 - val_acc: 0.5781\n",
            "Epoch 123/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.9509 - acc: 0.5281 - val_loss: 0.6921 - val_acc: 0.5625\n",
            "Epoch 124/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.9124 - acc: 0.5375 - val_loss: 0.7773 - val_acc: 0.4844\n",
            "Epoch 125/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.9142 - acc: 0.5562 - val_loss: 0.7539 - val_acc: 0.5156\n",
            "Epoch 126/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.8449 - acc: 0.5749 - val_loss: 0.6946 - val_acc: 0.5938\n",
            "Epoch 127/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.9129 - acc: 0.5412 - val_loss: 0.7390 - val_acc: 0.5000\n",
            "Epoch 128/1000\n",
            "9/9 [==============================] - 3s 267ms/step - loss: 0.9736 - acc: 0.5225 - val_loss: 0.6852 - val_acc: 0.5781\n",
            "Epoch 129/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.9165 - acc: 0.5524 - val_loss: 0.7591 - val_acc: 0.4688\n",
            "Epoch 130/1000\n",
            "9/9 [==============================] - 3s 175ms/step - loss: 0.9325 - acc: 0.5356 - val_loss: 0.6284 - val_acc: 0.6250\n",
            "Epoch 131/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.9202 - acc: 0.5581 - val_loss: 0.7063 - val_acc: 0.5938\n",
            "Epoch 132/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.8895 - acc: 0.5337 - val_loss: 0.7225 - val_acc: 0.5000\n",
            "Epoch 133/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.8910 - acc: 0.5693 - val_loss: 0.7636 - val_acc: 0.5156\n",
            "Epoch 134/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.8719 - acc: 0.5524 - val_loss: 0.7285 - val_acc: 0.6094\n",
            "Epoch 135/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.8334 - acc: 0.5730 - val_loss: 0.7824 - val_acc: 0.5469\n",
            "Epoch 136/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.8975 - acc: 0.5637 - val_loss: 0.7597 - val_acc: 0.4844\n",
            "Epoch 137/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.8810 - acc: 0.5431 - val_loss: 0.7869 - val_acc: 0.5312\n",
            "Epoch 138/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.8107 - acc: 0.5993 - val_loss: 0.7530 - val_acc: 0.5625\n",
            "Epoch 139/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.8759 - acc: 0.5655 - val_loss: 0.7429 - val_acc: 0.4844\n",
            "Epoch 140/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.8927 - acc: 0.5581 - val_loss: 0.7818 - val_acc: 0.5156\n",
            "Epoch 141/1000\n",
            "9/9 [==============================] - 3s 267ms/step - loss: 0.9054 - acc: 0.5487 - val_loss: 0.7283 - val_acc: 0.5000\n",
            "Epoch 142/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.8982 - acc: 0.5693 - val_loss: 0.7635 - val_acc: 0.4375\n",
            "Epoch 143/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.8688 - acc: 0.5637 - val_loss: 0.7532 - val_acc: 0.4844\n",
            "Epoch 144/1000\n",
            "9/9 [==============================] - 3s 182ms/step - loss: 0.9159 - acc: 0.5468 - val_loss: 0.6859 - val_acc: 0.5000\n",
            "Epoch 145/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.9257 - acc: 0.5225 - val_loss: 0.7201 - val_acc: 0.5000\n",
            "Epoch 146/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.9010 - acc: 0.5393 - val_loss: 0.7453 - val_acc: 0.5000\n",
            "Epoch 147/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.8723 - acc: 0.5599 - val_loss: 0.8291 - val_acc: 0.4219\n",
            "Epoch 148/1000\n",
            "9/9 [==============================] - 3s 183ms/step - loss: 0.8279 - acc: 0.6161 - val_loss: 0.7405 - val_acc: 0.4688\n",
            "Epoch 149/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.8524 - acc: 0.5660 - val_loss: 0.7541 - val_acc: 0.5000\n",
            "Epoch 150/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.9615 - acc: 0.5056 - val_loss: 0.7227 - val_acc: 0.5000\n",
            "Epoch 151/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.8427 - acc: 0.5936 - val_loss: 0.7441 - val_acc: 0.4688\n",
            "Epoch 152/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.8758 - acc: 0.5262 - val_loss: 0.7582 - val_acc: 0.4844\n",
            "Epoch 153/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.8677 - acc: 0.5693 - val_loss: 0.7672 - val_acc: 0.5156\n",
            "Epoch 154/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.8130 - acc: 0.6250 - val_loss: 0.7855 - val_acc: 0.4531\n",
            "Epoch 155/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.8395 - acc: 0.5637 - val_loss: 0.7237 - val_acc: 0.5156\n",
            "Epoch 156/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.8975 - acc: 0.5506 - val_loss: 0.7544 - val_acc: 0.4844\n",
            "Epoch 157/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.9139 - acc: 0.5431 - val_loss: 0.8062 - val_acc: 0.4062\n",
            "Epoch 158/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.8173 - acc: 0.5918 - val_loss: 0.7537 - val_acc: 0.5000\n",
            "Epoch 159/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8589 - acc: 0.5674 - val_loss: 0.7754 - val_acc: 0.4375\n",
            "Epoch 160/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.9237 - acc: 0.5431 - val_loss: 0.7066 - val_acc: 0.5469\n",
            "Epoch 161/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.8439 - acc: 0.5449 - val_loss: 0.7823 - val_acc: 0.4062\n",
            "Epoch 162/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.8617 - acc: 0.5393 - val_loss: 0.7430 - val_acc: 0.5000\n",
            "Epoch 163/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7867 - acc: 0.5918 - val_loss: 0.7712 - val_acc: 0.4844\n",
            "Epoch 164/1000\n",
            "9/9 [==============================] - 3s 174ms/step - loss: 0.8684 - acc: 0.5449 - val_loss: 0.7995 - val_acc: 0.4375\n",
            "Epoch 165/1000\n",
            "9/9 [==============================] - 3s 271ms/step - loss: 0.8021 - acc: 0.5993 - val_loss: 0.7946 - val_acc: 0.4531\n",
            "Epoch 166/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.8870 - acc: 0.5393 - val_loss: 0.8053 - val_acc: 0.4531\n",
            "Epoch 167/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8596 - acc: 0.5730 - val_loss: 0.8363 - val_acc: 0.3906\n",
            "Epoch 168/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.8049 - acc: 0.5936 - val_loss: 0.6990 - val_acc: 0.5469\n",
            "Epoch 169/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.8563 - acc: 0.5674 - val_loss: 0.7933 - val_acc: 0.4219\n",
            "Epoch 170/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.8797 - acc: 0.5449 - val_loss: 0.7583 - val_acc: 0.4531\n",
            "Epoch 171/1000\n",
            "9/9 [==============================] - 3s 190ms/step - loss: 0.8573 - acc: 0.5599 - val_loss: 0.7639 - val_acc: 0.4375\n",
            "Epoch 172/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.8408 - acc: 0.5562 - val_loss: 0.7871 - val_acc: 0.4531\n",
            "Epoch 173/1000\n",
            "9/9 [==============================] - 3s 185ms/step - loss: 0.8830 - acc: 0.5562 - val_loss: 0.7573 - val_acc: 0.4844\n",
            "Epoch 174/1000\n",
            "9/9 [==============================] - 3s 184ms/step - loss: 0.7964 - acc: 0.5674 - val_loss: 0.7229 - val_acc: 0.5469\n",
            "Epoch 175/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.9104 - acc: 0.5347 - val_loss: 0.8139 - val_acc: 0.4219\n",
            "Epoch 176/1000\n",
            "9/9 [==============================] - 3s 190ms/step - loss: 0.8166 - acc: 0.5749 - val_loss: 0.6698 - val_acc: 0.5625\n",
            "Epoch 177/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.8486 - acc: 0.5581 - val_loss: 0.7131 - val_acc: 0.5000\n",
            "Epoch 178/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.8286 - acc: 0.5637 - val_loss: 0.8251 - val_acc: 0.3438\n",
            "Epoch 179/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.8939 - acc: 0.5618 - val_loss: 0.8041 - val_acc: 0.4219\n",
            "Epoch 180/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8461 - acc: 0.5693 - val_loss: 0.7902 - val_acc: 0.4219\n",
            "Epoch 181/1000\n",
            "9/9 [==============================] - 3s 269ms/step - loss: 0.8527 - acc: 0.5787 - val_loss: 0.7931 - val_acc: 0.4688\n",
            "Epoch 182/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.9131 - acc: 0.5262 - val_loss: 0.7624 - val_acc: 0.4062\n",
            "Epoch 183/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.8805 - acc: 0.5417 - val_loss: 0.7709 - val_acc: 0.4531\n",
            "Epoch 184/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.8791 - acc: 0.5468 - val_loss: 0.7430 - val_acc: 0.4844\n",
            "Epoch 185/1000\n",
            "9/9 [==============================] - 3s 232ms/step - loss: 0.8847 - acc: 0.5521 - val_loss: 0.7568 - val_acc: 0.4531\n",
            "Epoch 186/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.8120 - acc: 0.5833 - val_loss: 0.7066 - val_acc: 0.5156\n",
            "Epoch 187/1000\n",
            "9/9 [==============================] - 3s 180ms/step - loss: 0.8393 - acc: 0.5843 - val_loss: 0.8342 - val_acc: 0.3906\n",
            "Epoch 188/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.8465 - acc: 0.5412 - val_loss: 0.7813 - val_acc: 0.4219\n",
            "Epoch 189/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7375 - acc: 0.5918 - val_loss: 0.7272 - val_acc: 0.4688\n",
            "Epoch 190/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.8409 - acc: 0.5880 - val_loss: 0.7862 - val_acc: 0.4375\n",
            "Epoch 191/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.8156 - acc: 0.5749 - val_loss: 0.8023 - val_acc: 0.3750\n",
            "Epoch 192/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.8641 - acc: 0.5637 - val_loss: 0.8016 - val_acc: 0.4219\n",
            "Epoch 193/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.8571 - acc: 0.5677 - val_loss: 0.8084 - val_acc: 0.3906\n",
            "Epoch 194/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7856 - acc: 0.5843 - val_loss: 0.7398 - val_acc: 0.4844\n",
            "Epoch 195/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.7983 - acc: 0.6161 - val_loss: 0.7595 - val_acc: 0.4375\n",
            "Epoch 196/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.8417 - acc: 0.5538 - val_loss: 0.7445 - val_acc: 0.4844\n",
            "Epoch 197/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.8335 - acc: 0.5768 - val_loss: 0.7975 - val_acc: 0.3750\n",
            "Epoch 198/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.8194 - acc: 0.6011 - val_loss: 0.7845 - val_acc: 0.3750\n",
            "Epoch 199/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8062 - acc: 0.5562 - val_loss: 0.7635 - val_acc: 0.4531\n",
            "Epoch 200/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.8527 - acc: 0.5749 - val_loss: 0.7713 - val_acc: 0.4219\n",
            "Epoch 201/1000\n",
            "9/9 [==============================] - 3s 232ms/step - loss: 0.8050 - acc: 0.6030 - val_loss: 0.7738 - val_acc: 0.4688\n",
            "Epoch 202/1000\n",
            "9/9 [==============================] - 3s 266ms/step - loss: 0.7916 - acc: 0.5805 - val_loss: 0.8228 - val_acc: 0.4062\n",
            "Epoch 203/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7698 - acc: 0.6311 - val_loss: 0.8089 - val_acc: 0.4375\n",
            "Epoch 204/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.8118 - acc: 0.5730 - val_loss: 0.8255 - val_acc: 0.4062\n",
            "Epoch 205/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.8176 - acc: 0.5787 - val_loss: 0.7809 - val_acc: 0.4531\n",
            "Epoch 206/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.8533 - acc: 0.5599 - val_loss: 0.8308 - val_acc: 0.3594\n",
            "Epoch 207/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.8287 - acc: 0.5824 - val_loss: 0.8179 - val_acc: 0.4062\n",
            "Epoch 208/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.8195 - acc: 0.5730 - val_loss: 0.7817 - val_acc: 0.4688\n",
            "Epoch 209/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.8542 - acc: 0.5693 - val_loss: 0.7927 - val_acc: 0.3906\n",
            "Epoch 210/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.8312 - acc: 0.5637 - val_loss: 0.7359 - val_acc: 0.4844\n",
            "Epoch 211/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7714 - acc: 0.6105 - val_loss: 0.7833 - val_acc: 0.3906\n",
            "Epoch 212/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.8168 - acc: 0.5764 - val_loss: 0.7886 - val_acc: 0.3906\n",
            "Epoch 213/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.8997 - acc: 0.5506 - val_loss: 0.7604 - val_acc: 0.4375\n",
            "Epoch 214/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.8248 - acc: 0.5749 - val_loss: 0.7692 - val_acc: 0.4219\n",
            "Epoch 215/1000\n",
            "9/9 [==============================] - 3s 266ms/step - loss: 0.7841 - acc: 0.5674 - val_loss: 0.7749 - val_acc: 0.4219\n",
            "Epoch 216/1000\n",
            "9/9 [==============================] - 3s 244ms/step - loss: 0.8183 - acc: 0.5538 - val_loss: 0.8008 - val_acc: 0.4219\n",
            "Epoch 217/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 0.7987 - acc: 0.5918 - val_loss: 0.7895 - val_acc: 0.4219\n",
            "Epoch 218/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.8463 - acc: 0.5730 - val_loss: 0.7649 - val_acc: 0.4375\n",
            "Epoch 219/1000\n",
            "9/9 [==============================] - 3s 185ms/step - loss: 0.8740 - acc: 0.5674 - val_loss: 0.7929 - val_acc: 0.3906\n",
            "Epoch 220/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7908 - acc: 0.6124 - val_loss: 0.8035 - val_acc: 0.4375\n",
            "Epoch 221/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.8148 - acc: 0.5749 - val_loss: 0.7955 - val_acc: 0.4062\n",
            "Epoch 222/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7935 - acc: 0.5955 - val_loss: 0.7548 - val_acc: 0.4688\n",
            "Epoch 223/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.8316 - acc: 0.5824 - val_loss: 0.8208 - val_acc: 0.3750\n",
            "Epoch 224/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.8429 - acc: 0.5599 - val_loss: 0.7465 - val_acc: 0.4375\n",
            "Epoch 225/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.8300 - acc: 0.5562 - val_loss: 0.7572 - val_acc: 0.5000\n",
            "Epoch 226/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.8494 - acc: 0.5337 - val_loss: 0.7988 - val_acc: 0.4219\n",
            "Epoch 227/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.8088 - acc: 0.5768 - val_loss: 0.7849 - val_acc: 0.4375\n",
            "Epoch 228/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.8731 - acc: 0.5618 - val_loss: 0.7779 - val_acc: 0.3906\n",
            "Epoch 229/1000\n",
            "9/9 [==============================] - 3s 236ms/step - loss: 0.7691 - acc: 0.5938 - val_loss: 0.7872 - val_acc: 0.4219\n",
            "Epoch 230/1000\n",
            "9/9 [==============================] - 3s 242ms/step - loss: 0.8042 - acc: 0.5833 - val_loss: 0.8721 - val_acc: 0.3438\n",
            "Epoch 231/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.8335 - acc: 0.5543 - val_loss: 0.7868 - val_acc: 0.4531\n",
            "Epoch 232/1000\n",
            "9/9 [==============================] - 3s 190ms/step - loss: 0.7566 - acc: 0.5936 - val_loss: 0.7948 - val_acc: 0.4219\n",
            "Epoch 233/1000\n",
            "9/9 [==============================] - 3s 289ms/step - loss: 0.7914 - acc: 0.5936 - val_loss: 0.7493 - val_acc: 0.4688\n",
            "Epoch 234/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.8034 - acc: 0.5730 - val_loss: 0.8003 - val_acc: 0.4062\n",
            "Epoch 235/1000\n",
            "9/9 [==============================] - 3s 280ms/step - loss: 0.8485 - acc: 0.5693 - val_loss: 0.7660 - val_acc: 0.4219\n",
            "Epoch 236/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7591 - acc: 0.6011 - val_loss: 0.7872 - val_acc: 0.4219\n",
            "Epoch 237/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.7683 - acc: 0.6161 - val_loss: 0.7899 - val_acc: 0.3750\n",
            "Epoch 238/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.8062 - acc: 0.6049 - val_loss: 0.7414 - val_acc: 0.4688\n",
            "Epoch 239/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.8600 - acc: 0.5655 - val_loss: 0.8148 - val_acc: 0.3438\n",
            "Epoch 240/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.8146 - acc: 0.5655 - val_loss: 0.7475 - val_acc: 0.4844\n",
            "Epoch 241/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.8254 - acc: 0.6086 - val_loss: 0.7482 - val_acc: 0.4688\n",
            "Epoch 242/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.7964 - acc: 0.5787 - val_loss: 0.7796 - val_acc: 0.4219\n",
            "Epoch 243/1000\n",
            "9/9 [==============================] - 3s 241ms/step - loss: 0.7750 - acc: 0.6128 - val_loss: 0.7833 - val_acc: 0.4531\n",
            "Epoch 244/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.8049 - acc: 0.5955 - val_loss: 0.7492 - val_acc: 0.4844\n",
            "Epoch 245/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7381 - acc: 0.6142 - val_loss: 0.7759 - val_acc: 0.4219\n",
            "Epoch 246/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.8417 - acc: 0.5747 - val_loss: 0.7807 - val_acc: 0.4375\n",
            "Epoch 247/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7745 - acc: 0.6086 - val_loss: 0.6947 - val_acc: 0.5000\n",
            "Epoch 248/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7732 - acc: 0.5936 - val_loss: 0.8127 - val_acc: 0.3906\n",
            "Epoch 249/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7847 - acc: 0.6161 - val_loss: 0.7991 - val_acc: 0.4375\n",
            "Epoch 250/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.7712 - acc: 0.6011 - val_loss: 0.7646 - val_acc: 0.4688\n",
            "Epoch 251/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7863 - acc: 0.5768 - val_loss: 0.8582 - val_acc: 0.3438\n",
            "Epoch 252/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.8198 - acc: 0.5936 - val_loss: 0.7718 - val_acc: 0.4688\n",
            "Epoch 253/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.7383 - acc: 0.6124 - val_loss: 0.8269 - val_acc: 0.3594\n",
            "Epoch 254/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7929 - acc: 0.5824 - val_loss: 0.7922 - val_acc: 0.3750\n",
            "Epoch 255/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.8351 - acc: 0.5764 - val_loss: 0.7804 - val_acc: 0.4219\n",
            "Epoch 256/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.8025 - acc: 0.5920 - val_loss: 0.8038 - val_acc: 0.4062\n",
            "Epoch 257/1000\n",
            "9/9 [==============================] - 3s 184ms/step - loss: 0.8403 - acc: 0.5768 - val_loss: 0.8124 - val_acc: 0.3750\n",
            "Epoch 258/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.8148 - acc: 0.5824 - val_loss: 0.8188 - val_acc: 0.4062\n",
            "Epoch 259/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.7977 - acc: 0.5955 - val_loss: 0.7516 - val_acc: 0.4531\n",
            "Epoch 260/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7788 - acc: 0.5955 - val_loss: 0.8046 - val_acc: 0.4062\n",
            "Epoch 261/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.8189 - acc: 0.5805 - val_loss: 0.8223 - val_acc: 0.3438\n",
            "Epoch 262/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.7892 - acc: 0.5918 - val_loss: 0.8541 - val_acc: 0.3594\n",
            "Epoch 263/1000\n",
            "9/9 [==============================] - 3s 280ms/step - loss: 0.8009 - acc: 0.5993 - val_loss: 0.7328 - val_acc: 0.4844\n",
            "Epoch 264/1000\n",
            "9/9 [==============================] - 3s 184ms/step - loss: 0.7483 - acc: 0.6311 - val_loss: 0.7833 - val_acc: 0.4531\n",
            "Epoch 265/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.8117 - acc: 0.6273 - val_loss: 0.7667 - val_acc: 0.4375\n",
            "Epoch 266/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.8256 - acc: 0.5618 - val_loss: 0.8241 - val_acc: 0.3438\n",
            "Epoch 267/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.8378 - acc: 0.5918 - val_loss: 0.7730 - val_acc: 0.4688\n",
            "Epoch 268/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.7679 - acc: 0.5899 - val_loss: 0.8145 - val_acc: 0.4062\n",
            "Epoch 269/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.8089 - acc: 0.5674 - val_loss: 0.8003 - val_acc: 0.3906\n",
            "Epoch 270/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7656 - acc: 0.6105 - val_loss: 0.7747 - val_acc: 0.4375\n",
            "Epoch 271/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.8157 - acc: 0.6128 - val_loss: 0.7807 - val_acc: 0.3906\n",
            "Epoch 272/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.8039 - acc: 0.5787 - val_loss: 0.8099 - val_acc: 0.3750\n",
            "Epoch 273/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.8044 - acc: 0.5993 - val_loss: 0.7895 - val_acc: 0.4219\n",
            "Epoch 274/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.8244 - acc: 0.5880 - val_loss: 0.7953 - val_acc: 0.4219\n",
            "Epoch 275/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7559 - acc: 0.6049 - val_loss: 0.7815 - val_acc: 0.4062\n",
            "Epoch 276/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7898 - acc: 0.6030 - val_loss: 0.7831 - val_acc: 0.4062\n",
            "Epoch 277/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.8082 - acc: 0.5955 - val_loss: 0.7842 - val_acc: 0.4219\n",
            "Epoch 278/1000\n",
            "9/9 [==============================] - 3s 185ms/step - loss: 0.7419 - acc: 0.6180 - val_loss: 0.7798 - val_acc: 0.4062\n",
            "Epoch 279/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.7756 - acc: 0.5880 - val_loss: 0.7675 - val_acc: 0.4375\n",
            "Epoch 280/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7486 - acc: 0.6142 - val_loss: 0.7856 - val_acc: 0.4375\n",
            "Epoch 281/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.8231 - acc: 0.5573 - val_loss: 0.8125 - val_acc: 0.3750\n",
            "Epoch 282/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.8013 - acc: 0.6011 - val_loss: 0.7647 - val_acc: 0.4219\n",
            "Epoch 283/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.7401 - acc: 0.6236 - val_loss: 0.7679 - val_acc: 0.4219\n",
            "Epoch 284/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7997 - acc: 0.5861 - val_loss: 0.7944 - val_acc: 0.4062\n",
            "Epoch 285/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7844 - acc: 0.6067 - val_loss: 0.7588 - val_acc: 0.4531\n",
            "Epoch 286/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.8014 - acc: 0.6011 - val_loss: 0.7096 - val_acc: 0.4844\n",
            "Epoch 287/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.8630 - acc: 0.5749 - val_loss: 0.7765 - val_acc: 0.4062\n",
            "Epoch 288/1000\n",
            "9/9 [==============================] - 3s 179ms/step - loss: 0.8079 - acc: 0.5955 - val_loss: 0.7417 - val_acc: 0.5312\n",
            "Epoch 289/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.8233 - acc: 0.5618 - val_loss: 0.7440 - val_acc: 0.4688\n",
            "Epoch 290/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.7591 - acc: 0.6180 - val_loss: 0.7567 - val_acc: 0.4375\n",
            "Epoch 291/1000\n",
            "9/9 [==============================] - 3s 234ms/step - loss: 0.7534 - acc: 0.6076 - val_loss: 0.7637 - val_acc: 0.3906\n",
            "Epoch 292/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.7718 - acc: 0.6067 - val_loss: 0.7981 - val_acc: 0.3750\n",
            "Epoch 293/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.7752 - acc: 0.6161 - val_loss: 0.8151 - val_acc: 0.4062\n",
            "Epoch 294/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7274 - acc: 0.6311 - val_loss: 0.7493 - val_acc: 0.4531\n",
            "Epoch 295/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7519 - acc: 0.6367 - val_loss: 0.8075 - val_acc: 0.4062\n",
            "Epoch 296/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7325 - acc: 0.6161 - val_loss: 0.8234 - val_acc: 0.4062\n",
            "Epoch 297/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.7792 - acc: 0.5824 - val_loss: 0.8253 - val_acc: 0.3750\n",
            "Epoch 298/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7622 - acc: 0.6011 - val_loss: 0.7340 - val_acc: 0.4688\n",
            "Epoch 299/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7988 - acc: 0.5730 - val_loss: 0.8079 - val_acc: 0.3906\n",
            "Epoch 300/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7815 - acc: 0.5880 - val_loss: 0.8137 - val_acc: 0.3906\n",
            "Epoch 301/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.8308 - acc: 0.5787 - val_loss: 0.7431 - val_acc: 0.4219\n",
            "Epoch 302/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7154 - acc: 0.6348 - val_loss: 0.7585 - val_acc: 0.4375\n",
            "Epoch 303/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7714 - acc: 0.6011 - val_loss: 0.7801 - val_acc: 0.4219\n",
            "Epoch 304/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7671 - acc: 0.5936 - val_loss: 0.8005 - val_acc: 0.3750\n",
            "Epoch 305/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7443 - acc: 0.6142 - val_loss: 0.7701 - val_acc: 0.4219\n",
            "Epoch 306/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7874 - acc: 0.5880 - val_loss: 0.7753 - val_acc: 0.4531\n",
            "Epoch 307/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.7645 - acc: 0.6059 - val_loss: 0.7812 - val_acc: 0.4375\n",
            "Epoch 308/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.7947 - acc: 0.5861 - val_loss: 0.7898 - val_acc: 0.4062\n",
            "Epoch 309/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.7804 - acc: 0.5899 - val_loss: 0.8153 - val_acc: 0.3750\n",
            "Epoch 310/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.7956 - acc: 0.5899 - val_loss: 0.7606 - val_acc: 0.4375\n",
            "Epoch 311/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.7688 - acc: 0.6105 - val_loss: 0.7715 - val_acc: 0.4531\n",
            "Epoch 312/1000\n",
            "9/9 [==============================] - 3s 233ms/step - loss: 0.7588 - acc: 0.6302 - val_loss: 0.7804 - val_acc: 0.4375\n",
            "Epoch 313/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.7405 - acc: 0.6161 - val_loss: 0.8137 - val_acc: 0.3750\n",
            "Epoch 314/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7512 - acc: 0.6105 - val_loss: 0.7680 - val_acc: 0.4375\n",
            "Epoch 315/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7866 - acc: 0.5805 - val_loss: 0.8181 - val_acc: 0.3906\n",
            "Epoch 316/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7597 - acc: 0.6142 - val_loss: 0.8538 - val_acc: 0.3594\n",
            "Epoch 317/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7910 - acc: 0.6086 - val_loss: 0.8215 - val_acc: 0.3594\n",
            "Epoch 318/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7215 - acc: 0.6236 - val_loss: 0.7905 - val_acc: 0.4062\n",
            "Epoch 319/1000\n",
            "9/9 [==============================] - 3s 277ms/step - loss: 0.7498 - acc: 0.6161 - val_loss: 0.8343 - val_acc: 0.3906\n",
            "Epoch 320/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7751 - acc: 0.6142 - val_loss: 0.8011 - val_acc: 0.3906\n",
            "Epoch 321/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.7254 - acc: 0.6442 - val_loss: 0.7931 - val_acc: 0.3750\n",
            "Epoch 322/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7466 - acc: 0.6404 - val_loss: 0.8029 - val_acc: 0.4219\n",
            "Epoch 323/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7779 - acc: 0.5974 - val_loss: 0.7937 - val_acc: 0.4219\n",
            "Epoch 324/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.8289 - acc: 0.5674 - val_loss: 0.8454 - val_acc: 0.3438\n",
            "Epoch 325/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7330 - acc: 0.6536 - val_loss: 0.7984 - val_acc: 0.3750\n",
            "Epoch 326/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7271 - acc: 0.6292 - val_loss: 0.7668 - val_acc: 0.4531\n",
            "Epoch 327/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 0.8036 - acc: 0.5824 - val_loss: 0.8206 - val_acc: 0.4062\n",
            "Epoch 328/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.7514 - acc: 0.6273 - val_loss: 0.7377 - val_acc: 0.4688\n",
            "Epoch 329/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7182 - acc: 0.6217 - val_loss: 0.7818 - val_acc: 0.4531\n",
            "Epoch 330/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6638 - acc: 0.6498 - val_loss: 0.7739 - val_acc: 0.4219\n",
            "Epoch 331/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.7818 - acc: 0.5955 - val_loss: 0.7884 - val_acc: 0.4531\n",
            "Epoch 332/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.8220 - acc: 0.6086 - val_loss: 0.8113 - val_acc: 0.4062\n",
            "Epoch 333/1000\n",
            "9/9 [==============================] - 3s 230ms/step - loss: 0.7633 - acc: 0.6215 - val_loss: 0.7905 - val_acc: 0.4062\n",
            "Epoch 334/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7511 - acc: 0.5936 - val_loss: 0.7183 - val_acc: 0.5000\n",
            "Epoch 335/1000\n",
            "9/9 [==============================] - 3s 240ms/step - loss: 0.8198 - acc: 0.5608 - val_loss: 0.7475 - val_acc: 0.4688\n",
            "Epoch 336/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.7925 - acc: 0.5918 - val_loss: 0.8304 - val_acc: 0.3906\n",
            "Epoch 337/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.7627 - acc: 0.6049 - val_loss: 0.7949 - val_acc: 0.4375\n",
            "Epoch 338/1000\n",
            "9/9 [==============================] - 3s 239ms/step - loss: 0.7786 - acc: 0.6111 - val_loss: 0.7974 - val_acc: 0.3906\n",
            "Epoch 339/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7656 - acc: 0.6067 - val_loss: 0.7363 - val_acc: 0.4531\n",
            "Epoch 340/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7337 - acc: 0.6049 - val_loss: 0.8494 - val_acc: 0.3750\n",
            "Epoch 341/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7442 - acc: 0.6255 - val_loss: 0.7738 - val_acc: 0.4375\n",
            "Epoch 342/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.7757 - acc: 0.6105 - val_loss: 0.7370 - val_acc: 0.5156\n",
            "Epoch 343/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.7325 - acc: 0.6217 - val_loss: 0.7511 - val_acc: 0.4531\n",
            "Epoch 344/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7460 - acc: 0.6142 - val_loss: 0.8590 - val_acc: 0.2969\n",
            "Epoch 345/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7601 - acc: 0.6161 - val_loss: 0.7757 - val_acc: 0.4062\n",
            "Epoch 346/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7489 - acc: 0.6105 - val_loss: 0.7908 - val_acc: 0.3906\n",
            "Epoch 347/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.7730 - acc: 0.6255 - val_loss: 0.7998 - val_acc: 0.4219\n",
            "Epoch 348/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7100 - acc: 0.6554 - val_loss: 0.7284 - val_acc: 0.4688\n",
            "Epoch 349/1000\n",
            "9/9 [==============================] - 3s 288ms/step - loss: 0.7237 - acc: 0.6105 - val_loss: 0.7323 - val_acc: 0.4844\n",
            "Epoch 350/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7323 - acc: 0.6124 - val_loss: 0.7404 - val_acc: 0.4531\n",
            "Epoch 351/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.7844 - acc: 0.5655 - val_loss: 0.7706 - val_acc: 0.4062\n",
            "Epoch 352/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.7180 - acc: 0.6215 - val_loss: 0.7944 - val_acc: 0.3906\n",
            "Epoch 353/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6809 - acc: 0.6498 - val_loss: 0.7432 - val_acc: 0.4844\n",
            "Epoch 354/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.7808 - acc: 0.6042 - val_loss: 0.7405 - val_acc: 0.4531\n",
            "Epoch 355/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.7506 - acc: 0.6302 - val_loss: 0.8101 - val_acc: 0.3750\n",
            "Epoch 356/1000\n",
            "9/9 [==============================] - 3s 234ms/step - loss: 0.7286 - acc: 0.6461 - val_loss: 0.8488 - val_acc: 0.3594\n",
            "Epoch 357/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7500 - acc: 0.6199 - val_loss: 0.7966 - val_acc: 0.4062\n",
            "Epoch 358/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.7398 - acc: 0.5974 - val_loss: 0.7955 - val_acc: 0.4688\n",
            "Epoch 359/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7332 - acc: 0.6124 - val_loss: 0.8117 - val_acc: 0.3594\n",
            "Epoch 360/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7365 - acc: 0.6386 - val_loss: 0.8095 - val_acc: 0.4062\n",
            "Epoch 361/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7385 - acc: 0.6348 - val_loss: 0.8443 - val_acc: 0.4062\n",
            "Epoch 362/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.7750 - acc: 0.6146 - val_loss: 0.8500 - val_acc: 0.3594\n",
            "Epoch 363/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7830 - acc: 0.5805 - val_loss: 0.7623 - val_acc: 0.4375\n",
            "Epoch 364/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.7060 - acc: 0.6217 - val_loss: 0.7988 - val_acc: 0.3906\n",
            "Epoch 365/1000\n",
            "9/9 [==============================] - 3s 240ms/step - loss: 0.7638 - acc: 0.6094 - val_loss: 0.8197 - val_acc: 0.3594\n",
            "Epoch 366/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.7094 - acc: 0.6330 - val_loss: 0.7354 - val_acc: 0.4688\n",
            "Epoch 367/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.8296 - acc: 0.5712 - val_loss: 0.7924 - val_acc: 0.4219\n",
            "Epoch 368/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7145 - acc: 0.6217 - val_loss: 0.7501 - val_acc: 0.4688\n",
            "Epoch 369/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7513 - acc: 0.6311 - val_loss: 0.7619 - val_acc: 0.4531\n",
            "Epoch 370/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7344 - acc: 0.6180 - val_loss: 0.7809 - val_acc: 0.4375\n",
            "Epoch 371/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.7603 - acc: 0.6161 - val_loss: 0.7780 - val_acc: 0.4375\n",
            "Epoch 372/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.7342 - acc: 0.6086 - val_loss: 0.8412 - val_acc: 0.4062\n",
            "Epoch 373/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.8107 - acc: 0.6030 - val_loss: 0.8197 - val_acc: 0.4062\n",
            "Epoch 374/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7517 - acc: 0.6067 - val_loss: 0.8296 - val_acc: 0.3594\n",
            "Epoch 375/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7476 - acc: 0.6199 - val_loss: 0.7795 - val_acc: 0.4531\n",
            "Epoch 376/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.7597 - acc: 0.6161 - val_loss: 0.7794 - val_acc: 0.4531\n",
            "Epoch 377/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7127 - acc: 0.6255 - val_loss: 0.7308 - val_acc: 0.4531\n",
            "Epoch 378/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.7496 - acc: 0.6217 - val_loss: 0.7161 - val_acc: 0.4688\n",
            "Epoch 379/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.7688 - acc: 0.5993 - val_loss: 0.8307 - val_acc: 0.3750\n",
            "Epoch 380/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7778 - acc: 0.6142 - val_loss: 0.7159 - val_acc: 0.5156\n",
            "Epoch 381/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.7587 - acc: 0.6161 - val_loss: 0.7421 - val_acc: 0.4844\n",
            "Epoch 382/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.7898 - acc: 0.5618 - val_loss: 0.7574 - val_acc: 0.5156\n",
            "Epoch 383/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.7176 - acc: 0.6442 - val_loss: 0.7681 - val_acc: 0.4531\n",
            "Epoch 384/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.7495 - acc: 0.6255 - val_loss: 0.7734 - val_acc: 0.4219\n",
            "Epoch 385/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7311 - acc: 0.6273 - val_loss: 0.7623 - val_acc: 0.4062\n",
            "Epoch 386/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7068 - acc: 0.6311 - val_loss: 0.7607 - val_acc: 0.4375\n",
            "Epoch 387/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.7202 - acc: 0.6273 - val_loss: 0.7568 - val_acc: 0.4844\n",
            "Epoch 388/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.7429 - acc: 0.6273 - val_loss: 0.7453 - val_acc: 0.4375\n",
            "Epoch 389/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.7242 - acc: 0.6458 - val_loss: 0.7708 - val_acc: 0.4219\n",
            "Epoch 390/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7746 - acc: 0.6030 - val_loss: 0.8392 - val_acc: 0.3750\n",
            "Epoch 391/1000\n",
            "9/9 [==============================] - 3s 266ms/step - loss: 0.7234 - acc: 0.6161 - val_loss: 0.8152 - val_acc: 0.3750\n",
            "Epoch 392/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.7350 - acc: 0.6086 - val_loss: 0.7840 - val_acc: 0.4219\n",
            "Epoch 393/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7436 - acc: 0.6311 - val_loss: 0.8122 - val_acc: 0.3906\n",
            "Epoch 394/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.8033 - acc: 0.5880 - val_loss: 0.8349 - val_acc: 0.3750\n",
            "Epoch 395/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.7241 - acc: 0.6442 - val_loss: 0.8016 - val_acc: 0.4219\n",
            "Epoch 396/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.6816 - acc: 0.6554 - val_loss: 0.7779 - val_acc: 0.4219\n",
            "Epoch 397/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7168 - acc: 0.6554 - val_loss: 0.7975 - val_acc: 0.3750\n",
            "Epoch 398/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7201 - acc: 0.6217 - val_loss: 0.8483 - val_acc: 0.3750\n",
            "Epoch 399/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.7530 - acc: 0.6386 - val_loss: 0.7746 - val_acc: 0.3750\n",
            "Epoch 400/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.6511 - acc: 0.6592 - val_loss: 0.7161 - val_acc: 0.4531\n",
            "Epoch 401/1000\n",
            "9/9 [==============================] - 3s 238ms/step - loss: 0.7433 - acc: 0.5955 - val_loss: 0.8181 - val_acc: 0.3906\n",
            "Epoch 402/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.7162 - acc: 0.6124 - val_loss: 0.7935 - val_acc: 0.4375\n",
            "Epoch 403/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7692 - acc: 0.5861 - val_loss: 0.8024 - val_acc: 0.3906\n",
            "Epoch 404/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 0.6973 - acc: 0.6573 - val_loss: 0.7673 - val_acc: 0.4531\n",
            "Epoch 405/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7764 - acc: 0.5918 - val_loss: 0.8028 - val_acc: 0.3906\n",
            "Epoch 406/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.7420 - acc: 0.6311 - val_loss: 0.7714 - val_acc: 0.4688\n",
            "Epoch 407/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.7328 - acc: 0.6124 - val_loss: 0.7820 - val_acc: 0.4375\n",
            "Epoch 408/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.7371 - acc: 0.6086 - val_loss: 0.8188 - val_acc: 0.4062\n",
            "Epoch 409/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.7118 - acc: 0.6105 - val_loss: 0.7746 - val_acc: 0.4531\n",
            "Epoch 410/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7131 - acc: 0.6142 - val_loss: 0.8290 - val_acc: 0.3594\n",
            "Epoch 411/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.7158 - acc: 0.6273 - val_loss: 0.7877 - val_acc: 0.4219\n",
            "Epoch 412/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7369 - acc: 0.6067 - val_loss: 0.8442 - val_acc: 0.3594\n",
            "Epoch 413/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.7836 - acc: 0.5903 - val_loss: 0.7582 - val_acc: 0.4531\n",
            "Epoch 414/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.7659 - acc: 0.6180 - val_loss: 0.7628 - val_acc: 0.4688\n",
            "Epoch 415/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7644 - acc: 0.6067 - val_loss: 0.7359 - val_acc: 0.5156\n",
            "Epoch 416/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6761 - acc: 0.6461 - val_loss: 0.8470 - val_acc: 0.3438\n",
            "Epoch 417/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.7252 - acc: 0.6217 - val_loss: 0.7979 - val_acc: 0.4219\n",
            "Epoch 418/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7388 - acc: 0.5974 - val_loss: 0.8254 - val_acc: 0.3750\n",
            "Epoch 419/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7477 - acc: 0.6273 - val_loss: 0.8173 - val_acc: 0.3906\n",
            "Epoch 420/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.8049 - acc: 0.5843 - val_loss: 0.7822 - val_acc: 0.4219\n",
            "Epoch 421/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7392 - acc: 0.6236 - val_loss: 0.7602 - val_acc: 0.4375\n",
            "Epoch 422/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7645 - acc: 0.6105 - val_loss: 0.8447 - val_acc: 0.3594\n",
            "Epoch 423/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7678 - acc: 0.5955 - val_loss: 0.7694 - val_acc: 0.4375\n",
            "Epoch 424/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7352 - acc: 0.6217 - val_loss: 0.8065 - val_acc: 0.4844\n",
            "Epoch 425/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7534 - acc: 0.5993 - val_loss: 0.7582 - val_acc: 0.4844\n",
            "Epoch 426/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6518 - acc: 0.6667 - val_loss: 0.8087 - val_acc: 0.4219\n",
            "Epoch 427/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.7783 - acc: 0.5972 - val_loss: 0.8682 - val_acc: 0.3594\n",
            "Epoch 428/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.7431 - acc: 0.6217 - val_loss: 0.7927 - val_acc: 0.4375\n",
            "Epoch 429/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.7046 - acc: 0.6386 - val_loss: 0.7613 - val_acc: 0.4844\n",
            "Epoch 430/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7814 - acc: 0.5843 - val_loss: 0.8814 - val_acc: 0.3438\n",
            "Epoch 431/1000\n",
            "9/9 [==============================] - 3s 243ms/step - loss: 0.6870 - acc: 0.6337 - val_loss: 0.8029 - val_acc: 0.4219\n",
            "Epoch 432/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.7721 - acc: 0.5955 - val_loss: 0.8085 - val_acc: 0.3906\n",
            "Epoch 433/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7165 - acc: 0.6461 - val_loss: 0.7594 - val_acc: 0.4688\n",
            "Epoch 434/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.7472 - acc: 0.5918 - val_loss: 0.7618 - val_acc: 0.4531\n",
            "Epoch 435/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7271 - acc: 0.6199 - val_loss: 0.8114 - val_acc: 0.4375\n",
            "Epoch 436/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.7577 - acc: 0.6233 - val_loss: 0.7617 - val_acc: 0.4844\n",
            "Epoch 437/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7135 - acc: 0.6330 - val_loss: 0.8333 - val_acc: 0.4062\n",
            "Epoch 438/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7390 - acc: 0.6124 - val_loss: 0.7602 - val_acc: 0.5000\n",
            "Epoch 439/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.7718 - acc: 0.6292 - val_loss: 0.7792 - val_acc: 0.4219\n",
            "Epoch 440/1000\n",
            "9/9 [==============================] - 3s 180ms/step - loss: 0.7094 - acc: 0.6330 - val_loss: 0.8258 - val_acc: 0.3438\n",
            "Epoch 441/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.7165 - acc: 0.6386 - val_loss: 0.7618 - val_acc: 0.4219\n",
            "Epoch 442/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6947 - acc: 0.6367 - val_loss: 0.8306 - val_acc: 0.3906\n",
            "Epoch 443/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7452 - acc: 0.6105 - val_loss: 0.8625 - val_acc: 0.3594\n",
            "Epoch 444/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6859 - acc: 0.6292 - val_loss: 0.7330 - val_acc: 0.4844\n",
            "Epoch 445/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6981 - acc: 0.6367 - val_loss: 0.8354 - val_acc: 0.3594\n",
            "Epoch 446/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.7155 - acc: 0.6236 - val_loss: 0.8054 - val_acc: 0.3906\n",
            "Epoch 447/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6663 - acc: 0.6573 - val_loss: 0.7241 - val_acc: 0.5312\n",
            "Epoch 448/1000\n",
            "9/9 [==============================] - 3s 246ms/step - loss: 0.7577 - acc: 0.6273 - val_loss: 0.7958 - val_acc: 0.4375\n",
            "Epoch 449/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7382 - acc: 0.6086 - val_loss: 0.7677 - val_acc: 0.4531\n",
            "Epoch 450/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.8056 - acc: 0.6030 - val_loss: 0.7927 - val_acc: 0.4062\n",
            "Epoch 451/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.7661 - acc: 0.5918 - val_loss: 0.7566 - val_acc: 0.4219\n",
            "Epoch 452/1000\n",
            "9/9 [==============================] - 3s 190ms/step - loss: 0.6854 - acc: 0.6517 - val_loss: 0.8064 - val_acc: 0.3750\n",
            "Epoch 453/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7439 - acc: 0.6049 - val_loss: 0.7738 - val_acc: 0.4219\n",
            "Epoch 454/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.7274 - acc: 0.6423 - val_loss: 0.7567 - val_acc: 0.4062\n",
            "Epoch 455/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7508 - acc: 0.6049 - val_loss: 0.7824 - val_acc: 0.4219\n",
            "Epoch 456/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6878 - acc: 0.6273 - val_loss: 0.8137 - val_acc: 0.3750\n",
            "Epoch 457/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7361 - acc: 0.6142 - val_loss: 0.7287 - val_acc: 0.4844\n",
            "Epoch 458/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7176 - acc: 0.6255 - val_loss: 0.7576 - val_acc: 0.4531\n",
            "Epoch 459/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7165 - acc: 0.6105 - val_loss: 0.7572 - val_acc: 0.4375\n",
            "Epoch 460/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7588 - acc: 0.6199 - val_loss: 0.7554 - val_acc: 0.4844\n",
            "Epoch 461/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.7503 - acc: 0.6111 - val_loss: 0.8075 - val_acc: 0.4062\n",
            "Epoch 462/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7112 - acc: 0.6536 - val_loss: 0.7684 - val_acc: 0.4062\n",
            "Epoch 463/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.7087 - acc: 0.6386 - val_loss: 0.7421 - val_acc: 0.4531\n",
            "Epoch 464/1000\n",
            "9/9 [==============================] - 3s 184ms/step - loss: 0.7288 - acc: 0.6330 - val_loss: 0.8302 - val_acc: 0.3438\n",
            "Epoch 465/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.6856 - acc: 0.6255 - val_loss: 0.7996 - val_acc: 0.4219\n",
            "Epoch 466/1000\n",
            "9/9 [==============================] - 3s 283ms/step - loss: 0.7135 - acc: 0.6610 - val_loss: 0.7960 - val_acc: 0.4219\n",
            "Epoch 467/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7530 - acc: 0.6030 - val_loss: 0.7838 - val_acc: 0.4062\n",
            "Epoch 468/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.7325 - acc: 0.6348 - val_loss: 0.7311 - val_acc: 0.4688\n",
            "Epoch 469/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7582 - acc: 0.6142 - val_loss: 0.8048 - val_acc: 0.4219\n",
            "Epoch 470/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6389 - acc: 0.6592 - val_loss: 0.7841 - val_acc: 0.4062\n",
            "Epoch 471/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6958 - acc: 0.6292 - val_loss: 0.7743 - val_acc: 0.4219\n",
            "Epoch 472/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6975 - acc: 0.6367 - val_loss: 0.8003 - val_acc: 0.4062\n",
            "Epoch 473/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.7066 - acc: 0.6311 - val_loss: 0.8070 - val_acc: 0.4219\n",
            "Epoch 474/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6973 - acc: 0.6423 - val_loss: 0.7676 - val_acc: 0.4219\n",
            "Epoch 475/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.7477 - acc: 0.6181 - val_loss: 0.7277 - val_acc: 0.4688\n",
            "Epoch 476/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.7302 - acc: 0.6255 - val_loss: 0.7851 - val_acc: 0.4062\n",
            "Epoch 477/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7193 - acc: 0.6161 - val_loss: 0.7569 - val_acc: 0.4688\n",
            "Epoch 478/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.6598 - acc: 0.6461 - val_loss: 0.7563 - val_acc: 0.4531\n",
            "Epoch 479/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6868 - acc: 0.6528 - val_loss: 0.8280 - val_acc: 0.4219\n",
            "Epoch 480/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.7562 - acc: 0.6161 - val_loss: 0.7779 - val_acc: 0.4375\n",
            "Epoch 481/1000\n",
            "9/9 [==============================] - 3s 273ms/step - loss: 0.7575 - acc: 0.6105 - val_loss: 0.7427 - val_acc: 0.5000\n",
            "Epoch 482/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7058 - acc: 0.6610 - val_loss: 0.8227 - val_acc: 0.3438\n",
            "Epoch 483/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6764 - acc: 0.6517 - val_loss: 0.7590 - val_acc: 0.4531\n",
            "Epoch 484/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6950 - acc: 0.6442 - val_loss: 0.7742 - val_acc: 0.4531\n",
            "Epoch 485/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7216 - acc: 0.6255 - val_loss: 0.7975 - val_acc: 0.4531\n",
            "Epoch 486/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7147 - acc: 0.6236 - val_loss: 0.7444 - val_acc: 0.4688\n",
            "Epoch 487/1000\n",
            "9/9 [==============================] - 3s 266ms/step - loss: 0.6541 - acc: 0.6667 - val_loss: 0.7868 - val_acc: 0.4219\n",
            "Epoch 488/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7044 - acc: 0.6573 - val_loss: 0.7655 - val_acc: 0.4688\n",
            "Epoch 489/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.7214 - acc: 0.6311 - val_loss: 0.7587 - val_acc: 0.4531\n",
            "Epoch 490/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.6958 - acc: 0.6517 - val_loss: 0.8090 - val_acc: 0.4062\n",
            "Epoch 491/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.7277 - acc: 0.6180 - val_loss: 0.8086 - val_acc: 0.3906\n",
            "Epoch 492/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.7439 - acc: 0.6049 - val_loss: 0.7924 - val_acc: 0.3906\n",
            "Epoch 493/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6857 - acc: 0.6273 - val_loss: 0.7623 - val_acc: 0.4688\n",
            "Epoch 494/1000\n",
            "9/9 [==============================] - 3s 238ms/step - loss: 0.6875 - acc: 0.6580 - val_loss: 0.8161 - val_acc: 0.4219\n",
            "Epoch 495/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.7049 - acc: 0.6554 - val_loss: 0.7313 - val_acc: 0.4688\n",
            "Epoch 496/1000\n",
            "9/9 [==============================] - 3s 275ms/step - loss: 0.7115 - acc: 0.6330 - val_loss: 0.7545 - val_acc: 0.4844\n",
            "Epoch 497/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7136 - acc: 0.6348 - val_loss: 0.8075 - val_acc: 0.3594\n",
            "Epoch 498/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.7205 - acc: 0.6030 - val_loss: 0.7487 - val_acc: 0.4688\n",
            "Epoch 499/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6986 - acc: 0.6199 - val_loss: 0.7709 - val_acc: 0.4219\n",
            "Epoch 500/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7229 - acc: 0.6423 - val_loss: 0.8225 - val_acc: 0.4219\n",
            "Epoch 501/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7365 - acc: 0.6161 - val_loss: 0.7806 - val_acc: 0.4219\n",
            "Epoch 502/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.6877 - acc: 0.6442 - val_loss: 0.7373 - val_acc: 0.4844\n",
            "Epoch 503/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.7159 - acc: 0.6255 - val_loss: 0.7643 - val_acc: 0.4844\n",
            "Epoch 504/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.7252 - acc: 0.6105 - val_loss: 0.6743 - val_acc: 0.5312\n",
            "Epoch 505/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6608 - acc: 0.6573 - val_loss: 0.7690 - val_acc: 0.4375\n",
            "Epoch 506/1000\n",
            "9/9 [==============================] - 3s 247ms/step - loss: 0.6588 - acc: 0.6736 - val_loss: 0.8225 - val_acc: 0.3594\n",
            "Epoch 507/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7134 - acc: 0.6236 - val_loss: 0.7860 - val_acc: 0.4531\n",
            "Epoch 508/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.7020 - acc: 0.6498 - val_loss: 0.8033 - val_acc: 0.4688\n",
            "Epoch 509/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.7491 - acc: 0.5861 - val_loss: 0.7469 - val_acc: 0.4688\n",
            "Epoch 510/1000\n",
            "9/9 [==============================] - 3s 227ms/step - loss: 0.6952 - acc: 0.6442 - val_loss: 0.8051 - val_acc: 0.3750\n",
            "Epoch 511/1000\n",
            "9/9 [==============================] - 3s 270ms/step - loss: 0.7370 - acc: 0.6199 - val_loss: 0.8271 - val_acc: 0.4062\n",
            "Epoch 512/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.7087 - acc: 0.6199 - val_loss: 0.7785 - val_acc: 0.4688\n",
            "Epoch 513/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.7166 - acc: 0.6086 - val_loss: 0.8074 - val_acc: 0.4531\n",
            "Epoch 514/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.7265 - acc: 0.6086 - val_loss: 0.8280 - val_acc: 0.4531\n",
            "Epoch 515/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6650 - acc: 0.6536 - val_loss: 0.7274 - val_acc: 0.4844\n",
            "Epoch 516/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 0.6572 - acc: 0.6910 - val_loss: 0.7967 - val_acc: 0.4375\n",
            "Epoch 517/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.6515 - acc: 0.6573 - val_loss: 0.8155 - val_acc: 0.3906\n",
            "Epoch 518/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.6803 - acc: 0.6217 - val_loss: 0.7939 - val_acc: 0.4375\n",
            "Epoch 519/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7009 - acc: 0.6348 - val_loss: 0.7434 - val_acc: 0.4531\n",
            "Epoch 520/1000\n",
            "9/9 [==============================] - 3s 271ms/step - loss: 0.7584 - acc: 0.6011 - val_loss: 0.8045 - val_acc: 0.4375\n",
            "Epoch 521/1000\n",
            "9/9 [==============================] - 3s 232ms/step - loss: 0.6954 - acc: 0.6406 - val_loss: 0.8357 - val_acc: 0.3750\n",
            "Epoch 522/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7245 - acc: 0.6255 - val_loss: 0.7715 - val_acc: 0.4688\n",
            "Epoch 523/1000\n",
            "9/9 [==============================] - 3s 233ms/step - loss: 0.6849 - acc: 0.6510 - val_loss: 0.7510 - val_acc: 0.5000\n",
            "Epoch 524/1000\n",
            "9/9 [==============================] - 3s 190ms/step - loss: 0.6300 - acc: 0.6629 - val_loss: 0.7337 - val_acc: 0.5000\n",
            "Epoch 525/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6927 - acc: 0.6386 - val_loss: 0.7833 - val_acc: 0.4219\n",
            "Epoch 526/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.6994 - acc: 0.6367 - val_loss: 0.7481 - val_acc: 0.5000\n",
            "Epoch 527/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.7192 - acc: 0.6236 - val_loss: 0.8229 - val_acc: 0.4062\n",
            "Epoch 528/1000\n",
            "9/9 [==============================] - 3s 240ms/step - loss: 0.6891 - acc: 0.6441 - val_loss: 0.7885 - val_acc: 0.4375\n",
            "Epoch 529/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6273 - acc: 0.6760 - val_loss: 0.7304 - val_acc: 0.5469\n",
            "Epoch 530/1000\n",
            "9/9 [==============================] - 3s 267ms/step - loss: 0.7220 - acc: 0.6215 - val_loss: 0.7580 - val_acc: 0.4688\n",
            "Epoch 531/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.7208 - acc: 0.6199 - val_loss: 0.7807 - val_acc: 0.4844\n",
            "Epoch 532/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7088 - acc: 0.6479 - val_loss: 0.7913 - val_acc: 0.4688\n",
            "Epoch 533/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.7061 - acc: 0.6423 - val_loss: 0.7829 - val_acc: 0.4531\n",
            "Epoch 534/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6875 - acc: 0.6517 - val_loss: 0.7846 - val_acc: 0.4375\n",
            "Epoch 535/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6668 - acc: 0.6723 - val_loss: 0.7714 - val_acc: 0.4531\n",
            "Epoch 536/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6762 - acc: 0.6423 - val_loss: 0.7783 - val_acc: 0.4688\n",
            "Epoch 537/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6807 - acc: 0.6610 - val_loss: 0.8086 - val_acc: 0.4219\n",
            "Epoch 538/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.6394 - acc: 0.6760 - val_loss: 0.7872 - val_acc: 0.3906\n",
            "Epoch 539/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6866 - acc: 0.6458 - val_loss: 0.7041 - val_acc: 0.5312\n",
            "Epoch 540/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.6739 - acc: 0.6386 - val_loss: 0.7954 - val_acc: 0.4219\n",
            "Epoch 541/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.7170 - acc: 0.6372 - val_loss: 0.7753 - val_acc: 0.4375\n",
            "Epoch 542/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6876 - acc: 0.6311 - val_loss: 0.7732 - val_acc: 0.4375\n",
            "Epoch 543/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7184 - acc: 0.6461 - val_loss: 0.8150 - val_acc: 0.4375\n",
            "Epoch 544/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6249 - acc: 0.6610 - val_loss: 0.7287 - val_acc: 0.5312\n",
            "Epoch 545/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6727 - acc: 0.6573 - val_loss: 0.8155 - val_acc: 0.3906\n",
            "Epoch 546/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7326 - acc: 0.6180 - val_loss: 0.7761 - val_acc: 0.4375\n",
            "Epoch 547/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.7215 - acc: 0.6311 - val_loss: 0.7725 - val_acc: 0.4219\n",
            "Epoch 548/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.7278 - acc: 0.6163 - val_loss: 0.7812 - val_acc: 0.4531\n",
            "Epoch 549/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6996 - acc: 0.6367 - val_loss: 0.7903 - val_acc: 0.4375\n",
            "Epoch 550/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.7094 - acc: 0.6367 - val_loss: 0.7703 - val_acc: 0.4688\n",
            "Epoch 551/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6638 - acc: 0.6648 - val_loss: 0.7793 - val_acc: 0.4844\n",
            "Epoch 552/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6222 - acc: 0.6779 - val_loss: 0.7299 - val_acc: 0.4844\n",
            "Epoch 553/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6937 - acc: 0.6330 - val_loss: 0.7603 - val_acc: 0.4844\n",
            "Epoch 554/1000\n",
            "9/9 [==============================] - 3s 273ms/step - loss: 0.6985 - acc: 0.6423 - val_loss: 0.7880 - val_acc: 0.4375\n",
            "Epoch 555/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6829 - acc: 0.6573 - val_loss: 0.7854 - val_acc: 0.4219\n",
            "Epoch 556/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6936 - acc: 0.6479 - val_loss: 0.7331 - val_acc: 0.5156\n",
            "Epoch 557/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6548 - acc: 0.6704 - val_loss: 0.7708 - val_acc: 0.4531\n",
            "Epoch 558/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.7349 - acc: 0.6086 - val_loss: 0.7862 - val_acc: 0.4531\n",
            "Epoch 559/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6893 - acc: 0.6498 - val_loss: 0.7165 - val_acc: 0.5156\n",
            "Epoch 560/1000\n",
            "9/9 [==============================] - 3s 233ms/step - loss: 0.6937 - acc: 0.6442 - val_loss: 0.8020 - val_acc: 0.4375\n",
            "Epoch 561/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6591 - acc: 0.6798 - val_loss: 0.7276 - val_acc: 0.5000\n",
            "Epoch 562/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6797 - acc: 0.6536 - val_loss: 0.7678 - val_acc: 0.4375\n",
            "Epoch 563/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6782 - acc: 0.6554 - val_loss: 0.8175 - val_acc: 0.4375\n",
            "Epoch 564/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6936 - acc: 0.6517 - val_loss: 0.7779 - val_acc: 0.4688\n",
            "Epoch 565/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6464 - acc: 0.6423 - val_loss: 0.8119 - val_acc: 0.4062\n",
            "Epoch 566/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6676 - acc: 0.6554 - val_loss: 0.7504 - val_acc: 0.4844\n",
            "Epoch 567/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7149 - acc: 0.6180 - val_loss: 0.7173 - val_acc: 0.5156\n",
            "Epoch 568/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7183 - acc: 0.6311 - val_loss: 0.7270 - val_acc: 0.4688\n",
            "Epoch 569/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6671 - acc: 0.6629 - val_loss: 0.8238 - val_acc: 0.4062\n",
            "Epoch 570/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6987 - acc: 0.6461 - val_loss: 0.7705 - val_acc: 0.4688\n",
            "Epoch 571/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.7116 - acc: 0.6180 - val_loss: 0.8191 - val_acc: 0.3750\n",
            "Epoch 572/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.6613 - acc: 0.6573 - val_loss: 0.8265 - val_acc: 0.4375\n",
            "Epoch 573/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.7625 - acc: 0.5936 - val_loss: 0.7636 - val_acc: 0.4219\n",
            "Epoch 574/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6631 - acc: 0.6610 - val_loss: 0.7723 - val_acc: 0.4531\n",
            "Epoch 575/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6714 - acc: 0.6592 - val_loss: 0.7522 - val_acc: 0.4688\n",
            "Epoch 576/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.6973 - acc: 0.6161 - val_loss: 0.7349 - val_acc: 0.5000\n",
            "Epoch 577/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6656 - acc: 0.6573 - val_loss: 0.7674 - val_acc: 0.4375\n",
            "Epoch 578/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6782 - acc: 0.6517 - val_loss: 0.7465 - val_acc: 0.4844\n",
            "Epoch 579/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6505 - acc: 0.6461 - val_loss: 0.7477 - val_acc: 0.4844\n",
            "Epoch 580/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 0.6947 - acc: 0.6423 - val_loss: 0.8161 - val_acc: 0.4375\n",
            "Epoch 581/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.7190 - acc: 0.6348 - val_loss: 0.7141 - val_acc: 0.4531\n",
            "Epoch 582/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6734 - acc: 0.6423 - val_loss: 0.8095 - val_acc: 0.4219\n",
            "Epoch 583/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6887 - acc: 0.6517 - val_loss: 0.7593 - val_acc: 0.4531\n",
            "Epoch 584/1000\n",
            "9/9 [==============================] - 3s 288ms/step - loss: 0.6786 - acc: 0.6685 - val_loss: 0.8149 - val_acc: 0.4062\n",
            "Epoch 585/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6896 - acc: 0.6367 - val_loss: 0.7490 - val_acc: 0.5000\n",
            "Epoch 586/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6823 - acc: 0.6319 - val_loss: 0.7612 - val_acc: 0.4844\n",
            "Epoch 587/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6481 - acc: 0.6629 - val_loss: 0.8209 - val_acc: 0.4219\n",
            "Epoch 588/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6960 - acc: 0.6554 - val_loss: 0.7859 - val_acc: 0.4062\n",
            "Epoch 589/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.7201 - acc: 0.6292 - val_loss: 0.7847 - val_acc: 0.4219\n",
            "Epoch 590/1000\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 0.6943 - acc: 0.6461 - val_loss: 0.8008 - val_acc: 0.4375\n",
            "Epoch 591/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6792 - acc: 0.6311 - val_loss: 0.7424 - val_acc: 0.4844\n",
            "Epoch 592/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6393 - acc: 0.6854 - val_loss: 0.7457 - val_acc: 0.4688\n",
            "Epoch 593/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6339 - acc: 0.6610 - val_loss: 0.7591 - val_acc: 0.4375\n",
            "Epoch 594/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6975 - acc: 0.6292 - val_loss: 0.7376 - val_acc: 0.5000\n",
            "Epoch 595/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6961 - acc: 0.6255 - val_loss: 0.7738 - val_acc: 0.4531\n",
            "Epoch 596/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6933 - acc: 0.6554 - val_loss: 0.7408 - val_acc: 0.5000\n",
            "Epoch 597/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6645 - acc: 0.6292 - val_loss: 0.7485 - val_acc: 0.5000\n",
            "Epoch 598/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6643 - acc: 0.6461 - val_loss: 0.7986 - val_acc: 0.4531\n",
            "Epoch 599/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.6845 - acc: 0.6610 - val_loss: 0.7597 - val_acc: 0.5000\n",
            "Epoch 600/1000\n",
            "9/9 [==============================] - 3s 272ms/step - loss: 0.6469 - acc: 0.6554 - val_loss: 0.7432 - val_acc: 0.4844\n",
            "Epoch 601/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.6662 - acc: 0.6442 - val_loss: 0.8160 - val_acc: 0.4062\n",
            "Epoch 602/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6139 - acc: 0.7079 - val_loss: 0.7770 - val_acc: 0.4531\n",
            "Epoch 603/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 0.6304 - acc: 0.6873 - val_loss: 0.7403 - val_acc: 0.4844\n",
            "Epoch 604/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6808 - acc: 0.6404 - val_loss: 0.7090 - val_acc: 0.4844\n",
            "Epoch 605/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6686 - acc: 0.6517 - val_loss: 0.7934 - val_acc: 0.4062\n",
            "Epoch 606/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6696 - acc: 0.6685 - val_loss: 0.7438 - val_acc: 0.4688\n",
            "Epoch 607/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6858 - acc: 0.6199 - val_loss: 0.7475 - val_acc: 0.4688\n",
            "Epoch 608/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6304 - acc: 0.6615 - val_loss: 0.7808 - val_acc: 0.4531\n",
            "Epoch 609/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.6776 - acc: 0.6367 - val_loss: 0.7768 - val_acc: 0.4375\n",
            "Epoch 610/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.6385 - acc: 0.6873 - val_loss: 0.7851 - val_acc: 0.4375\n",
            "Epoch 611/1000\n",
            "9/9 [==============================] - 3s 248ms/step - loss: 0.6661 - acc: 0.6554 - val_loss: 0.8010 - val_acc: 0.4062\n",
            "Epoch 612/1000\n",
            "9/9 [==============================] - 3s 183ms/step - loss: 0.6779 - acc: 0.6629 - val_loss: 0.7799 - val_acc: 0.4219\n",
            "Epoch 613/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6878 - acc: 0.6554 - val_loss: 0.7451 - val_acc: 0.5000\n",
            "Epoch 614/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6687 - acc: 0.6517 - val_loss: 0.7612 - val_acc: 0.4531\n",
            "Epoch 615/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6669 - acc: 0.6386 - val_loss: 0.7818 - val_acc: 0.4688\n",
            "Epoch 616/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6918 - acc: 0.6667 - val_loss: 0.7585 - val_acc: 0.4219\n",
            "Epoch 617/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6029 - acc: 0.6891 - val_loss: 0.7947 - val_acc: 0.4062\n",
            "Epoch 618/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7069 - acc: 0.6273 - val_loss: 0.7239 - val_acc: 0.4844\n",
            "Epoch 619/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6532 - acc: 0.6798 - val_loss: 0.7085 - val_acc: 0.5000\n",
            "Epoch 620/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6942 - acc: 0.6386 - val_loss: 0.8015 - val_acc: 0.4375\n",
            "Epoch 621/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 0.6667 - acc: 0.6704 - val_loss: 0.7777 - val_acc: 0.4219\n",
            "Epoch 622/1000\n",
            "9/9 [==============================] - 3s 193ms/step - loss: 0.6501 - acc: 0.6423 - val_loss: 0.7518 - val_acc: 0.4688\n",
            "Epoch 623/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.6904 - acc: 0.6528 - val_loss: 0.7483 - val_acc: 0.4844\n",
            "Epoch 624/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6453 - acc: 0.6854 - val_loss: 0.7518 - val_acc: 0.4844\n",
            "Epoch 625/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.6817 - acc: 0.6311 - val_loss: 0.7027 - val_acc: 0.5312\n",
            "Epoch 626/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6545 - acc: 0.6629 - val_loss: 0.7037 - val_acc: 0.5000\n",
            "Epoch 627/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6541 - acc: 0.6573 - val_loss: 0.7180 - val_acc: 0.4688\n",
            "Epoch 628/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6912 - acc: 0.6311 - val_loss: 0.7552 - val_acc: 0.4844\n",
            "Epoch 629/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6570 - acc: 0.6554 - val_loss: 0.7700 - val_acc: 0.4375\n",
            "Epoch 630/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6920 - acc: 0.6528 - val_loss: 0.7784 - val_acc: 0.4375\n",
            "Epoch 631/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6539 - acc: 0.6404 - val_loss: 0.7429 - val_acc: 0.4375\n",
            "Epoch 632/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6641 - acc: 0.6779 - val_loss: 0.7729 - val_acc: 0.4219\n",
            "Epoch 633/1000\n",
            "9/9 [==============================] - 3s 284ms/step - loss: 0.6507 - acc: 0.6610 - val_loss: 0.7691 - val_acc: 0.3906\n",
            "Epoch 634/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6540 - acc: 0.6536 - val_loss: 0.7608 - val_acc: 0.4844\n",
            "Epoch 635/1000\n",
            "9/9 [==============================] - 3s 243ms/step - loss: 0.7328 - acc: 0.6285 - val_loss: 0.8226 - val_acc: 0.4062\n",
            "Epoch 636/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6456 - acc: 0.6610 - val_loss: 0.7512 - val_acc: 0.4844\n",
            "Epoch 637/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6929 - acc: 0.6479 - val_loss: 0.7773 - val_acc: 0.4062\n",
            "Epoch 638/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6561 - acc: 0.6536 - val_loss: 0.7302 - val_acc: 0.4844\n",
            "Epoch 639/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6327 - acc: 0.6798 - val_loss: 0.7495 - val_acc: 0.4844\n",
            "Epoch 640/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.6543 - acc: 0.6592 - val_loss: 0.7737 - val_acc: 0.4688\n",
            "Epoch 641/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6482 - acc: 0.6854 - val_loss: 0.7586 - val_acc: 0.4688\n",
            "Epoch 642/1000\n",
            "9/9 [==============================] - 3s 283ms/step - loss: 0.6310 - acc: 0.6648 - val_loss: 0.7847 - val_acc: 0.4375\n",
            "Epoch 643/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6928 - acc: 0.6461 - val_loss: 0.7709 - val_acc: 0.4375\n",
            "Epoch 644/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.7052 - acc: 0.6442 - val_loss: 0.7014 - val_acc: 0.5156\n",
            "Epoch 645/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6643 - acc: 0.6685 - val_loss: 0.7993 - val_acc: 0.4375\n",
            "Epoch 646/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.7149 - acc: 0.6105 - val_loss: 0.7428 - val_acc: 0.5156\n",
            "Epoch 647/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.6889 - acc: 0.6536 - val_loss: 0.7974 - val_acc: 0.4219\n",
            "Epoch 648/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6761 - acc: 0.6554 - val_loss: 0.7161 - val_acc: 0.5312\n",
            "Epoch 649/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6356 - acc: 0.6760 - val_loss: 0.7180 - val_acc: 0.4844\n",
            "Epoch 650/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6347 - acc: 0.6423 - val_loss: 0.7892 - val_acc: 0.4062\n",
            "Epoch 651/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.7436 - acc: 0.6007 - val_loss: 0.7069 - val_acc: 0.5000\n",
            "Epoch 652/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7064 - acc: 0.6292 - val_loss: 0.7453 - val_acc: 0.4375\n",
            "Epoch 653/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6461 - acc: 0.6835 - val_loss: 0.7668 - val_acc: 0.4531\n",
            "Epoch 654/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.6759 - acc: 0.6517 - val_loss: 0.7250 - val_acc: 0.4844\n",
            "Epoch 655/1000\n",
            "9/9 [==============================] - 3s 241ms/step - loss: 0.6523 - acc: 0.6493 - val_loss: 0.7210 - val_acc: 0.5156\n",
            "Epoch 656/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6906 - acc: 0.6498 - val_loss: 0.7596 - val_acc: 0.4688\n",
            "Epoch 657/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6726 - acc: 0.6386 - val_loss: 0.7567 - val_acc: 0.4531\n",
            "Epoch 658/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.6537 - acc: 0.6330 - val_loss: 0.7165 - val_acc: 0.5000\n",
            "Epoch 659/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6423 - acc: 0.6685 - val_loss: 0.6797 - val_acc: 0.5156\n",
            "Epoch 660/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.7011 - acc: 0.6404 - val_loss: 0.7607 - val_acc: 0.4531\n",
            "Epoch 661/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6040 - acc: 0.6685 - val_loss: 0.7189 - val_acc: 0.5000\n",
            "Epoch 662/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6560 - acc: 0.6592 - val_loss: 0.8026 - val_acc: 0.4062\n",
            "Epoch 663/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6811 - acc: 0.6629 - val_loss: 0.7120 - val_acc: 0.4531\n",
            "Epoch 664/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.6403 - acc: 0.7004 - val_loss: 0.7453 - val_acc: 0.4688\n",
            "Epoch 665/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6885 - acc: 0.6142 - val_loss: 0.7694 - val_acc: 0.4375\n",
            "Epoch 666/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6410 - acc: 0.6685 - val_loss: 0.7344 - val_acc: 0.4531\n",
            "Epoch 667/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6553 - acc: 0.6685 - val_loss: 0.7225 - val_acc: 0.5000\n",
            "Epoch 668/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6852 - acc: 0.6498 - val_loss: 0.7765 - val_acc: 0.4375\n",
            "Epoch 669/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6208 - acc: 0.6910 - val_loss: 0.7249 - val_acc: 0.5000\n",
            "Epoch 670/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6249 - acc: 0.6798 - val_loss: 0.7780 - val_acc: 0.4844\n",
            "Epoch 671/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6370 - acc: 0.6742 - val_loss: 0.7328 - val_acc: 0.5000\n",
            "Epoch 672/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6800 - acc: 0.6685 - val_loss: 0.7880 - val_acc: 0.4062\n",
            "Epoch 673/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6741 - acc: 0.6667 - val_loss: 0.7591 - val_acc: 0.4375\n",
            "Epoch 674/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6507 - acc: 0.6610 - val_loss: 0.7536 - val_acc: 0.5000\n",
            "Epoch 675/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6234 - acc: 0.6910 - val_loss: 0.7666 - val_acc: 0.4688\n",
            "Epoch 676/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6460 - acc: 0.6479 - val_loss: 0.7191 - val_acc: 0.4844\n",
            "Epoch 677/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6484 - acc: 0.6760 - val_loss: 0.7798 - val_acc: 0.4375\n",
            "Epoch 678/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7061 - acc: 0.6461 - val_loss: 0.7351 - val_acc: 0.5000\n",
            "Epoch 679/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6817 - acc: 0.6592 - val_loss: 0.6899 - val_acc: 0.5156\n",
            "Epoch 680/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6656 - acc: 0.6629 - val_loss: 0.7275 - val_acc: 0.4844\n",
            "Epoch 681/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.6863 - acc: 0.6273 - val_loss: 0.7707 - val_acc: 0.4531\n",
            "Epoch 682/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.6629 - acc: 0.6386 - val_loss: 0.7905 - val_acc: 0.4531\n",
            "Epoch 683/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6317 - acc: 0.6873 - val_loss: 0.7659 - val_acc: 0.4531\n",
            "Epoch 684/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6276 - acc: 0.6779 - val_loss: 0.7672 - val_acc: 0.4219\n",
            "Epoch 685/1000\n",
            "9/9 [==============================] - 3s 238ms/step - loss: 0.6576 - acc: 0.6719 - val_loss: 0.7596 - val_acc: 0.4531\n",
            "Epoch 686/1000\n",
            "9/9 [==============================] - 3s 279ms/step - loss: 0.6688 - acc: 0.6592 - val_loss: 0.8117 - val_acc: 0.3594\n",
            "Epoch 687/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6357 - acc: 0.6667 - val_loss: 0.7677 - val_acc: 0.4531\n",
            "Epoch 688/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6534 - acc: 0.6573 - val_loss: 0.7250 - val_acc: 0.4531\n",
            "Epoch 689/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.7469 - acc: 0.5918 - val_loss: 0.7234 - val_acc: 0.4531\n",
            "Epoch 690/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.6287 - acc: 0.6667 - val_loss: 0.7619 - val_acc: 0.4688\n",
            "Epoch 691/1000\n",
            "9/9 [==============================] - 3s 256ms/step - loss: 0.6560 - acc: 0.6667 - val_loss: 0.7060 - val_acc: 0.5000\n",
            "Epoch 692/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.7080 - acc: 0.6236 - val_loss: 0.7352 - val_acc: 0.4688\n",
            "Epoch 693/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6297 - acc: 0.6816 - val_loss: 0.7592 - val_acc: 0.4219\n",
            "Epoch 694/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6442 - acc: 0.6573 - val_loss: 0.7477 - val_acc: 0.4688\n",
            "Epoch 695/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6940 - acc: 0.6367 - val_loss: 0.7596 - val_acc: 0.4531\n",
            "Epoch 696/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6901 - acc: 0.6348 - val_loss: 0.7741 - val_acc: 0.4062\n",
            "Epoch 697/1000\n",
            "9/9 [==============================] - 3s 226ms/step - loss: 0.6782 - acc: 0.6404 - val_loss: 0.7897 - val_acc: 0.4375\n",
            "Epoch 698/1000\n",
            "9/9 [==============================] - 3s 240ms/step - loss: 0.6662 - acc: 0.6423 - val_loss: 0.7979 - val_acc: 0.4219\n",
            "Epoch 699/1000\n",
            "9/9 [==============================] - 3s 279ms/step - loss: 0.6639 - acc: 0.6498 - val_loss: 0.7041 - val_acc: 0.5156\n",
            "Epoch 700/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6637 - acc: 0.6498 - val_loss: 0.7819 - val_acc: 0.4219\n",
            "Epoch 701/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.6060 - acc: 0.6966 - val_loss: 0.7164 - val_acc: 0.4844\n",
            "Epoch 702/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.6448 - acc: 0.6742 - val_loss: 0.6905 - val_acc: 0.5000\n",
            "Epoch 703/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6916 - acc: 0.6536 - val_loss: 0.7855 - val_acc: 0.4219\n",
            "Epoch 704/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.7010 - acc: 0.6124 - val_loss: 0.7512 - val_acc: 0.4375\n",
            "Epoch 705/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6425 - acc: 0.6592 - val_loss: 0.7132 - val_acc: 0.5000\n",
            "Epoch 706/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6117 - acc: 0.6667 - val_loss: 0.7473 - val_acc: 0.4531\n",
            "Epoch 707/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.5983 - acc: 0.6858 - val_loss: 0.6983 - val_acc: 0.5000\n",
            "Epoch 708/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.6350 - acc: 0.6873 - val_loss: 0.7934 - val_acc: 0.4062\n",
            "Epoch 709/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6664 - acc: 0.6685 - val_loss: 0.7139 - val_acc: 0.4844\n",
            "Epoch 710/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6319 - acc: 0.6573 - val_loss: 0.7611 - val_acc: 0.4844\n",
            "Epoch 711/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6260 - acc: 0.6927 - val_loss: 0.7838 - val_acc: 0.4062\n",
            "Epoch 712/1000\n",
            "9/9 [==============================] - 3s 227ms/step - loss: 0.6451 - acc: 0.6753 - val_loss: 0.8148 - val_acc: 0.3906\n",
            "Epoch 713/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.6394 - acc: 0.6629 - val_loss: 0.6846 - val_acc: 0.5156\n",
            "Epoch 714/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6968 - acc: 0.6404 - val_loss: 0.7277 - val_acc: 0.5000\n",
            "Epoch 715/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6251 - acc: 0.6910 - val_loss: 0.7196 - val_acc: 0.5000\n",
            "Epoch 716/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6462 - acc: 0.6667 - val_loss: 0.7754 - val_acc: 0.4375\n",
            "Epoch 717/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.7089 - acc: 0.6199 - val_loss: 0.7984 - val_acc: 0.4219\n",
            "Epoch 718/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6171 - acc: 0.6966 - val_loss: 0.7541 - val_acc: 0.4375\n",
            "Epoch 719/1000\n",
            "9/9 [==============================] - 3s 238ms/step - loss: 0.6003 - acc: 0.6875 - val_loss: 0.7132 - val_acc: 0.4688\n",
            "Epoch 720/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6304 - acc: 0.6816 - val_loss: 0.7502 - val_acc: 0.5000\n",
            "Epoch 721/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6547 - acc: 0.6629 - val_loss: 0.8271 - val_acc: 0.3750\n",
            "Epoch 722/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6780 - acc: 0.6461 - val_loss: 0.7883 - val_acc: 0.4375\n",
            "Epoch 723/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6674 - acc: 0.6330 - val_loss: 0.7506 - val_acc: 0.4531\n",
            "Epoch 724/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6399 - acc: 0.6648 - val_loss: 0.7498 - val_acc: 0.4531\n",
            "Epoch 725/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6708 - acc: 0.6330 - val_loss: 0.7611 - val_acc: 0.4531\n",
            "Epoch 726/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6528 - acc: 0.6404 - val_loss: 0.7326 - val_acc: 0.5000\n",
            "Epoch 727/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.6390 - acc: 0.6648 - val_loss: 0.7133 - val_acc: 0.4688\n",
            "Epoch 728/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6614 - acc: 0.6348 - val_loss: 0.7242 - val_acc: 0.4531\n",
            "Epoch 729/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6445 - acc: 0.6760 - val_loss: 0.7690 - val_acc: 0.4062\n",
            "Epoch 730/1000\n",
            "9/9 [==============================] - 3s 188ms/step - loss: 0.6082 - acc: 0.6723 - val_loss: 0.7533 - val_acc: 0.4844\n",
            "Epoch 731/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6633 - acc: 0.6217 - val_loss: 0.7423 - val_acc: 0.4531\n",
            "Epoch 732/1000\n",
            "9/9 [==============================] - 3s 240ms/step - loss: 0.5875 - acc: 0.6892 - val_loss: 0.7375 - val_acc: 0.4531\n",
            "Epoch 733/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6568 - acc: 0.6367 - val_loss: 0.7403 - val_acc: 0.4844\n",
            "Epoch 734/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6948 - acc: 0.6311 - val_loss: 0.7709 - val_acc: 0.4219\n",
            "Epoch 735/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6732 - acc: 0.6760 - val_loss: 0.7479 - val_acc: 0.4844\n",
            "Epoch 736/1000\n",
            "9/9 [==============================] - 3s 239ms/step - loss: 0.6523 - acc: 0.6510 - val_loss: 0.7608 - val_acc: 0.4844\n",
            "Epoch 737/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6379 - acc: 0.6944 - val_loss: 0.7142 - val_acc: 0.5156\n",
            "Epoch 738/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6503 - acc: 0.6723 - val_loss: 0.7542 - val_acc: 0.4844\n",
            "Epoch 739/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6749 - acc: 0.6442 - val_loss: 0.7871 - val_acc: 0.4062\n",
            "Epoch 740/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.6850 - acc: 0.6386 - val_loss: 0.7449 - val_acc: 0.4844\n",
            "Epoch 741/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6430 - acc: 0.6948 - val_loss: 0.6713 - val_acc: 0.5156\n",
            "Epoch 742/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6296 - acc: 0.6573 - val_loss: 0.6898 - val_acc: 0.5156\n",
            "Epoch 743/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6651 - acc: 0.6367 - val_loss: 0.7797 - val_acc: 0.4531\n",
            "Epoch 744/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6613 - acc: 0.6610 - val_loss: 0.7647 - val_acc: 0.4688\n",
            "Epoch 745/1000\n",
            "9/9 [==============================] - 3s 244ms/step - loss: 0.6646 - acc: 0.6476 - val_loss: 0.6975 - val_acc: 0.5312\n",
            "Epoch 746/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6489 - acc: 0.6685 - val_loss: 0.7504 - val_acc: 0.4219\n",
            "Epoch 747/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6279 - acc: 0.6816 - val_loss: 0.7472 - val_acc: 0.4688\n",
            "Epoch 748/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.6638 - acc: 0.6493 - val_loss: 0.7322 - val_acc: 0.5000\n",
            "Epoch 749/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6319 - acc: 0.6648 - val_loss: 0.7917 - val_acc: 0.4531\n",
            "Epoch 750/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.6153 - acc: 0.6854 - val_loss: 0.7827 - val_acc: 0.3750\n",
            "Epoch 751/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6252 - acc: 0.6742 - val_loss: 0.7509 - val_acc: 0.4688\n",
            "Epoch 752/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6429 - acc: 0.6760 - val_loss: 0.7222 - val_acc: 0.4844\n",
            "Epoch 753/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6722 - acc: 0.6592 - val_loss: 0.7349 - val_acc: 0.4688\n",
            "Epoch 754/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6554 - acc: 0.6386 - val_loss: 0.7741 - val_acc: 0.4062\n",
            "Epoch 755/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6763 - acc: 0.6330 - val_loss: 0.6831 - val_acc: 0.5000\n",
            "Epoch 756/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.6248 - acc: 0.6592 - val_loss: 0.7446 - val_acc: 0.4375\n",
            "Epoch 757/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6576 - acc: 0.6554 - val_loss: 0.7019 - val_acc: 0.4844\n",
            "Epoch 758/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6595 - acc: 0.6573 - val_loss: 0.7128 - val_acc: 0.5312\n",
            "Epoch 759/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6332 - acc: 0.6798 - val_loss: 0.7427 - val_acc: 0.4531\n",
            "Epoch 760/1000\n",
            "9/9 [==============================] - 3s 284ms/step - loss: 0.6327 - acc: 0.6498 - val_loss: 0.7362 - val_acc: 0.5156\n",
            "Epoch 761/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6581 - acc: 0.6554 - val_loss: 0.7946 - val_acc: 0.4531\n",
            "Epoch 762/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6646 - acc: 0.6760 - val_loss: 0.7343 - val_acc: 0.4844\n",
            "Epoch 763/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6351 - acc: 0.6442 - val_loss: 0.7490 - val_acc: 0.4531\n",
            "Epoch 764/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6139 - acc: 0.6667 - val_loss: 0.7205 - val_acc: 0.5156\n",
            "Epoch 765/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6149 - acc: 0.6891 - val_loss: 0.7650 - val_acc: 0.4531\n",
            "Epoch 766/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6797 - acc: 0.6479 - val_loss: 0.7322 - val_acc: 0.5000\n",
            "Epoch 767/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.5973 - acc: 0.6929 - val_loss: 0.7496 - val_acc: 0.4531\n",
            "Epoch 768/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6994 - acc: 0.6517 - val_loss: 0.7437 - val_acc: 0.4531\n",
            "Epoch 769/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6190 - acc: 0.6723 - val_loss: 0.7199 - val_acc: 0.5312\n",
            "Epoch 770/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6244 - acc: 0.6573 - val_loss: 0.7406 - val_acc: 0.4531\n",
            "Epoch 771/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6514 - acc: 0.6573 - val_loss: 0.8018 - val_acc: 0.4219\n",
            "Epoch 772/1000\n",
            "9/9 [==============================] - 3s 230ms/step - loss: 0.6545 - acc: 0.6423 - val_loss: 0.7284 - val_acc: 0.4844\n",
            "Epoch 773/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.6235 - acc: 0.6592 - val_loss: 0.7672 - val_acc: 0.4375\n",
            "Epoch 774/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.5817 - acc: 0.7135 - val_loss: 0.7530 - val_acc: 0.5156\n",
            "Epoch 775/1000\n",
            "9/9 [==============================] - 3s 230ms/step - loss: 0.6973 - acc: 0.6536 - val_loss: 0.7562 - val_acc: 0.4219\n",
            "Epoch 776/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6395 - acc: 0.6629 - val_loss: 0.7254 - val_acc: 0.5312\n",
            "Epoch 777/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6529 - acc: 0.6592 - val_loss: 0.7219 - val_acc: 0.4844\n",
            "Epoch 778/1000\n",
            "9/9 [==============================] - 4s 263ms/step - loss: 0.6519 - acc: 0.6536 - val_loss: 0.7789 - val_acc: 0.4375\n",
            "Epoch 779/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6493 - acc: 0.6573 - val_loss: 0.7404 - val_acc: 0.5000\n",
            "Epoch 780/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6829 - acc: 0.6423 - val_loss: 0.7762 - val_acc: 0.4844\n",
            "Epoch 781/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6204 - acc: 0.6891 - val_loss: 0.7811 - val_acc: 0.4375\n",
            "Epoch 782/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6612 - acc: 0.6517 - val_loss: 0.7111 - val_acc: 0.5156\n",
            "Epoch 783/1000\n",
            "9/9 [==============================] - 3s 234ms/step - loss: 0.6317 - acc: 0.6892 - val_loss: 0.7428 - val_acc: 0.5000\n",
            "Epoch 784/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.5977 - acc: 0.6948 - val_loss: 0.7510 - val_acc: 0.5000\n",
            "Epoch 785/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6261 - acc: 0.6910 - val_loss: 0.7718 - val_acc: 0.4375\n",
            "Epoch 786/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6401 - acc: 0.6760 - val_loss: 0.7728 - val_acc: 0.4531\n",
            "Epoch 787/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6686 - acc: 0.6461 - val_loss: 0.7726 - val_acc: 0.4688\n",
            "Epoch 788/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.6518 - acc: 0.6667 - val_loss: 0.7229 - val_acc: 0.5312\n",
            "Epoch 789/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.5978 - acc: 0.7060 - val_loss: 0.7141 - val_acc: 0.5469\n",
            "Epoch 790/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6367 - acc: 0.6760 - val_loss: 0.7757 - val_acc: 0.4688\n",
            "Epoch 791/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6327 - acc: 0.6704 - val_loss: 0.7676 - val_acc: 0.4531\n",
            "Epoch 792/1000\n",
            "9/9 [==============================] - 3s 278ms/step - loss: 0.6474 - acc: 0.6536 - val_loss: 0.7122 - val_acc: 0.5312\n",
            "Epoch 793/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6084 - acc: 0.6835 - val_loss: 0.7361 - val_acc: 0.4844\n",
            "Epoch 794/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6283 - acc: 0.6816 - val_loss: 0.7867 - val_acc: 0.4844\n",
            "Epoch 795/1000\n",
            "9/9 [==============================] - 3s 286ms/step - loss: 0.6573 - acc: 0.6629 - val_loss: 0.7469 - val_acc: 0.4688\n",
            "Epoch 796/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6169 - acc: 0.6835 - val_loss: 0.7479 - val_acc: 0.5312\n",
            "Epoch 797/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6622 - acc: 0.6685 - val_loss: 0.7542 - val_acc: 0.4844\n",
            "Epoch 798/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6406 - acc: 0.6667 - val_loss: 0.6990 - val_acc: 0.5469\n",
            "Epoch 799/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6554 - acc: 0.6854 - val_loss: 0.7128 - val_acc: 0.5000\n",
            "Epoch 800/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.5955 - acc: 0.6997 - val_loss: 0.7837 - val_acc: 0.4375\n",
            "Epoch 801/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6370 - acc: 0.6573 - val_loss: 0.7415 - val_acc: 0.4844\n",
            "Epoch 802/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6017 - acc: 0.7135 - val_loss: 0.7009 - val_acc: 0.5312\n",
            "Epoch 803/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.6211 - acc: 0.6835 - val_loss: 0.7339 - val_acc: 0.4844\n",
            "Epoch 804/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6844 - acc: 0.6498 - val_loss: 0.6977 - val_acc: 0.5625\n",
            "Epoch 805/1000\n",
            "9/9 [==============================] - 3s 277ms/step - loss: 0.6282 - acc: 0.6779 - val_loss: 0.7470 - val_acc: 0.4688\n",
            "Epoch 806/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6871 - acc: 0.6292 - val_loss: 0.6651 - val_acc: 0.5781\n",
            "Epoch 807/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6187 - acc: 0.6798 - val_loss: 0.7325 - val_acc: 0.5000\n",
            "Epoch 808/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6553 - acc: 0.6536 - val_loss: 0.7615 - val_acc: 0.4219\n",
            "Epoch 809/1000\n",
            "9/9 [==============================] - 3s 194ms/step - loss: 0.6555 - acc: 0.6536 - val_loss: 0.7320 - val_acc: 0.4844\n",
            "Epoch 810/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6052 - acc: 0.6854 - val_loss: 0.7419 - val_acc: 0.4688\n",
            "Epoch 811/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6226 - acc: 0.6873 - val_loss: 0.7403 - val_acc: 0.4531\n",
            "Epoch 812/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.5857 - acc: 0.6723 - val_loss: 0.7165 - val_acc: 0.5469\n",
            "Epoch 813/1000\n",
            "9/9 [==============================] - 3s 282ms/step - loss: 0.6486 - acc: 0.6610 - val_loss: 0.6751 - val_acc: 0.5625\n",
            "Epoch 814/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6254 - acc: 0.6835 - val_loss: 0.6908 - val_acc: 0.5625\n",
            "Epoch 815/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6296 - acc: 0.6835 - val_loss: 0.7587 - val_acc: 0.4844\n",
            "Epoch 816/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6459 - acc: 0.6929 - val_loss: 0.7004 - val_acc: 0.5312\n",
            "Epoch 817/1000\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.6086 - acc: 0.6875 - val_loss: 0.7416 - val_acc: 0.4844\n",
            "Epoch 818/1000\n",
            "9/9 [==============================] - 3s 236ms/step - loss: 0.5960 - acc: 0.6944 - val_loss: 0.7007 - val_acc: 0.5000\n",
            "Epoch 819/1000\n",
            "9/9 [==============================] - 3s 195ms/step - loss: 0.6281 - acc: 0.6779 - val_loss: 0.7535 - val_acc: 0.4688\n",
            "Epoch 820/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6018 - acc: 0.6966 - val_loss: 0.7784 - val_acc: 0.4375\n",
            "Epoch 821/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6240 - acc: 0.6966 - val_loss: 0.7598 - val_acc: 0.5000\n",
            "Epoch 822/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6609 - acc: 0.6685 - val_loss: 0.7198 - val_acc: 0.4688\n",
            "Epoch 823/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6615 - acc: 0.6423 - val_loss: 0.6958 - val_acc: 0.5469\n",
            "Epoch 824/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6031 - acc: 0.6910 - val_loss: 0.7308 - val_acc: 0.5312\n",
            "Epoch 825/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6134 - acc: 0.6760 - val_loss: 0.7129 - val_acc: 0.4688\n",
            "Epoch 826/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6495 - acc: 0.6367 - val_loss: 0.7630 - val_acc: 0.4844\n",
            "Epoch 827/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6585 - acc: 0.6779 - val_loss: 0.7852 - val_acc: 0.4375\n",
            "Epoch 828/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6573 - acc: 0.6292 - val_loss: 0.7353 - val_acc: 0.4375\n",
            "Epoch 829/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6022 - acc: 0.6854 - val_loss: 0.7529 - val_acc: 0.4844\n",
            "Epoch 830/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.6298 - acc: 0.6517 - val_loss: 0.7231 - val_acc: 0.5156\n",
            "Epoch 831/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6216 - acc: 0.6648 - val_loss: 0.7590 - val_acc: 0.5000\n",
            "Epoch 832/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6213 - acc: 0.6816 - val_loss: 0.7365 - val_acc: 0.5156\n",
            "Epoch 833/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6731 - acc: 0.6498 - val_loss: 0.6994 - val_acc: 0.5156\n",
            "Epoch 834/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6155 - acc: 0.6667 - val_loss: 0.7324 - val_acc: 0.5000\n",
            "Epoch 835/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6154 - acc: 0.6948 - val_loss: 0.7120 - val_acc: 0.5000\n",
            "Epoch 836/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6286 - acc: 0.6858 - val_loss: 0.7311 - val_acc: 0.5000\n",
            "Epoch 837/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6609 - acc: 0.6685 - val_loss: 0.7556 - val_acc: 0.4531\n",
            "Epoch 838/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6716 - acc: 0.6685 - val_loss: 0.7808 - val_acc: 0.4375\n",
            "Epoch 839/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6310 - acc: 0.6404 - val_loss: 0.7811 - val_acc: 0.4375\n",
            "Epoch 840/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6410 - acc: 0.6742 - val_loss: 0.7338 - val_acc: 0.4375\n",
            "Epoch 841/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6218 - acc: 0.6929 - val_loss: 0.7134 - val_acc: 0.4844\n",
            "Epoch 842/1000\n",
            "9/9 [==============================] - 3s 216ms/step - loss: 0.6294 - acc: 0.6580 - val_loss: 0.7326 - val_acc: 0.4688\n",
            "Epoch 843/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6294 - acc: 0.6798 - val_loss: 0.7605 - val_acc: 0.4688\n",
            "Epoch 844/1000\n",
            "9/9 [==============================] - 3s 234ms/step - loss: 0.6020 - acc: 0.6892 - val_loss: 0.6762 - val_acc: 0.5156\n",
            "Epoch 845/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6928 - acc: 0.6236 - val_loss: 0.7055 - val_acc: 0.5312\n",
            "Epoch 846/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6348 - acc: 0.6597 - val_loss: 0.7056 - val_acc: 0.5312\n",
            "Epoch 847/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.6454 - acc: 0.6840 - val_loss: 0.7494 - val_acc: 0.4219\n",
            "Epoch 848/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6314 - acc: 0.6742 - val_loss: 0.7858 - val_acc: 0.4062\n",
            "Epoch 849/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6426 - acc: 0.6610 - val_loss: 0.7380 - val_acc: 0.4375\n",
            "Epoch 850/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6152 - acc: 0.6985 - val_loss: 0.7050 - val_acc: 0.4844\n",
            "Epoch 851/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6388 - acc: 0.6536 - val_loss: 0.7803 - val_acc: 0.3906\n",
            "Epoch 852/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.6112 - acc: 0.6615 - val_loss: 0.6962 - val_acc: 0.5156\n",
            "Epoch 853/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.6813 - acc: 0.6517 - val_loss: 0.8425 - val_acc: 0.3125\n",
            "Epoch 854/1000\n",
            "9/9 [==============================] - 3s 348ms/step - loss: 0.6645 - acc: 0.6517 - val_loss: 0.7070 - val_acc: 0.4688\n",
            "Epoch 855/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6564 - acc: 0.6610 - val_loss: 0.7275 - val_acc: 0.4531\n",
            "Epoch 856/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6106 - acc: 0.7118 - val_loss: 0.7530 - val_acc: 0.4688\n",
            "Epoch 857/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.5992 - acc: 0.6985 - val_loss: 0.7410 - val_acc: 0.4531\n",
            "Epoch 858/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6545 - acc: 0.6723 - val_loss: 0.7454 - val_acc: 0.4688\n",
            "Epoch 859/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.6107 - acc: 0.6929 - val_loss: 0.7319 - val_acc: 0.4688\n",
            "Epoch 860/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6421 - acc: 0.6536 - val_loss: 0.7653 - val_acc: 0.4375\n",
            "Epoch 861/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6061 - acc: 0.6742 - val_loss: 0.7552 - val_acc: 0.4531\n",
            "Epoch 862/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6409 - acc: 0.6610 - val_loss: 0.7390 - val_acc: 0.4844\n",
            "Epoch 863/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.6474 - acc: 0.6648 - val_loss: 0.7540 - val_acc: 0.4688\n",
            "Epoch 864/1000\n",
            "9/9 [==============================] - 3s 197ms/step - loss: 0.6625 - acc: 0.6742 - val_loss: 0.7766 - val_acc: 0.4531\n",
            "Epoch 865/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6542 - acc: 0.6517 - val_loss: 0.7706 - val_acc: 0.3906\n",
            "Epoch 866/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6262 - acc: 0.6929 - val_loss: 0.6976 - val_acc: 0.4688\n",
            "Epoch 867/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6754 - acc: 0.6348 - val_loss: 0.7528 - val_acc: 0.4219\n",
            "Epoch 868/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6319 - acc: 0.6816 - val_loss: 0.7485 - val_acc: 0.5312\n",
            "Epoch 869/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6031 - acc: 0.7049 - val_loss: 0.7680 - val_acc: 0.4062\n",
            "Epoch 870/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.5863 - acc: 0.7041 - val_loss: 0.7749 - val_acc: 0.4375\n",
            "Epoch 871/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6788 - acc: 0.6554 - val_loss: 0.7108 - val_acc: 0.4844\n",
            "Epoch 872/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6233 - acc: 0.6779 - val_loss: 0.8068 - val_acc: 0.3750\n",
            "Epoch 873/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6034 - acc: 0.6966 - val_loss: 0.6917 - val_acc: 0.5156\n",
            "Epoch 874/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6519 - acc: 0.6592 - val_loss: 0.7176 - val_acc: 0.4844\n",
            "Epoch 875/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6637 - acc: 0.6461 - val_loss: 0.7259 - val_acc: 0.4844\n",
            "Epoch 876/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.5848 - acc: 0.7060 - val_loss: 0.7342 - val_acc: 0.5156\n",
            "Epoch 877/1000\n",
            "9/9 [==============================] - 3s 201ms/step - loss: 0.6224 - acc: 0.6704 - val_loss: 0.7208 - val_acc: 0.5156\n",
            "Epoch 878/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6340 - acc: 0.6498 - val_loss: 0.7042 - val_acc: 0.5000\n",
            "Epoch 879/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6345 - acc: 0.6479 - val_loss: 0.7449 - val_acc: 0.4531\n",
            "Epoch 880/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.5929 - acc: 0.6948 - val_loss: 0.6834 - val_acc: 0.5312\n",
            "Epoch 881/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6629 - acc: 0.6760 - val_loss: 0.7400 - val_acc: 0.4844\n",
            "Epoch 882/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6257 - acc: 0.6685 - val_loss: 0.7640 - val_acc: 0.4375\n",
            "Epoch 883/1000\n",
            "9/9 [==============================] - 3s 282ms/step - loss: 0.6658 - acc: 0.6479 - val_loss: 0.7476 - val_acc: 0.5312\n",
            "Epoch 884/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6219 - acc: 0.6723 - val_loss: 0.7567 - val_acc: 0.5000\n",
            "Epoch 885/1000\n",
            "9/9 [==============================] - 3s 229ms/step - loss: 0.6304 - acc: 0.6891 - val_loss: 0.7162 - val_acc: 0.5000\n",
            "Epoch 886/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.6374 - acc: 0.6723 - val_loss: 0.7695 - val_acc: 0.4844\n",
            "Epoch 887/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6496 - acc: 0.6760 - val_loss: 0.7628 - val_acc: 0.4844\n",
            "Epoch 888/1000\n",
            "9/9 [==============================] - 3s 279ms/step - loss: 0.6326 - acc: 0.6648 - val_loss: 0.6930 - val_acc: 0.5625\n",
            "Epoch 889/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6467 - acc: 0.6554 - val_loss: 0.7019 - val_acc: 0.5156\n",
            "Epoch 890/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6048 - acc: 0.6948 - val_loss: 0.7236 - val_acc: 0.4844\n",
            "Epoch 891/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6154 - acc: 0.6854 - val_loss: 0.7239 - val_acc: 0.4844\n",
            "Epoch 892/1000\n",
            "9/9 [==============================] - 3s 238ms/step - loss: 0.6223 - acc: 0.6458 - val_loss: 0.7379 - val_acc: 0.4688\n",
            "Epoch 893/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6122 - acc: 0.7079 - val_loss: 0.6933 - val_acc: 0.4688\n",
            "Epoch 894/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6246 - acc: 0.6742 - val_loss: 0.7450 - val_acc: 0.4688\n",
            "Epoch 895/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6278 - acc: 0.6723 - val_loss: 0.7505 - val_acc: 0.4219\n",
            "Epoch 896/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6358 - acc: 0.6479 - val_loss: 0.7066 - val_acc: 0.5000\n",
            "Epoch 897/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.6442 - acc: 0.6597 - val_loss: 0.7385 - val_acc: 0.4844\n",
            "Epoch 898/1000\n",
            "9/9 [==============================] - 3s 227ms/step - loss: 0.6294 - acc: 0.6910 - val_loss: 0.7402 - val_acc: 0.4219\n",
            "Epoch 899/1000\n",
            "9/9 [==============================] - 3s 198ms/step - loss: 0.6229 - acc: 0.6498 - val_loss: 0.7272 - val_acc: 0.4531\n",
            "Epoch 900/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6042 - acc: 0.6798 - val_loss: 0.7895 - val_acc: 0.4844\n",
            "Epoch 901/1000\n",
            "9/9 [==============================] - 3s 295ms/step - loss: 0.6261 - acc: 0.6536 - val_loss: 0.7586 - val_acc: 0.4688\n",
            "Epoch 902/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6047 - acc: 0.6760 - val_loss: 0.7580 - val_acc: 0.4688\n",
            "Epoch 903/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6538 - acc: 0.6816 - val_loss: 0.7836 - val_acc: 0.4375\n",
            "Epoch 904/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.6317 - acc: 0.6667 - val_loss: 0.7264 - val_acc: 0.5156\n",
            "Epoch 905/1000\n",
            "9/9 [==============================] - 3s 232ms/step - loss: 0.5821 - acc: 0.6985 - val_loss: 0.7437 - val_acc: 0.4688\n",
            "Epoch 906/1000\n",
            "9/9 [==============================] - 3s 199ms/step - loss: 0.6278 - acc: 0.6929 - val_loss: 0.7460 - val_acc: 0.5000\n",
            "Epoch 907/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6603 - acc: 0.6372 - val_loss: 0.7782 - val_acc: 0.4375\n",
            "Epoch 908/1000\n",
            "9/9 [==============================] - 3s 189ms/step - loss: 0.5737 - acc: 0.6760 - val_loss: 0.7412 - val_acc: 0.5000\n",
            "Epoch 909/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6030 - acc: 0.6592 - val_loss: 0.7682 - val_acc: 0.4219\n",
            "Epoch 910/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6109 - acc: 0.6592 - val_loss: 0.7499 - val_acc: 0.4688\n",
            "Epoch 911/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6116 - acc: 0.6742 - val_loss: 0.6857 - val_acc: 0.5156\n",
            "Epoch 912/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6086 - acc: 0.6816 - val_loss: 0.7414 - val_acc: 0.4844\n",
            "Epoch 913/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6150 - acc: 0.6854 - val_loss: 0.7304 - val_acc: 0.4844\n",
            "Epoch 914/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6276 - acc: 0.6592 - val_loss: 0.7367 - val_acc: 0.4531\n",
            "Epoch 915/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.6022 - acc: 0.6927 - val_loss: 0.7530 - val_acc: 0.4688\n",
            "Epoch 916/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6032 - acc: 0.6723 - val_loss: 0.7967 - val_acc: 0.4531\n",
            "Epoch 917/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.5956 - acc: 0.6929 - val_loss: 0.7238 - val_acc: 0.4844\n",
            "Epoch 918/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6337 - acc: 0.6648 - val_loss: 0.7245 - val_acc: 0.5312\n",
            "Epoch 919/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6153 - acc: 0.6816 - val_loss: 0.7350 - val_acc: 0.5156\n",
            "Epoch 920/1000\n",
            "9/9 [==============================] - 3s 237ms/step - loss: 0.6087 - acc: 0.6701 - val_loss: 0.7303 - val_acc: 0.4531\n",
            "Epoch 921/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6162 - acc: 0.6816 - val_loss: 0.7005 - val_acc: 0.5156\n",
            "Epoch 922/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.5964 - acc: 0.6648 - val_loss: 0.6965 - val_acc: 0.5312\n",
            "Epoch 923/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.5884 - acc: 0.6723 - val_loss: 0.7103 - val_acc: 0.4688\n",
            "Epoch 924/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6454 - acc: 0.6498 - val_loss: 0.7785 - val_acc: 0.3906\n",
            "Epoch 925/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.5999 - acc: 0.6648 - val_loss: 0.7855 - val_acc: 0.3750\n",
            "Epoch 926/1000\n",
            "9/9 [==============================] - 3s 211ms/step - loss: 0.6102 - acc: 0.6910 - val_loss: 0.7405 - val_acc: 0.4219\n",
            "Epoch 927/1000\n",
            "9/9 [==============================] - 3s 286ms/step - loss: 0.5948 - acc: 0.7079 - val_loss: 0.7435 - val_acc: 0.4531\n",
            "Epoch 928/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6209 - acc: 0.6685 - val_loss: 0.6801 - val_acc: 0.5469\n",
            "Epoch 929/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.6090 - acc: 0.6854 - val_loss: 0.7581 - val_acc: 0.4688\n",
            "Epoch 930/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.5901 - acc: 0.6760 - val_loss: 0.6762 - val_acc: 0.5156\n",
            "Epoch 931/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6316 - acc: 0.6779 - val_loss: 0.7126 - val_acc: 0.4531\n",
            "Epoch 932/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.5986 - acc: 0.7022 - val_loss: 0.7203 - val_acc: 0.5156\n",
            "Epoch 933/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.6189 - acc: 0.6985 - val_loss: 0.7791 - val_acc: 0.4062\n",
            "Epoch 934/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.5797 - acc: 0.7060 - val_loss: 0.7303 - val_acc: 0.5156\n",
            "Epoch 935/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.6030 - acc: 0.6835 - val_loss: 0.7438 - val_acc: 0.4375\n",
            "Epoch 936/1000\n",
            "9/9 [==============================] - 3s 245ms/step - loss: 0.6203 - acc: 0.6723 - val_loss: 0.7052 - val_acc: 0.4844\n",
            "Epoch 937/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6478 - acc: 0.6592 - val_loss: 0.7259 - val_acc: 0.4688\n",
            "Epoch 938/1000\n",
            "9/9 [==============================] - 3s 220ms/step - loss: 0.5976 - acc: 0.6873 - val_loss: 0.6839 - val_acc: 0.5312\n",
            "Epoch 939/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6400 - acc: 0.6816 - val_loss: 0.7330 - val_acc: 0.5156\n",
            "Epoch 940/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6446 - acc: 0.6498 - val_loss: 0.7316 - val_acc: 0.5000\n",
            "Epoch 941/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6173 - acc: 0.6816 - val_loss: 0.7578 - val_acc: 0.5000\n",
            "Epoch 942/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6258 - acc: 0.6835 - val_loss: 0.7523 - val_acc: 0.5000\n",
            "Epoch 943/1000\n",
            "9/9 [==============================] - 3s 207ms/step - loss: 0.6175 - acc: 0.6798 - val_loss: 0.7379 - val_acc: 0.4688\n",
            "Epoch 944/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6437 - acc: 0.6330 - val_loss: 0.7241 - val_acc: 0.5312\n",
            "Epoch 945/1000\n",
            "9/9 [==============================] - 3s 208ms/step - loss: 0.6003 - acc: 0.6704 - val_loss: 0.7179 - val_acc: 0.5312\n",
            "Epoch 946/1000\n",
            "9/9 [==============================] - 3s 196ms/step - loss: 0.6255 - acc: 0.6498 - val_loss: 0.7292 - val_acc: 0.4688\n",
            "Epoch 947/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.6183 - acc: 0.6667 - val_loss: 0.7614 - val_acc: 0.4688\n",
            "Epoch 948/1000\n",
            "9/9 [==============================] - 3s 192ms/step - loss: 0.6083 - acc: 0.6760 - val_loss: 0.7802 - val_acc: 0.3750\n",
            "Epoch 949/1000\n",
            "9/9 [==============================] - 3s 187ms/step - loss: 0.6237 - acc: 0.6798 - val_loss: 0.7585 - val_acc: 0.4531\n",
            "Epoch 950/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.6400 - acc: 0.6816 - val_loss: 0.6913 - val_acc: 0.5469\n",
            "Epoch 951/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6269 - acc: 0.6704 - val_loss: 0.7463 - val_acc: 0.5156\n",
            "Epoch 952/1000\n",
            "9/9 [==============================] - 3s 203ms/step - loss: 0.6085 - acc: 0.6966 - val_loss: 0.7494 - val_acc: 0.4688\n",
            "Epoch 953/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.6183 - acc: 0.6798 - val_loss: 0.7312 - val_acc: 0.4844\n",
            "Epoch 954/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.6326 - acc: 0.6610 - val_loss: 0.7499 - val_acc: 0.4375\n",
            "Epoch 955/1000\n",
            "9/9 [==============================] - 3s 209ms/step - loss: 0.5737 - acc: 0.7022 - val_loss: 0.7378 - val_acc: 0.4688\n",
            "Epoch 956/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.5646 - acc: 0.7135 - val_loss: 0.7097 - val_acc: 0.5000\n",
            "Epoch 957/1000\n",
            "9/9 [==============================] - 3s 215ms/step - loss: 0.5645 - acc: 0.7060 - val_loss: 0.6809 - val_acc: 0.5156\n",
            "Epoch 958/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.5950 - acc: 0.6948 - val_loss: 0.7533 - val_acc: 0.5000\n",
            "Epoch 959/1000\n",
            "9/9 [==============================] - 3s 282ms/step - loss: 0.6290 - acc: 0.6592 - val_loss: 0.6836 - val_acc: 0.5312\n",
            "Epoch 960/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6178 - acc: 0.6536 - val_loss: 0.7057 - val_acc: 0.5156\n",
            "Epoch 961/1000\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.6140 - acc: 0.6704 - val_loss: 0.7495 - val_acc: 0.5156\n",
            "Epoch 962/1000\n",
            "9/9 [==============================] - 3s 218ms/step - loss: 0.5500 - acc: 0.7116 - val_loss: 0.6942 - val_acc: 0.5312\n",
            "Epoch 963/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6014 - acc: 0.6910 - val_loss: 0.7352 - val_acc: 0.5000\n",
            "Epoch 964/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 0.5964 - acc: 0.6948 - val_loss: 0.6909 - val_acc: 0.5625\n",
            "Epoch 965/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.6205 - acc: 0.6667 - val_loss: 0.7252 - val_acc: 0.5156\n",
            "Epoch 966/1000\n",
            "9/9 [==============================] - 3s 206ms/step - loss: 0.6016 - acc: 0.6779 - val_loss: 0.7301 - val_acc: 0.5156\n",
            "Epoch 967/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.5946 - acc: 0.6910 - val_loss: 0.7528 - val_acc: 0.4844\n",
            "Epoch 968/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.6431 - acc: 0.6592 - val_loss: 0.7504 - val_acc: 0.4688\n",
            "Epoch 969/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6109 - acc: 0.6854 - val_loss: 0.7632 - val_acc: 0.4844\n",
            "Epoch 970/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.5945 - acc: 0.6816 - val_loss: 0.7695 - val_acc: 0.4688\n",
            "Epoch 971/1000\n",
            "9/9 [==============================] - 3s 235ms/step - loss: 0.6039 - acc: 0.6966 - val_loss: 0.6663 - val_acc: 0.5469\n",
            "Epoch 972/1000\n",
            "9/9 [==============================] - 3s 224ms/step - loss: 0.5937 - acc: 0.7004 - val_loss: 0.7489 - val_acc: 0.4688\n",
            "Epoch 973/1000\n",
            "9/9 [==============================] - 3s 222ms/step - loss: 0.6230 - acc: 0.6648 - val_loss: 0.7636 - val_acc: 0.4688\n",
            "Epoch 974/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.6381 - acc: 0.6873 - val_loss: 0.6549 - val_acc: 0.5781\n",
            "Epoch 975/1000\n",
            "9/9 [==============================] - 3s 200ms/step - loss: 0.5927 - acc: 0.6798 - val_loss: 0.7142 - val_acc: 0.5156\n",
            "Epoch 976/1000\n",
            "9/9 [==============================] - 3s 276ms/step - loss: 0.5894 - acc: 0.7097 - val_loss: 0.6890 - val_acc: 0.5312\n",
            "Epoch 977/1000\n",
            "9/9 [==============================] - 3s 242ms/step - loss: 0.5980 - acc: 0.6823 - val_loss: 0.7570 - val_acc: 0.4688\n",
            "Epoch 978/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6077 - acc: 0.6835 - val_loss: 0.7228 - val_acc: 0.5156\n",
            "Epoch 979/1000\n",
            "9/9 [==============================] - 3s 221ms/step - loss: 0.6517 - acc: 0.6404 - val_loss: 0.7417 - val_acc: 0.4531\n",
            "Epoch 980/1000\n",
            "9/9 [==============================] - 3s 228ms/step - loss: 0.5972 - acc: 0.6966 - val_loss: 0.7025 - val_acc: 0.5469\n",
            "Epoch 981/1000\n",
            "9/9 [==============================] - 3s 214ms/step - loss: 0.6180 - acc: 0.6779 - val_loss: 0.7043 - val_acc: 0.5000\n",
            "Epoch 982/1000\n",
            "9/9 [==============================] - 3s 244ms/step - loss: 0.6046 - acc: 0.6779 - val_loss: 0.7445 - val_acc: 0.4531\n",
            "Epoch 983/1000\n",
            "9/9 [==============================] - 3s 283ms/step - loss: 0.6051 - acc: 0.6966 - val_loss: 0.7127 - val_acc: 0.4688\n",
            "Epoch 984/1000\n",
            "9/9 [==============================] - 3s 202ms/step - loss: 0.5648 - acc: 0.7266 - val_loss: 0.7748 - val_acc: 0.4375\n",
            "Epoch 985/1000\n",
            "9/9 [==============================] - 3s 219ms/step - loss: 0.6042 - acc: 0.7004 - val_loss: 0.7437 - val_acc: 0.4062\n",
            "Epoch 986/1000\n",
            "9/9 [==============================] - 3s 210ms/step - loss: 0.6117 - acc: 0.6798 - val_loss: 0.7056 - val_acc: 0.4844\n",
            "Epoch 987/1000\n",
            "9/9 [==============================] - 3s 213ms/step - loss: 0.5866 - acc: 0.6985 - val_loss: 0.7090 - val_acc: 0.5000\n",
            "Epoch 988/1000\n",
            "9/9 [==============================] - 3s 205ms/step - loss: 0.6190 - acc: 0.6498 - val_loss: 0.7278 - val_acc: 0.5000\n",
            "Epoch 989/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.5933 - acc: 0.6948 - val_loss: 0.7323 - val_acc: 0.4375\n",
            "Epoch 990/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6200 - acc: 0.6891 - val_loss: 0.6920 - val_acc: 0.5156\n",
            "Epoch 991/1000\n",
            "9/9 [==============================] - 3s 225ms/step - loss: 0.6578 - acc: 0.6311 - val_loss: 0.7598 - val_acc: 0.4844\n",
            "Epoch 992/1000\n",
            "9/9 [==============================] - 3s 284ms/step - loss: 0.5829 - acc: 0.7135 - val_loss: 0.7495 - val_acc: 0.4844\n",
            "Epoch 993/1000\n",
            "9/9 [==============================] - 3s 223ms/step - loss: 0.6351 - acc: 0.6573 - val_loss: 0.6934 - val_acc: 0.5469\n",
            "Epoch 994/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6072 - acc: 0.6873 - val_loss: 0.7165 - val_acc: 0.4688\n",
            "Epoch 995/1000\n",
            "9/9 [==============================] - 3s 283ms/step - loss: 0.6187 - acc: 0.7022 - val_loss: 0.7139 - val_acc: 0.4844\n",
            "Epoch 996/1000\n",
            "9/9 [==============================] - 3s 281ms/step - loss: 0.5873 - acc: 0.6723 - val_loss: 0.7338 - val_acc: 0.5156\n",
            "Epoch 997/1000\n",
            "9/9 [==============================] - 3s 217ms/step - loss: 0.6132 - acc: 0.6704 - val_loss: 0.7509 - val_acc: 0.4844\n",
            "Epoch 998/1000\n",
            "9/9 [==============================] - 3s 212ms/step - loss: 0.6651 - acc: 0.6685 - val_loss: 0.6935 - val_acc: 0.5156\n",
            "Epoch 999/1000\n",
            "9/9 [==============================] - 3s 204ms/step - loss: 0.6037 - acc: 0.7097 - val_loss: 0.7297 - val_acc: 0.5156\n",
            "Epoch 1000/1000\n",
            "9/9 [==============================] - 3s 231ms/step - loss: 0.6186 - acc: 0.6684 - val_loss: 0.7387 - val_acc: 0.4688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "5b5a7073-d8a6-4222-912e-5c6079f68229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.187384843826294,\n",
              "  1.1527215242385864,\n",
              "  1.0837653875350952,\n",
              "  1.1960501670837402,\n",
              "  1.0534578561782837,\n",
              "  1.1424058675765991,\n",
              "  1.1627954244613647,\n",
              "  1.0695281028747559,\n",
              "  1.051576018333435,\n",
              "  1.148503303527832,\n",
              "  1.1157455444335938,\n",
              "  1.1685436964035034,\n",
              "  1.1658272743225098,\n",
              "  1.1086229085922241,\n",
              "  1.1048370599746704,\n",
              "  1.132653832435608,\n",
              "  1.1309717893600464,\n",
              "  1.0339314937591553,\n",
              "  1.1425851583480835,\n",
              "  1.0328998565673828,\n",
              "  1.0566864013671875,\n",
              "  1.111307144165039,\n",
              "  1.1167386770248413,\n",
              "  1.140394926071167,\n",
              "  1.0376288890838623,\n",
              "  1.1089537143707275,\n",
              "  1.1023359298706055,\n",
              "  1.1872419118881226,\n",
              "  1.0912747383117676,\n",
              "  1.0394474267959595,\n",
              "  1.1155622005462646,\n",
              "  1.1720962524414062,\n",
              "  1.0758227109909058,\n",
              "  1.1001074314117432,\n",
              "  1.007035255432129,\n",
              "  1.1015969514846802,\n",
              "  1.0044976472854614,\n",
              "  1.1213769912719727,\n",
              "  1.0626412630081177,\n",
              "  1.0483322143554688,\n",
              "  1.0422252416610718,\n",
              "  1.0327669382095337,\n",
              "  1.0461094379425049,\n",
              "  0.9890079498291016,\n",
              "  0.9881175756454468,\n",
              "  0.972524106502533,\n",
              "  1.0643237829208374,\n",
              "  1.1169590950012207,\n",
              "  1.0305089950561523,\n",
              "  0.9589477181434631,\n",
              "  1.0511730909347534,\n",
              "  1.0024006366729736,\n",
              "  1.007306694984436,\n",
              "  1.0361260175704956,\n",
              "  1.0130902528762817,\n",
              "  0.9984873533248901,\n",
              "  1.0076947212219238,\n",
              "  1.0209715366363525,\n",
              "  0.9931510090827942,\n",
              "  1.0069133043289185,\n",
              "  1.021256923675537,\n",
              "  1.0081969499588013,\n",
              "  1.007077693939209,\n",
              "  1.006556510925293,\n",
              "  0.9406854510307312,\n",
              "  0.9997770190238953,\n",
              "  1.0156123638153076,\n",
              "  0.9984663724899292,\n",
              "  1.0034970045089722,\n",
              "  0.9858382344245911,\n",
              "  0.9863251447677612,\n",
              "  0.9805727601051331,\n",
              "  1.0011143684387207,\n",
              "  1.0219203233718872,\n",
              "  1.0498380661010742,\n",
              "  1.0751243829727173,\n",
              "  0.8886350393295288,\n",
              "  0.9129226207733154,\n",
              "  0.9735738635063171,\n",
              "  0.9796136617660522,\n",
              "  0.9387982487678528,\n",
              "  1.012608528137207,\n",
              "  0.9765863418579102,\n",
              "  0.9010471105575562,\n",
              "  0.9343825578689575,\n",
              "  0.9841273427009583,\n",
              "  0.9401086568832397,\n",
              "  0.9528120756149292,\n",
              "  0.9724411368370056,\n",
              "  0.9680148363113403,\n",
              "  0.9310900568962097,\n",
              "  0.9428434371948242,\n",
              "  0.957744836807251,\n",
              "  0.962046205997467,\n",
              "  0.8999198079109192,\n",
              "  0.9651931524276733,\n",
              "  0.9319577813148499,\n",
              "  0.9629955291748047,\n",
              "  0.9638726711273193,\n",
              "  0.9664517045021057,\n",
              "  0.9223032593727112,\n",
              "  0.9251462817192078,\n",
              "  0.9870624542236328,\n",
              "  0.9318690299987793,\n",
              "  0.9100069999694824,\n",
              "  0.9268168807029724,\n",
              "  0.9170004725456238,\n",
              "  0.9083564281463623,\n",
              "  0.8867685794830322,\n",
              "  0.8919934034347534,\n",
              "  0.9041098356246948,\n",
              "  0.8573699593544006,\n",
              "  0.902371346950531,\n",
              "  0.9559886455535889,\n",
              "  0.9499185085296631,\n",
              "  0.9357850551605225,\n",
              "  0.8637096285820007,\n",
              "  0.8895344734191895,\n",
              "  0.9006140828132629,\n",
              "  0.9323792457580566,\n",
              "  0.941548228263855,\n",
              "  0.8955261707305908,\n",
              "  0.9508710503578186,\n",
              "  0.9123802781105042,\n",
              "  0.9142388701438904,\n",
              "  0.8448783755302429,\n",
              "  0.912922739982605,\n",
              "  0.9735848307609558,\n",
              "  0.9164898991584778,\n",
              "  0.9325082898139954,\n",
              "  0.9201597571372986,\n",
              "  0.8894978761672974,\n",
              "  0.8910154104232788,\n",
              "  0.8718979358673096,\n",
              "  0.8334284424781799,\n",
              "  0.8975225687026978,\n",
              "  0.8809646368026733,\n",
              "  0.8106582164764404,\n",
              "  0.8758781552314758,\n",
              "  0.8926817774772644,\n",
              "  0.9054052233695984,\n",
              "  0.8982479572296143,\n",
              "  0.8688368201255798,\n",
              "  0.9159223437309265,\n",
              "  0.9257027506828308,\n",
              "  0.900976836681366,\n",
              "  0.8722637891769409,\n",
              "  0.8278843760490417,\n",
              "  0.852432370185852,\n",
              "  0.9614565372467041,\n",
              "  0.8426561951637268,\n",
              "  0.8758423328399658,\n",
              "  0.867729127407074,\n",
              "  0.8130130767822266,\n",
              "  0.8394515514373779,\n",
              "  0.8975257277488708,\n",
              "  0.9138593077659607,\n",
              "  0.8173364400863647,\n",
              "  0.8588851094245911,\n",
              "  0.9236806035041809,\n",
              "  0.843943178653717,\n",
              "  0.8616659641265869,\n",
              "  0.7867453098297119,\n",
              "  0.8684064745903015,\n",
              "  0.8021485209465027,\n",
              "  0.8870441317558289,\n",
              "  0.8595651388168335,\n",
              "  0.8048550486564636,\n",
              "  0.8563157320022583,\n",
              "  0.8797255754470825,\n",
              "  0.857266902923584,\n",
              "  0.8408075571060181,\n",
              "  0.8830243349075317,\n",
              "  0.7964480519294739,\n",
              "  0.9103913903236389,\n",
              "  0.816571831703186,\n",
              "  0.8485503196716309,\n",
              "  0.8286270499229431,\n",
              "  0.893897533416748,\n",
              "  0.846096932888031,\n",
              "  0.8526625037193298,\n",
              "  0.913102388381958,\n",
              "  0.8805202841758728,\n",
              "  0.8790596127510071,\n",
              "  0.8846901655197144,\n",
              "  0.8120395541191101,\n",
              "  0.8393198251724243,\n",
              "  0.8465290069580078,\n",
              "  0.737460196018219,\n",
              "  0.8408557772636414,\n",
              "  0.8155988454818726,\n",
              "  0.864127516746521,\n",
              "  0.8570805788040161,\n",
              "  0.7856415510177612,\n",
              "  0.7983125448226929,\n",
              "  0.841689944267273,\n",
              "  0.8334833383560181,\n",
              "  0.8193764090538025,\n",
              "  0.8062360882759094,\n",
              "  0.8527454733848572,\n",
              "  0.8050463795661926,\n",
              "  0.7915629744529724,\n",
              "  0.769778847694397,\n",
              "  0.8118242621421814,\n",
              "  0.8175798654556274,\n",
              "  0.8532893061637878,\n",
              "  0.8286694288253784,\n",
              "  0.8194713592529297,\n",
              "  0.8542165160179138,\n",
              "  0.8312196135520935,\n",
              "  0.7713527083396912,\n",
              "  0.816758394241333,\n",
              "  0.8996644020080566,\n",
              "  0.8248090147972107,\n",
              "  0.78407222032547,\n",
              "  0.8183454275131226,\n",
              "  0.7987267971038818,\n",
              "  0.8463247418403625,\n",
              "  0.8739535212516785,\n",
              "  0.7907739281654358,\n",
              "  0.8148104548454285,\n",
              "  0.7934706211090088,\n",
              "  0.8315706849098206,\n",
              "  0.8428695201873779,\n",
              "  0.830024778842926,\n",
              "  0.8493852615356445,\n",
              "  0.8087831735610962,\n",
              "  0.873063325881958,\n",
              "  0.7690871357917786,\n",
              "  0.8041848540306091,\n",
              "  0.8335036635398865,\n",
              "  0.7566084265708923,\n",
              "  0.7913718819618225,\n",
              "  0.8033835887908936,\n",
              "  0.8484607934951782,\n",
              "  0.7591087818145752,\n",
              "  0.7682501673698425,\n",
              "  0.8062401413917542,\n",
              "  0.8600354790687561,\n",
              "  0.8145816326141357,\n",
              "  0.825366735458374,\n",
              "  0.7963629961013794,\n",
              "  0.7749649882316589,\n",
              "  0.8049084544181824,\n",
              "  0.7381262183189392,\n",
              "  0.8417090773582458,\n",
              "  0.7744788527488708,\n",
              "  0.7732424139976501,\n",
              "  0.7847387194633484,\n",
              "  0.7711892127990723,\n",
              "  0.7862566709518433,\n",
              "  0.8197542428970337,\n",
              "  0.7382550239562988,\n",
              "  0.7928563952445984,\n",
              "  0.835089921951294,\n",
              "  0.8024976253509521,\n",
              "  0.8402868509292603,\n",
              "  0.8147537112236023,\n",
              "  0.7976784110069275,\n",
              "  0.7787967920303345,\n",
              "  0.8188928961753845,\n",
              "  0.7891502976417542,\n",
              "  0.8008641004562378,\n",
              "  0.7483187317848206,\n",
              "  0.8117234110832214,\n",
              "  0.8255630731582642,\n",
              "  0.8377668261528015,\n",
              "  0.7678654789924622,\n",
              "  0.8089154958724976,\n",
              "  0.7655835151672363,\n",
              "  0.8157220482826233,\n",
              "  0.8038888573646545,\n",
              "  0.8044081926345825,\n",
              "  0.8243644833564758,\n",
              "  0.7559210062026978,\n",
              "  0.7897557616233826,\n",
              "  0.8082326650619507,\n",
              "  0.7418677806854248,\n",
              "  0.7756421566009521,\n",
              "  0.7486158609390259,\n",
              "  0.8230859637260437,\n",
              "  0.8013364672660828,\n",
              "  0.7400550842285156,\n",
              "  0.7996892333030701,\n",
              "  0.7844264507293701,\n",
              "  0.8013584613800049,\n",
              "  0.8630056381225586,\n",
              "  0.8078559637069702,\n",
              "  0.8233246803283691,\n",
              "  0.7590592503547668,\n",
              "  0.7534072399139404,\n",
              "  0.7717873454093933,\n",
              "  0.7751518487930298,\n",
              "  0.7274420261383057,\n",
              "  0.7519186735153198,\n",
              "  0.7325412631034851,\n",
              "  0.7792280912399292,\n",
              "  0.7621924877166748,\n",
              "  0.7987769842147827,\n",
              "  0.7815108895301819,\n",
              "  0.8308048248291016,\n",
              "  0.715418815612793,\n",
              "  0.7714322209358215,\n",
              "  0.7670609354972839,\n",
              "  0.7443175315856934,\n",
              "  0.7874122262001038,\n",
              "  0.7645050883293152,\n",
              "  0.7946655750274658,\n",
              "  0.7804375886917114,\n",
              "  0.7955504059791565,\n",
              "  0.768845796585083,\n",
              "  0.758782684803009,\n",
              "  0.7405083179473877,\n",
              "  0.7511653900146484,\n",
              "  0.7865603566169739,\n",
              "  0.7596678733825684,\n",
              "  0.7910246849060059,\n",
              "  0.7214520573616028,\n",
              "  0.7498026490211487,\n",
              "  0.7750944495201111,\n",
              "  0.7253961563110352,\n",
              "  0.7466152906417847,\n",
              "  0.7779490351676941,\n",
              "  0.8289168477058411,\n",
              "  0.7329927682876587,\n",
              "  0.7270758152008057,\n",
              "  0.8036248683929443,\n",
              "  0.7514455318450928,\n",
              "  0.7181537747383118,\n",
              "  0.6638031601905823,\n",
              "  0.7818440198898315,\n",
              "  0.8220250010490417,\n",
              "  0.7632582783699036,\n",
              "  0.7511134743690491,\n",
              "  0.8197863101959229,\n",
              "  0.7925388813018799,\n",
              "  0.7627080678939819,\n",
              "  0.7786012291908264,\n",
              "  0.7655606269836426,\n",
              "  0.7336854338645935,\n",
              "  0.7441755533218384,\n",
              "  0.7756506204605103,\n",
              "  0.7324546575546265,\n",
              "  0.7459662556648254,\n",
              "  0.7601061463356018,\n",
              "  0.7488679885864258,\n",
              "  0.7730332612991333,\n",
              "  0.7100153565406799,\n",
              "  0.723663866519928,\n",
              "  0.7323368787765503,\n",
              "  0.7844169735908508,\n",
              "  0.7179681062698364,\n",
              "  0.680877685546875,\n",
              "  0.7808336615562439,\n",
              "  0.7506422996520996,\n",
              "  0.7286032438278198,\n",
              "  0.7499616146087646,\n",
              "  0.7398092150688171,\n",
              "  0.7332298159599304,\n",
              "  0.7364631295204163,\n",
              "  0.7384922504425049,\n",
              "  0.7750177979469299,\n",
              "  0.7829963564872742,\n",
              "  0.7060282826423645,\n",
              "  0.7638208866119385,\n",
              "  0.7094137668609619,\n",
              "  0.8295546174049377,\n",
              "  0.7145071625709534,\n",
              "  0.7513295412063599,\n",
              "  0.7344428896903992,\n",
              "  0.7603135108947754,\n",
              "  0.7342309355735779,\n",
              "  0.8107046484947205,\n",
              "  0.7517294883728027,\n",
              "  0.7476272583007812,\n",
              "  0.7597366571426392,\n",
              "  0.7127112150192261,\n",
              "  0.7496021389961243,\n",
              "  0.7687502503395081,\n",
              "  0.7777772545814514,\n",
              "  0.758700966835022,\n",
              "  0.7897974252700806,\n",
              "  0.7176279425621033,\n",
              "  0.749528169631958,\n",
              "  0.7311197519302368,\n",
              "  0.7067974209785461,\n",
              "  0.720223605632782,\n",
              "  0.7428526282310486,\n",
              "  0.7242218255996704,\n",
              "  0.7746219635009766,\n",
              "  0.7234382629394531,\n",
              "  0.7350184917449951,\n",
              "  0.7435680031776428,\n",
              "  0.8033113479614258,\n",
              "  0.7240684628486633,\n",
              "  0.6815557479858398,\n",
              "  0.7167747020721436,\n",
              "  0.7201427221298218,\n",
              "  0.7530249953269958,\n",
              "  0.6511295437812805,\n",
              "  0.7432771921157837,\n",
              "  0.7162353992462158,\n",
              "  0.7692021131515503,\n",
              "  0.697291374206543,\n",
              "  0.7764265537261963,\n",
              "  0.7420257925987244,\n",
              "  0.7328078746795654,\n",
              "  0.7371001839637756,\n",
              "  0.711769163608551,\n",
              "  0.713079571723938,\n",
              "  0.7157876491546631,\n",
              "  0.7369146347045898,\n",
              "  0.7835596799850464,\n",
              "  0.7658693194389343,\n",
              "  0.7643568515777588,\n",
              "  0.676098644733429,\n",
              "  0.7251976728439331,\n",
              "  0.7388139963150024,\n",
              "  0.747651994228363,\n",
              "  0.8049325346946716,\n",
              "  0.7392014265060425,\n",
              "  0.7645304203033447,\n",
              "  0.7677686214447021,\n",
              "  0.7351688742637634,\n",
              "  0.7533519268035889,\n",
              "  0.6517834663391113,\n",
              "  0.7783343195915222,\n",
              "  0.7430821657180786,\n",
              "  0.7045806646347046,\n",
              "  0.78142911195755,\n",
              "  0.6870284676551819,\n",
              "  0.7721484899520874,\n",
              "  0.7164508700370789,\n",
              "  0.747184157371521,\n",
              "  0.72706139087677,\n",
              "  0.7577010989189148,\n",
              "  0.7135205268859863,\n",
              "  0.7390391826629639,\n",
              "  0.7717722058296204,\n",
              "  0.70941162109375,\n",
              "  0.7164766788482666,\n",
              "  0.6946650147438049,\n",
              "  0.7451558709144592,\n",
              "  0.6859143376350403,\n",
              "  0.6981450915336609,\n",
              "  0.7154684066772461,\n",
              "  0.66631019115448,\n",
              "  0.7576689124107361,\n",
              "  0.738213837146759,\n",
              "  0.8056005239486694,\n",
              "  0.7661144137382507,\n",
              "  0.6853940486907959,\n",
              "  0.7439079880714417,\n",
              "  0.7274284958839417,\n",
              "  0.7508403062820435,\n",
              "  0.687832236289978,\n",
              "  0.7360554337501526,\n",
              "  0.7176271080970764,\n",
              "  0.7164924144744873,\n",
              "  0.7588496208190918,\n",
              "  0.7502623796463013,\n",
              "  0.7111790776252747,\n",
              "  0.7087090611457825,\n",
              "  0.728816568851471,\n",
              "  0.6856199502944946,\n",
              "  0.7135398387908936,\n",
              "  0.7530351281166077,\n",
              "  0.73246830701828,\n",
              "  0.7581841349601746,\n",
              "  0.6388614177703857,\n",
              "  0.6958360075950623,\n",
              "  0.697543740272522,\n",
              "  0.7066109776496887,\n",
              "  0.6972617506980896,\n",
              "  0.74769526720047,\n",
              "  0.730204164981842,\n",
              "  0.7193222641944885,\n",
              "  0.6598327159881592,\n",
              "  0.6868286728858948,\n",
              "  0.7562171816825867,\n",
              "  0.7574693560600281,\n",
              "  0.7057955861091614,\n",
              "  0.6764246225357056,\n",
              "  0.6950065493583679,\n",
              "  0.7215746641159058,\n",
              "  0.7146656513214111,\n",
              "  0.6541116237640381,\n",
              "  0.7043680548667908,\n",
              "  0.7213743925094604,\n",
              "  0.6957511305809021,\n",
              "  0.7276982665061951,\n",
              "  0.7439185380935669,\n",
              "  0.6856987476348877,\n",
              "  0.6874926686286926,\n",
              "  0.7048749327659607,\n",
              "  0.7114579081535339,\n",
              "  0.7135629057884216,\n",
              "  0.7204940915107727,\n",
              "  0.6985780000686646,\n",
              "  0.722891628742218,\n",
              "  0.7365052700042725,\n",
              "  0.6877196431159973,\n",
              "  0.7158765196800232,\n",
              "  0.7251765727996826,\n",
              "  0.6608369946479797,\n",
              "  0.6588397026062012,\n",
              "  0.7133911848068237,\n",
              "  0.7020004391670227,\n",
              "  0.7491142153739929,\n",
              "  0.6951666474342346,\n",
              "  0.737038791179657,\n",
              "  0.708701491355896,\n",
              "  0.7165617346763611,\n",
              "  0.7265220284461975,\n",
              "  0.6650193929672241,\n",
              "  0.6572444438934326,\n",
              "  0.651494562625885,\n",
              "  0.6802839040756226,\n",
              "  0.7008538842201233,\n",
              "  0.7584075331687927,\n",
              "  0.6954448223114014,\n",
              "  0.7245101928710938,\n",
              "  0.6849057674407959,\n",
              "  0.6299768686294556,\n",
              "  0.6927230954170227,\n",
              "  0.6994498372077942,\n",
              "  0.7191880941390991,\n",
              "  0.6890540719032288,\n",
              "  0.6272920370101929,\n",
              "  0.722045361995697,\n",
              "  0.7208054661750793,\n",
              "  0.7088125944137573,\n",
              "  0.7060949802398682,\n",
              "  0.6875031590461731,\n",
              "  0.6668026447296143,\n",
              "  0.6761836409568787,\n",
              "  0.680659830570221,\n",
              "  0.6393559575080872,\n",
              "  0.6866333484649658,\n",
              "  0.673896312713623,\n",
              "  0.7169927358627319,\n",
              "  0.687578022480011,\n",
              "  0.7184499502182007,\n",
              "  0.6248753070831299,\n",
              "  0.6726751327514648,\n",
              "  0.7325961589813232,\n",
              "  0.721524178981781,\n",
              "  0.7277809977531433,\n",
              "  0.6995503902435303,\n",
              "  0.70943284034729,\n",
              "  0.6637976765632629,\n",
              "  0.6222276091575623,\n",
              "  0.6936735510826111,\n",
              "  0.6985118985176086,\n",
              "  0.6828691959381104,\n",
              "  0.6936132311820984,\n",
              "  0.6547889709472656,\n",
              "  0.7348755598068237,\n",
              "  0.6892900466918945,\n",
              "  0.6936925053596497,\n",
              "  0.659054160118103,\n",
              "  0.6796846985816956,\n",
              "  0.6781716346740723,\n",
              "  0.6935501098632812,\n",
              "  0.6464051604270935,\n",
              "  0.6676005125045776,\n",
              "  0.7148522734642029,\n",
              "  0.7183091044425964,\n",
              "  0.6671389937400818,\n",
              "  0.698677659034729,\n",
              "  0.7116361856460571,\n",
              "  0.6613059043884277,\n",
              "  0.7624609470367432,\n",
              "  0.6631150841712952,\n",
              "  0.671379566192627,\n",
              "  0.6973106861114502,\n",
              "  0.665556013584137,\n",
              "  0.678159236907959,\n",
              "  0.6504604816436768,\n",
              "  0.6947097778320312,\n",
              "  0.719036877155304,\n",
              "  0.6733697652816772,\n",
              "  0.688690185546875,\n",
              "  0.6785682439804077,\n",
              "  0.6895627975463867,\n",
              "  0.6822651624679565,\n",
              "  0.6480932235717773,\n",
              "  0.6959581971168518,\n",
              "  0.7201374769210815,\n",
              "  0.6942548155784607,\n",
              "  0.6791962385177612,\n",
              "  0.63933926820755,\n",
              "  0.6339064836502075,\n",
              "  0.6975089311599731,\n",
              "  0.6961311101913452,\n",
              "  0.6932733654975891,\n",
              "  0.6645002365112305,\n",
              "  0.6643469929695129,\n",
              "  0.6845003366470337,\n",
              "  0.6468906998634338,\n",
              "  0.6661754250526428,\n",
              "  0.6139452457427979,\n",
              "  0.6304078102111816,\n",
              "  0.680842936038971,\n",
              "  0.6685776710510254,\n",
              "  0.669631838798523,\n",
              "  0.6858014464378357,\n",
              "  0.630418062210083,\n",
              "  0.6776086688041687,\n",
              "  0.6384674906730652,\n",
              "  0.6660943031311035,\n",
              "  0.6778864860534668,\n",
              "  0.6877701878547668,\n",
              "  0.6686585545539856,\n",
              "  0.6668833494186401,\n",
              "  0.6918398141860962,\n",
              "  0.6029108762741089,\n",
              "  0.7069318890571594,\n",
              "  0.653188943862915,\n",
              "  0.694210410118103,\n",
              "  0.6666535139083862,\n",
              "  0.6500850915908813,\n",
              "  0.6904066205024719,\n",
              "  0.6452726721763611,\n",
              "  0.6817158460617065,\n",
              "  0.6544901728630066,\n",
              "  0.654125452041626,\n",
              "  0.6911942362785339,\n",
              "  0.6570258736610413,\n",
              "  0.6919873356819153,\n",
              "  0.6538620591163635,\n",
              "  0.664117157459259,\n",
              "  0.6507430672645569,\n",
              "  0.6540226340293884,\n",
              "  0.7328126430511475,\n",
              "  0.6456440091133118,\n",
              "  0.6929057836532593,\n",
              "  0.6561148166656494,\n",
              "  0.6327281594276428,\n",
              "  0.6543166637420654,\n",
              "  0.648237407207489,\n",
              "  0.6309570074081421,\n",
              "  0.6927633285522461,\n",
              "  0.7052034139633179,\n",
              "  0.6642961502075195,\n",
              "  0.7149335145950317,\n",
              "  0.6889315843582153,\n",
              "  0.6760714054107666,\n",
              "  0.6355990171432495,\n",
              "  0.6347189545631409,\n",
              "  0.7435591816902161,\n",
              "  0.7064152956008911,\n",
              "  0.6460662484169006,\n",
              "  0.6759445071220398,\n",
              "  0.6523411273956299,\n",
              "  0.6905785202980042,\n",
              "  0.6725969910621643,\n",
              "  0.653718113899231,\n",
              "  0.6422669291496277,\n",
              "  0.7010771036148071,\n",
              "  0.6040063500404358,\n",
              "  0.6559559106826782,\n",
              "  0.6811137199401855,\n",
              "  0.6402998566627502,\n",
              "  0.6885328888893127,\n",
              "  0.640971839427948,\n",
              "  0.6553384065628052,\n",
              "  0.6852108836174011,\n",
              "  0.6207535862922668,\n",
              "  0.6248636245727539,\n",
              "  0.6370378136634827,\n",
              "  0.679973840713501,\n",
              "  0.6740843057632446,\n",
              "  0.6507461071014404,\n",
              "  0.6234202980995178,\n",
              "  0.6460328102111816,\n",
              "  0.6484218239784241,\n",
              "  0.7061397433280945,\n",
              "  0.6816645264625549,\n",
              "  0.665606677532196,\n",
              "  0.6863300204277039,\n",
              "  0.6629007458686829,\n",
              "  0.6316629648208618,\n",
              "  0.6276364326477051,\n",
              "  0.6575924754142761,\n",
              "  0.6687994599342346,\n",
              "  0.6357181072235107,\n",
              "  0.6533573865890503,\n",
              "  0.7468556761741638,\n",
              "  0.6287028789520264,\n",
              "  0.6559682488441467,\n",
              "  0.7080184817314148,\n",
              "  0.6297482848167419,\n",
              "  0.6442447900772095,\n",
              "  0.6940072178840637,\n",
              "  0.6900853514671326,\n",
              "  0.6781882047653198,\n",
              "  0.6662002801895142,\n",
              "  0.6638768315315247,\n",
              "  0.6636545062065125,\n",
              "  0.6059862375259399,\n",
              "  0.6448374390602112,\n",
              "  0.6915603876113892,\n",
              "  0.7009605765342712,\n",
              "  0.6424787640571594,\n",
              "  0.6116520166397095,\n",
              "  0.5983362197875977,\n",
              "  0.6349707841873169,\n",
              "  0.6664073467254639,\n",
              "  0.6318823099136353,\n",
              "  0.6259960532188416,\n",
              "  0.6451336741447449,\n",
              "  0.639404833316803,\n",
              "  0.6968439817428589,\n",
              "  0.6250582933425903,\n",
              "  0.6461682319641113,\n",
              "  0.7089207172393799,\n",
              "  0.6170654296875,\n",
              "  0.6002785563468933,\n",
              "  0.6303720474243164,\n",
              "  0.6546964049339294,\n",
              "  0.6779663562774658,\n",
              "  0.6673851013183594,\n",
              "  0.6399182677268982,\n",
              "  0.6708003878593445,\n",
              "  0.6527913212776184,\n",
              "  0.6389616131782532,\n",
              "  0.6614158153533936,\n",
              "  0.6444857716560364,\n",
              "  0.6081617474555969,\n",
              "  0.6633437871932983,\n",
              "  0.5874932408332825,\n",
              "  0.6568101644515991,\n",
              "  0.6948080062866211,\n",
              "  0.6731788516044617,\n",
              "  0.6523135900497437,\n",
              "  0.6379013657569885,\n",
              "  0.6503307223320007,\n",
              "  0.6749023199081421,\n",
              "  0.6850010752677917,\n",
              "  0.6429807543754578,\n",
              "  0.6295815110206604,\n",
              "  0.6650718450546265,\n",
              "  0.6613091230392456,\n",
              "  0.6645511388778687,\n",
              "  0.6488701701164246,\n",
              "  0.6279036998748779,\n",
              "  0.6637563109397888,\n",
              "  0.6319102048873901,\n",
              "  0.615261971950531,\n",
              "  0.6251913905143738,\n",
              "  0.6429355144500732,\n",
              "  0.6722300052642822,\n",
              "  0.6553823351860046,\n",
              "  0.6763446927070618,\n",
              "  0.6248314380645752,\n",
              "  0.6576251983642578,\n",
              "  0.6594886183738708,\n",
              "  0.6331636309623718,\n",
              "  0.6326982975006104,\n",
              "  0.6581387519836426,\n",
              "  0.6645694375038147,\n",
              "  0.6351045966148376,\n",
              "  0.6139143109321594,\n",
              "  0.6149039268493652,\n",
              "  0.6797267198562622,\n",
              "  0.597320556640625,\n",
              "  0.6993921995162964,\n",
              "  0.6190323829650879,\n",
              "  0.6243644952774048,\n",
              "  0.6514224410057068,\n",
              "  0.6545026898384094,\n",
              "  0.6234569549560547,\n",
              "  0.581658124923706,\n",
              "  0.6972820162773132,\n",
              "  0.639481246471405,\n",
              "  0.6529301404953003,\n",
              "  0.6519086956977844,\n",
              "  0.6492647528648376,\n",
              "  0.6828835010528564,\n",
              "  0.6203505396842957,\n",
              "  0.6611999273300171,\n",
              "  0.6317154765129089,\n",
              "  0.5977290868759155,\n",
              "  0.6260682344436646,\n",
              "  0.6400733590126038,\n",
              "  0.6686455011367798,\n",
              "  0.6518317461013794,\n",
              "  0.5977723598480225,\n",
              "  0.6366780996322632,\n",
              "  0.6327484250068665,\n",
              "  0.6473562717437744,\n",
              "  0.6083976030349731,\n",
              "  0.6283421516418457,\n",
              "  0.6572509407997131,\n",
              "  0.6169403791427612,\n",
              "  0.6621893048286438,\n",
              "  0.6406241059303284,\n",
              "  0.6553865075111389,\n",
              "  0.5955162644386292,\n",
              "  0.637042224407196,\n",
              "  0.6016771793365479,\n",
              "  0.6211181879043579,\n",
              "  0.684441864490509,\n",
              "  0.6282007098197937,\n",
              "  0.6870725750923157,\n",
              "  0.6187010407447815,\n",
              "  0.6552981734275818,\n",
              "  0.6555211544036865,\n",
              "  0.6052446961402893,\n",
              "  0.6226091980934143,\n",
              "  0.5857052206993103,\n",
              "  0.6486461758613586,\n",
              "  0.6254478693008423,\n",
              "  0.6295694708824158,\n",
              "  0.6458568572998047,\n",
              "  0.6085538268089294,\n",
              "  0.5960428714752197,\n",
              "  0.6281351447105408,\n",
              "  0.6018497943878174,\n",
              "  0.6239659190177917,\n",
              "  0.6609097123146057,\n",
              "  0.6614637970924377,\n",
              "  0.603071928024292,\n",
              "  0.6134160757064819,\n",
              "  0.6494526863098145,\n",
              "  0.6585042476654053,\n",
              "  0.6572938561439514,\n",
              "  0.6021733283996582,\n",
              "  0.6297876238822937,\n",
              "  0.6216190457344055,\n",
              "  0.6212765574455261,\n",
              "  0.6730656623840332,\n",
              "  0.6155450344085693,\n",
              "  0.6153882145881653,\n",
              "  0.628576934337616,\n",
              "  0.6609221696853638,\n",
              "  0.6715566515922546,\n",
              "  0.6310438513755798,\n",
              "  0.6410370469093323,\n",
              "  0.6218450665473938,\n",
              "  0.629386842250824,\n",
              "  0.6294185519218445,\n",
              "  0.6019986867904663,\n",
              "  0.6927899122238159,\n",
              "  0.6347956657409668,\n",
              "  0.6453633904457092,\n",
              "  0.6314243674278259,\n",
              "  0.6426197290420532,\n",
              "  0.6152147054672241,\n",
              "  0.6387923955917358,\n",
              "  0.6111862659454346,\n",
              "  0.6813182830810547,\n",
              "  0.6644544005393982,\n",
              "  0.6564258337020874,\n",
              "  0.6105888485908508,\n",
              "  0.599170446395874,\n",
              "  0.6545168161392212,\n",
              "  0.6107136607170105,\n",
              "  0.6420546770095825,\n",
              "  0.6060500144958496,\n",
              "  0.6408861875534058,\n",
              "  0.647387683391571,\n",
              "  0.6624683737754822,\n",
              "  0.654232382774353,\n",
              "  0.6262323260307312,\n",
              "  0.6753548979759216,\n",
              "  0.631927490234375,\n",
              "  0.6030728816986084,\n",
              "  0.5862908959388733,\n",
              "  0.6788455247879028,\n",
              "  0.6233128905296326,\n",
              "  0.6033838987350464,\n",
              "  0.6518797874450684,\n",
              "  0.6637321710586548,\n",
              "  0.5848308205604553,\n",
              "  0.622443675994873,\n",
              "  0.6339650750160217,\n",
              "  0.6344783902168274,\n",
              "  0.5928536653518677,\n",
              "  0.6628965735435486,\n",
              "  0.62572181224823,\n",
              "  0.6658229231834412,\n",
              "  0.6218956708908081,\n",
              "  0.6303640007972717,\n",
              "  0.6373769640922546,\n",
              "  0.6495611667633057,\n",
              "  0.6325626969337463,\n",
              "  0.6467303037643433,\n",
              "  0.6048216223716736,\n",
              "  0.6153774857521057,\n",
              "  0.6223207116127014,\n",
              "  0.6121761202812195,\n",
              "  0.6245838403701782,\n",
              "  0.6277906894683838,\n",
              "  0.6357705593109131,\n",
              "  0.6442475318908691,\n",
              "  0.6293654441833496,\n",
              "  0.6228885054588318,\n",
              "  0.604186475276947,\n",
              "  0.6260925531387329,\n",
              "  0.6047042012214661,\n",
              "  0.6538316607475281,\n",
              "  0.6316880583763123,\n",
              "  0.5821148157119751,\n",
              "  0.6277652978897095,\n",
              "  0.6603034138679504,\n",
              "  0.5736847519874573,\n",
              "  0.6030011773109436,\n",
              "  0.6109219789505005,\n",
              "  0.6115763783454895,\n",
              "  0.6085759401321411,\n",
              "  0.614951491355896,\n",
              "  0.6276462078094482,\n",
              "  0.6022260785102844,\n",
              "  0.6031897068023682,\n",
              "  0.5955907106399536,\n",
              "  0.6336758136749268,\n",
              "  0.6152669191360474,\n",
              "  0.6087186932563782,\n",
              "  0.6162026524543762,\n",
              "  0.5963960289955139,\n",
              "  0.588350772857666,\n",
              "  0.6454393267631531,\n",
              "  0.5999103784561157,\n",
              "  0.6102350950241089,\n",
              "  0.594832718372345,\n",
              "  0.6208637952804565,\n",
              "  0.6089503765106201,\n",
              "  0.5900824069976807,\n",
              "  0.6316051483154297,\n",
              "  0.5985853672027588,\n",
              "  0.6188914775848389,\n",
              "  0.5797452926635742,\n",
              "  0.6030444502830505,\n",
              "  0.6203393936157227,\n",
              "  0.6477916240692139,\n",
              "  0.5976109504699707,\n",
              "  0.6400257349014282,\n",
              "  0.6445835828781128,\n",
              "  0.6172966361045837,\n",
              "  0.6258310675621033,\n",
              "  0.6174943447113037,\n",
              "  0.6437187194824219,\n",
              "  0.6003314852714539,\n",
              "  0.6255360841751099,\n",
              "  0.6182788610458374,\n",
              "  0.6082624793052673,\n",
              "  0.6236696839332581,\n",
              "  0.6400278210639954,\n",
              "  0.6268751621246338,\n",
              "  0.6085041165351868,\n",
              "  0.618259608745575,\n",
              "  0.6326484084129333,\n",
              "  0.5736576318740845,\n",
              "  0.5645824670791626,\n",
              "  0.5645337700843811,\n",
              "  0.5949693918228149,\n",
              "  0.6289862394332886,\n",
              "  0.6178238391876221,\n",
              "  0.6139848232269287,\n",
              "  0.549985945224762,\n",
              "  0.6013746857643127,\n",
              "  0.5964392423629761,\n",
              "  0.6205208897590637,\n",
              "  0.6016215682029724,\n",
              "  0.594588041305542,\n",
              "  0.6430837512016296,\n",
              "  0.6108787059783936,\n",
              "  0.5944810509681702,\n",
              "  0.6039065718650818,\n",
              "  0.5937318801879883,\n",
              "  0.6229584813117981,\n",
              "  0.6381100416183472,\n",
              "  0.5926782488822937,\n",
              "  0.5894066691398621,\n",
              "  0.5979779958724976,\n",
              "  0.6077059507369995,\n",
              "  0.6517137885093689,\n",
              "  0.5972417593002319,\n",
              "  0.6180037260055542,\n",
              "  0.6046132445335388,\n",
              "  0.6050620675086975,\n",
              "  0.5648179650306702,\n",
              "  0.6041638255119324,\n",
              "  0.6116666793823242,\n",
              "  0.5865921974182129,\n",
              "  0.6189754605293274,\n",
              "  0.5932731628417969,\n",
              "  0.6200385689735413,\n",
              "  0.6578375101089478,\n",
              "  0.5829234719276428,\n",
              "  0.6351198554039001,\n",
              "  0.6072130799293518,\n",
              "  0.6187431812286377,\n",
              "  0.5872759222984314,\n",
              "  0.6132068634033203,\n",
              "  0.6651450991630554,\n",
              "  0.6037395000457764,\n",
              "  0.6185939311981201],\n",
              " 'acc': [0.48876404762268066,\n",
              "  0.4981273412704468,\n",
              "  0.4981273412704468,\n",
              "  0.4812734127044678,\n",
              "  0.5449438095092773,\n",
              "  0.49625468254089355,\n",
              "  0.4756554365158081,\n",
              "  0.533707857131958,\n",
              "  0.5243445634841919,\n",
              "  0.4930555522441864,\n",
              "  0.5052083134651184,\n",
              "  0.4981273412704468,\n",
              "  0.4756944477558136,\n",
              "  0.4906367063522339,\n",
              "  0.4850187301635742,\n",
              "  0.47191011905670166,\n",
              "  0.47191011905670166,\n",
              "  0.5205992460250854,\n",
              "  0.48689138889312744,\n",
              "  0.5224719047546387,\n",
              "  0.5017361044883728,\n",
              "  0.5,\n",
              "  0.47752809524536133,\n",
              "  0.4895833432674408,\n",
              "  0.533707857131958,\n",
              "  0.5,\n",
              "  0.4850187301635742,\n",
              "  0.4569288492202759,\n",
              "  0.5,\n",
              "  0.5262172222137451,\n",
              "  0.48876404762268066,\n",
              "  0.47752809524536133,\n",
              "  0.5018726587295532,\n",
              "  0.47191011905670166,\n",
              "  0.5355805158615112,\n",
              "  0.48876404762268066,\n",
              "  0.5205992460250854,\n",
              "  0.4681648015975952,\n",
              "  0.49438202381134033,\n",
              "  0.5018726587295532,\n",
              "  0.5,\n",
              "  0.5056179761886597,\n",
              "  0.5224719047546387,\n",
              "  0.5149812698364258,\n",
              "  0.5393258333206177,\n",
              "  0.5355805158615112,\n",
              "  0.5074906349182129,\n",
              "  0.4947916567325592,\n",
              "  0.5093632936477661,\n",
              "  0.546875,\n",
              "  0.5262172222137451,\n",
              "  0.5093632936477661,\n",
              "  0.5205992460250854,\n",
              "  0.5037453174591064,\n",
              "  0.516853928565979,\n",
              "  0.5280898809432983,\n",
              "  0.5190972089767456,\n",
              "  0.5037453174591064,\n",
              "  0.5262172222137451,\n",
              "  0.5243445634841919,\n",
              "  0.48689138889312744,\n",
              "  0.5018726587295532,\n",
              "  0.4982638955116272,\n",
              "  0.49625468254089355,\n",
              "  0.5580524206161499,\n",
              "  0.5243445634841919,\n",
              "  0.5131086111068726,\n",
              "  0.5018726587295532,\n",
              "  0.5112359523773193,\n",
              "  0.533707857131958,\n",
              "  0.5224719047546387,\n",
              "  0.5524344444274902,\n",
              "  0.5187265872955322,\n",
              "  0.47940075397491455,\n",
              "  0.483146071434021,\n",
              "  0.5037453174591064,\n",
              "  0.5468164682388306,\n",
              "  0.5449438095092773,\n",
              "  0.5299625396728516,\n",
              "  0.5224719047546387,\n",
              "  0.533707857131958,\n",
              "  0.4925093650817871,\n",
              "  0.4965277910232544,\n",
              "  0.5561797618865967,\n",
              "  0.5243445634841919,\n",
              "  0.5318351984024048,\n",
              "  0.5524344444274902,\n",
              "  0.5411984920501709,\n",
              "  0.5295138955116272,\n",
              "  0.5112359523773193,\n",
              "  0.5280898809432983,\n",
              "  0.5131086111068726,\n",
              "  0.5208333134651184,\n",
              "  0.5449438095092773,\n",
              "  0.5434027910232544,\n",
              "  0.5393258333206177,\n",
              "  0.5524344444274902,\n",
              "  0.5355805158615112,\n",
              "  0.5430711507797241,\n",
              "  0.5149812698364258,\n",
              "  0.5486891269683838,\n",
              "  0.5149812698364258,\n",
              "  0.53125,\n",
              "  0.5430711507797241,\n",
              "  0.5093632936477661,\n",
              "  0.5561797618865967,\n",
              "  0.5318351984024048,\n",
              "  0.5617977380752563,\n",
              "  0.5543071031570435,\n",
              "  0.5416666865348816,\n",
              "  0.5692883729934692,\n",
              "  0.5694444179534912,\n",
              "  0.5543071031570435,\n",
              "  0.5149812698364258,\n",
              "  0.5393258333206177,\n",
              "  0.5299625396728516,\n",
              "  0.5636703968048096,\n",
              "  0.5243445634841919,\n",
              "  0.550561785697937,\n",
              "  0.5449438095092773,\n",
              "  0.5262172222137451,\n",
              "  0.5364583134651184,\n",
              "  0.5280898809432983,\n",
              "  0.5374531745910645,\n",
              "  0.5561797618865967,\n",
              "  0.5749063491821289,\n",
              "  0.5411984920501709,\n",
              "  0.5224719047546387,\n",
              "  0.5524344444274902,\n",
              "  0.5355805158615112,\n",
              "  0.5580524206161499,\n",
              "  0.533707857131958,\n",
              "  0.5692883729934692,\n",
              "  0.5524344444274902,\n",
              "  0.5730336904525757,\n",
              "  0.5636703968048096,\n",
              "  0.5430711507797241,\n",
              "  0.5992509126663208,\n",
              "  0.5655430555343628,\n",
              "  0.5580524206161499,\n",
              "  0.5486891269683838,\n",
              "  0.5692883729934692,\n",
              "  0.5636703968048096,\n",
              "  0.5468164682388306,\n",
              "  0.5224719047546387,\n",
              "  0.5393258333206177,\n",
              "  0.5599250793457031,\n",
              "  0.6161048412322998,\n",
              "  0.5659722089767456,\n",
              "  0.5056179761886597,\n",
              "  0.5936329364776611,\n",
              "  0.5262172222137451,\n",
              "  0.5692883729934692,\n",
              "  0.625,\n",
              "  0.5636703968048096,\n",
              "  0.550561785697937,\n",
              "  0.5430711507797241,\n",
              "  0.5917602777481079,\n",
              "  0.567415714263916,\n",
              "  0.5430711507797241,\n",
              "  0.5449438095092773,\n",
              "  0.5393258333206177,\n",
              "  0.5917602777481079,\n",
              "  0.5449438095092773,\n",
              "  0.5992509126663208,\n",
              "  0.5393258333206177,\n",
              "  0.5730336904525757,\n",
              "  0.5936329364776611,\n",
              "  0.567415714263916,\n",
              "  0.5449438095092773,\n",
              "  0.5599250793457031,\n",
              "  0.5561797618865967,\n",
              "  0.5561797618865967,\n",
              "  0.567415714263916,\n",
              "  0.5347222089767456,\n",
              "  0.5749063491821289,\n",
              "  0.5580524206161499,\n",
              "  0.5636703968048096,\n",
              "  0.5617977380752563,\n",
              "  0.5692883729934692,\n",
              "  0.5786516666412354,\n",
              "  0.5262172222137451,\n",
              "  0.5416666865348816,\n",
              "  0.5468164682388306,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.584269642829895,\n",
              "  0.5411984920501709,\n",
              "  0.5917602777481079,\n",
              "  0.5880149602890015,\n",
              "  0.5749063491821289,\n",
              "  0.5636703968048096,\n",
              "  0.5677083134651184,\n",
              "  0.584269642829895,\n",
              "  0.6161048412322998,\n",
              "  0.5538194179534912,\n",
              "  0.5767790079116821,\n",
              "  0.601123571395874,\n",
              "  0.5561797618865967,\n",
              "  0.5749063491821289,\n",
              "  0.6029962301254272,\n",
              "  0.5805243253707886,\n",
              "  0.6310861706733704,\n",
              "  0.5730336904525757,\n",
              "  0.5786516666412354,\n",
              "  0.5599250793457031,\n",
              "  0.5823969841003418,\n",
              "  0.5730336904525757,\n",
              "  0.5692883729934692,\n",
              "  0.5636703968048096,\n",
              "  0.6104868650436401,\n",
              "  0.5763888955116272,\n",
              "  0.550561785697937,\n",
              "  0.5749063491821289,\n",
              "  0.567415714263916,\n",
              "  0.5538194179534912,\n",
              "  0.5917602777481079,\n",
              "  0.5730336904525757,\n",
              "  0.567415714263916,\n",
              "  0.6123595237731934,\n",
              "  0.5749063491821289,\n",
              "  0.5955055952072144,\n",
              "  0.5823969841003418,\n",
              "  0.5599250793457031,\n",
              "  0.5561797618865967,\n",
              "  0.533707857131958,\n",
              "  0.5767790079116821,\n",
              "  0.5617977380752563,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.5543071031570435,\n",
              "  0.5936329364776611,\n",
              "  0.5936329364776611,\n",
              "  0.5730336904525757,\n",
              "  0.5692883729934692,\n",
              "  0.601123571395874,\n",
              "  0.6161048412322998,\n",
              "  0.6048688888549805,\n",
              "  0.5655430555343628,\n",
              "  0.5655430555343628,\n",
              "  0.6086142063140869,\n",
              "  0.5786516666412354,\n",
              "  0.6128472089767456,\n",
              "  0.5955055952072144,\n",
              "  0.6142321825027466,\n",
              "  0.5746527910232544,\n",
              "  0.6086142063140869,\n",
              "  0.5936329364776611,\n",
              "  0.6161048412322998,\n",
              "  0.601123571395874,\n",
              "  0.5767790079116821,\n",
              "  0.5936329364776611,\n",
              "  0.6123595237731934,\n",
              "  0.5823969841003418,\n",
              "  0.5763888955116272,\n",
              "  0.5920138955116272,\n",
              "  0.5767790079116821,\n",
              "  0.5823969841003418,\n",
              "  0.5955055952072144,\n",
              "  0.5955055952072144,\n",
              "  0.5805243253707886,\n",
              "  0.5917602777481079,\n",
              "  0.5992509126663208,\n",
              "  0.6310861706733704,\n",
              "  0.6273408532142639,\n",
              "  0.5617977380752563,\n",
              "  0.5917602777481079,\n",
              "  0.5898876190185547,\n",
              "  0.567415714263916,\n",
              "  0.6104868650436401,\n",
              "  0.6128472089767456,\n",
              "  0.5786516666412354,\n",
              "  0.5992509126663208,\n",
              "  0.5880149602890015,\n",
              "  0.6048688888549805,\n",
              "  0.6029962301254272,\n",
              "  0.5955055952072144,\n",
              "  0.617977499961853,\n",
              "  0.5880149602890015,\n",
              "  0.6142321825027466,\n",
              "  0.5572916865348816,\n",
              "  0.601123571395874,\n",
              "  0.6235954761505127,\n",
              "  0.5861423015594482,\n",
              "  0.6067415475845337,\n",
              "  0.601123571395874,\n",
              "  0.5749063491821289,\n",
              "  0.5955055952072144,\n",
              "  0.5617977380752563,\n",
              "  0.617977499961853,\n",
              "  0.6076388955116272,\n",
              "  0.6067415475845337,\n",
              "  0.6161048412322998,\n",
              "  0.6310861706733704,\n",
              "  0.63670414686203,\n",
              "  0.6161048412322998,\n",
              "  0.5823969841003418,\n",
              "  0.601123571395874,\n",
              "  0.5730336904525757,\n",
              "  0.5880149602890015,\n",
              "  0.5786516666412354,\n",
              "  0.6348314881324768,\n",
              "  0.601123571395874,\n",
              "  0.5936329364776611,\n",
              "  0.6142321825027466,\n",
              "  0.5880149602890015,\n",
              "  0.6059027910232544,\n",
              "  0.5861423015594482,\n",
              "  0.5898876190185547,\n",
              "  0.5898876190185547,\n",
              "  0.6104868650436401,\n",
              "  0.6302083134651184,\n",
              "  0.6161048412322998,\n",
              "  0.6104868650436401,\n",
              "  0.5805243253707886,\n",
              "  0.6142321825027466,\n",
              "  0.6086142063140869,\n",
              "  0.6235954761505127,\n",
              "  0.6161048412322998,\n",
              "  0.6142321825027466,\n",
              "  0.6441947817802429,\n",
              "  0.6404494643211365,\n",
              "  0.5973782539367676,\n",
              "  0.567415714263916,\n",
              "  0.653558075428009,\n",
              "  0.6292135119438171,\n",
              "  0.5823969841003418,\n",
              "  0.6273408532142639,\n",
              "  0.6217228174209595,\n",
              "  0.6498127579689026,\n",
              "  0.5955055952072144,\n",
              "  0.6086142063140869,\n",
              "  0.6215277910232544,\n",
              "  0.5936329364776611,\n",
              "  0.5607638955116272,\n",
              "  0.5917602777481079,\n",
              "  0.6048688888549805,\n",
              "  0.6111111044883728,\n",
              "  0.6067415475845337,\n",
              "  0.6048688888549805,\n",
              "  0.6254681944847107,\n",
              "  0.6104868650436401,\n",
              "  0.6217228174209595,\n",
              "  0.6142321825027466,\n",
              "  0.6161048412322998,\n",
              "  0.6104868650436401,\n",
              "  0.6254681944847107,\n",
              "  0.6554307341575623,\n",
              "  0.6104868650436401,\n",
              "  0.6123595237731934,\n",
              "  0.5655430555343628,\n",
              "  0.6215277910232544,\n",
              "  0.6498127579689026,\n",
              "  0.6041666865348816,\n",
              "  0.6302083134651184,\n",
              "  0.6460674405097961,\n",
              "  0.6198501586914062,\n",
              "  0.5973782539367676,\n",
              "  0.6123595237731934,\n",
              "  0.6385768055915833,\n",
              "  0.6348314881324768,\n",
              "  0.6145833134651184,\n",
              "  0.5805243253707886,\n",
              "  0.6217228174209595,\n",
              "  0.609375,\n",
              "  0.6329588294029236,\n",
              "  0.5711610317230225,\n",
              "  0.6217228174209595,\n",
              "  0.6310861706733704,\n",
              "  0.617977499961853,\n",
              "  0.6161048412322998,\n",
              "  0.6086142063140869,\n",
              "  0.6029962301254272,\n",
              "  0.6067415475845337,\n",
              "  0.6198501586914062,\n",
              "  0.6161048412322998,\n",
              "  0.6254681944847107,\n",
              "  0.6217228174209595,\n",
              "  0.5992509126663208,\n",
              "  0.6142321825027466,\n",
              "  0.6161048412322998,\n",
              "  0.5617977380752563,\n",
              "  0.6441947817802429,\n",
              "  0.6254681944847107,\n",
              "  0.6273408532142639,\n",
              "  0.6310861706733704,\n",
              "  0.6273408532142639,\n",
              "  0.6273408532142639,\n",
              "  0.6458333134651184,\n",
              "  0.6029962301254272,\n",
              "  0.6161048412322998,\n",
              "  0.6086142063140869,\n",
              "  0.6310861706733704,\n",
              "  0.5880149602890015,\n",
              "  0.6441947817802429,\n",
              "  0.6554307341575623,\n",
              "  0.6554307341575623,\n",
              "  0.6217228174209595,\n",
              "  0.6385768055915833,\n",
              "  0.6591760516166687,\n",
              "  0.5954861044883728,\n",
              "  0.6123595237731934,\n",
              "  0.5861423015594482,\n",
              "  0.6573033928871155,\n",
              "  0.5917602777481079,\n",
              "  0.6310861706733704,\n",
              "  0.6123595237731934,\n",
              "  0.6086142063140869,\n",
              "  0.6104868650436401,\n",
              "  0.6142321825027466,\n",
              "  0.6273408532142639,\n",
              "  0.6067415475845337,\n",
              "  0.5902777910232544,\n",
              "  0.617977499961853,\n",
              "  0.6067415475845337,\n",
              "  0.6460674405097961,\n",
              "  0.6217228174209595,\n",
              "  0.5973782539367676,\n",
              "  0.6273408532142639,\n",
              "  0.584269642829895,\n",
              "  0.6235954761505127,\n",
              "  0.6104868650436401,\n",
              "  0.5955055952072144,\n",
              "  0.6217228174209595,\n",
              "  0.5992509126663208,\n",
              "  0.6666666865348816,\n",
              "  0.5972222089767456,\n",
              "  0.6217228174209595,\n",
              "  0.6385768055915833,\n",
              "  0.584269642829895,\n",
              "  0.6336805820465088,\n",
              "  0.5955055952072144,\n",
              "  0.6460674405097961,\n",
              "  0.5917602777481079,\n",
              "  0.6198501586914062,\n",
              "  0.6232638955116272,\n",
              "  0.6329588294029236,\n",
              "  0.6123595237731934,\n",
              "  0.6292135119438171,\n",
              "  0.6329588294029236,\n",
              "  0.6385768055915833,\n",
              "  0.63670414686203,\n",
              "  0.6104868650436401,\n",
              "  0.6292135119438171,\n",
              "  0.63670414686203,\n",
              "  0.6235954761505127,\n",
              "  0.6573033928871155,\n",
              "  0.6273408532142639,\n",
              "  0.6086142063140869,\n",
              "  0.6029962301254272,\n",
              "  0.5917602777481079,\n",
              "  0.6516854166984558,\n",
              "  0.6048688888549805,\n",
              "  0.6423221230506897,\n",
              "  0.6048688888549805,\n",
              "  0.6273408532142639,\n",
              "  0.6142321825027466,\n",
              "  0.6254681944847107,\n",
              "  0.6104868650436401,\n",
              "  0.6198501586914062,\n",
              "  0.6111111044883728,\n",
              "  0.653558075428009,\n",
              "  0.6385768055915833,\n",
              "  0.6329588294029236,\n",
              "  0.6254681944847107,\n",
              "  0.6610487103462219,\n",
              "  0.6029962301254272,\n",
              "  0.6348314881324768,\n",
              "  0.6142321825027466,\n",
              "  0.6591760516166687,\n",
              "  0.6292135119438171,\n",
              "  0.63670414686203,\n",
              "  0.6310861706733704,\n",
              "  0.6423221230506897,\n",
              "  0.6180555820465088,\n",
              "  0.6254681944847107,\n",
              "  0.6161048412322998,\n",
              "  0.6460674405097961,\n",
              "  0.6527777910232544,\n",
              "  0.6161048412322998,\n",
              "  0.6104868650436401,\n",
              "  0.6610487103462219,\n",
              "  0.6516854166984558,\n",
              "  0.6441947817802429,\n",
              "  0.6254681944847107,\n",
              "  0.6235954761505127,\n",
              "  0.6666666865348816,\n",
              "  0.6573033928871155,\n",
              "  0.6310861706733704,\n",
              "  0.6516854166984558,\n",
              "  0.617977499961853,\n",
              "  0.6048688888549805,\n",
              "  0.6273408532142639,\n",
              "  0.6579861044883728,\n",
              "  0.6554307341575623,\n",
              "  0.6329588294029236,\n",
              "  0.6348314881324768,\n",
              "  0.6029962301254272,\n",
              "  0.6198501586914062,\n",
              "  0.6423221230506897,\n",
              "  0.6161048412322998,\n",
              "  0.6441947817802429,\n",
              "  0.6254681944847107,\n",
              "  0.6104868650436401,\n",
              "  0.6573033928871155,\n",
              "  0.6736111044883728,\n",
              "  0.6235954761505127,\n",
              "  0.6498127579689026,\n",
              "  0.5861423015594482,\n",
              "  0.6441947817802429,\n",
              "  0.6198501586914062,\n",
              "  0.6198501586914062,\n",
              "  0.6086142063140869,\n",
              "  0.6086142063140869,\n",
              "  0.653558075428009,\n",
              "  0.6910112500190735,\n",
              "  0.6573033928871155,\n",
              "  0.6217228174209595,\n",
              "  0.6348314881324768,\n",
              "  0.601123571395874,\n",
              "  0.640625,\n",
              "  0.6254681944847107,\n",
              "  0.6510416865348816,\n",
              "  0.6629213690757751,\n",
              "  0.6385768055915833,\n",
              "  0.63670414686203,\n",
              "  0.6235954761505127,\n",
              "  0.6440972089767456,\n",
              "  0.6760299801826477,\n",
              "  0.6215277910232544,\n",
              "  0.6198501586914062,\n",
              "  0.6479400992393494,\n",
              "  0.6423221230506897,\n",
              "  0.6516854166984558,\n",
              "  0.6722846627235413,\n",
              "  0.6423221230506897,\n",
              "  0.6610487103462219,\n",
              "  0.6760299801826477,\n",
              "  0.6458333134651184,\n",
              "  0.6385768055915833,\n",
              "  0.6371527910232544,\n",
              "  0.6310861706733704,\n",
              "  0.6460674405097961,\n",
              "  0.6610487103462219,\n",
              "  0.6573033928871155,\n",
              "  0.617977499961853,\n",
              "  0.6310861706733704,\n",
              "  0.6163194179534912,\n",
              "  0.63670414686203,\n",
              "  0.63670414686203,\n",
              "  0.6647940278053284,\n",
              "  0.6779026389122009,\n",
              "  0.6329588294029236,\n",
              "  0.6423221230506897,\n",
              "  0.6573033928871155,\n",
              "  0.6479400992393494,\n",
              "  0.670412003993988,\n",
              "  0.6086142063140869,\n",
              "  0.6498127579689026,\n",
              "  0.6441947817802429,\n",
              "  0.6797752976417542,\n",
              "  0.653558075428009,\n",
              "  0.6554307341575623,\n",
              "  0.6516854166984558,\n",
              "  0.6423221230506897,\n",
              "  0.6554307341575623,\n",
              "  0.617977499961853,\n",
              "  0.6310861706733704,\n",
              "  0.6629213690757751,\n",
              "  0.6460674405097961,\n",
              "  0.617977499961853,\n",
              "  0.6573033928871155,\n",
              "  0.5936329364776611,\n",
              "  0.6610487103462219,\n",
              "  0.6591760516166687,\n",
              "  0.6161048412322998,\n",
              "  0.6573033928871155,\n",
              "  0.6516854166984558,\n",
              "  0.6460674405097961,\n",
              "  0.6423221230506897,\n",
              "  0.6348314881324768,\n",
              "  0.6423221230506897,\n",
              "  0.6516854166984558,\n",
              "  0.6685393452644348,\n",
              "  0.63670414686203,\n",
              "  0.6319444179534912,\n",
              "  0.6629213690757751,\n",
              "  0.6554307341575623,\n",
              "  0.6292135119438171,\n",
              "  0.6460674405097961,\n",
              "  0.6310861706733704,\n",
              "  0.6853932738304138,\n",
              "  0.6610487103462219,\n",
              "  0.6292135119438171,\n",
              "  0.6254681944847107,\n",
              "  0.6554307341575623,\n",
              "  0.6292135119438171,\n",
              "  0.6460674405097961,\n",
              "  0.6610487103462219,\n",
              "  0.6554307341575623,\n",
              "  0.6441947817802429,\n",
              "  0.7078651785850525,\n",
              "  0.687265932559967,\n",
              "  0.6404494643211365,\n",
              "  0.6516854166984558,\n",
              "  0.6685393452644348,\n",
              "  0.6198501586914062,\n",
              "  0.6614583134651184,\n",
              "  0.63670414686203,\n",
              "  0.687265932559967,\n",
              "  0.6554307341575623,\n",
              "  0.6629213690757751,\n",
              "  0.6554307341575623,\n",
              "  0.6516854166984558,\n",
              "  0.6385768055915833,\n",
              "  0.6666666865348816,\n",
              "  0.6891385912895203,\n",
              "  0.6273408532142639,\n",
              "  0.6797752976417542,\n",
              "  0.6385768055915833,\n",
              "  0.670412003993988,\n",
              "  0.6423221230506897,\n",
              "  0.6527777910232544,\n",
              "  0.6853932738304138,\n",
              "  0.6310861706733704,\n",
              "  0.6629213690757751,\n",
              "  0.6573033928871155,\n",
              "  0.6310861706733704,\n",
              "  0.6554307341575623,\n",
              "  0.6527777910232544,\n",
              "  0.6404494643211365,\n",
              "  0.6779026389122009,\n",
              "  0.6610487103462219,\n",
              "  0.653558075428009,\n",
              "  0.6284722089767456,\n",
              "  0.6610487103462219,\n",
              "  0.6479400992393494,\n",
              "  0.653558075428009,\n",
              "  0.6797752976417542,\n",
              "  0.6591760516166687,\n",
              "  0.6853932738304138,\n",
              "  0.6647940278053284,\n",
              "  0.6460674405097961,\n",
              "  0.6441947817802429,\n",
              "  0.6685393452644348,\n",
              "  0.6104868650436401,\n",
              "  0.653558075428009,\n",
              "  0.6554307341575623,\n",
              "  0.6760299801826477,\n",
              "  0.6423221230506897,\n",
              "  0.6006944179534912,\n",
              "  0.6292135119438171,\n",
              "  0.6835206151008606,\n",
              "  0.6516854166984558,\n",
              "  0.6493055820465088,\n",
              "  0.6498127579689026,\n",
              "  0.6385768055915833,\n",
              "  0.6329588294029236,\n",
              "  0.6685393452644348,\n",
              "  0.6404494643211365,\n",
              "  0.6685393452644348,\n",
              "  0.6591760516166687,\n",
              "  0.6629213690757751,\n",
              "  0.7003745436668396,\n",
              "  0.6142321825027466,\n",
              "  0.6685393452644348,\n",
              "  0.6685393452644348,\n",
              "  0.6498127579689026,\n",
              "  0.6910112500190735,\n",
              "  0.6797752976417542,\n",
              "  0.6741573214530945,\n",
              "  0.6685393452644348,\n",
              "  0.6666666865348816,\n",
              "  0.6610487103462219,\n",
              "  0.6910112500190735,\n",
              "  0.6479400992393494,\n",
              "  0.6760299801826477,\n",
              "  0.6460674405097961,\n",
              "  0.6591760516166687,\n",
              "  0.6629213690757751,\n",
              "  0.6273408532142639,\n",
              "  0.6385768055915833,\n",
              "  0.687265932559967,\n",
              "  0.6779026389122009,\n",
              "  0.671875,\n",
              "  0.6591760516166687,\n",
              "  0.6666666865348816,\n",
              "  0.6573033928871155,\n",
              "  0.5917602777481079,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6235954761505127,\n",
              "  0.6816479563713074,\n",
              "  0.6573033928871155,\n",
              "  0.63670414686203,\n",
              "  0.6348314881324768,\n",
              "  0.6404494643211365,\n",
              "  0.6423221230506897,\n",
              "  0.6498127579689026,\n",
              "  0.6498127579689026,\n",
              "  0.6966292262077332,\n",
              "  0.6741573214530945,\n",
              "  0.653558075428009,\n",
              "  0.6123595237731934,\n",
              "  0.6591760516166687,\n",
              "  0.6666666865348816,\n",
              "  0.6857638955116272,\n",
              "  0.687265932559967,\n",
              "  0.6685393452644348,\n",
              "  0.6573033928871155,\n",
              "  0.6927083134651184,\n",
              "  0.6753472089767456,\n",
              "  0.6629213690757751,\n",
              "  0.6404494643211365,\n",
              "  0.6910112500190735,\n",
              "  0.6666666865348816,\n",
              "  0.6198501586914062,\n",
              "  0.6966292262077332,\n",
              "  0.6875,\n",
              "  0.6816479563713074,\n",
              "  0.6629213690757751,\n",
              "  0.6460674405097961,\n",
              "  0.6329588294029236,\n",
              "  0.6647940278053284,\n",
              "  0.6329588294029236,\n",
              "  0.6404494643211365,\n",
              "  0.6647940278053284,\n",
              "  0.6348314881324768,\n",
              "  0.6760299801826477,\n",
              "  0.6722846627235413,\n",
              "  0.6217228174209595,\n",
              "  0.6892361044883728,\n",
              "  0.63670414686203,\n",
              "  0.6310861706733704,\n",
              "  0.6760299801826477,\n",
              "  0.6510416865348816,\n",
              "  0.6944444179534912,\n",
              "  0.6722846627235413,\n",
              "  0.6441947817802429,\n",
              "  0.6385768055915833,\n",
              "  0.6947565674781799,\n",
              "  0.6573033928871155,\n",
              "  0.63670414686203,\n",
              "  0.6610487103462219,\n",
              "  0.6475694179534912,\n",
              "  0.6685393452644348,\n",
              "  0.6816479563713074,\n",
              "  0.6493055820465088,\n",
              "  0.6647940278053284,\n",
              "  0.6853932738304138,\n",
              "  0.6741573214530945,\n",
              "  0.6760299801826477,\n",
              "  0.6591760516166687,\n",
              "  0.6385768055915833,\n",
              "  0.6329588294029236,\n",
              "  0.6591760516166687,\n",
              "  0.6554307341575623,\n",
              "  0.6573033928871155,\n",
              "  0.6797752976417542,\n",
              "  0.6498127579689026,\n",
              "  0.6554307341575623,\n",
              "  0.6760299801826477,\n",
              "  0.6441947817802429,\n",
              "  0.6666666865348816,\n",
              "  0.6891385912895203,\n",
              "  0.6479400992393494,\n",
              "  0.6928839087486267,\n",
              "  0.6516854166984558,\n",
              "  0.6722846627235413,\n",
              "  0.6573033928871155,\n",
              "  0.6573033928871155,\n",
              "  0.6423221230506897,\n",
              "  0.6591760516166687,\n",
              "  0.7134831547737122,\n",
              "  0.653558075428009,\n",
              "  0.6629213690757751,\n",
              "  0.6591760516166687,\n",
              "  0.653558075428009,\n",
              "  0.6573033928871155,\n",
              "  0.6423221230506897,\n",
              "  0.6891385912895203,\n",
              "  0.6516854166984558,\n",
              "  0.6892361044883728,\n",
              "  0.6947565674781799,\n",
              "  0.6909722089767456,\n",
              "  0.6760299801826477,\n",
              "  0.6460674405097961,\n",
              "  0.6666666865348816,\n",
              "  0.7059925198554993,\n",
              "  0.6760299801826477,\n",
              "  0.670412003993988,\n",
              "  0.653558075428009,\n",
              "  0.6835206151008606,\n",
              "  0.6816479563713074,\n",
              "  0.6629213690757751,\n",
              "  0.6835206151008606,\n",
              "  0.6685393452644348,\n",
              "  0.6666666865348816,\n",
              "  0.6853932738304138,\n",
              "  0.6996527910232544,\n",
              "  0.6573033928871155,\n",
              "  0.7134831547737122,\n",
              "  0.6835206151008606,\n",
              "  0.6498127579689026,\n",
              "  0.6779026389122009,\n",
              "  0.6292135119438171,\n",
              "  0.6797752976417542,\n",
              "  0.653558075428009,\n",
              "  0.653558075428009,\n",
              "  0.6853932738304138,\n",
              "  0.687265932559967,\n",
              "  0.6722846627235413,\n",
              "  0.6610487103462219,\n",
              "  0.6835206151008606,\n",
              "  0.6835206151008606,\n",
              "  0.6928839087486267,\n",
              "  0.6875,\n",
              "  0.6944444179534912,\n",
              "  0.6779026389122009,\n",
              "  0.6966292262077332,\n",
              "  0.6966292262077332,\n",
              "  0.6685393452644348,\n",
              "  0.6423221230506897,\n",
              "  0.6910112500190735,\n",
              "  0.6760299801826477,\n",
              "  0.63670414686203,\n",
              "  0.6779026389122009,\n",
              "  0.6292135119438171,\n",
              "  0.6853932738304138,\n",
              "  0.6516854166984558,\n",
              "  0.6647940278053284,\n",
              "  0.6816479563713074,\n",
              "  0.6498127579689026,\n",
              "  0.6666666865348816,\n",
              "  0.6947565674781799,\n",
              "  0.6857638955116272,\n",
              "  0.6685393452644348,\n",
              "  0.6685393452644348,\n",
              "  0.6404494643211365,\n",
              "  0.6741573214530945,\n",
              "  0.6928839087486267,\n",
              "  0.6579861044883728,\n",
              "  0.6797752976417542,\n",
              "  0.6892361044883728,\n",
              "  0.6235954761505127,\n",
              "  0.6597222089767456,\n",
              "  0.6840277910232544,\n",
              "  0.6741573214530945,\n",
              "  0.6610487103462219,\n",
              "  0.6985018849372864,\n",
              "  0.653558075428009,\n",
              "  0.6614583134651184,\n",
              "  0.6516854166984558,\n",
              "  0.6516854166984558,\n",
              "  0.6610487103462219,\n",
              "  0.7118055820465088,\n",
              "  0.6985018849372864,\n",
              "  0.6722846627235413,\n",
              "  0.6928839087486267,\n",
              "  0.653558075428009,\n",
              "  0.6741573214530945,\n",
              "  0.6610487103462219,\n",
              "  0.6647940278053284,\n",
              "  0.6741573214530945,\n",
              "  0.6516854166984558,\n",
              "  0.6928839087486267,\n",
              "  0.6348314881324768,\n",
              "  0.6816479563713074,\n",
              "  0.7048611044883728,\n",
              "  0.704119861125946,\n",
              "  0.6554307341575623,\n",
              "  0.6779026389122009,\n",
              "  0.6966292262077332,\n",
              "  0.6591760516166687,\n",
              "  0.6460674405097961,\n",
              "  0.7059925198554993,\n",
              "  0.670412003993988,\n",
              "  0.6498127579689026,\n",
              "  0.6479400992393494,\n",
              "  0.6947565674781799,\n",
              "  0.6760299801826477,\n",
              "  0.6685393452644348,\n",
              "  0.6479400992393494,\n",
              "  0.6722846627235413,\n",
              "  0.6891385912895203,\n",
              "  0.6722846627235413,\n",
              "  0.6760299801826477,\n",
              "  0.6647940278053284,\n",
              "  0.6554307341575623,\n",
              "  0.6947565674781799,\n",
              "  0.6853932738304138,\n",
              "  0.6458333134651184,\n",
              "  0.7078651785850525,\n",
              "  0.6741573214530945,\n",
              "  0.6722846627235413,\n",
              "  0.6479400992393494,\n",
              "  0.6597222089767456,\n",
              "  0.6910112500190735,\n",
              "  0.6498127579689026,\n",
              "  0.6797752976417542,\n",
              "  0.653558075428009,\n",
              "  0.6760299801826477,\n",
              "  0.6816479563713074,\n",
              "  0.6666666865348816,\n",
              "  0.6985018849372864,\n",
              "  0.6928839087486267,\n",
              "  0.6371527910232544,\n",
              "  0.6760299801826477,\n",
              "  0.6591760516166687,\n",
              "  0.6591760516166687,\n",
              "  0.6741573214530945,\n",
              "  0.6816479563713074,\n",
              "  0.6853932738304138,\n",
              "  0.6591760516166687,\n",
              "  0.6927083134651184,\n",
              "  0.6722846627235413,\n",
              "  0.6928839087486267,\n",
              "  0.6647940278053284,\n",
              "  0.6816479563713074,\n",
              "  0.6701388955116272,\n",
              "  0.6816479563713074,\n",
              "  0.6647940278053284,\n",
              "  0.6722846627235413,\n",
              "  0.6498127579689026,\n",
              "  0.6647940278053284,\n",
              "  0.6910112500190735,\n",
              "  0.7078651785850525,\n",
              "  0.6685393452644348,\n",
              "  0.6853932738304138,\n",
              "  0.6760299801826477,\n",
              "  0.6779026389122009,\n",
              "  0.7022472023963928,\n",
              "  0.6985018849372864,\n",
              "  0.7059925198554993,\n",
              "  0.6835206151008606,\n",
              "  0.6722846627235413,\n",
              "  0.6591760516166687,\n",
              "  0.687265932559967,\n",
              "  0.6816479563713074,\n",
              "  0.6498127579689026,\n",
              "  0.6816479563713074,\n",
              "  0.6835206151008606,\n",
              "  0.6797752976417542,\n",
              "  0.6329588294029236,\n",
              "  0.670412003993988,\n",
              "  0.6498127579689026,\n",
              "  0.6666666865348816,\n",
              "  0.6760299801826477,\n",
              "  0.6797752976417542,\n",
              "  0.6816479563713074,\n",
              "  0.670412003993988,\n",
              "  0.6966292262077332,\n",
              "  0.6797752976417542,\n",
              "  0.6610487103462219,\n",
              "  0.7022472023963928,\n",
              "  0.7134831547737122,\n",
              "  0.7059925198554993,\n",
              "  0.6947565674781799,\n",
              "  0.6591760516166687,\n",
              "  0.653558075428009,\n",
              "  0.670412003993988,\n",
              "  0.7116104960441589,\n",
              "  0.6910112500190735,\n",
              "  0.6947565674781799,\n",
              "  0.6666666865348816,\n",
              "  0.6779026389122009,\n",
              "  0.6910112500190735,\n",
              "  0.6591760516166687,\n",
              "  0.6853932738304138,\n",
              "  0.6816479563713074,\n",
              "  0.6966292262077332,\n",
              "  0.7003745436668396,\n",
              "  0.6647940278053284,\n",
              "  0.687265932559967,\n",
              "  0.6797752976417542,\n",
              "  0.7097378373146057,\n",
              "  0.6822916865348816,\n",
              "  0.6835206151008606,\n",
              "  0.6404494643211365,\n",
              "  0.6966292262077332,\n",
              "  0.6779026389122009,\n",
              "  0.6779026389122009,\n",
              "  0.6966292262077332,\n",
              "  0.7265917658805847,\n",
              "  0.7003745436668396,\n",
              "  0.6797752976417542,\n",
              "  0.6985018849372864,\n",
              "  0.6498127579689026,\n",
              "  0.6947565674781799,\n",
              "  0.6891385912895203,\n",
              "  0.6310861706733704,\n",
              "  0.7134831547737122,\n",
              "  0.6573033928871155,\n",
              "  0.687265932559967,\n",
              "  0.7022472023963928,\n",
              "  0.6722846627235413,\n",
              "  0.670412003993988,\n",
              "  0.6685393452644348,\n",
              "  0.7097378373146057,\n",
              "  0.6684027910232544],\n",
              " 'val_loss': [0.5531365275382996,\n",
              "  0.4817996621131897,\n",
              "  0.6193966865539551,\n",
              "  0.5663193464279175,\n",
              "  0.5244669914245605,\n",
              "  0.5732920169830322,\n",
              "  0.5784757137298584,\n",
              "  0.5916372537612915,\n",
              "  0.48307493329048157,\n",
              "  0.5897136926651001,\n",
              "  0.5472298860549927,\n",
              "  0.5630686283111572,\n",
              "  0.5586695671081543,\n",
              "  0.6053571701049805,\n",
              "  0.5803791284561157,\n",
              "  0.555898129940033,\n",
              "  0.6088510155677795,\n",
              "  0.557815670967102,\n",
              "  0.5939046740531921,\n",
              "  0.5830733776092529,\n",
              "  0.5774883031845093,\n",
              "  0.5820721387863159,\n",
              "  0.6230558156967163,\n",
              "  0.6137133836746216,\n",
              "  0.6444172859191895,\n",
              "  0.6009331941604614,\n",
              "  0.5806386470794678,\n",
              "  0.595244824886322,\n",
              "  0.5750961899757385,\n",
              "  0.6587390899658203,\n",
              "  0.6532877683639526,\n",
              "  0.572653591632843,\n",
              "  0.6133872270584106,\n",
              "  0.5562076568603516,\n",
              "  0.6012590527534485,\n",
              "  0.6169464588165283,\n",
              "  0.5967705249786377,\n",
              "  0.6582211852073669,\n",
              "  0.6120045185089111,\n",
              "  0.6183886528015137,\n",
              "  0.6349102258682251,\n",
              "  0.6108242273330688,\n",
              "  0.6357263922691345,\n",
              "  0.6315372586250305,\n",
              "  0.6437526345252991,\n",
              "  0.5591961145401001,\n",
              "  0.607187807559967,\n",
              "  0.611986517906189,\n",
              "  0.6291203498840332,\n",
              "  0.6657893657684326,\n",
              "  0.6336559057235718,\n",
              "  0.6016726493835449,\n",
              "  0.6021170616149902,\n",
              "  0.5979354381561279,\n",
              "  0.6574980020523071,\n",
              "  0.6716694831848145,\n",
              "  0.6725466251373291,\n",
              "  0.6429454684257507,\n",
              "  0.6304919123649597,\n",
              "  0.6529112458229065,\n",
              "  0.6083439588546753,\n",
              "  0.6147816181182861,\n",
              "  0.6622719764709473,\n",
              "  0.6942198872566223,\n",
              "  0.6301099061965942,\n",
              "  0.6030136346817017,\n",
              "  0.617091178894043,\n",
              "  0.602814257144928,\n",
              "  0.6171648502349854,\n",
              "  0.6433376669883728,\n",
              "  0.6706314086914062,\n",
              "  0.6367546319961548,\n",
              "  0.6761529445648193,\n",
              "  0.6184232831001282,\n",
              "  0.6851848363876343,\n",
              "  0.6462734341621399,\n",
              "  0.6598262786865234,\n",
              "  0.6441391706466675,\n",
              "  0.7025201320648193,\n",
              "  0.6793899536132812,\n",
              "  0.6832332611083984,\n",
              "  0.6791585683822632,\n",
              "  0.7126126289367676,\n",
              "  0.7334340214729309,\n",
              "  0.6540610790252686,\n",
              "  0.7048835754394531,\n",
              "  0.7001882791519165,\n",
              "  0.6825799942016602,\n",
              "  0.688328206539154,\n",
              "  0.6838708519935608,\n",
              "  0.7065715789794922,\n",
              "  0.6682105660438538,\n",
              "  0.6726067066192627,\n",
              "  0.6905121803283691,\n",
              "  0.6604589223861694,\n",
              "  0.621937096118927,\n",
              "  0.720318078994751,\n",
              "  0.739453911781311,\n",
              "  0.6741782426834106,\n",
              "  0.7146154642105103,\n",
              "  0.6806384325027466,\n",
              "  0.7133357524871826,\n",
              "  0.6958817839622498,\n",
              "  0.7522987127304077,\n",
              "  0.7139083743095398,\n",
              "  0.6849331855773926,\n",
              "  0.7114500999450684,\n",
              "  0.6737588047981262,\n",
              "  0.7572614550590515,\n",
              "  0.7560470104217529,\n",
              "  0.7512754797935486,\n",
              "  0.6917690634727478,\n",
              "  0.7656627893447876,\n",
              "  0.6711618900299072,\n",
              "  0.7659887075424194,\n",
              "  0.7370715141296387,\n",
              "  0.7395360469818115,\n",
              "  0.745057225227356,\n",
              "  0.6969848871231079,\n",
              "  0.7163556814193726,\n",
              "  0.7488759160041809,\n",
              "  0.7258926630020142,\n",
              "  0.6920698881149292,\n",
              "  0.777332067489624,\n",
              "  0.7539176940917969,\n",
              "  0.6946424245834351,\n",
              "  0.7389978170394897,\n",
              "  0.6852105855941772,\n",
              "  0.7591061592102051,\n",
              "  0.628363311290741,\n",
              "  0.7062852382659912,\n",
              "  0.7225467562675476,\n",
              "  0.763567328453064,\n",
              "  0.7285453677177429,\n",
              "  0.7824229598045349,\n",
              "  0.759684681892395,\n",
              "  0.7868920564651489,\n",
              "  0.7529790997505188,\n",
              "  0.7429181337356567,\n",
              "  0.7817826271057129,\n",
              "  0.728320300579071,\n",
              "  0.7634809017181396,\n",
              "  0.7532484531402588,\n",
              "  0.6859464049339294,\n",
              "  0.7200607657432556,\n",
              "  0.7453423738479614,\n",
              "  0.8291457891464233,\n",
              "  0.7405461072921753,\n",
              "  0.7540968060493469,\n",
              "  0.7226884365081787,\n",
              "  0.7440981864929199,\n",
              "  0.7581838965415955,\n",
              "  0.7672237157821655,\n",
              "  0.7854949235916138,\n",
              "  0.7237223386764526,\n",
              "  0.7544155120849609,\n",
              "  0.8062393069267273,\n",
              "  0.7537297010421753,\n",
              "  0.7753787636756897,\n",
              "  0.7065796256065369,\n",
              "  0.7822996377944946,\n",
              "  0.7430374622344971,\n",
              "  0.7712070941925049,\n",
              "  0.7994861602783203,\n",
              "  0.7945761680603027,\n",
              "  0.8052915930747986,\n",
              "  0.8363175988197327,\n",
              "  0.6990190744400024,\n",
              "  0.7932605743408203,\n",
              "  0.7582667469978333,\n",
              "  0.7638981342315674,\n",
              "  0.7871301770210266,\n",
              "  0.7573224306106567,\n",
              "  0.7228659391403198,\n",
              "  0.8138988614082336,\n",
              "  0.6698058247566223,\n",
              "  0.7130805850028992,\n",
              "  0.8251349329948425,\n",
              "  0.8041152954101562,\n",
              "  0.7902014851570129,\n",
              "  0.7931199073791504,\n",
              "  0.7624255418777466,\n",
              "  0.7708754539489746,\n",
              "  0.743034839630127,\n",
              "  0.756784200668335,\n",
              "  0.706572949886322,\n",
              "  0.8341505527496338,\n",
              "  0.7812900543212891,\n",
              "  0.7272254228591919,\n",
              "  0.786226749420166,\n",
              "  0.8022981882095337,\n",
              "  0.8016034364700317,\n",
              "  0.808440089225769,\n",
              "  0.7398366332054138,\n",
              "  0.7595460414886475,\n",
              "  0.7445008158683777,\n",
              "  0.7975385189056396,\n",
              "  0.7844900488853455,\n",
              "  0.7634730339050293,\n",
              "  0.7713426351547241,\n",
              "  0.773795485496521,\n",
              "  0.8228336572647095,\n",
              "  0.8089237213134766,\n",
              "  0.8255329132080078,\n",
              "  0.7809362411499023,\n",
              "  0.8308011889457703,\n",
              "  0.8179042935371399,\n",
              "  0.7816884517669678,\n",
              "  0.7927002906799316,\n",
              "  0.7358884811401367,\n",
              "  0.7832835912704468,\n",
              "  0.7885634899139404,\n",
              "  0.7603954076766968,\n",
              "  0.7692163586616516,\n",
              "  0.7748767137527466,\n",
              "  0.8008092641830444,\n",
              "  0.7895117402076721,\n",
              "  0.7648804783821106,\n",
              "  0.7929076552391052,\n",
              "  0.8034968376159668,\n",
              "  0.7955357432365417,\n",
              "  0.7548495531082153,\n",
              "  0.8207670450210571,\n",
              "  0.7465040683746338,\n",
              "  0.7571629285812378,\n",
              "  0.7987959384918213,\n",
              "  0.7848953008651733,\n",
              "  0.7778690457344055,\n",
              "  0.7871521711349487,\n",
              "  0.8720986247062683,\n",
              "  0.7867856025695801,\n",
              "  0.7948064804077148,\n",
              "  0.749319314956665,\n",
              "  0.8003146648406982,\n",
              "  0.7659568786621094,\n",
              "  0.7871776223182678,\n",
              "  0.7899055480957031,\n",
              "  0.7413679361343384,\n",
              "  0.8147685527801514,\n",
              "  0.7475361227989197,\n",
              "  0.7481791973114014,\n",
              "  0.779582142829895,\n",
              "  0.783291220664978,\n",
              "  0.7492353916168213,\n",
              "  0.775938868522644,\n",
              "  0.7806623578071594,\n",
              "  0.6947179436683655,\n",
              "  0.8126780986785889,\n",
              "  0.7990841269493103,\n",
              "  0.7646344900131226,\n",
              "  0.8581721782684326,\n",
              "  0.7717607617378235,\n",
              "  0.8269109129905701,\n",
              "  0.7922230362892151,\n",
              "  0.7803820967674255,\n",
              "  0.8038445711135864,\n",
              "  0.8124176263809204,\n",
              "  0.8188278079032898,\n",
              "  0.7515997886657715,\n",
              "  0.8045717477798462,\n",
              "  0.8222521543502808,\n",
              "  0.8540788888931274,\n",
              "  0.7328024506568909,\n",
              "  0.783326268196106,\n",
              "  0.7666684985160828,\n",
              "  0.8240509629249573,\n",
              "  0.7730280160903931,\n",
              "  0.8144843578338623,\n",
              "  0.8002835512161255,\n",
              "  0.774722158908844,\n",
              "  0.7807333469390869,\n",
              "  0.8099021911621094,\n",
              "  0.7894718647003174,\n",
              "  0.7953488230705261,\n",
              "  0.7815341353416443,\n",
              "  0.7830834984779358,\n",
              "  0.7841670513153076,\n",
              "  0.7798411846160889,\n",
              "  0.7675108909606934,\n",
              "  0.7856341600418091,\n",
              "  0.8124533295631409,\n",
              "  0.7647465467453003,\n",
              "  0.767943799495697,\n",
              "  0.794380784034729,\n",
              "  0.758771538734436,\n",
              "  0.7096306085586548,\n",
              "  0.7764674425125122,\n",
              "  0.741657555103302,\n",
              "  0.7439955472946167,\n",
              "  0.7567063570022583,\n",
              "  0.7636511921882629,\n",
              "  0.7980585098266602,\n",
              "  0.815096914768219,\n",
              "  0.7493132948875427,\n",
              "  0.8074657320976257,\n",
              "  0.8233581185340881,\n",
              "  0.8252543807029724,\n",
              "  0.7340319156646729,\n",
              "  0.8079430460929871,\n",
              "  0.8137164115905762,\n",
              "  0.7430567145347595,\n",
              "  0.7585423588752747,\n",
              "  0.7801260352134705,\n",
              "  0.8005301356315613,\n",
              "  0.7701259255409241,\n",
              "  0.7752702832221985,\n",
              "  0.7812475562095642,\n",
              "  0.7897615432739258,\n",
              "  0.8152976036071777,\n",
              "  0.7606323957443237,\n",
              "  0.7715260982513428,\n",
              "  0.7804224491119385,\n",
              "  0.8137193322181702,\n",
              "  0.7679527997970581,\n",
              "  0.8180692791938782,\n",
              "  0.8538273572921753,\n",
              "  0.8214666843414307,\n",
              "  0.7905063629150391,\n",
              "  0.8342647552490234,\n",
              "  0.8010731935501099,\n",
              "  0.793103039264679,\n",
              "  0.8029245138168335,\n",
              "  0.7937036752700806,\n",
              "  0.8454467058181763,\n",
              "  0.7983590364456177,\n",
              "  0.7667813301086426,\n",
              "  0.8205849528312683,\n",
              "  0.7377435564994812,\n",
              "  0.7818148732185364,\n",
              "  0.7738680243492126,\n",
              "  0.7883926630020142,\n",
              "  0.8112922310829163,\n",
              "  0.7905452251434326,\n",
              "  0.7183218002319336,\n",
              "  0.7474595904350281,\n",
              "  0.8304089903831482,\n",
              "  0.794913113117218,\n",
              "  0.7974236607551575,\n",
              "  0.7363373637199402,\n",
              "  0.8494071960449219,\n",
              "  0.7738012075424194,\n",
              "  0.7370152473449707,\n",
              "  0.7510973215103149,\n",
              "  0.8590439558029175,\n",
              "  0.7757136821746826,\n",
              "  0.790825605392456,\n",
              "  0.7998026609420776,\n",
              "  0.7283831834793091,\n",
              "  0.7323086261749268,\n",
              "  0.7403565049171448,\n",
              "  0.7705596685409546,\n",
              "  0.794421911239624,\n",
              "  0.7431557178497314,\n",
              "  0.7404587268829346,\n",
              "  0.8100879192352295,\n",
              "  0.8488410711288452,\n",
              "  0.7966204881668091,\n",
              "  0.7954872846603394,\n",
              "  0.8117243051528931,\n",
              "  0.809524655342102,\n",
              "  0.8443182706832886,\n",
              "  0.8500102162361145,\n",
              "  0.7622870206832886,\n",
              "  0.7987720966339111,\n",
              "  0.8197468519210815,\n",
              "  0.7353802919387817,\n",
              "  0.792361855506897,\n",
              "  0.7501397132873535,\n",
              "  0.7619272470474243,\n",
              "  0.7808548808097839,\n",
              "  0.7780164480209351,\n",
              "  0.8411655426025391,\n",
              "  0.8197429776191711,\n",
              "  0.8296066522598267,\n",
              "  0.7795420289039612,\n",
              "  0.7794072031974792,\n",
              "  0.7308133840560913,\n",
              "  0.7161191701889038,\n",
              "  0.8306788206100464,\n",
              "  0.7158721685409546,\n",
              "  0.7421490550041199,\n",
              "  0.7574222087860107,\n",
              "  0.7681449055671692,\n",
              "  0.7734129428863525,\n",
              "  0.7623125314712524,\n",
              "  0.7607489824295044,\n",
              "  0.7568470239639282,\n",
              "  0.7452974319458008,\n",
              "  0.7708022594451904,\n",
              "  0.8392288684844971,\n",
              "  0.8151900768280029,\n",
              "  0.7839855551719666,\n",
              "  0.8122126460075378,\n",
              "  0.8348565101623535,\n",
              "  0.8015629053115845,\n",
              "  0.7778588533401489,\n",
              "  0.7974665760993958,\n",
              "  0.8482996225357056,\n",
              "  0.7746402025222778,\n",
              "  0.7161107063293457,\n",
              "  0.8180755972862244,\n",
              "  0.7934942245483398,\n",
              "  0.8024319410324097,\n",
              "  0.7673231363296509,\n",
              "  0.8028432726860046,\n",
              "  0.7713881731033325,\n",
              "  0.7819708585739136,\n",
              "  0.8187993764877319,\n",
              "  0.7746233940124512,\n",
              "  0.8289905190467834,\n",
              "  0.7877330780029297,\n",
              "  0.8442076444625854,\n",
              "  0.7581895589828491,\n",
              "  0.7628405094146729,\n",
              "  0.7359477281570435,\n",
              "  0.8470406532287598,\n",
              "  0.7978513240814209,\n",
              "  0.825427234172821,\n",
              "  0.8173153400421143,\n",
              "  0.7822399735450745,\n",
              "  0.7602434158325195,\n",
              "  0.8446600437164307,\n",
              "  0.7694413661956787,\n",
              "  0.8065271377563477,\n",
              "  0.7581720948219299,\n",
              "  0.8086965084075928,\n",
              "  0.8682223558425903,\n",
              "  0.792667806148529,\n",
              "  0.7613407969474792,\n",
              "  0.8814210891723633,\n",
              "  0.8029329776763916,\n",
              "  0.808509111404419,\n",
              "  0.7593649625778198,\n",
              "  0.7618023157119751,\n",
              "  0.8113871812820435,\n",
              "  0.7616731524467468,\n",
              "  0.8332768678665161,\n",
              "  0.7601738572120667,\n",
              "  0.7791684865951538,\n",
              "  0.8258131146430969,\n",
              "  0.7617706060409546,\n",
              "  0.8306199908256531,\n",
              "  0.8624709844589233,\n",
              "  0.7329758405685425,\n",
              "  0.8353511691093445,\n",
              "  0.805351972579956,\n",
              "  0.7241221070289612,\n",
              "  0.7958316206932068,\n",
              "  0.7677435874938965,\n",
              "  0.7927113175392151,\n",
              "  0.7566047310829163,\n",
              "  0.8063861727714539,\n",
              "  0.7738497853279114,\n",
              "  0.7567124962806702,\n",
              "  0.7824393510818481,\n",
              "  0.8137089014053345,\n",
              "  0.7287126779556274,\n",
              "  0.7576479315757751,\n",
              "  0.757151186466217,\n",
              "  0.7553644180297852,\n",
              "  0.8075098991394043,\n",
              "  0.7684102058410645,\n",
              "  0.7421064972877502,\n",
              "  0.830233633518219,\n",
              "  0.7995555400848389,\n",
              "  0.795983076095581,\n",
              "  0.7837814092636108,\n",
              "  0.7310652732849121,\n",
              "  0.8048499822616577,\n",
              "  0.784054160118103,\n",
              "  0.7743490934371948,\n",
              "  0.8003076314926147,\n",
              "  0.8070132732391357,\n",
              "  0.7676043510437012,\n",
              "  0.7277358174324036,\n",
              "  0.7851423621177673,\n",
              "  0.7568570971488953,\n",
              "  0.7563029527664185,\n",
              "  0.8279871344566345,\n",
              "  0.7778710722923279,\n",
              "  0.7426966428756714,\n",
              "  0.822731614112854,\n",
              "  0.7589669227600098,\n",
              "  0.7741833329200745,\n",
              "  0.7975208759307861,\n",
              "  0.7444310188293457,\n",
              "  0.7868437767028809,\n",
              "  0.7654536962509155,\n",
              "  0.7587497234344482,\n",
              "  0.8089945912361145,\n",
              "  0.8086362481117249,\n",
              "  0.7923887968063354,\n",
              "  0.7623220086097717,\n",
              "  0.8161202073097229,\n",
              "  0.731295108795166,\n",
              "  0.7545480132102966,\n",
              "  0.8075037002563477,\n",
              "  0.7487075328826904,\n",
              "  0.7709452509880066,\n",
              "  0.8225139379501343,\n",
              "  0.7806441783905029,\n",
              "  0.7372564077377319,\n",
              "  0.764280378818512,\n",
              "  0.674331784248352,\n",
              "  0.769001841545105,\n",
              "  0.8224984407424927,\n",
              "  0.7859770059585571,\n",
              "  0.8033446073532104,\n",
              "  0.7469193935394287,\n",
              "  0.805072546005249,\n",
              "  0.8271408081054688,\n",
              "  0.7785049676895142,\n",
              "  0.8073843717575073,\n",
              "  0.8280397057533264,\n",
              "  0.7273929119110107,\n",
              "  0.7966886758804321,\n",
              "  0.815521240234375,\n",
              "  0.7938748002052307,\n",
              "  0.7434334754943848,\n",
              "  0.8044517040252686,\n",
              "  0.8356568813323975,\n",
              "  0.7714537978172302,\n",
              "  0.7510471940040588,\n",
              "  0.7336801290512085,\n",
              "  0.7833425998687744,\n",
              "  0.7481018304824829,\n",
              "  0.822859525680542,\n",
              "  0.7885129451751709,\n",
              "  0.7303794622421265,\n",
              "  0.7580393552780151,\n",
              "  0.7807068824768066,\n",
              "  0.7912599444389343,\n",
              "  0.7829184532165527,\n",
              "  0.7845914363861084,\n",
              "  0.7713973522186279,\n",
              "  0.7782866954803467,\n",
              "  0.8085926175117493,\n",
              "  0.7871679067611694,\n",
              "  0.7040987014770508,\n",
              "  0.7953545451164246,\n",
              "  0.7753444910049438,\n",
              "  0.7732335329055786,\n",
              "  0.8150067329406738,\n",
              "  0.728719174861908,\n",
              "  0.8155461549758911,\n",
              "  0.7761027216911316,\n",
              "  0.7725492715835571,\n",
              "  0.781169056892395,\n",
              "  0.7903304100036621,\n",
              "  0.7702649235725403,\n",
              "  0.7792750597000122,\n",
              "  0.7298944592475891,\n",
              "  0.7603386640548706,\n",
              "  0.7879927158355713,\n",
              "  0.7853797674179077,\n",
              "  0.733098030090332,\n",
              "  0.770790696144104,\n",
              "  0.7862406373023987,\n",
              "  0.7164711356163025,\n",
              "  0.8020323514938354,\n",
              "  0.7275716066360474,\n",
              "  0.7678497433662415,\n",
              "  0.8175079226493835,\n",
              "  0.7778620719909668,\n",
              "  0.8119341731071472,\n",
              "  0.7503649592399597,\n",
              "  0.7173162698745728,\n",
              "  0.7269805073738098,\n",
              "  0.8238354325294495,\n",
              "  0.7705204486846924,\n",
              "  0.8190687298774719,\n",
              "  0.826462984085083,\n",
              "  0.7635510563850403,\n",
              "  0.7723073363304138,\n",
              "  0.7521758675575256,\n",
              "  0.734947681427002,\n",
              "  0.7673630714416504,\n",
              "  0.7464567422866821,\n",
              "  0.7476819753646851,\n",
              "  0.8160783052444458,\n",
              "  0.7140723466873169,\n",
              "  0.8094590902328491,\n",
              "  0.7592509388923645,\n",
              "  0.8148955702781677,\n",
              "  0.7490454316139221,\n",
              "  0.7612473964691162,\n",
              "  0.8208975791931152,\n",
              "  0.7858983278274536,\n",
              "  0.784692645072937,\n",
              "  0.8007509112358093,\n",
              "  0.7424048185348511,\n",
              "  0.7456507086753845,\n",
              "  0.7591437101364136,\n",
              "  0.7376288175582886,\n",
              "  0.7737601399421692,\n",
              "  0.740807056427002,\n",
              "  0.7485336661338806,\n",
              "  0.7986172437667847,\n",
              "  0.7597140073776245,\n",
              "  0.7432019114494324,\n",
              "  0.8159754276275635,\n",
              "  0.776994526386261,\n",
              "  0.7402894496917725,\n",
              "  0.7089852094650269,\n",
              "  0.7934304475784302,\n",
              "  0.7437979578971863,\n",
              "  0.7475275993347168,\n",
              "  0.780840277671814,\n",
              "  0.776826024055481,\n",
              "  0.7851170897483826,\n",
              "  0.8009630441665649,\n",
              "  0.7799012660980225,\n",
              "  0.7451061606407166,\n",
              "  0.7611522674560547,\n",
              "  0.7817637920379639,\n",
              "  0.7585102915763855,\n",
              "  0.7947419285774231,\n",
              "  0.7239219546318054,\n",
              "  0.7084814310073853,\n",
              "  0.8015332818031311,\n",
              "  0.7776644229888916,\n",
              "  0.751801609992981,\n",
              "  0.7482884526252747,\n",
              "  0.7518040537834167,\n",
              "  0.7026897668838501,\n",
              "  0.7037186026573181,\n",
              "  0.7180078625679016,\n",
              "  0.7552197575569153,\n",
              "  0.7699540853500366,\n",
              "  0.7783679962158203,\n",
              "  0.742914080619812,\n",
              "  0.7728689312934875,\n",
              "  0.7691380977630615,\n",
              "  0.760778546333313,\n",
              "  0.8225717544555664,\n",
              "  0.7512077689170837,\n",
              "  0.7772841453552246,\n",
              "  0.730211079120636,\n",
              "  0.7494986057281494,\n",
              "  0.7736997604370117,\n",
              "  0.75864177942276,\n",
              "  0.784651517868042,\n",
              "  0.7709158062934875,\n",
              "  0.7014075517654419,\n",
              "  0.7992650866508484,\n",
              "  0.7428287267684937,\n",
              "  0.7974196076393127,\n",
              "  0.7160584330558777,\n",
              "  0.7180437445640564,\n",
              "  0.7891884446144104,\n",
              "  0.7068774700164795,\n",
              "  0.7453304529190063,\n",
              "  0.7668090462684631,\n",
              "  0.7250201106071472,\n",
              "  0.721036434173584,\n",
              "  0.7595903873443604,\n",
              "  0.756712794303894,\n",
              "  0.7165297269821167,\n",
              "  0.6797243356704712,\n",
              "  0.7607167363166809,\n",
              "  0.7189099788665771,\n",
              "  0.8026324510574341,\n",
              "  0.7119654417037964,\n",
              "  0.7453295588493347,\n",
              "  0.7693840861320496,\n",
              "  0.7344350814819336,\n",
              "  0.7224925756454468,\n",
              "  0.7765183448791504,\n",
              "  0.724864661693573,\n",
              "  0.7779684066772461,\n",
              "  0.7328110933303833,\n",
              "  0.7880343198776245,\n",
              "  0.7591078281402588,\n",
              "  0.7535762786865234,\n",
              "  0.7665774822235107,\n",
              "  0.7190978527069092,\n",
              "  0.779811680316925,\n",
              "  0.7351395487785339,\n",
              "  0.6898803114891052,\n",
              "  0.7274507880210876,\n",
              "  0.7706551551818848,\n",
              "  0.7905194759368896,\n",
              "  0.7658625245094299,\n",
              "  0.7671598792076111,\n",
              "  0.7596031427383423,\n",
              "  0.8116900324821472,\n",
              "  0.7677216529846191,\n",
              "  0.7250027656555176,\n",
              "  0.7233562469482422,\n",
              "  0.7619113922119141,\n",
              "  0.7060426473617554,\n",
              "  0.7351954579353333,\n",
              "  0.7592267394065857,\n",
              "  0.7477312088012695,\n",
              "  0.7595687508583069,\n",
              "  0.77408367395401,\n",
              "  0.789655327796936,\n",
              "  0.7978933453559875,\n",
              "  0.7040989398956299,\n",
              "  0.7818818688392639,\n",
              "  0.7164055109024048,\n",
              "  0.6904501914978027,\n",
              "  0.7854940891265869,\n",
              "  0.7512050271034241,\n",
              "  0.713154673576355,\n",
              "  0.7472583055496216,\n",
              "  0.6982603669166565,\n",
              "  0.7934343814849854,\n",
              "  0.7138548493385315,\n",
              "  0.7610863447189331,\n",
              "  0.7837908267974854,\n",
              "  0.8147776126861572,\n",
              "  0.6846272349357605,\n",
              "  0.727703332901001,\n",
              "  0.719592809677124,\n",
              "  0.7754493355751038,\n",
              "  0.7983970046043396,\n",
              "  0.7541182041168213,\n",
              "  0.7131527662277222,\n",
              "  0.7501541376113892,\n",
              "  0.8271498680114746,\n",
              "  0.7882693409919739,\n",
              "  0.7506257891654968,\n",
              "  0.7497581243515015,\n",
              "  0.7611294984817505,\n",
              "  0.732563316822052,\n",
              "  0.7133024334907532,\n",
              "  0.7241535186767578,\n",
              "  0.769044816493988,\n",
              "  0.7533016204833984,\n",
              "  0.7423195242881775,\n",
              "  0.7375383377075195,\n",
              "  0.7402546405792236,\n",
              "  0.7709039449691772,\n",
              "  0.7479199171066284,\n",
              "  0.7608125805854797,\n",
              "  0.7141907215118408,\n",
              "  0.7542275786399841,\n",
              "  0.787101149559021,\n",
              "  0.7448682188987732,\n",
              "  0.6713441014289856,\n",
              "  0.6897883415222168,\n",
              "  0.7797448635101318,\n",
              "  0.7647380828857422,\n",
              "  0.6974892616271973,\n",
              "  0.750423014163971,\n",
              "  0.7471551895141602,\n",
              "  0.7322049140930176,\n",
              "  0.7916924953460693,\n",
              "  0.7827308773994446,\n",
              "  0.7508679628372192,\n",
              "  0.7221959233283997,\n",
              "  0.7349498271942139,\n",
              "  0.774124264717102,\n",
              "  0.6831250190734863,\n",
              "  0.7445635795593262,\n",
              "  0.7018551826477051,\n",
              "  0.7127635478973389,\n",
              "  0.7427034378051758,\n",
              "  0.7362164258956909,\n",
              "  0.7945630550384521,\n",
              "  0.7342699766159058,\n",
              "  0.7489831447601318,\n",
              "  0.7205029726028442,\n",
              "  0.7649593353271484,\n",
              "  0.7321556806564331,\n",
              "  0.7495566606521606,\n",
              "  0.7436727285385132,\n",
              "  0.7199122309684753,\n",
              "  0.7405866384506226,\n",
              "  0.8018356561660767,\n",
              "  0.7284306287765503,\n",
              "  0.7671728134155273,\n",
              "  0.7530311942100525,\n",
              "  0.7561711072921753,\n",
              "  0.7254095673561096,\n",
              "  0.7219326496124268,\n",
              "  0.778896689414978,\n",
              "  0.7403661608695984,\n",
              "  0.7761577367782593,\n",
              "  0.7811344861984253,\n",
              "  0.7110775709152222,\n",
              "  0.7428262233734131,\n",
              "  0.7510362267494202,\n",
              "  0.7717904448509216,\n",
              "  0.7727724313735962,\n",
              "  0.7725995182991028,\n",
              "  0.7229092121124268,\n",
              "  0.7141391038894653,\n",
              "  0.7757073044776917,\n",
              "  0.7676299810409546,\n",
              "  0.7121789455413818,\n",
              "  0.7361346483230591,\n",
              "  0.7866703271865845,\n",
              "  0.7469350695610046,\n",
              "  0.7479277849197388,\n",
              "  0.7542145252227783,\n",
              "  0.6990177631378174,\n",
              "  0.7127660512924194,\n",
              "  0.7836660742759705,\n",
              "  0.7415419816970825,\n",
              "  0.7009103298187256,\n",
              "  0.7338868975639343,\n",
              "  0.6976935863494873,\n",
              "  0.7469923496246338,\n",
              "  0.6650818586349487,\n",
              "  0.7324690818786621,\n",
              "  0.7615442276000977,\n",
              "  0.7320183515548706,\n",
              "  0.741855800151825,\n",
              "  0.7402737140655518,\n",
              "  0.7165242433547974,\n",
              "  0.6750665903091431,\n",
              "  0.6907768845558167,\n",
              "  0.7586508393287659,\n",
              "  0.7003704309463501,\n",
              "  0.7415589690208435,\n",
              "  0.7006970643997192,\n",
              "  0.7534695863723755,\n",
              "  0.7783609628677368,\n",
              "  0.7597577571868896,\n",
              "  0.7198113203048706,\n",
              "  0.6957917809486389,\n",
              "  0.730793833732605,\n",
              "  0.7129207849502563,\n",
              "  0.7629818916320801,\n",
              "  0.7851777076721191,\n",
              "  0.7352883815765381,\n",
              "  0.7529072165489197,\n",
              "  0.7231222987174988,\n",
              "  0.7590309381484985,\n",
              "  0.7364768385887146,\n",
              "  0.6993936896324158,\n",
              "  0.7323929071426392,\n",
              "  0.7119551301002502,\n",
              "  0.7310623526573181,\n",
              "  0.7556352019309998,\n",
              "  0.7807811498641968,\n",
              "  0.7811139822006226,\n",
              "  0.7337946891784668,\n",
              "  0.71340012550354,\n",
              "  0.7325621247291565,\n",
              "  0.7604945302009583,\n",
              "  0.6761981844902039,\n",
              "  0.705495297908783,\n",
              "  0.7055882811546326,\n",
              "  0.7493593096733093,\n",
              "  0.7857558131217957,\n",
              "  0.7380444407463074,\n",
              "  0.7050344944000244,\n",
              "  0.7803480625152588,\n",
              "  0.6961588859558105,\n",
              "  0.8424707651138306,\n",
              "  0.7069845199584961,\n",
              "  0.7275146245956421,\n",
              "  0.753049910068512,\n",
              "  0.7409719824790955,\n",
              "  0.7453769445419312,\n",
              "  0.7318804264068604,\n",
              "  0.7653107643127441,\n",
              "  0.7552176117897034,\n",
              "  0.7389823198318481,\n",
              "  0.7539504766464233,\n",
              "  0.7765583992004395,\n",
              "  0.7705720067024231,\n",
              "  0.69761723279953,\n",
              "  0.7528224587440491,\n",
              "  0.7485313415527344,\n",
              "  0.7679814696311951,\n",
              "  0.774925947189331,\n",
              "  0.7107681632041931,\n",
              "  0.8068110942840576,\n",
              "  0.6917141675949097,\n",
              "  0.71761155128479,\n",
              "  0.7259159088134766,\n",
              "  0.7341885566711426,\n",
              "  0.7207663059234619,\n",
              "  0.7041866779327393,\n",
              "  0.7448561191558838,\n",
              "  0.6833868026733398,\n",
              "  0.739997386932373,\n",
              "  0.7639597654342651,\n",
              "  0.7476246953010559,\n",
              "  0.7567354440689087,\n",
              "  0.7162114381790161,\n",
              "  0.7694770693778992,\n",
              "  0.7628319263458252,\n",
              "  0.6929926872253418,\n",
              "  0.7019496560096741,\n",
              "  0.7235701084136963,\n",
              "  0.7238975763320923,\n",
              "  0.7379100322723389,\n",
              "  0.6933449506759644,\n",
              "  0.7450065016746521,\n",
              "  0.7505065202713013,\n",
              "  0.7065812945365906,\n",
              "  0.7384708523750305,\n",
              "  0.7402124404907227,\n",
              "  0.7271710634231567,\n",
              "  0.7895164489746094,\n",
              "  0.7585612535476685,\n",
              "  0.7579523324966431,\n",
              "  0.7835845947265625,\n",
              "  0.7264468669891357,\n",
              "  0.7437034845352173,\n",
              "  0.7459570169448853,\n",
              "  0.7782168388366699,\n",
              "  0.7411553263664246,\n",
              "  0.7682369351387024,\n",
              "  0.7498811483383179,\n",
              "  0.6857236623764038,\n",
              "  0.7414047718048096,\n",
              "  0.7304110527038574,\n",
              "  0.7366989850997925,\n",
              "  0.7529969215393066,\n",
              "  0.7967239618301392,\n",
              "  0.7237676382064819,\n",
              "  0.724472165107727,\n",
              "  0.7349957823753357,\n",
              "  0.7302701473236084,\n",
              "  0.7004925012588501,\n",
              "  0.6965388059616089,\n",
              "  0.7102512121200562,\n",
              "  0.7785102725028992,\n",
              "  0.7854773998260498,\n",
              "  0.7405396699905396,\n",
              "  0.7435052394866943,\n",
              "  0.6800917983055115,\n",
              "  0.7581197023391724,\n",
              "  0.6762142777442932,\n",
              "  0.7125750780105591,\n",
              "  0.7202926278114319,\n",
              "  0.7790606021881104,\n",
              "  0.7302915453910828,\n",
              "  0.7437577247619629,\n",
              "  0.7052179574966431,\n",
              "  0.7259321808815002,\n",
              "  0.6838778257369995,\n",
              "  0.7330150008201599,\n",
              "  0.7315554618835449,\n",
              "  0.7577908039093018,\n",
              "  0.7522644996643066,\n",
              "  0.7378987073898315,\n",
              "  0.7241113781929016,\n",
              "  0.7178689241409302,\n",
              "  0.7292422652244568,\n",
              "  0.761385440826416,\n",
              "  0.7802069187164307,\n",
              "  0.7585129737854004,\n",
              "  0.691314160823822,\n",
              "  0.7463281154632568,\n",
              "  0.749401330947876,\n",
              "  0.7312276363372803,\n",
              "  0.7498514652252197,\n",
              "  0.737839937210083,\n",
              "  0.7096689939498901,\n",
              "  0.6808929443359375,\n",
              "  0.7533407211303711,\n",
              "  0.683603048324585,\n",
              "  0.7057478427886963,\n",
              "  0.7495262622833252,\n",
              "  0.6942279934883118,\n",
              "  0.7351678609848022,\n",
              "  0.6909229755401611,\n",
              "  0.7252452373504639,\n",
              "  0.7300724983215332,\n",
              "  0.7527765035629272,\n",
              "  0.750373125076294,\n",
              "  0.7631652355194092,\n",
              "  0.7695165276527405,\n",
              "  0.666346549987793,\n",
              "  0.7489184141159058,\n",
              "  0.7635952234268188,\n",
              "  0.6549391746520996,\n",
              "  0.7142210006713867,\n",
              "  0.6889703273773193,\n",
              "  0.7570317983627319,\n",
              "  0.7228362560272217,\n",
              "  0.7417413592338562,\n",
              "  0.7024590969085693,\n",
              "  0.7043238282203674,\n",
              "  0.7445482015609741,\n",
              "  0.7126572132110596,\n",
              "  0.7748236656188965,\n",
              "  0.7436678409576416,\n",
              "  0.7056245803833008,\n",
              "  0.7090461850166321,\n",
              "  0.7277786731719971,\n",
              "  0.7323495149612427,\n",
              "  0.6920146942138672,\n",
              "  0.7597751021385193,\n",
              "  0.7495295405387878,\n",
              "  0.6933741569519043,\n",
              "  0.71647709608078,\n",
              "  0.7139025330543518,\n",
              "  0.7337784767150879,\n",
              "  0.7509028911590576,\n",
              "  0.6935124397277832,\n",
              "  0.7296592593193054,\n",
              "  0.7386687994003296],\n",
              " 'val_acc': [0.6875,\n",
              "  0.796875,\n",
              "  0.65625,\n",
              "  0.703125,\n",
              "  0.734375,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.71875,\n",
              "  0.78125,\n",
              "  0.71875,\n",
              "  0.78125,\n",
              "  0.765625,\n",
              "  0.734375,\n",
              "  0.640625,\n",
              "  0.71875,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.71875,\n",
              "  0.6875,\n",
              "  0.734375,\n",
              "  0.71875,\n",
              "  0.765625,\n",
              "  0.671875,\n",
              "  0.703125,\n",
              "  0.671875,\n",
              "  0.71875,\n",
              "  0.703125,\n",
              "  0.703125,\n",
              "  0.734375,\n",
              "  0.640625,\n",
              "  0.640625,\n",
              "  0.71875,\n",
              "  0.703125,\n",
              "  0.75,\n",
              "  0.703125,\n",
              "  0.6875,\n",
              "  0.640625,\n",
              "  0.640625,\n",
              "  0.71875,\n",
              "  0.65625,\n",
              "  0.671875,\n",
              "  0.71875,\n",
              "  0.671875,\n",
              "  0.671875,\n",
              "  0.65625,\n",
              "  0.71875,\n",
              "  0.6875,\n",
              "  0.640625,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.703125,\n",
              "  0.625,\n",
              "  0.609375,\n",
              "  0.640625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.671875,\n",
              "  0.609375,\n",
              "  0.578125,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.703125,\n",
              "  0.640625,\n",
              "  0.671875,\n",
              "  0.625,\n",
              "  0.671875,\n",
              "  0.625,\n",
              "  0.671875,\n",
              "  0.5625,\n",
              "  0.640625,\n",
              "  0.640625,\n",
              "  0.640625,\n",
              "  0.640625,\n",
              "  0.65625,\n",
              "  0.5625,\n",
              "  0.609375,\n",
              "  0.625,\n",
              "  0.546875,\n",
              "  0.640625,\n",
              "  0.625,\n",
              "  0.609375,\n",
              "  0.671875,\n",
              "  0.625,\n",
              "  0.640625,\n",
              "  0.578125,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.609375,\n",
              "  0.640625,\n",
              "  0.65625,\n",
              "  0.578125,\n",
              "  0.546875,\n",
              "  0.609375,\n",
              "  0.546875,\n",
              "  0.578125,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.546875,\n",
              "  0.65625,\n",
              "  0.546875,\n",
              "  0.640625,\n",
              "  0.578125,\n",
              "  0.53125,\n",
              "  0.515625,\n",
              "  0.546875,\n",
              "  0.515625,\n",
              "  0.625,\n",
              "  0.53125,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.578125,\n",
              "  0.59375,\n",
              "  0.578125,\n",
              "  0.5625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.59375,\n",
              "  0.5,\n",
              "  0.578125,\n",
              "  0.46875,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.5,\n",
              "  0.515625,\n",
              "  0.609375,\n",
              "  0.546875,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.546875,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.390625,\n",
              "  0.546875,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.546875,\n",
              "  0.421875,\n",
              "  0.5625,\n",
              "  0.5,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.359375,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.390625,\n",
              "  0.484375,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.46875,\n",
              "  0.34375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.34375,\n",
              "  0.46875,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.34375,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.46875,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.453125,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.359375,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.453125,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.453125,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.359375,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.515625,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.359375,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.484375,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.53125,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.375,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.34375,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.34375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.359375,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.421875,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.4375,\n",
              "  0.359375,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.375,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.5,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.546875,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.53125,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.53125,\n",
              "  0.390625,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.421875,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.390625,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.421875,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.40625,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.359375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.515625,\n",
              "  0.421875,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.390625,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.421875,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.375,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.421875,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.53125,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.375,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.40625,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.453125,\n",
              "  0.53125,\n",
              "  0.453125,\n",
              "  0.421875,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.421875,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.53125,\n",
              "  0.546875,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.546875,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.5625,\n",
              "  0.46875,\n",
              "  0.578125,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.546875,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.546875,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.515625,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.421875,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.390625,\n",
              "  0.515625,\n",
              "  0.3125,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.390625,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.53125,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.375,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.53125,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.5625,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.421875,\n",
              "  0.5,\n",
              "  0.484375,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.484375,\n",
              "  0.53125,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.453125,\n",
              "  0.546875,\n",
              "  0.46875,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.515625,\n",
              "  0.40625,\n",
              "  0.515625,\n",
              "  0.4375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.53125,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.46875,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.375,\n",
              "  0.453125,\n",
              "  0.546875,\n",
              "  0.515625,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.515625,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5625,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.546875,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.578125,\n",
              "  0.515625,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.515625,\n",
              "  0.453125,\n",
              "  0.546875,\n",
              "  0.5,\n",
              "  0.453125,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.484375,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.546875,\n",
              "  0.46875,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.484375,\n",
              "  0.515625,\n",
              "  0.515625,\n",
              "  0.46875]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "ccdd67ac-568c-4260-e22f-90e5e0c91586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "46604ca2-993b-4520-9000-3d40c87fe3ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ed04aa1c-2291-490d-b299-1176a7aac659\", \"2Class.h5\", 16605296)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}