{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPO1tnX21MRefspkHRIB6M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/2Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "e14cb30e-2089-4a3c-848f-e2280b815b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "94306840-4702-4257-a8ec-54d0ce79f45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd06d81d-3b4c-4a7f-a902-1863dc3313b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd06d81d-3b4c-4a7f-a902-1863dc3313b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd06d81d-3b4c-4a7f-a902-1863dc3313b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd06d81d-3b4c-4a7f-a902-1863dc3313b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "5f6d340f-ed80-4562-9f99-e5c80577424f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "ab2d7015-0eeb-4ade-cdfd-15a3e67eda6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "8e4b5b95-11b6-4b72-eb19-070cf24ba385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "168499be-f5e9-4b3a-de91-43e421eb12a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-800')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '801-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-800')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '801-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-800')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '801-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-800']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='801-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-800']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='801-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-800']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='801-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "f7fbe266-a2ab-4e18-b9dc-bfc8a83efe10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "515216b7-fd02-447e-befd-d315cfbdfd1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_195 (Conv2D)            (None, 75, 75, 32)   864         ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_147 (Batch  (None, 75, 75, 32)  128         ['conv2d_195[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_147 (Swish)              (None, 75, 75, 32)   0           ['batch_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_48 (Depthwise  (None, 75, 75, 32)  288         ['swish_147[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_148 (Batch  (None, 75, 75, 32)  128         ['depthwise_conv2d_48[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_148 (Swish)              (None, 75, 75, 32)   0           ['batch_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " lambda_48 (Lambda)             (None, 1, 1, 32)     0           ['swish_148[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_196 (Conv2D)            (None, 1, 1, 8)      264         ['lambda_48[0][0]']              \n",
            "                                                                                                  \n",
            " swish_149 (Swish)              (None, 1, 1, 8)      0           ['conv2d_196[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_197 (Conv2D)            (None, 1, 1, 32)     288         ['swish_149[0][0]']              \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 1, 1, 32)     0           ['conv2d_197[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_48 (Multiply)         (None, 75, 75, 32)   0           ['activation_48[0][0]',          \n",
            "                                                                  'swish_148[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_198 (Conv2D)            (None, 75, 75, 16)   512         ['multiply_48[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_149 (Batch  (None, 75, 75, 16)  64          ['conv2d_198[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_199 (Conv2D)            (None, 75, 75, 96)   1536        ['batch_normalization_149[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_150 (Batch  (None, 75, 75, 96)  384         ['conv2d_199[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_150 (Swish)              (None, 75, 75, 96)   0           ['batch_normalization_150[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_49 (Depthwise  (None, 38, 38, 96)  864         ['swish_150[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_151 (Batch  (None, 38, 38, 96)  384         ['depthwise_conv2d_49[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_151 (Swish)              (None, 38, 38, 96)   0           ['batch_normalization_151[0][0]']\n",
            "                                                                                                  \n",
            " lambda_49 (Lambda)             (None, 1, 1, 96)     0           ['swish_151[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)            (None, 1, 1, 4)      388         ['lambda_49[0][0]']              \n",
            "                                                                                                  \n",
            " swish_152 (Swish)              (None, 1, 1, 4)      0           ['conv2d_200[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)            (None, 1, 1, 96)     480         ['swish_152[0][0]']              \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 1, 1, 96)     0           ['conv2d_201[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_49 (Multiply)         (None, 38, 38, 96)   0           ['activation_49[0][0]',          \n",
            "                                                                  'swish_151[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)            (None, 38, 38, 24)   2304        ['multiply_49[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_152 (Batch  (None, 38, 38, 24)  96          ['conv2d_202[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_203 (Conv2D)            (None, 38, 38, 144)  3456        ['batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_153 (Batch  (None, 38, 38, 144)  576        ['conv2d_203[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_153 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_153[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_50 (Depthwise  (None, 38, 38, 144)  1296       ['swish_153[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_154 (Batch  (None, 38, 38, 144)  576        ['depthwise_conv2d_50[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_154 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_154[0][0]']\n",
            "                                                                                                  \n",
            " lambda_50 (Lambda)             (None, 1, 1, 144)    0           ['swish_154[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_50[0][0]']              \n",
            "                                                                                                  \n",
            " swish_155 (Swish)              (None, 1, 1, 6)      0           ['conv2d_204[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_155[0][0]']              \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 1, 1, 144)    0           ['conv2d_205[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_50 (Multiply)         (None, 38, 38, 144)  0           ['activation_50[0][0]',          \n",
            "                                                                  'swish_154[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)            (None, 38, 38, 24)   3456        ['multiply_50[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_155 (Batch  (None, 38, 38, 24)  96          ['conv2d_206[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_27 (DropConnect)  (None, 38, 38, 24)   0           ['batch_normalization_155[0][0]']\n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 38, 38, 24)   0           ['drop_connect_27[0][0]',        \n",
            "                                                                  'batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)            (None, 38, 38, 144)  3456        ['add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_156 (Batch  (None, 38, 38, 144)  576        ['conv2d_207[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_156 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_156[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_51 (Depthwise  (None, 19, 19, 144)  3600       ['swish_156[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_157 (Batch  (None, 19, 19, 144)  576        ['depthwise_conv2d_51[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_157 (Swish)              (None, 19, 19, 144)  0           ['batch_normalization_157[0][0]']\n",
            "                                                                                                  \n",
            " lambda_51 (Lambda)             (None, 1, 1, 144)    0           ['swish_157[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_51[0][0]']              \n",
            "                                                                                                  \n",
            " swish_158 (Swish)              (None, 1, 1, 6)      0           ['conv2d_208[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_158[0][0]']              \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 1, 1, 144)    0           ['conv2d_209[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_51 (Multiply)         (None, 19, 19, 144)  0           ['activation_51[0][0]',          \n",
            "                                                                  'swish_157[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, 19, 19, 40)   5760        ['multiply_51[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_158 (Batch  (None, 19, 19, 40)  160         ['conv2d_210[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)            (None, 19, 19, 240)  9600        ['batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_159 (Batch  (None, 19, 19, 240)  960        ['conv2d_211[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_159 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_159[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_52 (Depthwise  (None, 19, 19, 240)  6000       ['swish_159[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_160 (Batch  (None, 19, 19, 240)  960        ['depthwise_conv2d_52[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_160 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_160[0][0]']\n",
            "                                                                                                  \n",
            " lambda_52 (Lambda)             (None, 1, 1, 240)    0           ['swish_160[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_52[0][0]']              \n",
            "                                                                                                  \n",
            " swish_161 (Swish)              (None, 1, 1, 10)     0           ['conv2d_212[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_161[0][0]']              \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 1, 1, 240)    0           ['conv2d_213[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_52 (Multiply)         (None, 19, 19, 240)  0           ['activation_52[0][0]',          \n",
            "                                                                  'swish_160[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)            (None, 19, 19, 40)   9600        ['multiply_52[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_161 (Batch  (None, 19, 19, 40)  160         ['conv2d_214[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_28 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_161[0][0]']\n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_28[0][0]',        \n",
            "                                                                  'batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)            (None, 19, 19, 240)  9600        ['add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_162 (Batch  (None, 19, 19, 240)  960        ['conv2d_215[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_162 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_162[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_53 (Depthwise  (None, 10, 10, 240)  2160       ['swish_162[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_163 (Batch  (None, 10, 10, 240)  960        ['depthwise_conv2d_53[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_163 (Swish)              (None, 10, 10, 240)  0           ['batch_normalization_163[0][0]']\n",
            "                                                                                                  \n",
            " lambda_53 (Lambda)             (None, 1, 1, 240)    0           ['swish_163[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_53[0][0]']              \n",
            "                                                                                                  \n",
            " swish_164 (Swish)              (None, 1, 1, 10)     0           ['conv2d_216[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_164[0][0]']              \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 1, 1, 240)    0           ['conv2d_217[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_53 (Multiply)         (None, 10, 10, 240)  0           ['activation_53[0][0]',          \n",
            "                                                                  'swish_163[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_218 (Conv2D)            (None, 10, 10, 80)   19200       ['multiply_53[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_164 (Batch  (None, 10, 10, 80)  320         ['conv2d_218[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)            (None, 10, 10, 480)  38400       ['batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_165 (Batch  (None, 10, 10, 480)  1920       ['conv2d_219[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_165 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_165[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_54 (Depthwise  (None, 10, 10, 480)  4320       ['swish_165[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_166 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_54[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_166 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_166[0][0]']\n",
            "                                                                                                  \n",
            " lambda_54 (Lambda)             (None, 1, 1, 480)    0           ['swish_166[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_54[0][0]']              \n",
            "                                                                                                  \n",
            " swish_167 (Swish)              (None, 1, 1, 20)     0           ['conv2d_220[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_167[0][0]']              \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 1, 1, 480)    0           ['conv2d_221[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_54 (Multiply)         (None, 10, 10, 480)  0           ['activation_54[0][0]',          \n",
            "                                                                  'swish_166[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_54[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_167 (Batch  (None, 10, 10, 80)  320         ['conv2d_222[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_29 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_29[0][0]',        \n",
            "                                                                  'batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_223 (Conv2D)            (None, 10, 10, 480)  38400       ['add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_168 (Batch  (None, 10, 10, 480)  1920       ['conv2d_223[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_168 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_168[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_55 (Depthwise  (None, 10, 10, 480)  4320       ['swish_168[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_169 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_55[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_169 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_169[0][0]']\n",
            "                                                                                                  \n",
            " lambda_55 (Lambda)             (None, 1, 1, 480)    0           ['swish_169[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_55[0][0]']              \n",
            "                                                                                                  \n",
            " swish_170 (Swish)              (None, 1, 1, 20)     0           ['conv2d_224[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_170[0][0]']              \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 1, 1, 480)    0           ['conv2d_225[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_55 (Multiply)         (None, 10, 10, 480)  0           ['activation_55[0][0]',          \n",
            "                                                                  'swish_169[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_55[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_170 (Batch  (None, 10, 10, 80)  320         ['conv2d_226[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_30 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_170[0][0]']\n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_30[0][0]',        \n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)            (None, 10, 10, 480)  38400       ['add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_171 (Batch  (None, 10, 10, 480)  1920       ['conv2d_227[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_171 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_171[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_56 (Depthwise  (None, 10, 10, 480)  12000      ['swish_171[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_172 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_56[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_172 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_172[0][0]']\n",
            "                                                                                                  \n",
            " lambda_56 (Lambda)             (None, 1, 1, 480)    0           ['swish_172[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_228 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_56[0][0]']              \n",
            "                                                                                                  \n",
            " swish_173 (Swish)              (None, 1, 1, 20)     0           ['conv2d_228[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_229 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_173[0][0]']              \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 1, 1, 480)    0           ['conv2d_229[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_56 (Multiply)         (None, 10, 10, 480)  0           ['activation_56[0][0]',          \n",
            "                                                                  'swish_172[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_230 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_56[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_173 (Batch  (None, 10, 10, 112)  448        ['conv2d_230[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_231 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_174 (Batch  (None, 10, 10, 672)  2688       ['conv2d_231[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_174 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_174[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_57 (Depthwise  (None, 10, 10, 672)  16800      ['swish_174[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_175 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_57[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_175 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_175[0][0]']\n",
            "                                                                                                  \n",
            " lambda_57 (Lambda)             (None, 1, 1, 672)    0           ['swish_175[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_232 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_57[0][0]']              \n",
            "                                                                                                  \n",
            " swish_176 (Swish)              (None, 1, 1, 28)     0           ['conv2d_232[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_233 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_176[0][0]']              \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 1, 1, 672)    0           ['conv2d_233[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_57 (Multiply)         (None, 10, 10, 672)  0           ['activation_57[0][0]',          \n",
            "                                                                  'swish_175[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_234 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_57[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_176 (Batch  (None, 10, 10, 112)  448        ['conv2d_234[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_31 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_176[0][0]']\n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_31[0][0]',        \n",
            "                                                                  'batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_235 (Conv2D)            (None, 10, 10, 672)  75264       ['add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_177 (Batch  (None, 10, 10, 672)  2688       ['conv2d_235[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_177 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_177[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_58 (Depthwise  (None, 10, 10, 672)  16800      ['swish_177[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_178 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_58[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_178 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_178[0][0]']\n",
            "                                                                                                  \n",
            " lambda_58 (Lambda)             (None, 1, 1, 672)    0           ['swish_178[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_236 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_58[0][0]']              \n",
            "                                                                                                  \n",
            " swish_179 (Swish)              (None, 1, 1, 28)     0           ['conv2d_236[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_237 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_179[0][0]']              \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 1, 1, 672)    0           ['conv2d_237[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_58 (Multiply)         (None, 10, 10, 672)  0           ['activation_58[0][0]',          \n",
            "                                                                  'swish_178[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_238 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_58[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_179 (Batch  (None, 10, 10, 112)  448        ['conv2d_238[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_32 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_179[0][0]']\n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_32[0][0]',        \n",
            "                                                                  'add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_239 (Conv2D)            (None, 10, 10, 672)  75264       ['add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_180 (Batch  (None, 10, 10, 672)  2688       ['conv2d_239[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_180 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_180[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_59 (Depthwise  (None, 5, 5, 672)   16800       ['swish_180[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_181 (Batch  (None, 5, 5, 672)   2688        ['depthwise_conv2d_59[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_181 (Swish)              (None, 5, 5, 672)    0           ['batch_normalization_181[0][0]']\n",
            "                                                                                                  \n",
            " lambda_59 (Lambda)             (None, 1, 1, 672)    0           ['swish_181[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_240 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_59[0][0]']              \n",
            "                                                                                                  \n",
            " swish_182 (Swish)              (None, 1, 1, 28)     0           ['conv2d_240[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_241 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_182[0][0]']              \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 1, 1, 672)    0           ['conv2d_241[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_59 (Multiply)         (None, 5, 5, 672)    0           ['activation_59[0][0]',          \n",
            "                                                                  'swish_181[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_242 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_59[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_182 (Batch  (None, 5, 5, 192)   768         ['conv2d_242[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_243 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_183 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_243[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_183 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_183[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_60 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_183[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_184 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_60[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_184 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_184[0][0]']\n",
            "                                                                                                  \n",
            " lambda_60 (Lambda)             (None, 1, 1, 1152)   0           ['swish_184[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_244 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_60[0][0]']              \n",
            "                                                                                                  \n",
            " swish_185 (Swish)              (None, 1, 1, 48)     0           ['conv2d_244[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_245 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_185[0][0]']              \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_245[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_60 (Multiply)         (None, 5, 5, 1152)   0           ['activation_60[0][0]',          \n",
            "                                                                  'swish_184[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_246 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_60[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_185 (Batch  (None, 5, 5, 192)   768         ['conv2d_246[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_33 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_185[0][0]']\n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_33[0][0]',        \n",
            "                                                                  'batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_247 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_186 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_247[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_186 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_61 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_186[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_187 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_61[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_187 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_187[0][0]']\n",
            "                                                                                                  \n",
            " lambda_61 (Lambda)             (None, 1, 1, 1152)   0           ['swish_187[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_248 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_61[0][0]']              \n",
            "                                                                                                  \n",
            " swish_188 (Swish)              (None, 1, 1, 48)     0           ['conv2d_248[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_249 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_188[0][0]']              \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_249[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_61 (Multiply)         (None, 5, 5, 1152)   0           ['activation_61[0][0]',          \n",
            "                                                                  'swish_187[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_250 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_61[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_188 (Batch  (None, 5, 5, 192)   768         ['conv2d_250[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_34 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_188[0][0]']\n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_34[0][0]',        \n",
            "                                                                  'add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_251 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_189 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_251[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_189 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_62 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_189[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_190 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_62[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_190 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_190[0][0]']\n",
            "                                                                                                  \n",
            " lambda_62 (Lambda)             (None, 1, 1, 1152)   0           ['swish_190[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_252 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_62[0][0]']              \n",
            "                                                                                                  \n",
            " swish_191 (Swish)              (None, 1, 1, 48)     0           ['conv2d_252[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_253 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_191[0][0]']              \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_253[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_62 (Multiply)         (None, 5, 5, 1152)   0           ['activation_62[0][0]',          \n",
            "                                                                  'swish_190[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_254 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_62[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_191 (Batch  (None, 5, 5, 192)   768         ['conv2d_254[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_35 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_191[0][0]']\n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_35[0][0]',        \n",
            "                                                                  'add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_255 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_192 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_255[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_192 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_192[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_63 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_192[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_193 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_63[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_193 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_193[0][0]']\n",
            "                                                                                                  \n",
            " lambda_63 (Lambda)             (None, 1, 1, 1152)   0           ['swish_193[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_256 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_63[0][0]']              \n",
            "                                                                                                  \n",
            " swish_194 (Swish)              (None, 1, 1, 48)     0           ['conv2d_256[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_257 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_194[0][0]']              \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_257[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_63 (Multiply)         (None, 5, 5, 1152)   0           ['activation_63[0][0]',          \n",
            "                                                                  'swish_193[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_258 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_63[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_194 (Batch  (None, 5, 5, 320)   1280        ['conv2d_258[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_259 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_194[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_195 (Batch  (None, 5, 5, 1280)  5120        ['conv2d_259[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_195 (Swish)              (None, 5, 5, 1280)   0           ['batch_normalization_195[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "4ce7beaa-bbf6-4e6a-fe74-e16c6a0d5eb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "f34f2886-a8a3-4f16-b1d8-89980461fbff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "0df47026-aad6-4f0d-c548-95d5b155b82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c421360-fff0-4419-f0a5-9cc769106552"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-85-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 10s 114ms/step - loss: 1.1836 - acc: 0.5309 - val_loss: 1.0530 - val_acc: 0.4062\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1.1654 - acc: 0.5241 - val_loss: 1.0236 - val_acc: 0.3750\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1287 - acc: 0.5447 - val_loss: 1.0257 - val_acc: 0.3333\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.2095 - acc: 0.5172 - val_loss: 1.0180 - val_acc: 0.3438\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1828 - acc: 0.4966 - val_loss: 1.0607 - val_acc: 0.3021\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.2383 - acc: 0.5103 - val_loss: 1.0439 - val_acc: 0.3021\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1519 - acc: 0.5361 - val_loss: 1.0388 - val_acc: 0.3125\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1106 - acc: 0.5378 - val_loss: 1.0293 - val_acc: 0.3125\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1465 - acc: 0.5206 - val_loss: 1.0457 - val_acc: 0.3229\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1778 - acc: 0.4966 - val_loss: 1.0542 - val_acc: 0.3125\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 1.1367 - acc: 0.5258 - val_loss: 1.0722 - val_acc: 0.3021\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 3s 68ms/step - loss: 1.2212 - acc: 0.4863 - val_loss: 1.0321 - val_acc: 0.3125\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0706 - acc: 0.5361 - val_loss: 1.0649 - val_acc: 0.3125\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1453 - acc: 0.5275 - val_loss: 1.0567 - val_acc: 0.3125\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1340 - acc: 0.5223 - val_loss: 1.0623 - val_acc: 0.3125\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0845 - acc: 0.5619 - val_loss: 1.0641 - val_acc: 0.3125\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1483 - acc: 0.5309 - val_loss: 1.0783 - val_acc: 0.3021\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0759 - acc: 0.5619 - val_loss: 1.0769 - val_acc: 0.3021\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1479 - acc: 0.5344 - val_loss: 1.0515 - val_acc: 0.3125\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0945 - acc: 0.5206 - val_loss: 1.0569 - val_acc: 0.3021\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1.1523 - acc: 0.5258 - val_loss: 1.0786 - val_acc: 0.2812\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.1512 - acc: 0.5412 - val_loss: 1.0578 - val_acc: 0.2917\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1311 - acc: 0.5344 - val_loss: 1.0774 - val_acc: 0.2812\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.1248 - acc: 0.5275 - val_loss: 1.0706 - val_acc: 0.2917\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0537 - acc: 0.5636 - val_loss: 1.0494 - val_acc: 0.2812\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0960 - acc: 0.5344 - val_loss: 1.0702 - val_acc: 0.2708\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0717 - acc: 0.5670 - val_loss: 1.0701 - val_acc: 0.2917\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0155 - acc: 0.5945 - val_loss: 1.0712 - val_acc: 0.2812\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.1067 - acc: 0.5241 - val_loss: 1.0908 - val_acc: 0.2708\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9858 - acc: 0.5653 - val_loss: 1.0790 - val_acc: 0.2812\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1.0892 - acc: 0.5321 - val_loss: 1.0811 - val_acc: 0.2708\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0687 - acc: 0.5464 - val_loss: 1.0807 - val_acc: 0.2708\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1.0572 - acc: 0.5344 - val_loss: 1.0907 - val_acc: 0.2500\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0820 - acc: 0.5653 - val_loss: 1.0856 - val_acc: 0.2812\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0350 - acc: 0.5344 - val_loss: 1.0958 - val_acc: 0.2708\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0228 - acc: 0.5206 - val_loss: 1.0782 - val_acc: 0.2604\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1.0254 - acc: 0.5481 - val_loss: 1.0717 - val_acc: 0.2812\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9081 - acc: 0.5704 - val_loss: 1.0795 - val_acc: 0.2708\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.1069 - acc: 0.5378 - val_loss: 1.0652 - val_acc: 0.2708\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.9882 - acc: 0.5584 - val_loss: 1.1076 - val_acc: 0.2500\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9823 - acc: 0.5876 - val_loss: 1.0607 - val_acc: 0.2708\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0538 - acc: 0.5584 - val_loss: 1.0743 - val_acc: 0.2604\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0789 - acc: 0.5533 - val_loss: 1.0780 - val_acc: 0.2708\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9442 - acc: 0.5808 - val_loss: 1.0823 - val_acc: 0.2708\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9819 - acc: 0.5773 - val_loss: 1.0890 - val_acc: 0.2604\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0699 - acc: 0.5481 - val_loss: 1.0886 - val_acc: 0.2500\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9827 - acc: 0.5636 - val_loss: 1.0787 - val_acc: 0.2500\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1.0167 - acc: 0.5550 - val_loss: 1.0792 - val_acc: 0.2500\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9861 - acc: 0.5533 - val_loss: 1.0844 - val_acc: 0.2604\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0177 - acc: 0.5515 - val_loss: 1.0776 - val_acc: 0.2396\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9533 - acc: 0.5636 - val_loss: 1.0691 - val_acc: 0.2708\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0961 - acc: 0.5361 - val_loss: 1.1084 - val_acc: 0.2292\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9834 - acc: 0.5739 - val_loss: 1.0878 - val_acc: 0.2604\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 3s 68ms/step - loss: 1.0238 - acc: 0.5395 - val_loss: 1.0838 - val_acc: 0.2500\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0130 - acc: 0.5808 - val_loss: 1.0988 - val_acc: 0.2292\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0525 - acc: 0.5456 - val_loss: 1.0878 - val_acc: 0.2500\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9814 - acc: 0.5653 - val_loss: 1.0971 - val_acc: 0.2292\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9267 - acc: 0.6134 - val_loss: 1.1058 - val_acc: 0.2188\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9775 - acc: 0.5756 - val_loss: 1.1096 - val_acc: 0.2292\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0232 - acc: 0.5309 - val_loss: 1.1060 - val_acc: 0.2188\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9836 - acc: 0.5642 - val_loss: 1.1083 - val_acc: 0.2396\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9164 - acc: 0.5859 - val_loss: 1.0934 - val_acc: 0.2604\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1.0186 - acc: 0.5636 - val_loss: 1.0997 - val_acc: 0.2396\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9690 - acc: 0.5704 - val_loss: 1.0844 - val_acc: 0.2500\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0394 - acc: 0.5619 - val_loss: 1.0925 - val_acc: 0.2604\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9809 - acc: 0.5808 - val_loss: 1.1026 - val_acc: 0.2188\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.9247 - acc: 0.5842 - val_loss: 1.0861 - val_acc: 0.2604\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9811 - acc: 0.5636 - val_loss: 1.1050 - val_acc: 0.2500\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9804 - acc: 0.5584 - val_loss: 1.0864 - val_acc: 0.2292\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9916 - acc: 0.5326 - val_loss: 1.0873 - val_acc: 0.2396\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0055 - acc: 0.5464 - val_loss: 1.0749 - val_acc: 0.2500\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1.0024 - acc: 0.5704 - val_loss: 1.0896 - val_acc: 0.2292\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0522 - acc: 0.5756 - val_loss: 1.0760 - val_acc: 0.2396\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9809 - acc: 0.5636 - val_loss: 1.0709 - val_acc: 0.2396\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8522 - acc: 0.5911 - val_loss: 1.0784 - val_acc: 0.2292\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 1.0077 - acc: 0.5498 - val_loss: 1.0671 - val_acc: 0.2396\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9852 - acc: 0.5601 - val_loss: 1.0698 - val_acc: 0.2500\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9186 - acc: 0.5825 - val_loss: 1.0679 - val_acc: 0.2396\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9261 - acc: 0.5928 - val_loss: 1.0866 - val_acc: 0.2396\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9778 - acc: 0.5722 - val_loss: 1.0773 - val_acc: 0.2500\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9547 - acc: 0.5893 - val_loss: 1.0769 - val_acc: 0.2396\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1.0333 - acc: 0.5481 - val_loss: 1.0777 - val_acc: 0.2292\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9586 - acc: 0.5739 - val_loss: 1.0963 - val_acc: 0.2396\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9504 - acc: 0.5722 - val_loss: 1.0767 - val_acc: 0.2396\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9284 - acc: 0.5756 - val_loss: 1.0831 - val_acc: 0.2500\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9363 - acc: 0.5859 - val_loss: 1.1105 - val_acc: 0.1979\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9031 - acc: 0.6031 - val_loss: 1.0944 - val_acc: 0.2083\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9165 - acc: 0.5704 - val_loss: 1.0763 - val_acc: 0.2292\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9734 - acc: 0.5601 - val_loss: 1.0720 - val_acc: 0.2292\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8758 - acc: 0.5928 - val_loss: 1.0750 - val_acc: 0.2500\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.9878 - acc: 0.5653 - val_loss: 1.0468 - val_acc: 0.2396\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9405 - acc: 0.5756 - val_loss: 1.0612 - val_acc: 0.2396\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8582 - acc: 0.6048 - val_loss: 1.0933 - val_acc: 0.2292\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8735 - acc: 0.5842 - val_loss: 1.0681 - val_acc: 0.2292\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9847 - acc: 0.5722 - val_loss: 1.0576 - val_acc: 0.2500\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8823 - acc: 0.6065 - val_loss: 1.0616 - val_acc: 0.2396\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9421 - acc: 0.5739 - val_loss: 1.0816 - val_acc: 0.2500\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9875 - acc: 0.5928 - val_loss: 1.0788 - val_acc: 0.2396\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.9254 - acc: 0.5859 - val_loss: 1.0663 - val_acc: 0.2292\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9353 - acc: 0.5790 - val_loss: 1.0614 - val_acc: 0.2396\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9456 - acc: 0.5687 - val_loss: 1.0791 - val_acc: 0.2396\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9460 - acc: 0.5876 - val_loss: 1.1013 - val_acc: 0.2396\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9640 - acc: 0.5859 - val_loss: 1.0884 - val_acc: 0.2396\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9495 - acc: 0.5997 - val_loss: 1.0915 - val_acc: 0.2500\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9119 - acc: 0.5687 - val_loss: 1.0793 - val_acc: 0.2396\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9065 - acc: 0.5722 - val_loss: 1.0486 - val_acc: 0.2708\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9559 - acc: 0.5842 - val_loss: 1.0686 - val_acc: 0.2292\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9213 - acc: 0.5962 - val_loss: 1.0864 - val_acc: 0.2500\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8398 - acc: 0.6151 - val_loss: 1.0679 - val_acc: 0.2500\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9493 - acc: 0.5790 - val_loss: 1.0849 - val_acc: 0.2396\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8945 - acc: 0.5756 - val_loss: 1.0698 - val_acc: 0.2292\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9543 - acc: 0.5842 - val_loss: 1.0674 - val_acc: 0.2500\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9050 - acc: 0.5726 - val_loss: 1.0737 - val_acc: 0.2500\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8668 - acc: 0.5842 - val_loss: 1.0731 - val_acc: 0.2500\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8875 - acc: 0.6186 - val_loss: 1.0769 - val_acc: 0.2396\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9725 - acc: 0.5619 - val_loss: 1.0650 - val_acc: 0.2604\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9102 - acc: 0.5790 - val_loss: 1.0560 - val_acc: 0.2500\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8565 - acc: 0.5962 - val_loss: 1.0902 - val_acc: 0.2292\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.9924 - acc: 0.5378 - val_loss: 1.0726 - val_acc: 0.2708\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9538 - acc: 0.5464 - val_loss: 1.0624 - val_acc: 0.2500\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9364 - acc: 0.5653 - val_loss: 1.0870 - val_acc: 0.2396\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9304 - acc: 0.5704 - val_loss: 1.0646 - val_acc: 0.2500\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9241 - acc: 0.5756 - val_loss: 1.0585 - val_acc: 0.2500\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8686 - acc: 0.5979 - val_loss: 1.0495 - val_acc: 0.2500\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8625 - acc: 0.5945 - val_loss: 1.0609 - val_acc: 0.2500\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9581 - acc: 0.5670 - val_loss: 1.0694 - val_acc: 0.2396\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8891 - acc: 0.5756 - val_loss: 1.0425 - val_acc: 0.2812\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8874 - acc: 0.5945 - val_loss: 1.0700 - val_acc: 0.2500\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8547 - acc: 0.5670 - val_loss: 1.0652 - val_acc: 0.2500\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9101 - acc: 0.5412 - val_loss: 1.0427 - val_acc: 0.2604\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.9021 - acc: 0.5859 - val_loss: 1.0524 - val_acc: 0.2604\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.8251 - acc: 0.6117 - val_loss: 1.0269 - val_acc: 0.2604\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.8845 - acc: 0.5773 - val_loss: 1.0673 - val_acc: 0.2500\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8856 - acc: 0.5893 - val_loss: 1.0517 - val_acc: 0.2500\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9628 - acc: 0.5773 - val_loss: 1.0382 - val_acc: 0.2708\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.8782 - acc: 0.5962 - val_loss: 1.0602 - val_acc: 0.2500\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8721 - acc: 0.5773 - val_loss: 1.0388 - val_acc: 0.2708\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.9722 - acc: 0.5704 - val_loss: 1.0451 - val_acc: 0.2708\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9067 - acc: 0.6014 - val_loss: 1.0329 - val_acc: 0.2812\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8486 - acc: 0.6134 - val_loss: 1.0481 - val_acc: 0.2604\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.8776 - acc: 0.5928 - val_loss: 1.0279 - val_acc: 0.2708\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8488 - acc: 0.6100 - val_loss: 1.0542 - val_acc: 0.2604\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.9148 - acc: 0.5773 - val_loss: 1.0550 - val_acc: 0.2604\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8426 - acc: 0.6082 - val_loss: 1.0436 - val_acc: 0.2500\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8577 - acc: 0.5825 - val_loss: 1.0249 - val_acc: 0.2604\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8274 - acc: 0.6284 - val_loss: 1.0401 - val_acc: 0.2708\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8676 - acc: 0.6117 - val_loss: 1.0236 - val_acc: 0.2604\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8239 - acc: 0.6271 - val_loss: 1.0488 - val_acc: 0.2604\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8771 - acc: 0.5928 - val_loss: 1.0436 - val_acc: 0.2500\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8406 - acc: 0.5893 - val_loss: 1.0100 - val_acc: 0.2917\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8948 - acc: 0.5876 - val_loss: 1.0369 - val_acc: 0.2917\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8387 - acc: 0.5997 - val_loss: 1.0428 - val_acc: 0.2396\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8465 - acc: 0.6014 - val_loss: 1.0471 - val_acc: 0.2500\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8151 - acc: 0.6237 - val_loss: 1.0306 - val_acc: 0.2812\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8566 - acc: 0.6082 - val_loss: 1.0295 - val_acc: 0.2500\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8450 - acc: 0.6082 - val_loss: 1.0407 - val_acc: 0.2708\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8274 - acc: 0.5979 - val_loss: 1.0226 - val_acc: 0.2917\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7862 - acc: 0.6203 - val_loss: 1.0411 - val_acc: 0.2708\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8529 - acc: 0.5928 - val_loss: 1.0230 - val_acc: 0.2708\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8781 - acc: 0.5584 - val_loss: 1.0223 - val_acc: 0.2917\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8389 - acc: 0.5945 - val_loss: 1.0450 - val_acc: 0.2708\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.9065 - acc: 0.5756 - val_loss: 1.0267 - val_acc: 0.2708\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8061 - acc: 0.6031 - val_loss: 1.0222 - val_acc: 0.2812\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8597 - acc: 0.5790 - val_loss: 1.0404 - val_acc: 0.2604\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8333 - acc: 0.6048 - val_loss: 1.0223 - val_acc: 0.2604\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8250 - acc: 0.6117 - val_loss: 1.0145 - val_acc: 0.2708\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8694 - acc: 0.5945 - val_loss: 1.0314 - val_acc: 0.2604\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7792 - acc: 0.6048 - val_loss: 1.0405 - val_acc: 0.2500\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7639 - acc: 0.6271 - val_loss: 1.0151 - val_acc: 0.2917\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8022 - acc: 0.6134 - val_loss: 1.0010 - val_acc: 0.2812\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7929 - acc: 0.6134 - val_loss: 1.0010 - val_acc: 0.2917\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8037 - acc: 0.6186 - val_loss: 1.0083 - val_acc: 0.2812\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.8430 - acc: 0.6100 - val_loss: 0.9947 - val_acc: 0.2812\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8322 - acc: 0.5893 - val_loss: 1.0058 - val_acc: 0.2917\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8041 - acc: 0.5945 - val_loss: 1.0108 - val_acc: 0.2812\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8601 - acc: 0.6100 - val_loss: 1.0000 - val_acc: 0.3021\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8337 - acc: 0.5722 - val_loss: 1.0089 - val_acc: 0.2812\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8414 - acc: 0.5945 - val_loss: 1.0256 - val_acc: 0.2812\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8564 - acc: 0.5704 - val_loss: 1.0062 - val_acc: 0.3021\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8783 - acc: 0.5704 - val_loss: 1.0025 - val_acc: 0.3125\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7555 - acc: 0.6237 - val_loss: 1.0111 - val_acc: 0.2917\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7424 - acc: 0.6357 - val_loss: 1.0154 - val_acc: 0.2917\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8270 - acc: 0.6203 - val_loss: 0.9858 - val_acc: 0.3229\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7832 - acc: 0.6220 - val_loss: 1.0063 - val_acc: 0.2917\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8297 - acc: 0.6014 - val_loss: 0.9707 - val_acc: 0.3229\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8293 - acc: 0.6048 - val_loss: 1.0109 - val_acc: 0.2812\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7940 - acc: 0.6048 - val_loss: 1.0149 - val_acc: 0.3125\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8221 - acc: 0.6100 - val_loss: 1.0043 - val_acc: 0.3021\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.8168 - acc: 0.6186 - val_loss: 1.0168 - val_acc: 0.3229\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8098 - acc: 0.5997 - val_loss: 1.0088 - val_acc: 0.2812\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7026 - acc: 0.6598 - val_loss: 1.0108 - val_acc: 0.2917\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7722 - acc: 0.6564 - val_loss: 1.0117 - val_acc: 0.3229\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8691 - acc: 0.5378 - val_loss: 1.0077 - val_acc: 0.3125\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.7798 - acc: 0.6186 - val_loss: 1.0019 - val_acc: 0.3021\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7473 - acc: 0.6375 - val_loss: 0.9812 - val_acc: 0.3333\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7983 - acc: 0.6168 - val_loss: 1.0022 - val_acc: 0.3021\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.8149 - acc: 0.6117 - val_loss: 0.9777 - val_acc: 0.3229\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.8094 - acc: 0.6237 - val_loss: 0.9867 - val_acc: 0.3021\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8192 - acc: 0.6048 - val_loss: 0.9783 - val_acc: 0.3021\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7646 - acc: 0.6254 - val_loss: 0.9964 - val_acc: 0.3021\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7642 - acc: 0.6220 - val_loss: 0.9839 - val_acc: 0.3021\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8228 - acc: 0.5979 - val_loss: 1.0018 - val_acc: 0.2812\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8270 - acc: 0.6392 - val_loss: 1.0079 - val_acc: 0.2917\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7650 - acc: 0.6460 - val_loss: 0.9913 - val_acc: 0.2917\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7934 - acc: 0.6220 - val_loss: 0.9803 - val_acc: 0.3229\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8041 - acc: 0.5808 - val_loss: 0.9977 - val_acc: 0.2917\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7937 - acc: 0.5911 - val_loss: 0.9820 - val_acc: 0.3021\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8202 - acc: 0.6289 - val_loss: 0.9967 - val_acc: 0.3333\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7528 - acc: 0.6375 - val_loss: 0.9846 - val_acc: 0.3021\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7841 - acc: 0.6065 - val_loss: 0.9972 - val_acc: 0.3229\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8502 - acc: 0.5997 - val_loss: 0.9763 - val_acc: 0.3021\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7924 - acc: 0.5979 - val_loss: 0.9566 - val_acc: 0.3125\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8156 - acc: 0.6151 - val_loss: 0.9648 - val_acc: 0.3229\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.8209 - acc: 0.5893 - val_loss: 0.9673 - val_acc: 0.3333\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7871 - acc: 0.6203 - val_loss: 0.9779 - val_acc: 0.3021\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7776 - acc: 0.6443 - val_loss: 0.9736 - val_acc: 0.3125\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6990 - acc: 0.6529 - val_loss: 0.9943 - val_acc: 0.3333\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7747 - acc: 0.6031 - val_loss: 0.9751 - val_acc: 0.3646\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.8044 - acc: 0.6117 - val_loss: 0.9578 - val_acc: 0.3542\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7441 - acc: 0.6478 - val_loss: 0.9765 - val_acc: 0.3542\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7925 - acc: 0.5997 - val_loss: 0.9850 - val_acc: 0.3438\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7838 - acc: 0.6254 - val_loss: 0.9659 - val_acc: 0.3438\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7694 - acc: 0.6478 - val_loss: 0.9487 - val_acc: 0.3021\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7958 - acc: 0.5997 - val_loss: 0.9499 - val_acc: 0.3646\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7442 - acc: 0.6392 - val_loss: 0.9867 - val_acc: 0.3438\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7645 - acc: 0.6220 - val_loss: 0.9662 - val_acc: 0.3438\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7378 - acc: 0.6289 - val_loss: 0.9856 - val_acc: 0.3438\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7352 - acc: 0.6168 - val_loss: 0.9738 - val_acc: 0.3542\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8298 - acc: 0.5739 - val_loss: 0.9541 - val_acc: 0.3542\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7272 - acc: 0.6409 - val_loss: 0.9794 - val_acc: 0.3646\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7857 - acc: 0.6271 - val_loss: 0.9603 - val_acc: 0.3542\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8291 - acc: 0.6048 - val_loss: 0.9803 - val_acc: 0.3125\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7231 - acc: 0.6392 - val_loss: 0.9699 - val_acc: 0.3229\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7178 - acc: 0.6443 - val_loss: 0.9726 - val_acc: 0.3021\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6804 - acc: 0.6667 - val_loss: 0.9640 - val_acc: 0.3438\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7938 - acc: 0.6100 - val_loss: 0.9657 - val_acc: 0.3229\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7901 - acc: 0.6220 - val_loss: 0.9654 - val_acc: 0.2917\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8203 - acc: 0.6186 - val_loss: 0.9852 - val_acc: 0.3125\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7758 - acc: 0.6065 - val_loss: 0.9918 - val_acc: 0.3438\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6887 - acc: 0.6701 - val_loss: 0.9690 - val_acc: 0.3542\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7564 - acc: 0.6289 - val_loss: 0.9753 - val_acc: 0.3646\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7190 - acc: 0.6392 - val_loss: 0.9827 - val_acc: 0.3438\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7290 - acc: 0.6546 - val_loss: 0.9669 - val_acc: 0.3646\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8163 - acc: 0.6014 - val_loss: 0.9704 - val_acc: 0.3021\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8015 - acc: 0.5945 - val_loss: 0.9700 - val_acc: 0.3125\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.8117 - acc: 0.5945 - val_loss: 0.9719 - val_acc: 0.2812\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.8305 - acc: 0.6289 - val_loss: 0.9658 - val_acc: 0.3333\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.7570 - acc: 0.6271 - val_loss: 0.9757 - val_acc: 0.3646\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7665 - acc: 0.6512 - val_loss: 0.9476 - val_acc: 0.3750\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7776 - acc: 0.6306 - val_loss: 0.9548 - val_acc: 0.3438\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6973 - acc: 0.6753 - val_loss: 0.9660 - val_acc: 0.3438\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7439 - acc: 0.6392 - val_loss: 0.9672 - val_acc: 0.3333\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7898 - acc: 0.5962 - val_loss: 0.9651 - val_acc: 0.3542\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7451 - acc: 0.6220 - val_loss: 0.9529 - val_acc: 0.3438\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7552 - acc: 0.6220 - val_loss: 0.9535 - val_acc: 0.3125\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7761 - acc: 0.6100 - val_loss: 0.9429 - val_acc: 0.3542\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7313 - acc: 0.6375 - val_loss: 0.9579 - val_acc: 0.3750\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7353 - acc: 0.6220 - val_loss: 0.9590 - val_acc: 0.3229\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.7125 - acc: 0.6632 - val_loss: 0.9524 - val_acc: 0.3438\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7489 - acc: 0.6546 - val_loss: 0.9495 - val_acc: 0.3750\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7824 - acc: 0.6203 - val_loss: 0.9706 - val_acc: 0.3438\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.7342 - acc: 0.6168 - val_loss: 0.9426 - val_acc: 0.3750\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.7454 - acc: 0.6426 - val_loss: 0.9639 - val_acc: 0.3125\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7298 - acc: 0.6409 - val_loss: 0.9533 - val_acc: 0.3333\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7403 - acc: 0.6100 - val_loss: 0.9604 - val_acc: 0.3229\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7395 - acc: 0.6443 - val_loss: 0.9471 - val_acc: 0.3542\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7204 - acc: 0.6237 - val_loss: 0.9575 - val_acc: 0.3229\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7496 - acc: 0.6289 - val_loss: 0.9657 - val_acc: 0.3125\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7154 - acc: 0.6495 - val_loss: 0.9516 - val_acc: 0.3438\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7670 - acc: 0.5979 - val_loss: 0.9606 - val_acc: 0.3438\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7556 - acc: 0.6186 - val_loss: 0.9347 - val_acc: 0.3646\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7499 - acc: 0.6220 - val_loss: 0.9275 - val_acc: 0.3854\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7260 - acc: 0.6495 - val_loss: 0.9304 - val_acc: 0.3542\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7314 - acc: 0.6357 - val_loss: 0.9520 - val_acc: 0.3333\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7074 - acc: 0.6546 - val_loss: 0.9395 - val_acc: 0.3542\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7164 - acc: 0.6443 - val_loss: 0.9349 - val_acc: 0.3750\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7438 - acc: 0.5962 - val_loss: 0.9401 - val_acc: 0.3438\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7154 - acc: 0.6581 - val_loss: 0.9399 - val_acc: 0.3438\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7137 - acc: 0.6306 - val_loss: 0.9466 - val_acc: 0.3438\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7602 - acc: 0.6186 - val_loss: 0.9397 - val_acc: 0.3750\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7684 - acc: 0.6082 - val_loss: 0.9476 - val_acc: 0.3229\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7533 - acc: 0.6289 - val_loss: 0.9393 - val_acc: 0.3229\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7057 - acc: 0.6443 - val_loss: 0.9157 - val_acc: 0.3333\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7367 - acc: 0.6351 - val_loss: 0.9337 - val_acc: 0.3438\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7623 - acc: 0.6254 - val_loss: 0.9307 - val_acc: 0.3438\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7264 - acc: 0.6615 - val_loss: 0.9391 - val_acc: 0.3646\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7721 - acc: 0.6082 - val_loss: 0.9358 - val_acc: 0.3542\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7388 - acc: 0.6306 - val_loss: 0.9319 - val_acc: 0.3542\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6942 - acc: 0.6460 - val_loss: 0.9179 - val_acc: 0.3438\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7654 - acc: 0.6186 - val_loss: 0.9245 - val_acc: 0.3646\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7721 - acc: 0.6168 - val_loss: 0.9261 - val_acc: 0.3750\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7865 - acc: 0.6271 - val_loss: 0.9332 - val_acc: 0.3438\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7121 - acc: 0.6512 - val_loss: 0.9308 - val_acc: 0.3333\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7067 - acc: 0.6340 - val_loss: 0.9229 - val_acc: 0.3542\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7265 - acc: 0.6443 - val_loss: 0.9143 - val_acc: 0.3438\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6802 - acc: 0.6615 - val_loss: 0.9150 - val_acc: 0.3646\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7733 - acc: 0.6151 - val_loss: 0.9282 - val_acc: 0.3438\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7144 - acc: 0.6512 - val_loss: 0.9243 - val_acc: 0.3646\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7166 - acc: 0.6512 - val_loss: 0.9179 - val_acc: 0.3333\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6881 - acc: 0.6564 - val_loss: 0.9102 - val_acc: 0.3333\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6837 - acc: 0.6392 - val_loss: 0.9071 - val_acc: 0.3854\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7639 - acc: 0.6220 - val_loss: 0.9251 - val_acc: 0.3438\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7139 - acc: 0.6564 - val_loss: 0.9182 - val_acc: 0.3333\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7318 - acc: 0.6254 - val_loss: 0.9327 - val_acc: 0.3125\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7192 - acc: 0.6306 - val_loss: 0.9242 - val_acc: 0.3438\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7149 - acc: 0.6392 - val_loss: 0.9251 - val_acc: 0.3542\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7065 - acc: 0.6443 - val_loss: 0.9299 - val_acc: 0.3542\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6623 - acc: 0.6632 - val_loss: 0.9272 - val_acc: 0.3229\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6922 - acc: 0.6529 - val_loss: 0.8957 - val_acc: 0.3333\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7232 - acc: 0.6151 - val_loss: 0.9095 - val_acc: 0.3750\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7014 - acc: 0.6701 - val_loss: 0.9213 - val_acc: 0.3542\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6701 - acc: 0.6649 - val_loss: 0.9101 - val_acc: 0.3750\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6907 - acc: 0.6426 - val_loss: 0.9238 - val_acc: 0.3229\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7165 - acc: 0.6340 - val_loss: 0.9038 - val_acc: 0.3750\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 0.7339 - acc: 0.6289 - val_loss: 0.9196 - val_acc: 0.3229\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 3s 69ms/step - loss: 0.6800 - acc: 0.6598 - val_loss: 0.9083 - val_acc: 0.3854\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7061 - acc: 0.6478 - val_loss: 0.9157 - val_acc: 0.3542\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6926 - acc: 0.6529 - val_loss: 0.9141 - val_acc: 0.3438\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6857 - acc: 0.6581 - val_loss: 0.9113 - val_acc: 0.3438\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6626 - acc: 0.6649 - val_loss: 0.9131 - val_acc: 0.3229\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7383 - acc: 0.6478 - val_loss: 0.8859 - val_acc: 0.3333\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6950 - acc: 0.6392 - val_loss: 0.9250 - val_acc: 0.3125\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7267 - acc: 0.6409 - val_loss: 0.9125 - val_acc: 0.3229\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6977 - acc: 0.6151 - val_loss: 0.9198 - val_acc: 0.3646\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6873 - acc: 0.6375 - val_loss: 0.8907 - val_acc: 0.3958\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6536 - acc: 0.6718 - val_loss: 0.8926 - val_acc: 0.3333\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.7641 - acc: 0.6048 - val_loss: 0.8957 - val_acc: 0.3229\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7332 - acc: 0.6426 - val_loss: 0.9252 - val_acc: 0.3021\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7668 - acc: 0.6237 - val_loss: 0.8794 - val_acc: 0.3542\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6528 - acc: 0.6581 - val_loss: 0.9044 - val_acc: 0.3646\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6904 - acc: 0.6649 - val_loss: 0.8987 - val_acc: 0.3542\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7921 - acc: 0.5859 - val_loss: 0.8983 - val_acc: 0.3438\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7065 - acc: 0.6426 - val_loss: 0.8784 - val_acc: 0.3333\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7151 - acc: 0.6203 - val_loss: 0.9066 - val_acc: 0.3229\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6866 - acc: 0.6667 - val_loss: 0.8862 - val_acc: 0.3438\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6551 - acc: 0.6838 - val_loss: 0.8947 - val_acc: 0.3646\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7347 - acc: 0.6409 - val_loss: 0.8866 - val_acc: 0.3542\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7055 - acc: 0.6289 - val_loss: 0.8817 - val_acc: 0.3646\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6510 - acc: 0.6770 - val_loss: 0.8833 - val_acc: 0.3646\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7131 - acc: 0.6323 - val_loss: 0.8842 - val_acc: 0.3438\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7337 - acc: 0.6134 - val_loss: 0.8866 - val_acc: 0.3750\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6971 - acc: 0.6615 - val_loss: 0.9064 - val_acc: 0.3542\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6568 - acc: 0.6735 - val_loss: 0.8756 - val_acc: 0.3542\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6581 - acc: 0.6701 - val_loss: 0.8993 - val_acc: 0.3333\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.7233 - acc: 0.6340 - val_loss: 0.8921 - val_acc: 0.3438\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6536 - acc: 0.6787 - val_loss: 0.8868 - val_acc: 0.3646\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6558 - acc: 0.6546 - val_loss: 0.9024 - val_acc: 0.3333\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6779 - acc: 0.6357 - val_loss: 0.8784 - val_acc: 0.3750\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7350 - acc: 0.6284 - val_loss: 0.8763 - val_acc: 0.3854\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6570 - acc: 0.6588 - val_loss: 0.8887 - val_acc: 0.3542\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7208 - acc: 0.6529 - val_loss: 0.8785 - val_acc: 0.3646\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6682 - acc: 0.6409 - val_loss: 0.8791 - val_acc: 0.3438\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6935 - acc: 0.6443 - val_loss: 0.8818 - val_acc: 0.3750\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7225 - acc: 0.6254 - val_loss: 0.8882 - val_acc: 0.3646\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7265 - acc: 0.6409 - val_loss: 0.8920 - val_acc: 0.3646\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6663 - acc: 0.6289 - val_loss: 0.8727 - val_acc: 0.3646\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6899 - acc: 0.6667 - val_loss: 0.8817 - val_acc: 0.3750\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6738 - acc: 0.6598 - val_loss: 0.8768 - val_acc: 0.3958\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6836 - acc: 0.6254 - val_loss: 0.8949 - val_acc: 0.3438\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7007 - acc: 0.6478 - val_loss: 0.8791 - val_acc: 0.3750\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7427 - acc: 0.6151 - val_loss: 0.8821 - val_acc: 0.3750\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6981 - acc: 0.6503 - val_loss: 0.8623 - val_acc: 0.3854\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6660 - acc: 0.6478 - val_loss: 0.8946 - val_acc: 0.3646\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6789 - acc: 0.6392 - val_loss: 0.8881 - val_acc: 0.3750\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6922 - acc: 0.6357 - val_loss: 0.8885 - val_acc: 0.3646\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6657 - acc: 0.6838 - val_loss: 0.8853 - val_acc: 0.3646\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6639 - acc: 0.6529 - val_loss: 0.8850 - val_acc: 0.3646\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6890 - acc: 0.6564 - val_loss: 0.8882 - val_acc: 0.3750\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.7198 - acc: 0.6375 - val_loss: 0.8907 - val_acc: 0.3646\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7085 - acc: 0.6529 - val_loss: 0.8733 - val_acc: 0.3854\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.7042 - acc: 0.6512 - val_loss: 0.8664 - val_acc: 0.3750\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6949 - acc: 0.6718 - val_loss: 0.8786 - val_acc: 0.3750\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.7374 - acc: 0.6151 - val_loss: 0.8822 - val_acc: 0.3854\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6485 - acc: 0.6718 - val_loss: 0.8921 - val_acc: 0.3646\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6918 - acc: 0.6460 - val_loss: 0.8973 - val_acc: 0.3750\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6888 - acc: 0.6392 - val_loss: 0.8815 - val_acc: 0.4062\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6691 - acc: 0.6735 - val_loss: 0.8981 - val_acc: 0.3646\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6462 - acc: 0.6787 - val_loss: 0.8652 - val_acc: 0.4062\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6194 - acc: 0.6684 - val_loss: 0.8788 - val_acc: 0.3750\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6444 - acc: 0.6632 - val_loss: 0.8811 - val_acc: 0.3854\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6747 - acc: 0.6667 - val_loss: 0.8881 - val_acc: 0.4062\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6674 - acc: 0.6375 - val_loss: 0.8836 - val_acc: 0.3646\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6372 - acc: 0.6735 - val_loss: 0.8768 - val_acc: 0.3854\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6574 - acc: 0.6409 - val_loss: 0.8583 - val_acc: 0.4062\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6378 - acc: 0.6546 - val_loss: 0.8782 - val_acc: 0.4062\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6435 - acc: 0.6873 - val_loss: 0.8570 - val_acc: 0.4167\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6399 - acc: 0.6907 - val_loss: 0.8785 - val_acc: 0.3854\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6476 - acc: 0.6787 - val_loss: 0.8800 - val_acc: 0.3958\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6575 - acc: 0.6718 - val_loss: 0.8804 - val_acc: 0.3750\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.6820 - acc: 0.6392 - val_loss: 0.8661 - val_acc: 0.3958\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6504 - acc: 0.6753 - val_loss: 0.8647 - val_acc: 0.3750\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6282 - acc: 0.6735 - val_loss: 0.8512 - val_acc: 0.3958\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6797 - acc: 0.6409 - val_loss: 0.8729 - val_acc: 0.3646\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6899 - acc: 0.6351 - val_loss: 0.8424 - val_acc: 0.4167\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6883 - acc: 0.6581 - val_loss: 0.8693 - val_acc: 0.3854\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6921 - acc: 0.6460 - val_loss: 0.8702 - val_acc: 0.3750\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6869 - acc: 0.6701 - val_loss: 0.8594 - val_acc: 0.3854\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6713 - acc: 0.6546 - val_loss: 0.8748 - val_acc: 0.3750\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6184 - acc: 0.6993 - val_loss: 0.8548 - val_acc: 0.3854\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6705 - acc: 0.6615 - val_loss: 0.8843 - val_acc: 0.3646\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6372 - acc: 0.6649 - val_loss: 0.8738 - val_acc: 0.3646\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6541 - acc: 0.6581 - val_loss: 0.8623 - val_acc: 0.3750\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7263 - acc: 0.6426 - val_loss: 0.8622 - val_acc: 0.3958\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6457 - acc: 0.6770 - val_loss: 0.8808 - val_acc: 0.3542\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6992 - acc: 0.6409 - val_loss: 0.8665 - val_acc: 0.4062\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6136 - acc: 0.6787 - val_loss: 0.8737 - val_acc: 0.3750\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6569 - acc: 0.6770 - val_loss: 0.8814 - val_acc: 0.3750\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6912 - acc: 0.6392 - val_loss: 0.8510 - val_acc: 0.3958\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6848 - acc: 0.6701 - val_loss: 0.8586 - val_acc: 0.4062\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6319 - acc: 0.6718 - val_loss: 0.8592 - val_acc: 0.3958\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6986 - acc: 0.6065 - val_loss: 0.8335 - val_acc: 0.4062\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6223 - acc: 0.6856 - val_loss: 0.8702 - val_acc: 0.4062\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6454 - acc: 0.6512 - val_loss: 0.8504 - val_acc: 0.4062\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6388 - acc: 0.6804 - val_loss: 0.8440 - val_acc: 0.4062\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6886 - acc: 0.6254 - val_loss: 0.8351 - val_acc: 0.4167\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6777 - acc: 0.6667 - val_loss: 0.8694 - val_acc: 0.3854\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6313 - acc: 0.6804 - val_loss: 0.8713 - val_acc: 0.3854\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6639 - acc: 0.6426 - val_loss: 0.8591 - val_acc: 0.4062\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.7031 - acc: 0.6323 - val_loss: 0.8751 - val_acc: 0.3958\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6188 - acc: 0.7199 - val_loss: 0.8588 - val_acc: 0.4062\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6914 - acc: 0.6478 - val_loss: 0.8575 - val_acc: 0.4062\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6401 - acc: 0.6564 - val_loss: 0.8503 - val_acc: 0.4062\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6148 - acc: 0.6804 - val_loss: 0.8589 - val_acc: 0.3958\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6467 - acc: 0.6460 - val_loss: 0.8681 - val_acc: 0.4062\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6635 - acc: 0.6512 - val_loss: 0.8795 - val_acc: 0.3750\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6616 - acc: 0.6649 - val_loss: 0.8616 - val_acc: 0.3958\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6724 - acc: 0.6529 - val_loss: 0.8728 - val_acc: 0.3854\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6383 - acc: 0.6907 - val_loss: 0.8574 - val_acc: 0.3958\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6249 - acc: 0.6976 - val_loss: 0.8556 - val_acc: 0.3854\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6997 - acc: 0.6409 - val_loss: 0.8758 - val_acc: 0.3750\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6460 - acc: 0.6667 - val_loss: 0.8635 - val_acc: 0.3958\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6614 - acc: 0.6495 - val_loss: 0.8536 - val_acc: 0.3854\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6324 - acc: 0.6804 - val_loss: 0.8494 - val_acc: 0.3958\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6465 - acc: 0.6735 - val_loss: 0.8496 - val_acc: 0.4062\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6227 - acc: 0.6667 - val_loss: 0.8638 - val_acc: 0.3958\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6915 - acc: 0.6495 - val_loss: 0.8458 - val_acc: 0.4062\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6484 - acc: 0.6495 - val_loss: 0.8637 - val_acc: 0.3958\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6387 - acc: 0.6821 - val_loss: 0.8545 - val_acc: 0.4062\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6318 - acc: 0.6735 - val_loss: 0.8616 - val_acc: 0.3958\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6233 - acc: 0.6838 - val_loss: 0.8406 - val_acc: 0.4167\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6349 - acc: 0.6787 - val_loss: 0.8349 - val_acc: 0.4167\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6653 - acc: 0.6564 - val_loss: 0.8590 - val_acc: 0.3958\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6351 - acc: 0.6770 - val_loss: 0.8439 - val_acc: 0.4271\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6768 - acc: 0.6392 - val_loss: 0.8460 - val_acc: 0.4167\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6208 - acc: 0.6873 - val_loss: 0.8436 - val_acc: 0.4062\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6039 - acc: 0.6959 - val_loss: 0.8543 - val_acc: 0.4062\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6330 - acc: 0.6667 - val_loss: 0.8534 - val_acc: 0.4062\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6568 - acc: 0.6460 - val_loss: 0.8386 - val_acc: 0.4271\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6660 - acc: 0.6753 - val_loss: 0.8552 - val_acc: 0.4271\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6679 - acc: 0.6787 - val_loss: 0.8542 - val_acc: 0.4271\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6249 - acc: 0.6856 - val_loss: 0.8519 - val_acc: 0.4167\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6446 - acc: 0.6632 - val_loss: 0.8450 - val_acc: 0.4167\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.6249 - acc: 0.6924 - val_loss: 0.8328 - val_acc: 0.4167\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6528 - acc: 0.6718 - val_loss: 0.8300 - val_acc: 0.4271\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5902 - acc: 0.7216 - val_loss: 0.8557 - val_acc: 0.4167\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6622 - acc: 0.6667 - val_loss: 0.8353 - val_acc: 0.4271\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6279 - acc: 0.6838 - val_loss: 0.8273 - val_acc: 0.4583\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6469 - acc: 0.6649 - val_loss: 0.8452 - val_acc: 0.4062\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6618 - acc: 0.6718 - val_loss: 0.8297 - val_acc: 0.4375\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6284 - acc: 0.6890 - val_loss: 0.8522 - val_acc: 0.3854\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5808 - acc: 0.6804 - val_loss: 0.8271 - val_acc: 0.4167\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6070 - acc: 0.6959 - val_loss: 0.8220 - val_acc: 0.4583\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6245 - acc: 0.6838 - val_loss: 0.8426 - val_acc: 0.4167\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6367 - acc: 0.6649 - val_loss: 0.8104 - val_acc: 0.4271\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6301 - acc: 0.6649 - val_loss: 0.8515 - val_acc: 0.4062\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6062 - acc: 0.6924 - val_loss: 0.8518 - val_acc: 0.3958\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6536 - acc: 0.6581 - val_loss: 0.8463 - val_acc: 0.4062\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6376 - acc: 0.6924 - val_loss: 0.8331 - val_acc: 0.4167\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6501 - acc: 0.6443 - val_loss: 0.8295 - val_acc: 0.4375\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6569 - acc: 0.6495 - val_loss: 0.8237 - val_acc: 0.4167\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6655 - acc: 0.6409 - val_loss: 0.8319 - val_acc: 0.4375\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6792 - acc: 0.6495 - val_loss: 0.8170 - val_acc: 0.4167\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6176 - acc: 0.6649 - val_loss: 0.8322 - val_acc: 0.4062\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6298 - acc: 0.6787 - val_loss: 0.8414 - val_acc: 0.4375\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5730 - acc: 0.7062 - val_loss: 0.8078 - val_acc: 0.4271\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6150 - acc: 0.6838 - val_loss: 0.8174 - val_acc: 0.4375\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6433 - acc: 0.6564 - val_loss: 0.8325 - val_acc: 0.4375\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6083 - acc: 0.6856 - val_loss: 0.8298 - val_acc: 0.4375\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6536 - acc: 0.6615 - val_loss: 0.8473 - val_acc: 0.3958\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6159 - acc: 0.6907 - val_loss: 0.8352 - val_acc: 0.4062\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6376 - acc: 0.6821 - val_loss: 0.8271 - val_acc: 0.4271\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6333 - acc: 0.6546 - val_loss: 0.8380 - val_acc: 0.4167\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6705 - acc: 0.6323 - val_loss: 0.8298 - val_acc: 0.4375\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6126 - acc: 0.6838 - val_loss: 0.8381 - val_acc: 0.4062\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6230 - acc: 0.6890 - val_loss: 0.8435 - val_acc: 0.3958\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5937 - acc: 0.6890 - val_loss: 0.8336 - val_acc: 0.3958\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6322 - acc: 0.6478 - val_loss: 0.8279 - val_acc: 0.4167\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5948 - acc: 0.6924 - val_loss: 0.8261 - val_acc: 0.4375\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6562 - acc: 0.6426 - val_loss: 0.8063 - val_acc: 0.4583\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6751 - acc: 0.6546 - val_loss: 0.8304 - val_acc: 0.4271\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6421 - acc: 0.6770 - val_loss: 0.8331 - val_acc: 0.4167\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.6495 - acc: 0.6667 - val_loss: 0.8478 - val_acc: 0.3854\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6203 - acc: 0.6684 - val_loss: 0.8306 - val_acc: 0.4167\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5913 - acc: 0.6924 - val_loss: 0.8320 - val_acc: 0.4375\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6269 - acc: 0.6838 - val_loss: 0.8204 - val_acc: 0.4271\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6249 - acc: 0.6787 - val_loss: 0.8314 - val_acc: 0.4062\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6335 - acc: 0.6632 - val_loss: 0.8138 - val_acc: 0.4375\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6417 - acc: 0.6512 - val_loss: 0.8294 - val_acc: 0.4271\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6488 - acc: 0.6959 - val_loss: 0.8113 - val_acc: 0.4479\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6355 - acc: 0.6632 - val_loss: 0.8199 - val_acc: 0.4271\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5933 - acc: 0.6821 - val_loss: 0.8479 - val_acc: 0.4167\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6642 - acc: 0.6735 - val_loss: 0.8143 - val_acc: 0.4375\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6371 - acc: 0.6838 - val_loss: 0.8347 - val_acc: 0.4271\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6269 - acc: 0.6632 - val_loss: 0.8460 - val_acc: 0.4167\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6261 - acc: 0.6873 - val_loss: 0.8222 - val_acc: 0.4375\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6372 - acc: 0.6632 - val_loss: 0.8047 - val_acc: 0.4583\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6069 - acc: 0.6873 - val_loss: 0.8203 - val_acc: 0.4167\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6540 - acc: 0.6770 - val_loss: 0.8241 - val_acc: 0.4375\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6520 - acc: 0.6581 - val_loss: 0.8135 - val_acc: 0.4479\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6195 - acc: 0.6873 - val_loss: 0.8253 - val_acc: 0.4479\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6308 - acc: 0.6537 - val_loss: 0.8151 - val_acc: 0.4479\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5970 - acc: 0.6890 - val_loss: 0.8296 - val_acc: 0.4375\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5862 - acc: 0.6959 - val_loss: 0.8335 - val_acc: 0.4375\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5870 - acc: 0.7079 - val_loss: 0.8276 - val_acc: 0.4479\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.6173 - acc: 0.6821 - val_loss: 0.8118 - val_acc: 0.4479\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.7087 - acc: 0.6357 - val_loss: 0.8177 - val_acc: 0.4375\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6558 - acc: 0.6770 - val_loss: 0.8307 - val_acc: 0.4062\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6331 - acc: 0.6684 - val_loss: 0.8222 - val_acc: 0.4375\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6554 - acc: 0.6495 - val_loss: 0.8230 - val_acc: 0.4583\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5898 - acc: 0.7062 - val_loss: 0.8264 - val_acc: 0.4375\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5973 - acc: 0.6804 - val_loss: 0.8211 - val_acc: 0.4583\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6040 - acc: 0.6701 - val_loss: 0.8177 - val_acc: 0.4271\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5678 - acc: 0.7182 - val_loss: 0.8276 - val_acc: 0.4375\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6265 - acc: 0.6770 - val_loss: 0.8113 - val_acc: 0.4479\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6469 - acc: 0.6804 - val_loss: 0.8220 - val_acc: 0.4375\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6197 - acc: 0.6821 - val_loss: 0.8201 - val_acc: 0.4375\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5908 - acc: 0.7010 - val_loss: 0.8077 - val_acc: 0.4479\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5709 - acc: 0.7216 - val_loss: 0.8153 - val_acc: 0.4479\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5777 - acc: 0.6787 - val_loss: 0.8331 - val_acc: 0.4271\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6260 - acc: 0.6718 - val_loss: 0.8209 - val_acc: 0.4375\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6160 - acc: 0.6942 - val_loss: 0.8240 - val_acc: 0.4271\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6315 - acc: 0.6770 - val_loss: 0.8000 - val_acc: 0.4583\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6421 - acc: 0.6804 - val_loss: 0.8131 - val_acc: 0.4479\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5744 - acc: 0.6821 - val_loss: 0.8199 - val_acc: 0.4167\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5649 - acc: 0.7096 - val_loss: 0.7851 - val_acc: 0.4479\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6335 - acc: 0.6770 - val_loss: 0.8175 - val_acc: 0.4375\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5869 - acc: 0.6838 - val_loss: 0.8099 - val_acc: 0.4479\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6301 - acc: 0.6443 - val_loss: 0.8065 - val_acc: 0.4375\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5885 - acc: 0.6856 - val_loss: 0.8152 - val_acc: 0.4271\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6273 - acc: 0.6667 - val_loss: 0.7934 - val_acc: 0.4479\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6250 - acc: 0.6684 - val_loss: 0.7829 - val_acc: 0.4479\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6279 - acc: 0.6701 - val_loss: 0.8051 - val_acc: 0.4271\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6082 - acc: 0.6701 - val_loss: 0.8123 - val_acc: 0.4375\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6110 - acc: 0.6632 - val_loss: 0.7874 - val_acc: 0.4271\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6109 - acc: 0.6581 - val_loss: 0.8114 - val_acc: 0.4271\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6389 - acc: 0.6873 - val_loss: 0.8052 - val_acc: 0.4583\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6227 - acc: 0.6821 - val_loss: 0.7980 - val_acc: 0.4375\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6079 - acc: 0.6787 - val_loss: 0.8055 - val_acc: 0.4167\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6131 - acc: 0.6774 - val_loss: 0.8200 - val_acc: 0.4375\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6027 - acc: 0.6890 - val_loss: 0.8064 - val_acc: 0.4375\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6201 - acc: 0.6649 - val_loss: 0.7999 - val_acc: 0.4375\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6191 - acc: 0.6718 - val_loss: 0.8004 - val_acc: 0.4479\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5844 - acc: 0.7027 - val_loss: 0.8115 - val_acc: 0.4271\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5808 - acc: 0.6804 - val_loss: 0.7989 - val_acc: 0.4688\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5648 - acc: 0.6907 - val_loss: 0.8105 - val_acc: 0.4271\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6216 - acc: 0.6959 - val_loss: 0.8015 - val_acc: 0.4375\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6188 - acc: 0.6804 - val_loss: 0.8069 - val_acc: 0.4375\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6200 - acc: 0.6804 - val_loss: 0.7926 - val_acc: 0.4896\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6350 - acc: 0.6753 - val_loss: 0.8040 - val_acc: 0.4375\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6038 - acc: 0.6821 - val_loss: 0.7854 - val_acc: 0.4583\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5830 - acc: 0.6821 - val_loss: 0.7978 - val_acc: 0.4583\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6327 - acc: 0.6701 - val_loss: 0.8019 - val_acc: 0.4271\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6119 - acc: 0.6976 - val_loss: 0.8118 - val_acc: 0.4271\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6251 - acc: 0.6598 - val_loss: 0.7868 - val_acc: 0.4479\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6632 - acc: 0.6615 - val_loss: 0.7944 - val_acc: 0.4375\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6220 - acc: 0.6873 - val_loss: 0.7867 - val_acc: 0.4583\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6172 - acc: 0.6873 - val_loss: 0.7942 - val_acc: 0.4583\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5817 - acc: 0.6890 - val_loss: 0.7882 - val_acc: 0.4375\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5858 - acc: 0.7095 - val_loss: 0.7688 - val_acc: 0.4583\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6331 - acc: 0.6478 - val_loss: 0.7989 - val_acc: 0.4792\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5959 - acc: 0.7010 - val_loss: 0.7983 - val_acc: 0.4271\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6553 - acc: 0.6546 - val_loss: 0.7903 - val_acc: 0.4583\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6107 - acc: 0.6787 - val_loss: 0.7846 - val_acc: 0.4688\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6207 - acc: 0.6924 - val_loss: 0.7988 - val_acc: 0.4271\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6083 - acc: 0.6821 - val_loss: 0.7997 - val_acc: 0.4271\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6387 - acc: 0.6667 - val_loss: 0.7846 - val_acc: 0.4375\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.6027 - acc: 0.6753 - val_loss: 0.7929 - val_acc: 0.4271\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5530 - acc: 0.7182 - val_loss: 0.7949 - val_acc: 0.4271\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6127 - acc: 0.7062 - val_loss: 0.8107 - val_acc: 0.4375\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5869 - acc: 0.7010 - val_loss: 0.8113 - val_acc: 0.4375\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6089 - acc: 0.6873 - val_loss: 0.7879 - val_acc: 0.4583\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6160 - acc: 0.6740 - val_loss: 0.7978 - val_acc: 0.4271\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6297 - acc: 0.6701 - val_loss: 0.8015 - val_acc: 0.4375\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5922 - acc: 0.6924 - val_loss: 0.7899 - val_acc: 0.4583\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6217 - acc: 0.6684 - val_loss: 0.8038 - val_acc: 0.4479\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5908 - acc: 0.7096 - val_loss: 0.7821 - val_acc: 0.4688\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6017 - acc: 0.7010 - val_loss: 0.7939 - val_acc: 0.4479\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6197 - acc: 0.6924 - val_loss: 0.7988 - val_acc: 0.4375\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6086 - acc: 0.6838 - val_loss: 0.8091 - val_acc: 0.4375\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5950 - acc: 0.6718 - val_loss: 0.7914 - val_acc: 0.4375\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6103 - acc: 0.6993 - val_loss: 0.8041 - val_acc: 0.4167\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5709 - acc: 0.7320 - val_loss: 0.7994 - val_acc: 0.4479\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6236 - acc: 0.6735 - val_loss: 0.7813 - val_acc: 0.4583\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5445 - acc: 0.7234 - val_loss: 0.7839 - val_acc: 0.4479\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5988 - acc: 0.7113 - val_loss: 0.8040 - val_acc: 0.4167\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5708 - acc: 0.7131 - val_loss: 0.8115 - val_acc: 0.3958\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5969 - acc: 0.6838 - val_loss: 0.7966 - val_acc: 0.4167\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5696 - acc: 0.7354 - val_loss: 0.7943 - val_acc: 0.4375\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5716 - acc: 0.7320 - val_loss: 0.7911 - val_acc: 0.4375\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6209 - acc: 0.6529 - val_loss: 0.7959 - val_acc: 0.4271\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5862 - acc: 0.6942 - val_loss: 0.7848 - val_acc: 0.4479\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6050 - acc: 0.6875 - val_loss: 0.7937 - val_acc: 0.4375\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6052 - acc: 0.6890 - val_loss: 0.7907 - val_acc: 0.4375\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5958 - acc: 0.6856 - val_loss: 0.7810 - val_acc: 0.4375\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5614 - acc: 0.7216 - val_loss: 0.7691 - val_acc: 0.4583\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6116 - acc: 0.6907 - val_loss: 0.7758 - val_acc: 0.4688\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6004 - acc: 0.6856 - val_loss: 0.7835 - val_acc: 0.4271\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6176 - acc: 0.6632 - val_loss: 0.7970 - val_acc: 0.4375\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.5933 - acc: 0.7079 - val_loss: 0.7731 - val_acc: 0.4375\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6011 - acc: 0.6976 - val_loss: 0.7539 - val_acc: 0.4792\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5923 - acc: 0.7113 - val_loss: 0.7822 - val_acc: 0.4479\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5989 - acc: 0.6942 - val_loss: 0.7683 - val_acc: 0.4375\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5831 - acc: 0.7079 - val_loss: 0.7929 - val_acc: 0.4479\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5927 - acc: 0.6873 - val_loss: 0.7614 - val_acc: 0.4479\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.6185 - acc: 0.6787 - val_loss: 0.7810 - val_acc: 0.4375\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5967 - acc: 0.6856 - val_loss: 0.7850 - val_acc: 0.4167\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5851 - acc: 0.6804 - val_loss: 0.7852 - val_acc: 0.5208\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6070 - acc: 0.6907 - val_loss: 0.7893 - val_acc: 0.4167\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5815 - acc: 0.6821 - val_loss: 0.7819 - val_acc: 0.4271\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5671 - acc: 0.7079 - val_loss: 0.7838 - val_acc: 0.4271\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6037 - acc: 0.6821 - val_loss: 0.7739 - val_acc: 0.4479\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5940 - acc: 0.6718 - val_loss: 0.7862 - val_acc: 0.4583\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5806 - acc: 0.7045 - val_loss: 0.7776 - val_acc: 0.4479\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6309 - acc: 0.6581 - val_loss: 0.7739 - val_acc: 0.4479\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5864 - acc: 0.7131 - val_loss: 0.7738 - val_acc: 0.4375\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6005 - acc: 0.6787 - val_loss: 0.7678 - val_acc: 0.4479\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5864 - acc: 0.7027 - val_loss: 0.7703 - val_acc: 0.4062\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5924 - acc: 0.7182 - val_loss: 0.7789 - val_acc: 0.4583\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6122 - acc: 0.6873 - val_loss: 0.7825 - val_acc: 0.4167\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5784 - acc: 0.7096 - val_loss: 0.7621 - val_acc: 0.4375\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6129 - acc: 0.6875 - val_loss: 0.7709 - val_acc: 0.4583\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6231 - acc: 0.6684 - val_loss: 0.7620 - val_acc: 0.4688\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5669 - acc: 0.7199 - val_loss: 0.7561 - val_acc: 0.4479\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5933 - acc: 0.6873 - val_loss: 0.7827 - val_acc: 0.4271\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5947 - acc: 0.7045 - val_loss: 0.7774 - val_acc: 0.4896\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5877 - acc: 0.6959 - val_loss: 0.7751 - val_acc: 0.4479\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5658 - acc: 0.7045 - val_loss: 0.7672 - val_acc: 0.4479\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.5902 - acc: 0.6718 - val_loss: 0.7907 - val_acc: 0.4271\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6090 - acc: 0.7113 - val_loss: 0.7737 - val_acc: 0.5000\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5831 - acc: 0.6924 - val_loss: 0.7827 - val_acc: 0.4167\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5856 - acc: 0.7079 - val_loss: 0.7645 - val_acc: 0.4583\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5805 - acc: 0.6993 - val_loss: 0.7782 - val_acc: 0.4583\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5760 - acc: 0.6976 - val_loss: 0.7863 - val_acc: 0.4792\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5562 - acc: 0.7045 - val_loss: 0.7737 - val_acc: 0.5104\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5667 - acc: 0.7010 - val_loss: 0.7852 - val_acc: 0.4167\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5996 - acc: 0.6890 - val_loss: 0.7648 - val_acc: 0.4583\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5984 - acc: 0.6615 - val_loss: 0.7476 - val_acc: 0.4792\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5886 - acc: 0.6976 - val_loss: 0.7734 - val_acc: 0.4583\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5831 - acc: 0.7027 - val_loss: 0.7641 - val_acc: 0.4583\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5710 - acc: 0.7079 - val_loss: 0.7618 - val_acc: 0.4375\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5788 - acc: 0.7131 - val_loss: 0.7602 - val_acc: 0.4479\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5983 - acc: 0.6959 - val_loss: 0.7698 - val_acc: 0.5000\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5901 - acc: 0.7010 - val_loss: 0.7773 - val_acc: 0.4375\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5606 - acc: 0.7027 - val_loss: 0.7608 - val_acc: 0.4479\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5716 - acc: 0.6907 - val_loss: 0.7693 - val_acc: 0.4583\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6158 - acc: 0.6873 - val_loss: 0.7687 - val_acc: 0.4583\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5815 - acc: 0.7010 - val_loss: 0.7723 - val_acc: 0.4688\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5909 - acc: 0.6753 - val_loss: 0.7680 - val_acc: 0.4479\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5365 - acc: 0.7234 - val_loss: 0.7788 - val_acc: 0.4688\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5559 - acc: 0.7131 - val_loss: 0.7651 - val_acc: 0.4271\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5792 - acc: 0.7182 - val_loss: 0.7711 - val_acc: 0.4271\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5452 - acc: 0.7320 - val_loss: 0.7605 - val_acc: 0.4688\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5489 - acc: 0.7251 - val_loss: 0.7734 - val_acc: 0.4375\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5710 - acc: 0.7062 - val_loss: 0.7469 - val_acc: 0.4479\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5818 - acc: 0.6924 - val_loss: 0.7607 - val_acc: 0.4583\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5797 - acc: 0.6907 - val_loss: 0.7606 - val_acc: 0.4479\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6238 - acc: 0.6753 - val_loss: 0.7776 - val_acc: 0.4375\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5523 - acc: 0.7388 - val_loss: 0.7674 - val_acc: 0.4479\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6091 - acc: 0.6976 - val_loss: 0.7626 - val_acc: 0.4583\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5994 - acc: 0.6821 - val_loss: 0.7750 - val_acc: 0.4479\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5536 - acc: 0.6942 - val_loss: 0.7674 - val_acc: 0.4583\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5526 - acc: 0.7079 - val_loss: 0.7793 - val_acc: 0.4479\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5761 - acc: 0.6993 - val_loss: 0.7720 - val_acc: 0.4688\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5729 - acc: 0.7251 - val_loss: 0.7749 - val_acc: 0.4479\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5844 - acc: 0.7096 - val_loss: 0.7756 - val_acc: 0.4479\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5691 - acc: 0.7079 - val_loss: 0.7826 - val_acc: 0.4271\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5813 - acc: 0.7045 - val_loss: 0.7553 - val_acc: 0.4688\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5818 - acc: 0.7131 - val_loss: 0.7778 - val_acc: 0.4583\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6225 - acc: 0.6924 - val_loss: 0.7752 - val_acc: 0.4271\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5806 - acc: 0.6959 - val_loss: 0.7648 - val_acc: 0.4479\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5366 - acc: 0.7405 - val_loss: 0.7715 - val_acc: 0.4479\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.6153 - acc: 0.6770 - val_loss: 0.7574 - val_acc: 0.4896\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5886 - acc: 0.6838 - val_loss: 0.7724 - val_acc: 0.4688\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5403 - acc: 0.7491 - val_loss: 0.7669 - val_acc: 0.4688\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5718 - acc: 0.7096 - val_loss: 0.7542 - val_acc: 0.4583\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5604 - acc: 0.7079 - val_loss: 0.7757 - val_acc: 0.4583\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5679 - acc: 0.7045 - val_loss: 0.7651 - val_acc: 0.4583\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5364 - acc: 0.7162 - val_loss: 0.7582 - val_acc: 0.4583\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5636 - acc: 0.7285 - val_loss: 0.7748 - val_acc: 0.4583\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5626 - acc: 0.7148 - val_loss: 0.7718 - val_acc: 0.4479\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5857 - acc: 0.6993 - val_loss: 0.7757 - val_acc: 0.4583\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5602 - acc: 0.7320 - val_loss: 0.7728 - val_acc: 0.4583\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5522 - acc: 0.7251 - val_loss: 0.7780 - val_acc: 0.4271\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5906 - acc: 0.6907 - val_loss: 0.7723 - val_acc: 0.4271\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6091 - acc: 0.6632 - val_loss: 0.7806 - val_acc: 0.4167\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5754 - acc: 0.6821 - val_loss: 0.7610 - val_acc: 0.4479\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5630 - acc: 0.7148 - val_loss: 0.7657 - val_acc: 0.4479\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5656 - acc: 0.7113 - val_loss: 0.7656 - val_acc: 0.4688\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.5621 - acc: 0.7199 - val_loss: 0.7785 - val_acc: 0.4271\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5546 - acc: 0.7165 - val_loss: 0.7802 - val_acc: 0.4375\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5605 - acc: 0.7285 - val_loss: 0.7792 - val_acc: 0.4375\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5511 - acc: 0.7423 - val_loss: 0.7574 - val_acc: 0.4688\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5557 - acc: 0.7079 - val_loss: 0.7780 - val_acc: 0.4479\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5979 - acc: 0.6770 - val_loss: 0.7764 - val_acc: 0.4271\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6429 - acc: 0.6615 - val_loss: 0.7514 - val_acc: 0.4688\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5766 - acc: 0.7096 - val_loss: 0.7565 - val_acc: 0.5000\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6174 - acc: 0.6804 - val_loss: 0.7667 - val_acc: 0.4583\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.6219 - acc: 0.6787 - val_loss: 0.7641 - val_acc: 0.4479\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5823 - acc: 0.7199 - val_loss: 0.7564 - val_acc: 0.4583\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5942 - acc: 0.6873 - val_loss: 0.7540 - val_acc: 0.5208\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5812 - acc: 0.6890 - val_loss: 0.7713 - val_acc: 0.4896\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5494 - acc: 0.7199 - val_loss: 0.7558 - val_acc: 0.4896\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5684 - acc: 0.7165 - val_loss: 0.7738 - val_acc: 0.5208\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5952 - acc: 0.6959 - val_loss: 0.7692 - val_acc: 0.4896\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5783 - acc: 0.6924 - val_loss: 0.7511 - val_acc: 0.5000\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5654 - acc: 0.7182 - val_loss: 0.7580 - val_acc: 0.5312\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5929 - acc: 0.6924 - val_loss: 0.7567 - val_acc: 0.4583\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5448 - acc: 0.7131 - val_loss: 0.7652 - val_acc: 0.4375\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.6001 - acc: 0.6838 - val_loss: 0.7707 - val_acc: 0.4479\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5897 - acc: 0.6890 - val_loss: 0.7726 - val_acc: 0.4896\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5691 - acc: 0.7045 - val_loss: 0.7574 - val_acc: 0.5104\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5941 - acc: 0.7045 - val_loss: 0.7711 - val_acc: 0.4583\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5802 - acc: 0.6959 - val_loss: 0.7787 - val_acc: 0.4896\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5703 - acc: 0.6993 - val_loss: 0.7624 - val_acc: 0.4792\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5830 - acc: 0.7045 - val_loss: 0.7699 - val_acc: 0.4792\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5734 - acc: 0.7079 - val_loss: 0.7615 - val_acc: 0.5000\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5444 - acc: 0.7251 - val_loss: 0.7668 - val_acc: 0.5208\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5708 - acc: 0.7302 - val_loss: 0.7630 - val_acc: 0.5000\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5567 - acc: 0.7044 - val_loss: 0.7543 - val_acc: 0.5000\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5997 - acc: 0.6804 - val_loss: 0.7668 - val_acc: 0.5104\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5716 - acc: 0.6907 - val_loss: 0.7673 - val_acc: 0.5000\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5419 - acc: 0.7199 - val_loss: 0.7456 - val_acc: 0.5312\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5815 - acc: 0.6873 - val_loss: 0.7530 - val_acc: 0.4688\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5967 - acc: 0.6838 - val_loss: 0.7433 - val_acc: 0.4792\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.5429 - acc: 0.7199 - val_loss: 0.7591 - val_acc: 0.4688\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5616 - acc: 0.7162 - val_loss: 0.7607 - val_acc: 0.4583\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5512 - acc: 0.7079 - val_loss: 0.7409 - val_acc: 0.4583\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5995 - acc: 0.6821 - val_loss: 0.7582 - val_acc: 0.4896\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5749 - acc: 0.7148 - val_loss: 0.7433 - val_acc: 0.5000\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6035 - acc: 0.6804 - val_loss: 0.7543 - val_acc: 0.4375\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.6121 - acc: 0.6787 - val_loss: 0.7464 - val_acc: 0.4583\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5683 - acc: 0.7131 - val_loss: 0.7535 - val_acc: 0.4479\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5540 - acc: 0.7148 - val_loss: 0.7380 - val_acc: 0.5104\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5588 - acc: 0.6907 - val_loss: 0.7367 - val_acc: 0.5104\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5669 - acc: 0.6907 - val_loss: 0.7456 - val_acc: 0.5000\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5703 - acc: 0.7199 - val_loss: 0.7509 - val_acc: 0.5104\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5732 - acc: 0.7079 - val_loss: 0.7452 - val_acc: 0.4688\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5413 - acc: 0.7234 - val_loss: 0.7580 - val_acc: 0.4792\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5753 - acc: 0.6873 - val_loss: 0.7451 - val_acc: 0.4792\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5666 - acc: 0.7131 - val_loss: 0.7253 - val_acc: 0.5208\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5786 - acc: 0.7405 - val_loss: 0.7434 - val_acc: 0.4792\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5548 - acc: 0.6942 - val_loss: 0.7239 - val_acc: 0.5104\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5648 - acc: 0.7045 - val_loss: 0.7395 - val_acc: 0.5208\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5454 - acc: 0.6976 - val_loss: 0.7289 - val_acc: 0.5521\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5307 - acc: 0.7199 - val_loss: 0.7474 - val_acc: 0.5000\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5764 - acc: 0.6976 - val_loss: 0.7364 - val_acc: 0.4896\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5301 - acc: 0.7595 - val_loss: 0.7410 - val_acc: 0.4792\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5899 - acc: 0.6976 - val_loss: 0.7445 - val_acc: 0.4896\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.5388 - acc: 0.7423 - val_loss: 0.7373 - val_acc: 0.4688\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5371 - acc: 0.7131 - val_loss: 0.7251 - val_acc: 0.4792\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5642 - acc: 0.7010 - val_loss: 0.7376 - val_acc: 0.4896\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5346 - acc: 0.7440 - val_loss: 0.7405 - val_acc: 0.5208\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5721 - acc: 0.6976 - val_loss: 0.7304 - val_acc: 0.4688\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5419 - acc: 0.7165 - val_loss: 0.7493 - val_acc: 0.5000\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5613 - acc: 0.7285 - val_loss: 0.7360 - val_acc: 0.5312\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5733 - acc: 0.7113 - val_loss: 0.7642 - val_acc: 0.4271\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5663 - acc: 0.7216 - val_loss: 0.7590 - val_acc: 0.4896\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.6439 - acc: 0.6667 - val_loss: 0.7517 - val_acc: 0.4688\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5297 - acc: 0.7165 - val_loss: 0.7355 - val_acc: 0.5208\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5677 - acc: 0.6976 - val_loss: 0.7481 - val_acc: 0.4583\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5296 - acc: 0.7251 - val_loss: 0.7544 - val_acc: 0.5000\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5836 - acc: 0.7027 - val_loss: 0.7570 - val_acc: 0.4792\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5788 - acc: 0.7079 - val_loss: 0.7540 - val_acc: 0.5312\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5705 - acc: 0.7027 - val_loss: 0.7598 - val_acc: 0.5104\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5764 - acc: 0.6907 - val_loss: 0.7578 - val_acc: 0.5208\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5429 - acc: 0.7213 - val_loss: 0.7314 - val_acc: 0.5000\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5460 - acc: 0.7320 - val_loss: 0.7411 - val_acc: 0.5208\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5810 - acc: 0.6993 - val_loss: 0.7395 - val_acc: 0.4688\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5338 - acc: 0.7251 - val_loss: 0.7520 - val_acc: 0.4792\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5964 - acc: 0.6924 - val_loss: 0.7331 - val_acc: 0.4792\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5266 - acc: 0.7337 - val_loss: 0.7436 - val_acc: 0.5104\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.6037 - acc: 0.6892 - val_loss: 0.7521 - val_acc: 0.4792\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5491 - acc: 0.7268 - val_loss: 0.7311 - val_acc: 0.4896\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5720 - acc: 0.7027 - val_loss: 0.7455 - val_acc: 0.4792\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5616 - acc: 0.7199 - val_loss: 0.7446 - val_acc: 0.5208\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5854 - acc: 0.6976 - val_loss: 0.7545 - val_acc: 0.5312\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5863 - acc: 0.6838 - val_loss: 0.7369 - val_acc: 0.5312\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5705 - acc: 0.6890 - val_loss: 0.7384 - val_acc: 0.4583\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5985 - acc: 0.6907 - val_loss: 0.7268 - val_acc: 0.5000\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5380 - acc: 0.7371 - val_loss: 0.7252 - val_acc: 0.5000\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5498 - acc: 0.7354 - val_loss: 0.7325 - val_acc: 0.4792\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5525 - acc: 0.7388 - val_loss: 0.7538 - val_acc: 0.4688\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5611 - acc: 0.7182 - val_loss: 0.7501 - val_acc: 0.4583\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5536 - acc: 0.7131 - val_loss: 0.7360 - val_acc: 0.5104\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5611 - acc: 0.7251 - val_loss: 0.7525 - val_acc: 0.5312\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5328 - acc: 0.7371 - val_loss: 0.7560 - val_acc: 0.5312\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5581 - acc: 0.7131 - val_loss: 0.7440 - val_acc: 0.5208\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5426 - acc: 0.7216 - val_loss: 0.7270 - val_acc: 0.5000\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5680 - acc: 0.7165 - val_loss: 0.7513 - val_acc: 0.4792\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5519 - acc: 0.7199 - val_loss: 0.7458 - val_acc: 0.5417\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5480 - acc: 0.7148 - val_loss: 0.7382 - val_acc: 0.5312\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5298 - acc: 0.7354 - val_loss: 0.7385 - val_acc: 0.5000\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5686 - acc: 0.6787 - val_loss: 0.7351 - val_acc: 0.5417\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5752 - acc: 0.6976 - val_loss: 0.7370 - val_acc: 0.5417\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5718 - acc: 0.6787 - val_loss: 0.7285 - val_acc: 0.5729\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5840 - acc: 0.7062 - val_loss: 0.7210 - val_acc: 0.5000\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5406 - acc: 0.7199 - val_loss: 0.7507 - val_acc: 0.4792\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5643 - acc: 0.7079 - val_loss: 0.7472 - val_acc: 0.5312\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5492 - acc: 0.7165 - val_loss: 0.7294 - val_acc: 0.5625\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5686 - acc: 0.7096 - val_loss: 0.7363 - val_acc: 0.4896\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5644 - acc: 0.7062 - val_loss: 0.7302 - val_acc: 0.5521\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5546 - acc: 0.7199 - val_loss: 0.7280 - val_acc: 0.5521\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5544 - acc: 0.7234 - val_loss: 0.7318 - val_acc: 0.5521\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5580 - acc: 0.7079 - val_loss: 0.7297 - val_acc: 0.4792\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5716 - acc: 0.7131 - val_loss: 0.7453 - val_acc: 0.4688\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5550 - acc: 0.7113 - val_loss: 0.7316 - val_acc: 0.4896\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5582 - acc: 0.7027 - val_loss: 0.7304 - val_acc: 0.5000\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5667 - acc: 0.7182 - val_loss: 0.7458 - val_acc: 0.4583\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5572 - acc: 0.7062 - val_loss: 0.7283 - val_acc: 0.5208\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.5469 - acc: 0.7268 - val_loss: 0.7180 - val_acc: 0.4896\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5641 - acc: 0.7131 - val_loss: 0.7464 - val_acc: 0.4792\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5321 - acc: 0.7216 - val_loss: 0.7418 - val_acc: 0.4896\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5860 - acc: 0.6942 - val_loss: 0.7438 - val_acc: 0.4792\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5233 - acc: 0.7405 - val_loss: 0.7416 - val_acc: 0.4792\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5456 - acc: 0.7199 - val_loss: 0.7479 - val_acc: 0.4792\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5709 - acc: 0.7027 - val_loss: 0.7396 - val_acc: 0.4896\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5550 - acc: 0.7165 - val_loss: 0.7289 - val_acc: 0.5000\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5715 - acc: 0.6924 - val_loss: 0.7430 - val_acc: 0.4896\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5776 - acc: 0.7079 - val_loss: 0.7359 - val_acc: 0.4896\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5455 - acc: 0.7285 - val_loss: 0.7348 - val_acc: 0.5208\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5789 - acc: 0.7096 - val_loss: 0.7360 - val_acc: 0.4896\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5878 - acc: 0.6907 - val_loss: 0.7445 - val_acc: 0.4896\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5245 - acc: 0.7302 - val_loss: 0.7395 - val_acc: 0.5000\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5386 - acc: 0.7423 - val_loss: 0.7389 - val_acc: 0.5417\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5586 - acc: 0.7079 - val_loss: 0.7287 - val_acc: 0.5000\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5511 - acc: 0.7148 - val_loss: 0.7384 - val_acc: 0.5521\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5570 - acc: 0.7268 - val_loss: 0.7537 - val_acc: 0.4896\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5297 - acc: 0.7337 - val_loss: 0.7397 - val_acc: 0.5104\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5914 - acc: 0.6873 - val_loss: 0.7501 - val_acc: 0.5521\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5429 - acc: 0.7337 - val_loss: 0.7507 - val_acc: 0.5312\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5392 - acc: 0.7268 - val_loss: 0.7381 - val_acc: 0.5208\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5824 - acc: 0.7079 - val_loss: 0.7297 - val_acc: 0.4896\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5498 - acc: 0.7182 - val_loss: 0.7517 - val_acc: 0.4583\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5945 - acc: 0.7010 - val_loss: 0.7283 - val_acc: 0.5208\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5805 - acc: 0.7079 - val_loss: 0.7408 - val_acc: 0.4792\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5415 - acc: 0.7182 - val_loss: 0.7255 - val_acc: 0.5312\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5508 - acc: 0.7079 - val_loss: 0.7346 - val_acc: 0.5521\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5533 - acc: 0.7010 - val_loss: 0.7374 - val_acc: 0.5417\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5321 - acc: 0.7320 - val_loss: 0.7379 - val_acc: 0.5521\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5779 - acc: 0.6959 - val_loss: 0.7480 - val_acc: 0.5521\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5498 - acc: 0.7165 - val_loss: 0.7459 - val_acc: 0.5208\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5777 - acc: 0.7131 - val_loss: 0.7307 - val_acc: 0.5417\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5521 - acc: 0.7148 - val_loss: 0.7367 - val_acc: 0.5312\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5533 - acc: 0.7216 - val_loss: 0.7223 - val_acc: 0.5521\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5780 - acc: 0.7113 - val_loss: 0.7471 - val_acc: 0.4896\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5338 - acc: 0.7337 - val_loss: 0.7296 - val_acc: 0.5312\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5704 - acc: 0.6993 - val_loss: 0.7204 - val_acc: 0.5312\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5465 - acc: 0.7371 - val_loss: 0.7365 - val_acc: 0.5208\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5532 - acc: 0.7148 - val_loss: 0.7370 - val_acc: 0.5000\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5828 - acc: 0.6907 - val_loss: 0.7268 - val_acc: 0.5104\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5657 - acc: 0.7062 - val_loss: 0.7372 - val_acc: 0.4896\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5779 - acc: 0.6959 - val_loss: 0.7392 - val_acc: 0.5000\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5823 - acc: 0.6907 - val_loss: 0.7419 - val_acc: 0.5104\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5346 - acc: 0.7268 - val_loss: 0.7290 - val_acc: 0.4896\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5351 - acc: 0.7320 - val_loss: 0.7294 - val_acc: 0.4792\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5819 - acc: 0.6873 - val_loss: 0.7233 - val_acc: 0.4792\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5822 - acc: 0.7234 - val_loss: 0.7316 - val_acc: 0.4792\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5653 - acc: 0.7199 - val_loss: 0.7345 - val_acc: 0.5000\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5575 - acc: 0.7268 - val_loss: 0.7337 - val_acc: 0.5208\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5545 - acc: 0.7354 - val_loss: 0.7355 - val_acc: 0.4583\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5788 - acc: 0.7045 - val_loss: 0.7481 - val_acc: 0.4583\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5868 - acc: 0.7182 - val_loss: 0.7395 - val_acc: 0.5104\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5438 - acc: 0.7285 - val_loss: 0.7614 - val_acc: 0.5000\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5220 - acc: 0.7457 - val_loss: 0.7335 - val_acc: 0.5208\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5585 - acc: 0.7062 - val_loss: 0.7330 - val_acc: 0.5208\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5528 - acc: 0.7234 - val_loss: 0.7075 - val_acc: 0.5417\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5630 - acc: 0.6976 - val_loss: 0.7378 - val_acc: 0.5104\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5386 - acc: 0.7216 - val_loss: 0.7191 - val_acc: 0.5312\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.5389 - acc: 0.7371 - val_loss: 0.7283 - val_acc: 0.5208\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.5545 - acc: 0.7027 - val_loss: 0.7250 - val_acc: 0.5208\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5193 - acc: 0.7320 - val_loss: 0.7274 - val_acc: 0.4896\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5457 - acc: 0.7131 - val_loss: 0.7266 - val_acc: 0.5312\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5485 - acc: 0.6993 - val_loss: 0.7351 - val_acc: 0.5312\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5472 - acc: 0.7268 - val_loss: 0.7281 - val_acc: 0.5104\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5123 - acc: 0.7405 - val_loss: 0.7311 - val_acc: 0.5104\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5303 - acc: 0.7440 - val_loss: 0.7133 - val_acc: 0.5312\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5569 - acc: 0.7010 - val_loss: 0.7147 - val_acc: 0.5104\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5505 - acc: 0.7216 - val_loss: 0.7320 - val_acc: 0.5208\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5673 - acc: 0.6976 - val_loss: 0.7172 - val_acc: 0.5417\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5470 - acc: 0.7285 - val_loss: 0.7185 - val_acc: 0.5312\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5408 - acc: 0.7131 - val_loss: 0.7455 - val_acc: 0.5104\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5310 - acc: 0.7285 - val_loss: 0.7171 - val_acc: 0.5521\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5741 - acc: 0.6959 - val_loss: 0.7166 - val_acc: 0.5417\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5838 - acc: 0.7062 - val_loss: 0.7404 - val_acc: 0.5104\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5215 - acc: 0.7234 - val_loss: 0.7383 - val_acc: 0.5208\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5379 - acc: 0.7440 - val_loss: 0.7375 - val_acc: 0.5208\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5444 - acc: 0.6976 - val_loss: 0.7308 - val_acc: 0.5312\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5305 - acc: 0.7354 - val_loss: 0.7198 - val_acc: 0.5312\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5277 - acc: 0.7337 - val_loss: 0.7314 - val_acc: 0.5312\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5574 - acc: 0.7251 - val_loss: 0.7176 - val_acc: 0.5417\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5309 - acc: 0.7234 - val_loss: 0.7245 - val_acc: 0.4792\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5474 - acc: 0.7148 - val_loss: 0.7297 - val_acc: 0.5521\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5161 - acc: 0.7509 - val_loss: 0.7385 - val_acc: 0.5208\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5501 - acc: 0.7199 - val_loss: 0.6984 - val_acc: 0.5521\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5827 - acc: 0.7096 - val_loss: 0.6982 - val_acc: 0.5625\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5607 - acc: 0.7027 - val_loss: 0.7203 - val_acc: 0.5312\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5633 - acc: 0.6993 - val_loss: 0.7186 - val_acc: 0.5625\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5724 - acc: 0.6959 - val_loss: 0.7184 - val_acc: 0.4896\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5711 - acc: 0.7182 - val_loss: 0.7285 - val_acc: 0.5312\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5547 - acc: 0.7354 - val_loss: 0.7338 - val_acc: 0.5312\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5822 - acc: 0.7096 - val_loss: 0.7317 - val_acc: 0.5312\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5688 - acc: 0.7131 - val_loss: 0.7221 - val_acc: 0.5417\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5593 - acc: 0.7045 - val_loss: 0.7170 - val_acc: 0.5417\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5594 - acc: 0.7268 - val_loss: 0.7245 - val_acc: 0.5417\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5673 - acc: 0.7302 - val_loss: 0.7258 - val_acc: 0.5417\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5485 - acc: 0.7113 - val_loss: 0.6982 - val_acc: 0.5521\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5612 - acc: 0.7062 - val_loss: 0.7128 - val_acc: 0.5208\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5459 - acc: 0.7216 - val_loss: 0.7147 - val_acc: 0.5208\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5080 - acc: 0.7423 - val_loss: 0.7193 - val_acc: 0.5312\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5277 - acc: 0.7251 - val_loss: 0.6979 - val_acc: 0.5312\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5635 - acc: 0.7199 - val_loss: 0.7277 - val_acc: 0.5208\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5462 - acc: 0.7096 - val_loss: 0.7079 - val_acc: 0.5625\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5250 - acc: 0.7285 - val_loss: 0.7134 - val_acc: 0.5417\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5053 - acc: 0.7491 - val_loss: 0.7177 - val_acc: 0.5312\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5553 - acc: 0.7302 - val_loss: 0.7096 - val_acc: 0.5417\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5757 - acc: 0.7251 - val_loss: 0.7140 - val_acc: 0.5625\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5493 - acc: 0.7320 - val_loss: 0.7232 - val_acc: 0.5208\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5844 - acc: 0.6959 - val_loss: 0.7097 - val_acc: 0.5000\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5587 - acc: 0.7285 - val_loss: 0.7114 - val_acc: 0.5312\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5272 - acc: 0.7302 - val_loss: 0.7254 - val_acc: 0.4792\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5564 - acc: 0.7216 - val_loss: 0.7047 - val_acc: 0.5417\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5482 - acc: 0.7079 - val_loss: 0.7202 - val_acc: 0.5729\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5552 - acc: 0.7182 - val_loss: 0.7062 - val_acc: 0.5625\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5541 - acc: 0.7302 - val_loss: 0.7120 - val_acc: 0.5521\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.4996 - acc: 0.7491 - val_loss: 0.7149 - val_acc: 0.5312\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5340 - acc: 0.7457 - val_loss: 0.7195 - val_acc: 0.5417\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5304 - acc: 0.7216 - val_loss: 0.7109 - val_acc: 0.5000\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5871 - acc: 0.6924 - val_loss: 0.6985 - val_acc: 0.5729\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.5639 - acc: 0.7354 - val_loss: 0.7239 - val_acc: 0.5312\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5683 - acc: 0.7010 - val_loss: 0.6937 - val_acc: 0.5729\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5389 - acc: 0.7182 - val_loss: 0.7040 - val_acc: 0.5521\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5742 - acc: 0.7096 - val_loss: 0.7104 - val_acc: 0.5104\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5186 - acc: 0.7268 - val_loss: 0.7137 - val_acc: 0.5208\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5262 - acc: 0.7663 - val_loss: 0.7131 - val_acc: 0.5521\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5697 - acc: 0.6838 - val_loss: 0.7153 - val_acc: 0.5312\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5452 - acc: 0.7179 - val_loss: 0.7186 - val_acc: 0.5312\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5468 - acc: 0.7165 - val_loss: 0.7170 - val_acc: 0.5208\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5322 - acc: 0.7045 - val_loss: 0.7184 - val_acc: 0.5417\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5690 - acc: 0.6873 - val_loss: 0.7030 - val_acc: 0.5625\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5523 - acc: 0.7113 - val_loss: 0.7141 - val_acc: 0.5417\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5306 - acc: 0.7285 - val_loss: 0.7323 - val_acc: 0.5417\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5368 - acc: 0.7337 - val_loss: 0.7252 - val_acc: 0.5312\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5996 - acc: 0.6753 - val_loss: 0.7229 - val_acc: 0.5312\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5431 - acc: 0.7165 - val_loss: 0.7039 - val_acc: 0.5521\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5370 - acc: 0.7251 - val_loss: 0.7187 - val_acc: 0.5208\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5675 - acc: 0.6942 - val_loss: 0.7105 - val_acc: 0.5208\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5602 - acc: 0.7268 - val_loss: 0.7089 - val_acc: 0.5312\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5707 - acc: 0.7113 - val_loss: 0.7349 - val_acc: 0.5104\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5592 - acc: 0.7320 - val_loss: 0.7341 - val_acc: 0.5000\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5123 - acc: 0.7509 - val_loss: 0.7115 - val_acc: 0.5417\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5556 - acc: 0.6890 - val_loss: 0.7132 - val_acc: 0.5208\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5537 - acc: 0.7165 - val_loss: 0.7139 - val_acc: 0.5417\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5284 - acc: 0.7234 - val_loss: 0.7175 - val_acc: 0.5417\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5304 - acc: 0.7251 - val_loss: 0.7373 - val_acc: 0.5417\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5929 - acc: 0.7062 - val_loss: 0.7156 - val_acc: 0.5312\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5445 - acc: 0.7388 - val_loss: 0.7025 - val_acc: 0.5417\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5186 - acc: 0.7285 - val_loss: 0.7256 - val_acc: 0.5000\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5717 - acc: 0.7113 - val_loss: 0.7041 - val_acc: 0.5625\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5234 - acc: 0.7595 - val_loss: 0.7141 - val_acc: 0.5104\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5807 - acc: 0.7096 - val_loss: 0.7213 - val_acc: 0.5417\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5486 - acc: 0.7234 - val_loss: 0.7259 - val_acc: 0.5417\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5217 - acc: 0.7337 - val_loss: 0.7069 - val_acc: 0.5833\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5241 - acc: 0.7216 - val_loss: 0.7268 - val_acc: 0.5312\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5398 - acc: 0.7199 - val_loss: 0.7191 - val_acc: 0.5417\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5320 - acc: 0.7213 - val_loss: 0.7085 - val_acc: 0.5312\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5112 - acc: 0.7491 - val_loss: 0.6938 - val_acc: 0.5521\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5421 - acc: 0.7337 - val_loss: 0.7042 - val_acc: 0.5521\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5213 - acc: 0.7526 - val_loss: 0.7246 - val_acc: 0.5000\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5122 - acc: 0.7285 - val_loss: 0.7329 - val_acc: 0.5312\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5335 - acc: 0.7199 - val_loss: 0.7253 - val_acc: 0.5521\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.5378 - acc: 0.7251 - val_loss: 0.7339 - val_acc: 0.5312\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5216 - acc: 0.7526 - val_loss: 0.7052 - val_acc: 0.5312\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4879 - acc: 0.7612 - val_loss: 0.7189 - val_acc: 0.5417\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.5348 - acc: 0.7165 - val_loss: 0.7138 - val_acc: 0.5729\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5254 - acc: 0.7234 - val_loss: 0.7199 - val_acc: 0.5625\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5501 - acc: 0.7251 - val_loss: 0.7135 - val_acc: 0.5521\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5173 - acc: 0.7131 - val_loss: 0.7300 - val_acc: 0.5417\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5299 - acc: 0.7199 - val_loss: 0.7235 - val_acc: 0.5729\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5295 - acc: 0.7302 - val_loss: 0.7242 - val_acc: 0.5521\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5177 - acc: 0.7354 - val_loss: 0.7188 - val_acc: 0.5417\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.5348 - acc: 0.7320 - val_loss: 0.7253 - val_acc: 0.5312\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5628 - acc: 0.7131 - val_loss: 0.7139 - val_acc: 0.5625\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5277 - acc: 0.7629 - val_loss: 0.7025 - val_acc: 0.5521\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5303 - acc: 0.7165 - val_loss: 0.7199 - val_acc: 0.5312\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.5182 - acc: 0.7337 - val_loss: 0.7162 - val_acc: 0.5312\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5486 - acc: 0.7216 - val_loss: 0.7024 - val_acc: 0.5625\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.5191 - acc: 0.7354 - val_loss: 0.7129 - val_acc: 0.5417\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.5566 - acc: 0.7131 - val_loss: 0.6947 - val_acc: 0.5521\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5403 - acc: 0.7388 - val_loss: 0.7010 - val_acc: 0.5521\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.5668 - acc: 0.7027 - val_loss: 0.7028 - val_acc: 0.5521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "2fe6c09e-25a9-4f33-f450-4c2a6b81d3b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.1836185455322266,\n",
              "  1.1653876304626465,\n",
              "  1.128720998764038,\n",
              "  1.20948326587677,\n",
              "  1.1828243732452393,\n",
              "  1.2382646799087524,\n",
              "  1.1518685817718506,\n",
              "  1.1105759143829346,\n",
              "  1.1464678049087524,\n",
              "  1.1778051853179932,\n",
              "  1.1366721391677856,\n",
              "  1.2212432622909546,\n",
              "  1.0705535411834717,\n",
              "  1.1452710628509521,\n",
              "  1.1339830160140991,\n",
              "  1.084529995918274,\n",
              "  1.1482640504837036,\n",
              "  1.075944423675537,\n",
              "  1.1479260921478271,\n",
              "  1.0944818258285522,\n",
              "  1.152325987815857,\n",
              "  1.1512306928634644,\n",
              "  1.1311368942260742,\n",
              "  1.1248130798339844,\n",
              "  1.0537097454071045,\n",
              "  1.0960341691970825,\n",
              "  1.0717421770095825,\n",
              "  1.0154789686203003,\n",
              "  1.1067090034484863,\n",
              "  0.985805332660675,\n",
              "  1.089207410812378,\n",
              "  1.0687116384506226,\n",
              "  1.057243824005127,\n",
              "  1.0819634199142456,\n",
              "  1.0349924564361572,\n",
              "  1.0227751731872559,\n",
              "  1.025407314300537,\n",
              "  0.9081463813781738,\n",
              "  1.1068893671035767,\n",
              "  0.9882253408432007,\n",
              "  0.9822778105735779,\n",
              "  1.0537629127502441,\n",
              "  1.0789008140563965,\n",
              "  0.9442137479782104,\n",
              "  0.9818554520606995,\n",
              "  1.069888710975647,\n",
              "  0.9827467799186707,\n",
              "  1.01669180393219,\n",
              "  0.9860750436782837,\n",
              "  1.0176650285720825,\n",
              "  0.9532521963119507,\n",
              "  1.0960805416107178,\n",
              "  0.983411431312561,\n",
              "  1.0237699747085571,\n",
              "  1.0130480527877808,\n",
              "  1.052498459815979,\n",
              "  0.9813749194145203,\n",
              "  0.9266716837882996,\n",
              "  0.9774746894836426,\n",
              "  1.0231897830963135,\n",
              "  0.9835543036460876,\n",
              "  0.9163519740104675,\n",
              "  1.0186457633972168,\n",
              "  0.9689763188362122,\n",
              "  1.0394401550292969,\n",
              "  0.9809268116950989,\n",
              "  0.9247068166732788,\n",
              "  0.9811402559280396,\n",
              "  0.9803721904754639,\n",
              "  0.9915554523468018,\n",
              "  1.0055313110351562,\n",
              "  1.0023990869522095,\n",
              "  1.0521785020828247,\n",
              "  0.9808881282806396,\n",
              "  0.8521538972854614,\n",
              "  1.0077241659164429,\n",
              "  0.985171914100647,\n",
              "  0.9186451435089111,\n",
              "  0.9260714054107666,\n",
              "  0.9777597188949585,\n",
              "  0.9546810984611511,\n",
              "  1.0332881212234497,\n",
              "  0.9586344957351685,\n",
              "  0.950368344783783,\n",
              "  0.9283676147460938,\n",
              "  0.9362943172454834,\n",
              "  0.9030681252479553,\n",
              "  0.9165294766426086,\n",
              "  0.9734305143356323,\n",
              "  0.8758196234703064,\n",
              "  0.987751841545105,\n",
              "  0.9405496716499329,\n",
              "  0.858222484588623,\n",
              "  0.8735479712486267,\n",
              "  0.9847413301467896,\n",
              "  0.8823301792144775,\n",
              "  0.9421472549438477,\n",
              "  0.9875159859657288,\n",
              "  0.9254473447799683,\n",
              "  0.9353426098823547,\n",
              "  0.9456369876861572,\n",
              "  0.9460349678993225,\n",
              "  0.9639673233032227,\n",
              "  0.9495091438293457,\n",
              "  0.9119171500205994,\n",
              "  0.9064750671386719,\n",
              "  0.9558595418930054,\n",
              "  0.9213038682937622,\n",
              "  0.8398261666297913,\n",
              "  0.9492836594581604,\n",
              "  0.8945050239562988,\n",
              "  0.9542967081069946,\n",
              "  0.9049847722053528,\n",
              "  0.8667881488800049,\n",
              "  0.8875470161437988,\n",
              "  0.9725329279899597,\n",
              "  0.9102069139480591,\n",
              "  0.8564531803131104,\n",
              "  0.992434024810791,\n",
              "  0.9537762403488159,\n",
              "  0.936369776725769,\n",
              "  0.930391788482666,\n",
              "  0.9240732192993164,\n",
              "  0.8686299324035645,\n",
              "  0.8624587059020996,\n",
              "  0.9581016898155212,\n",
              "  0.8891433477401733,\n",
              "  0.8873647451400757,\n",
              "  0.8546649217605591,\n",
              "  0.9100967049598694,\n",
              "  0.9020679593086243,\n",
              "  0.8250556588172913,\n",
              "  0.8845271468162537,\n",
              "  0.885572075843811,\n",
              "  0.96284019947052,\n",
              "  0.8781899809837341,\n",
              "  0.8720844984054565,\n",
              "  0.9722424149513245,\n",
              "  0.9067319631576538,\n",
              "  0.8485692739486694,\n",
              "  0.8775516748428345,\n",
              "  0.848764181137085,\n",
              "  0.9148204326629639,\n",
              "  0.8425503969192505,\n",
              "  0.857743501663208,\n",
              "  0.8273767828941345,\n",
              "  0.8676048517227173,\n",
              "  0.8238873481750488,\n",
              "  0.8770953416824341,\n",
              "  0.8405684232711792,\n",
              "  0.8948115706443787,\n",
              "  0.8387487530708313,\n",
              "  0.8464835286140442,\n",
              "  0.8151130080223083,\n",
              "  0.8565554022789001,\n",
              "  0.8450106382369995,\n",
              "  0.8273597955703735,\n",
              "  0.7862083315849304,\n",
              "  0.8529446125030518,\n",
              "  0.8780691027641296,\n",
              "  0.8388771414756775,\n",
              "  0.9065199494361877,\n",
              "  0.8061031103134155,\n",
              "  0.8596750497817993,\n",
              "  0.8332852125167847,\n",
              "  0.8250267505645752,\n",
              "  0.8694450855255127,\n",
              "  0.7791902422904968,\n",
              "  0.763859748840332,\n",
              "  0.8021501302719116,\n",
              "  0.7928948402404785,\n",
              "  0.8036784529685974,\n",
              "  0.842985987663269,\n",
              "  0.832194983959198,\n",
              "  0.8041061162948608,\n",
              "  0.8600804209709167,\n",
              "  0.8336693644523621,\n",
              "  0.8414485454559326,\n",
              "  0.856444776058197,\n",
              "  0.8783424496650696,\n",
              "  0.7555183172225952,\n",
              "  0.7424048185348511,\n",
              "  0.8269526958465576,\n",
              "  0.783190131187439,\n",
              "  0.8296709060668945,\n",
              "  0.829268753528595,\n",
              "  0.7940192818641663,\n",
              "  0.8221319913864136,\n",
              "  0.8168385028839111,\n",
              "  0.8097588419914246,\n",
              "  0.7026074528694153,\n",
              "  0.7722188830375671,\n",
              "  0.869120180606842,\n",
              "  0.7798429727554321,\n",
              "  0.7473107576370239,\n",
              "  0.7983040809631348,\n",
              "  0.8149173855781555,\n",
              "  0.8094196915626526,\n",
              "  0.8191761374473572,\n",
              "  0.7646056413650513,\n",
              "  0.7642317414283752,\n",
              "  0.8227877020835876,\n",
              "  0.8269681334495544,\n",
              "  0.7650420069694519,\n",
              "  0.793447732925415,\n",
              "  0.8040679097175598,\n",
              "  0.793673038482666,\n",
              "  0.8201756477355957,\n",
              "  0.7527602910995483,\n",
              "  0.784112811088562,\n",
              "  0.8502458930015564,\n",
              "  0.7923631072044373,\n",
              "  0.815610945224762,\n",
              "  0.8209337592124939,\n",
              "  0.7871452569961548,\n",
              "  0.7776439785957336,\n",
              "  0.6990187168121338,\n",
              "  0.7746635675430298,\n",
              "  0.8044137954711914,\n",
              "  0.7440751194953918,\n",
              "  0.7924789786338806,\n",
              "  0.7838264107704163,\n",
              "  0.7694106101989746,\n",
              "  0.7957754731178284,\n",
              "  0.7442331910133362,\n",
              "  0.7644720673561096,\n",
              "  0.7378071546554565,\n",
              "  0.7352027893066406,\n",
              "  0.829770565032959,\n",
              "  0.7272140383720398,\n",
              "  0.7856682538986206,\n",
              "  0.8290757536888123,\n",
              "  0.7230669260025024,\n",
              "  0.7178311944007874,\n",
              "  0.6804300546646118,\n",
              "  0.7937523126602173,\n",
              "  0.7901493906974792,\n",
              "  0.8202580213546753,\n",
              "  0.775801420211792,\n",
              "  0.688731849193573,\n",
              "  0.7563858032226562,\n",
              "  0.7189897298812866,\n",
              "  0.7289742827415466,\n",
              "  0.8163122534751892,\n",
              "  0.8014723658561707,\n",
              "  0.8116902112960815,\n",
              "  0.8304812908172607,\n",
              "  0.7569944262504578,\n",
              "  0.7665220499038696,\n",
              "  0.7775918841362,\n",
              "  0.6972625851631165,\n",
              "  0.7439393997192383,\n",
              "  0.7898183465003967,\n",
              "  0.7451390027999878,\n",
              "  0.7552123665809631,\n",
              "  0.7761114835739136,\n",
              "  0.7313020825386047,\n",
              "  0.7353298664093018,\n",
              "  0.7125420570373535,\n",
              "  0.7489396333694458,\n",
              "  0.7823876738548279,\n",
              "  0.7342333793640137,\n",
              "  0.7454193234443665,\n",
              "  0.7298430800437927,\n",
              "  0.7403093576431274,\n",
              "  0.739547848701477,\n",
              "  0.7204368710517883,\n",
              "  0.7495548725128174,\n",
              "  0.7154420614242554,\n",
              "  0.7670033574104309,\n",
              "  0.755560040473938,\n",
              "  0.7498534917831421,\n",
              "  0.7259537577629089,\n",
              "  0.7313635945320129,\n",
              "  0.707385241985321,\n",
              "  0.7163975238800049,\n",
              "  0.7437620162963867,\n",
              "  0.7154162526130676,\n",
              "  0.7137060761451721,\n",
              "  0.7602165341377258,\n",
              "  0.7683534622192383,\n",
              "  0.7532914280891418,\n",
              "  0.7057341933250427,\n",
              "  0.7366678714752197,\n",
              "  0.7622629404067993,\n",
              "  0.726405143737793,\n",
              "  0.7720674872398376,\n",
              "  0.7387735843658447,\n",
              "  0.6942436695098877,\n",
              "  0.7654123902320862,\n",
              "  0.7720719575881958,\n",
              "  0.7864660620689392,\n",
              "  0.7121003866195679,\n",
              "  0.7066812515258789,\n",
              "  0.7264798879623413,\n",
              "  0.680229127407074,\n",
              "  0.7732841968536377,\n",
              "  0.7144114375114441,\n",
              "  0.7165722250938416,\n",
              "  0.688089907169342,\n",
              "  0.683657169342041,\n",
              "  0.763857364654541,\n",
              "  0.7138984203338623,\n",
              "  0.7317607998847961,\n",
              "  0.7192477583885193,\n",
              "  0.7148933410644531,\n",
              "  0.7065005898475647,\n",
              "  0.6623225808143616,\n",
              "  0.6921955943107605,\n",
              "  0.7232437133789062,\n",
              "  0.7014293074607849,\n",
              "  0.6701448559761047,\n",
              "  0.6907044649124146,\n",
              "  0.7165272235870361,\n",
              "  0.7339261174201965,\n",
              "  0.679957926273346,\n",
              "  0.7061212062835693,\n",
              "  0.692570149898529,\n",
              "  0.6856575608253479,\n",
              "  0.6625574827194214,\n",
              "  0.738309919834137,\n",
              "  0.6949684619903564,\n",
              "  0.7266904711723328,\n",
              "  0.6977171301841736,\n",
              "  0.6873076558113098,\n",
              "  0.6535809636116028,\n",
              "  0.7640910744667053,\n",
              "  0.733150839805603,\n",
              "  0.7668116688728333,\n",
              "  0.6528425216674805,\n",
              "  0.6904025077819824,\n",
              "  0.7921295166015625,\n",
              "  0.7065269947052002,\n",
              "  0.7151243090629578,\n",
              "  0.6866332292556763,\n",
              "  0.6551389694213867,\n",
              "  0.7346513867378235,\n",
              "  0.7055332660675049,\n",
              "  0.6510180234909058,\n",
              "  0.7130910158157349,\n",
              "  0.733677864074707,\n",
              "  0.6970584392547607,\n",
              "  0.6568019986152649,\n",
              "  0.6581447124481201,\n",
              "  0.7233339548110962,\n",
              "  0.6535693407058716,\n",
              "  0.6558419466018677,\n",
              "  0.6778932809829712,\n",
              "  0.7350226044654846,\n",
              "  0.6569972038269043,\n",
              "  0.7208110094070435,\n",
              "  0.6682050824165344,\n",
              "  0.6934685111045837,\n",
              "  0.7224581241607666,\n",
              "  0.7265148758888245,\n",
              "  0.6663214564323425,\n",
              "  0.6898548603057861,\n",
              "  0.6738237142562866,\n",
              "  0.6836373805999756,\n",
              "  0.7006770968437195,\n",
              "  0.7427468299865723,\n",
              "  0.6980531811714172,\n",
              "  0.66599440574646,\n",
              "  0.6789461970329285,\n",
              "  0.6921969056129456,\n",
              "  0.6656596660614014,\n",
              "  0.6639289855957031,\n",
              "  0.6889531016349792,\n",
              "  0.7197583913803101,\n",
              "  0.7084552645683289,\n",
              "  0.7041734457015991,\n",
              "  0.6949250102043152,\n",
              "  0.737402081489563,\n",
              "  0.6485463380813599,\n",
              "  0.6918449997901917,\n",
              "  0.6888492107391357,\n",
              "  0.6690924167633057,\n",
              "  0.6461643576622009,\n",
              "  0.619449257850647,\n",
              "  0.6444233655929565,\n",
              "  0.6747393608093262,\n",
              "  0.6674445271492004,\n",
              "  0.6372394561767578,\n",
              "  0.6573876142501831,\n",
              "  0.6378185749053955,\n",
              "  0.6434793472290039,\n",
              "  0.639863133430481,\n",
              "  0.6475562453269958,\n",
              "  0.6574603915214539,\n",
              "  0.682026207447052,\n",
              "  0.65043705701828,\n",
              "  0.6282042860984802,\n",
              "  0.67967289686203,\n",
              "  0.6898934245109558,\n",
              "  0.6883013248443604,\n",
              "  0.6920766234397888,\n",
              "  0.6868588328361511,\n",
              "  0.6713497638702393,\n",
              "  0.6184289455413818,\n",
              "  0.6704828143119812,\n",
              "  0.6371906399726868,\n",
              "  0.6541346311569214,\n",
              "  0.726312518119812,\n",
              "  0.6457327604293823,\n",
              "  0.6991608142852783,\n",
              "  0.613645076751709,\n",
              "  0.6569226384162903,\n",
              "  0.6912224292755127,\n",
              "  0.6847618222236633,\n",
              "  0.6319464445114136,\n",
              "  0.6986265778541565,\n",
              "  0.6223152875900269,\n",
              "  0.6453907489776611,\n",
              "  0.638822078704834,\n",
              "  0.6885726451873779,\n",
              "  0.6776688098907471,\n",
              "  0.6312680840492249,\n",
              "  0.6639237999916077,\n",
              "  0.7031041979789734,\n",
              "  0.6188448667526245,\n",
              "  0.6913899183273315,\n",
              "  0.6401260495185852,\n",
              "  0.6147712469100952,\n",
              "  0.6466682553291321,\n",
              "  0.6634618043899536,\n",
              "  0.6615749001502991,\n",
              "  0.6724124550819397,\n",
              "  0.6382888555526733,\n",
              "  0.6248520612716675,\n",
              "  0.6997085213661194,\n",
              "  0.6460400819778442,\n",
              "  0.6613680124282837,\n",
              "  0.6324023604393005,\n",
              "  0.6465092301368713,\n",
              "  0.622702956199646,\n",
              "  0.6914945840835571,\n",
              "  0.648394763469696,\n",
              "  0.6386682987213135,\n",
              "  0.6317604184150696,\n",
              "  0.6232838034629822,\n",
              "  0.6348554491996765,\n",
              "  0.6653369665145874,\n",
              "  0.6351494193077087,\n",
              "  0.6767904162406921,\n",
              "  0.6207798719406128,\n",
              "  0.6039447784423828,\n",
              "  0.6329988837242126,\n",
              "  0.6567816734313965,\n",
              "  0.6660056710243225,\n",
              "  0.667935848236084,\n",
              "  0.6249045729637146,\n",
              "  0.6446214914321899,\n",
              "  0.6248536705970764,\n",
              "  0.6528058052062988,\n",
              "  0.5901501774787903,\n",
              "  0.6621540188789368,\n",
              "  0.6278656721115112,\n",
              "  0.646930456161499,\n",
              "  0.6618214845657349,\n",
              "  0.6284474730491638,\n",
              "  0.5808484554290771,\n",
              "  0.6069954633712769,\n",
              "  0.6245341300964355,\n",
              "  0.6366523504257202,\n",
              "  0.6301184296607971,\n",
              "  0.606198251247406,\n",
              "  0.6536272168159485,\n",
              "  0.6376150846481323,\n",
              "  0.6501175165176392,\n",
              "  0.6569066643714905,\n",
              "  0.6654917597770691,\n",
              "  0.6792452335357666,\n",
              "  0.6175938248634338,\n",
              "  0.6297850012779236,\n",
              "  0.5730481147766113,\n",
              "  0.6150488257408142,\n",
              "  0.6433002352714539,\n",
              "  0.6082653403282166,\n",
              "  0.6536276936531067,\n",
              "  0.6158541440963745,\n",
              "  0.6375603079795837,\n",
              "  0.6332811713218689,\n",
              "  0.6705108880996704,\n",
              "  0.6125807166099548,\n",
              "  0.6229784488677979,\n",
              "  0.5936688780784607,\n",
              "  0.6322441101074219,\n",
              "  0.594811201095581,\n",
              "  0.6561914086341858,\n",
              "  0.6750935316085815,\n",
              "  0.6420760750770569,\n",
              "  0.6495044827461243,\n",
              "  0.6203082799911499,\n",
              "  0.5913490653038025,\n",
              "  0.626928985118866,\n",
              "  0.6249164938926697,\n",
              "  0.633456826210022,\n",
              "  0.6416619420051575,\n",
              "  0.648816704750061,\n",
              "  0.6354832649230957,\n",
              "  0.5933144688606262,\n",
              "  0.6641849875450134,\n",
              "  0.6371331810951233,\n",
              "  0.6268777847290039,\n",
              "  0.6260654926300049,\n",
              "  0.6371711492538452,\n",
              "  0.606913149356842,\n",
              "  0.6539884209632874,\n",
              "  0.6519516110420227,\n",
              "  0.6195244789123535,\n",
              "  0.6307673454284668,\n",
              "  0.5970256328582764,\n",
              "  0.5862385630607605,\n",
              "  0.5870341062545776,\n",
              "  0.6172962188720703,\n",
              "  0.7087138891220093,\n",
              "  0.6558005213737488,\n",
              "  0.6331146955490112,\n",
              "  0.6553623676300049,\n",
              "  0.5898067355155945,\n",
              "  0.5972966551780701,\n",
              "  0.6039983034133911,\n",
              "  0.5678210854530334,\n",
              "  0.6264920234680176,\n",
              "  0.6468760967254639,\n",
              "  0.6197091341018677,\n",
              "  0.5908005237579346,\n",
              "  0.5709287524223328,\n",
              "  0.5776599645614624,\n",
              "  0.6259981393814087,\n",
              "  0.6159696578979492,\n",
              "  0.6314960718154907,\n",
              "  0.6420801281929016,\n",
              "  0.5743972659111023,\n",
              "  0.5649446845054626,\n",
              "  0.6335325241088867,\n",
              "  0.586866021156311,\n",
              "  0.6301277875900269,\n",
              "  0.5884546637535095,\n",
              "  0.6272525787353516,\n",
              "  0.6250166296958923,\n",
              "  0.62785804271698,\n",
              "  0.608185887336731,\n",
              "  0.6109954118728638,\n",
              "  0.6109029650688171,\n",
              "  0.6388522982597351,\n",
              "  0.6226964592933655,\n",
              "  0.6079064607620239,\n",
              "  0.613097071647644,\n",
              "  0.6026630997657776,\n",
              "  0.6200869679450989,\n",
              "  0.6191328167915344,\n",
              "  0.5844365358352661,\n",
              "  0.5807554125785828,\n",
              "  0.5647729635238647,\n",
              "  0.621570885181427,\n",
              "  0.61875319480896,\n",
              "  0.6200140714645386,\n",
              "  0.6350049376487732,\n",
              "  0.6037903428077698,\n",
              "  0.5830433964729309,\n",
              "  0.6327034831047058,\n",
              "  0.6119112968444824,\n",
              "  0.6251243948936462,\n",
              "  0.6631585359573364,\n",
              "  0.6219730377197266,\n",
              "  0.6172380447387695,\n",
              "  0.5817370414733887,\n",
              "  0.5857558846473694,\n",
              "  0.6331433057785034,\n",
              "  0.5959323644638062,\n",
              "  0.6552780270576477,\n",
              "  0.6107316613197327,\n",
              "  0.6206721067428589,\n",
              "  0.6082780361175537,\n",
              "  0.6387407183647156,\n",
              "  0.6026726365089417,\n",
              "  0.5529674291610718,\n",
              "  0.6127283573150635,\n",
              "  0.5868815779685974,\n",
              "  0.6089197397232056,\n",
              "  0.6159759759902954,\n",
              "  0.6296627521514893,\n",
              "  0.5921981334686279,\n",
              "  0.6217150092124939,\n",
              "  0.5907928943634033,\n",
              "  0.6016652584075928,\n",
              "  0.6196504831314087,\n",
              "  0.6086135506629944,\n",
              "  0.5949874520301819,\n",
              "  0.6102885007858276,\n",
              "  0.5709193348884583,\n",
              "  0.6236094832420349,\n",
              "  0.5444861054420471,\n",
              "  0.5987765192985535,\n",
              "  0.5708282589912415,\n",
              "  0.5968524217605591,\n",
              "  0.5696294903755188,\n",
              "  0.5715897679328918,\n",
              "  0.6209135055541992,\n",
              "  0.5862051248550415,\n",
              "  0.6050242185592651,\n",
              "  0.6052248477935791,\n",
              "  0.5957763195037842,\n",
              "  0.5614335536956787,\n",
              "  0.6116113662719727,\n",
              "  0.6004298329353333,\n",
              "  0.6176229119300842,\n",
              "  0.593333899974823,\n",
              "  0.6010944843292236,\n",
              "  0.5922702550888062,\n",
              "  0.5989482402801514,\n",
              "  0.5830944776535034,\n",
              "  0.5926865935325623,\n",
              "  0.618547260761261,\n",
              "  0.596717119216919,\n",
              "  0.5850905179977417,\n",
              "  0.6070171594619751,\n",
              "  0.5814518332481384,\n",
              "  0.5670576691627502,\n",
              "  0.6036925315856934,\n",
              "  0.5939775705337524,\n",
              "  0.580596387386322,\n",
              "  0.6309297680854797,\n",
              "  0.5863937735557556,\n",
              "  0.6005117297172546,\n",
              "  0.5863538980484009,\n",
              "  0.5923827886581421,\n",
              "  0.6121605038642883,\n",
              "  0.578378438949585,\n",
              "  0.6128732562065125,\n",
              "  0.6230795383453369,\n",
              "  0.5668736696243286,\n",
              "  0.5932545065879822,\n",
              "  0.5947255492210388,\n",
              "  0.5876871347427368,\n",
              "  0.5658321976661682,\n",
              "  0.5902343988418579,\n",
              "  0.6089755892753601,\n",
              "  0.5830978751182556,\n",
              "  0.5855633020401001,\n",
              "  0.5805187821388245,\n",
              "  0.5760279893875122,\n",
              "  0.5561816096305847,\n",
              "  0.5667304396629333,\n",
              "  0.5995897054672241,\n",
              "  0.5983976125717163,\n",
              "  0.5885736346244812,\n",
              "  0.5830840468406677,\n",
              "  0.5710271596908569,\n",
              "  0.5787922739982605,\n",
              "  0.598342776298523,\n",
              "  0.5901216864585876,\n",
              "  0.5605558156967163,\n",
              "  0.5715633034706116,\n",
              "  0.6158199906349182,\n",
              "  0.5814552307128906,\n",
              "  0.5908521413803101,\n",
              "  0.5365216135978699,\n",
              "  0.555897057056427,\n",
              "  0.5792396664619446,\n",
              "  0.5452426075935364,\n",
              "  0.54891037940979,\n",
              "  0.5709704756736755,\n",
              "  0.5817888975143433,\n",
              "  0.5797072052955627,\n",
              "  0.6238029599189758,\n",
              "  0.5523177981376648,\n",
              "  0.6091369390487671,\n",
              "  0.59937983751297,\n",
              "  0.5535655617713928,\n",
              "  0.5526478886604309,\n",
              "  0.5760979056358337,\n",
              "  0.5729129910469055,\n",
              "  0.5844249129295349,\n",
              "  0.5691429972648621,\n",
              "  0.5813012719154358,\n",
              "  0.5817525386810303,\n",
              "  0.6225335001945496,\n",
              "  0.5805658102035522,\n",
              "  0.5366112589836121,\n",
              "  0.6153214573860168,\n",
              "  0.5886446237564087,\n",
              "  0.5402754545211792,\n",
              "  0.5718300342559814,\n",
              "  0.5604405999183655,\n",
              "  0.5678974986076355,\n",
              "  0.536389172077179,\n",
              "  0.5636289119720459,\n",
              "  0.5626022815704346,\n",
              "  0.5856530070304871,\n",
              "  0.5602226257324219,\n",
              "  0.5521959066390991,\n",
              "  0.5905699729919434,\n",
              "  0.6091449856758118,\n",
              "  0.5753594040870667,\n",
              "  0.563033938407898,\n",
              "  0.5656344890594482,\n",
              "  0.5621179342269897,\n",
              "  0.5546318292617798,\n",
              "  0.5604654550552368,\n",
              "  0.5511215925216675,\n",
              "  0.5556675791740417,\n",
              "  0.5978960990905762,\n",
              "  0.6428838968276978,\n",
              "  0.5766380429267883,\n",
              "  0.6174022555351257,\n",
              "  0.6218857169151306,\n",
              "  0.5823200345039368,\n",
              "  0.5941769480705261,\n",
              "  0.5811612606048584,\n",
              "  0.549400806427002,\n",
              "  0.5683707594871521,\n",
              "  0.5951915383338928,\n",
              "  0.578336238861084,\n",
              "  0.5653505325317383,\n",
              "  0.5929046273231506,\n",
              "  0.5447616577148438,\n",
              "  0.6000918745994568,\n",
              "  0.5896713137626648,\n",
              "  0.5690645575523376,\n",
              "  0.5941125750541687,\n",
              "  0.5802117586135864,\n",
              "  0.5703285932540894,\n",
              "  0.5829813480377197,\n",
              "  0.5733673572540283,\n",
              "  0.5444040894508362,\n",
              "  0.5707626342773438,\n",
              "  0.5567273497581482,\n",
              "  0.5997208952903748,\n",
              "  0.5716352462768555,\n",
              "  0.5419080257415771,\n",
              "  0.5815474390983582,\n",
              "  0.5967209935188293,\n",
              "  0.5428583025932312,\n",
              "  0.5616043210029602,\n",
              "  0.5511991381645203,\n",
              "  0.5994608402252197,\n",
              "  0.5748859643936157,\n",
              "  0.6034680604934692,\n",
              "  0.6121291518211365,\n",
              "  0.5683068633079529,\n",
              "  0.5540273189544678,\n",
              "  0.558812141418457,\n",
              "  0.5669443011283875,\n",
              "  0.570323646068573,\n",
              "  0.5732117295265198,\n",
              "  0.5412708520889282,\n",
              "  0.575285792350769,\n",
              "  0.5665990114212036,\n",
              "  0.5785960555076599,\n",
              "  0.5548089146614075,\n",
              "  0.5648143291473389,\n",
              "  0.5453855991363525,\n",
              "  0.5306538343429565,\n",
              "  0.5763702988624573,\n",
              "  0.5300852656364441,\n",
              "  0.5899263024330139,\n",
              "  0.5388310551643372,\n",
              "  0.5370628237724304,\n",
              "  0.5642449259757996,\n",
              "  0.5346394181251526,\n",
              "  0.5720848441123962,\n",
              "  0.5418777465820312,\n",
              "  0.5613280534744263,\n",
              "  0.5733108520507812,\n",
              "  0.5662977695465088,\n",
              "  0.6438589692115784,\n",
              "  0.5297117233276367,\n",
              "  0.5676827430725098,\n",
              "  0.5295693278312683,\n",
              "  0.5835771560668945,\n",
              "  0.578820526599884,\n",
              "  0.570545494556427,\n",
              "  0.5763652324676514,\n",
              "  0.5429171323776245,\n",
              "  0.5460179448127747,\n",
              "  0.5809587836265564,\n",
              "  0.5338498950004578,\n",
              "  0.5964497327804565,\n",
              "  0.5266483426094055,\n",
              "  0.6037435531616211,\n",
              "  0.5490795373916626,\n",
              "  0.5719549655914307,\n",
              "  0.5615598559379578,\n",
              "  0.5853786468505859,\n",
              "  0.5863308906555176,\n",
              "  0.5705288052558899,\n",
              "  0.5984864234924316,\n",
              "  0.5379540324211121,\n",
              "  0.5497769713401794,\n",
              "  0.5525258779525757,\n",
              "  0.5611096024513245,\n",
              "  0.5535833835601807,\n",
              "  0.561103343963623,\n",
              "  0.5327950716018677,\n",
              "  0.5580717325210571,\n",
              "  0.5425567626953125,\n",
              "  0.567982017993927,\n",
              "  0.5519145727157593,\n",
              "  0.5480411052703857,\n",
              "  0.5297532677650452,\n",
              "  0.5685933232307434,\n",
              "  0.5752255320549011,\n",
              "  0.571759819984436,\n",
              "  0.5839653611183167,\n",
              "  0.5406110286712646,\n",
              "  0.5643090605735779,\n",
              "  0.5491943955421448,\n",
              "  0.5685627460479736,\n",
              "  0.5643864870071411,\n",
              "  0.5545783042907715,\n",
              "  0.5544407367706299,\n",
              "  0.557951807975769,\n",
              "  0.5716452598571777,\n",
              "  0.5550159811973572,\n",
              "  0.5582453608512878,\n",
              "  0.5667442083358765,\n",
              "  0.5572077631950378,\n",
              "  0.5468838214874268,\n",
              "  0.5641346573829651,\n",
              "  0.532096266746521,\n",
              "  0.5859819054603577,\n",
              "  0.5232914090156555,\n",
              "  0.545625627040863,\n",
              "  0.5708558559417725,\n",
              "  0.5549695491790771,\n",
              "  0.5714532732963562,\n",
              "  0.5776078104972839,\n",
              "  0.5454554557800293,\n",
              "  0.5789449214935303,\n",
              "  0.5878151059150696,\n",
              "  0.5245308876037598,\n",
              "  0.5386145114898682,\n",
              "  0.5585632920265198,\n",
              "  0.5510503053665161,\n",
              "  0.5570164322853088,\n",
              "  0.5297216176986694,\n",
              "  0.5914044976234436,\n",
              "  0.5428712368011475,\n",
              "  0.5391892194747925,\n",
              "  0.5823735594749451,\n",
              "  0.5498272180557251,\n",
              "  0.5945473909378052,\n",
              "  0.580474317073822,\n",
              "  0.5415102243423462,\n",
              "  0.5508083701133728,\n",
              "  0.5532692670822144,\n",
              "  0.5321247577667236,\n",
              "  0.5779075622558594,\n",
              "  0.5497809052467346,\n",
              "  0.5776875019073486,\n",
              "  0.5520840883255005,\n",
              "  0.5532634854316711,\n",
              "  0.5779601335525513,\n",
              "  0.5338286757469177,\n",
              "  0.5703961253166199,\n",
              "  0.5464684963226318,\n",
              "  0.5531855225563049,\n",
              "  0.5827893018722534,\n",
              "  0.5656631588935852,\n",
              "  0.5778602957725525,\n",
              "  0.5822582840919495,\n",
              "  0.534644603729248,\n",
              "  0.53514164686203,\n",
              "  0.5818747878074646,\n",
              "  0.582213282585144,\n",
              "  0.5652694702148438,\n",
              "  0.5574800968170166,\n",
              "  0.5544922351837158,\n",
              "  0.5787839889526367,\n",
              "  0.5868275165557861,\n",
              "  0.5438040494918823,\n",
              "  0.5219995975494385,\n",
              "  0.5585314035415649,\n",
              "  0.5528360605239868,\n",
              "  0.5630438327789307,\n",
              "  0.5385681390762329,\n",
              "  0.5389099717140198,\n",
              "  0.5544946193695068,\n",
              "  0.5192742347717285,\n",
              "  0.5457088947296143,\n",
              "  0.5485373139381409,\n",
              "  0.5471693277359009,\n",
              "  0.5122966170310974,\n",
              "  0.5302965641021729,\n",
              "  0.5568771958351135,\n",
              "  0.5505263209342957,\n",
              "  0.5673141479492188,\n",
              "  0.5470195412635803,\n",
              "  0.5408486127853394,\n",
              "  0.5309513807296753,\n",
              "  0.5741126537322998,\n",
              "  0.5837689638137817,\n",
              "  0.5215377807617188,\n",
              "  0.5379063487052917,\n",
              "  0.5443537831306458,\n",
              "  0.5305212140083313,\n",
              "  0.5276881456375122,\n",
              "  0.5574085712432861,\n",
              "  0.5309396386146545,\n",
              "  0.5474002957344055,\n",
              "  0.5160685181617737,\n",
              "  0.5501127243041992,\n",
              "  0.5827467441558838,\n",
              "  0.5606892704963684,\n",
              "  0.5632644891738892,\n",
              "  0.5723815560340881,\n",
              "  0.5710750222206116,\n",
              "  0.5546574592590332,\n",
              "  0.5822163820266724,\n",
              "  0.5687693953514099,\n",
              "  0.5593170523643494,\n",
              "  0.5594105124473572,\n",
              "  0.567284882068634,\n",
              "  0.5484954118728638,\n",
              "  0.5612166523933411,\n",
              "  0.5458742380142212,\n",
              "  0.5080437064170837,\n",
              "  0.5276517271995544,\n",
              "  0.5635470747947693,\n",
              "  0.5462097525596619,\n",
              "  0.5249738693237305,\n",
              "  0.5053037405014038,\n",
              "  0.5552853941917419,\n",
              "  0.5757341384887695,\n",
              "  0.5493209958076477,\n",
              "  0.5844263434410095,\n",
              "  0.5587372779846191,\n",
              "  0.5272497534751892,\n",
              "  0.5563955903053284,\n",
              "  0.5481511354446411,\n",
              "  0.5552297830581665,\n",
              "  0.5541031360626221,\n",
              "  0.4995569586753845,\n",
              "  0.5340433120727539,\n",
              "  0.5304304361343384,\n",
              "  0.5871056914329529,\n",
              "  0.5639017820358276,\n",
              "  0.5682877898216248,\n",
              "  0.5388513803482056,\n",
              "  0.5742361545562744,\n",
              "  0.5186030268669128,\n",
              "  0.52616947889328,\n",
              "  0.5696734189987183,\n",
              "  0.5451856851577759,\n",
              "  0.5468177795410156,\n",
              "  0.5321732759475708,\n",
              "  0.5689814686775208,\n",
              "  0.5523282289505005,\n",
              "  0.5306047797203064,\n",
              "  0.536780595779419,\n",
              "  0.5996097922325134,\n",
              "  0.543093204498291,\n",
              "  0.5369556546211243,\n",
              "  0.5674785375595093,\n",
              "  0.5601540803909302,\n",
              "  0.5706894397735596,\n",
              "  0.5591970086097717,\n",
              "  0.512253999710083,\n",
              "  0.5555694699287415,\n",
              "  0.5537056922912598,\n",
              "  0.5284364819526672,\n",
              "  0.5304355621337891,\n",
              "  0.5928904414176941,\n",
              "  0.5445055365562439,\n",
              "  0.5185657143592834,\n",
              "  0.5716697573661804,\n",
              "  0.5233675837516785,\n",
              "  0.580678403377533,\n",
              "  0.5485912561416626,\n",
              "  0.5216708779335022,\n",
              "  0.5240758657455444,\n",
              "  0.5397905707359314,\n",
              "  0.5320264101028442,\n",
              "  0.5111504197120667,\n",
              "  0.5420970320701599,\n",
              "  0.5213426351547241,\n",
              "  0.5121628046035767,\n",
              "  0.5335295796394348,\n",
              "  0.5378193259239197,\n",
              "  0.5215790271759033,\n",
              "  0.4879474937915802,\n",
              "  0.5347954630851746,\n",
              "  0.5253576636314392,\n",
              "  0.5500721335411072,\n",
              "  0.5172675848007202,\n",
              "  0.5299063324928284,\n",
              "  0.5295454263687134,\n",
              "  0.5176790356636047,\n",
              "  0.5348248481750488,\n",
              "  0.5628007650375366,\n",
              "  0.5276755094528198,\n",
              "  0.5303168296813965,\n",
              "  0.5181558132171631,\n",
              "  0.5486384034156799,\n",
              "  0.5191327929496765,\n",
              "  0.5565863847732544,\n",
              "  0.5402560234069824,\n",
              "  0.5668321847915649],\n",
              " 'acc': [0.530927836894989,\n",
              "  0.524055004119873,\n",
              "  0.5446735620498657,\n",
              "  0.5171821117401123,\n",
              "  0.496563583612442,\n",
              "  0.5103092789649963,\n",
              "  0.5360824465751648,\n",
              "  0.537800669670105,\n",
              "  0.5206185579299927,\n",
              "  0.496563583612442,\n",
              "  0.5257731676101685,\n",
              "  0.4862543046474457,\n",
              "  0.5360824465751648,\n",
              "  0.5274913907051086,\n",
              "  0.5223367810249329,\n",
              "  0.561855673789978,\n",
              "  0.530927836894989,\n",
              "  0.561855673789978,\n",
              "  0.5343642830848694,\n",
              "  0.5206185579299927,\n",
              "  0.5257731676101685,\n",
              "  0.5412371158599854,\n",
              "  0.5343642830848694,\n",
              "  0.5274913907051086,\n",
              "  0.5635738968849182,\n",
              "  0.5343642830848694,\n",
              "  0.5670102834701538,\n",
              "  0.5945017337799072,\n",
              "  0.524055004119873,\n",
              "  0.5652921199798584,\n",
              "  0.5320945978164673,\n",
              "  0.5463917255401611,\n",
              "  0.5343642830848694,\n",
              "  0.5652921199798584,\n",
              "  0.5343642830848694,\n",
              "  0.5206185579299927,\n",
              "  0.5481099486351013,\n",
              "  0.5704467296600342,\n",
              "  0.537800669670105,\n",
              "  0.5584192276000977,\n",
              "  0.5876288414001465,\n",
              "  0.5584192276000977,\n",
              "  0.5532646179199219,\n",
              "  0.5807560086250305,\n",
              "  0.5773195624351501,\n",
              "  0.5481099486351013,\n",
              "  0.5635738968849182,\n",
              "  0.5549828410148621,\n",
              "  0.5532646179199219,\n",
              "  0.5515463948249817,\n",
              "  0.5635738968849182,\n",
              "  0.5360824465751648,\n",
              "  0.5738831758499146,\n",
              "  0.5395188927650452,\n",
              "  0.5807560086250305,\n",
              "  0.5456081032752991,\n",
              "  0.5652921199798584,\n",
              "  0.6134020686149597,\n",
              "  0.5756013989448547,\n",
              "  0.530927836894989,\n",
              "  0.5641891956329346,\n",
              "  0.5859106779098511,\n",
              "  0.5635738968849182,\n",
              "  0.5704467296600342,\n",
              "  0.561855673789978,\n",
              "  0.5807560086250305,\n",
              "  0.5841924548149109,\n",
              "  0.5635738968849182,\n",
              "  0.5584192276000977,\n",
              "  0.5326460599899292,\n",
              "  0.5463917255401611,\n",
              "  0.5704467296600342,\n",
              "  0.5756013989448547,\n",
              "  0.5635738968849182,\n",
              "  0.5910652875900269,\n",
              "  0.5498281717300415,\n",
              "  0.5601374506950378,\n",
              "  0.5824742317199707,\n",
              "  0.592783510684967,\n",
              "  0.5721649527549744,\n",
              "  0.5893470644950867,\n",
              "  0.5481099486351013,\n",
              "  0.5738831758499146,\n",
              "  0.5721649527549744,\n",
              "  0.5756013989448547,\n",
              "  0.5859106779098511,\n",
              "  0.6030927896499634,\n",
              "  0.5704467296600342,\n",
              "  0.5601374506950378,\n",
              "  0.592783510684967,\n",
              "  0.5652921199798584,\n",
              "  0.5756013989448547,\n",
              "  0.6048110127449036,\n",
              "  0.5841924548149109,\n",
              "  0.5721649527549744,\n",
              "  0.6065292358398438,\n",
              "  0.5738831758499146,\n",
              "  0.592783510684967,\n",
              "  0.5859106779098511,\n",
              "  0.5790377855300903,\n",
              "  0.568728506565094,\n",
              "  0.5876288414001465,\n",
              "  0.5859106779098511,\n",
              "  0.599656343460083,\n",
              "  0.568728506565094,\n",
              "  0.5721649527549744,\n",
              "  0.5841924548149109,\n",
              "  0.5962199568748474,\n",
              "  0.6151202917098999,\n",
              "  0.5790377855300903,\n",
              "  0.5756013989448547,\n",
              "  0.5841924548149109,\n",
              "  0.5726351141929626,\n",
              "  0.5841924548149109,\n",
              "  0.6185566782951355,\n",
              "  0.561855673789978,\n",
              "  0.5790377855300903,\n",
              "  0.5962199568748474,\n",
              "  0.537800669670105,\n",
              "  0.5463917255401611,\n",
              "  0.5652921199798584,\n",
              "  0.5704467296600342,\n",
              "  0.5756013989448547,\n",
              "  0.5979381203651428,\n",
              "  0.5945017337799072,\n",
              "  0.5670102834701538,\n",
              "  0.5756013989448547,\n",
              "  0.5945017337799072,\n",
              "  0.5670102834701538,\n",
              "  0.5412371158599854,\n",
              "  0.5859106779098511,\n",
              "  0.6116838455200195,\n",
              "  0.5773195624351501,\n",
              "  0.5893470644950867,\n",
              "  0.5773195624351501,\n",
              "  0.5962199568748474,\n",
              "  0.5773195624351501,\n",
              "  0.5704467296600342,\n",
              "  0.6013745665550232,\n",
              "  0.6134020686149597,\n",
              "  0.592783510684967,\n",
              "  0.6099656224250793,\n",
              "  0.5773195624351501,\n",
              "  0.6082473993301392,\n",
              "  0.5824742317199707,\n",
              "  0.6283783912658691,\n",
              "  0.6116838455200195,\n",
              "  0.6271477937698364,\n",
              "  0.592783510684967,\n",
              "  0.5893470644950867,\n",
              "  0.5876288414001465,\n",
              "  0.599656343460083,\n",
              "  0.6013745665550232,\n",
              "  0.623711347579956,\n",
              "  0.6082473993301392,\n",
              "  0.6082473993301392,\n",
              "  0.5979381203651428,\n",
              "  0.6202749013900757,\n",
              "  0.592783510684967,\n",
              "  0.5584192276000977,\n",
              "  0.5945017337799072,\n",
              "  0.5756013989448547,\n",
              "  0.6030927896499634,\n",
              "  0.5790377855300903,\n",
              "  0.6048110127449036,\n",
              "  0.6116838455200195,\n",
              "  0.5945017337799072,\n",
              "  0.6048110127449036,\n",
              "  0.6271477937698364,\n",
              "  0.6134020686149597,\n",
              "  0.6134020686149597,\n",
              "  0.6185566782951355,\n",
              "  0.6099656224250793,\n",
              "  0.5893470644950867,\n",
              "  0.5945017337799072,\n",
              "  0.6099656224250793,\n",
              "  0.5721649527549744,\n",
              "  0.5945017337799072,\n",
              "  0.5704467296600342,\n",
              "  0.5704467296600342,\n",
              "  0.623711347579956,\n",
              "  0.6357388496398926,\n",
              "  0.6202749013900757,\n",
              "  0.6219931244850159,\n",
              "  0.6013745665550232,\n",
              "  0.6048110127449036,\n",
              "  0.6048110127449036,\n",
              "  0.6099656224250793,\n",
              "  0.6185566782951355,\n",
              "  0.599656343460083,\n",
              "  0.6597937941551208,\n",
              "  0.6563574075698853,\n",
              "  0.537800669670105,\n",
              "  0.6185566782951355,\n",
              "  0.6374570727348328,\n",
              "  0.6168385148048401,\n",
              "  0.6116838455200195,\n",
              "  0.623711347579956,\n",
              "  0.6048110127449036,\n",
              "  0.6254295706748962,\n",
              "  0.6219931244850159,\n",
              "  0.5979381203651428,\n",
              "  0.6391752362251282,\n",
              "  0.6460481286048889,\n",
              "  0.6219931244850159,\n",
              "  0.5807560086250305,\n",
              "  0.5910652875900269,\n",
              "  0.6288659572601318,\n",
              "  0.6374570727348328,\n",
              "  0.6065292358398438,\n",
              "  0.599656343460083,\n",
              "  0.5979381203651428,\n",
              "  0.6151202917098999,\n",
              "  0.5893470644950867,\n",
              "  0.6202749013900757,\n",
              "  0.6443299055099487,\n",
              "  0.6529209613800049,\n",
              "  0.6030927896499634,\n",
              "  0.6116838455200195,\n",
              "  0.6477663516998291,\n",
              "  0.599656343460083,\n",
              "  0.6254295706748962,\n",
              "  0.6477663516998291,\n",
              "  0.599656343460083,\n",
              "  0.6391752362251282,\n",
              "  0.6219931244850159,\n",
              "  0.6288659572601318,\n",
              "  0.6168385148048401,\n",
              "  0.5738831758499146,\n",
              "  0.6408934593200684,\n",
              "  0.6271477937698364,\n",
              "  0.6048110127449036,\n",
              "  0.6391752362251282,\n",
              "  0.6443299055099487,\n",
              "  0.6666666865348816,\n",
              "  0.6099656224250793,\n",
              "  0.6219931244850159,\n",
              "  0.6185566782951355,\n",
              "  0.6065292358398438,\n",
              "  0.6701030731201172,\n",
              "  0.6288659572601318,\n",
              "  0.6391752362251282,\n",
              "  0.6546391844749451,\n",
              "  0.6013745665550232,\n",
              "  0.5945017337799072,\n",
              "  0.5945017337799072,\n",
              "  0.6288659572601318,\n",
              "  0.6271477937698364,\n",
              "  0.6512027382850647,\n",
              "  0.630584180355072,\n",
              "  0.6752577424049377,\n",
              "  0.6391752362251282,\n",
              "  0.5962199568748474,\n",
              "  0.6219931244850159,\n",
              "  0.6219931244850159,\n",
              "  0.6099656224250793,\n",
              "  0.6374570727348328,\n",
              "  0.6219931244850159,\n",
              "  0.6632302403450012,\n",
              "  0.6546391844749451,\n",
              "  0.6202749013900757,\n",
              "  0.6168385148048401,\n",
              "  0.6426116824150085,\n",
              "  0.6408934593200684,\n",
              "  0.6099656224250793,\n",
              "  0.6443299055099487,\n",
              "  0.623711347579956,\n",
              "  0.6288659572601318,\n",
              "  0.6494845151901245,\n",
              "  0.5979381203651428,\n",
              "  0.6185566782951355,\n",
              "  0.6219931244850159,\n",
              "  0.6494845151901245,\n",
              "  0.6357388496398926,\n",
              "  0.6546391844749451,\n",
              "  0.6443299055099487,\n",
              "  0.5962199568748474,\n",
              "  0.6580756306648254,\n",
              "  0.630584180355072,\n",
              "  0.6185566782951355,\n",
              "  0.6082473993301392,\n",
              "  0.6288659572601318,\n",
              "  0.6443299055099487,\n",
              "  0.6351351141929626,\n",
              "  0.6254295706748962,\n",
              "  0.661512017250061,\n",
              "  0.6082473993301392,\n",
              "  0.630584180355072,\n",
              "  0.6460481286048889,\n",
              "  0.6185566782951355,\n",
              "  0.6168385148048401,\n",
              "  0.6271477937698364,\n",
              "  0.6512027382850647,\n",
              "  0.6340206265449524,\n",
              "  0.6443299055099487,\n",
              "  0.661512017250061,\n",
              "  0.6151202917098999,\n",
              "  0.6512027382850647,\n",
              "  0.6512027382850647,\n",
              "  0.6563574075698853,\n",
              "  0.6391752362251282,\n",
              "  0.6219931244850159,\n",
              "  0.6563574075698853,\n",
              "  0.6254295706748962,\n",
              "  0.630584180355072,\n",
              "  0.6391752362251282,\n",
              "  0.6443299055099487,\n",
              "  0.6632302403450012,\n",
              "  0.6529209613800049,\n",
              "  0.6151202917098999,\n",
              "  0.6701030731201172,\n",
              "  0.6649484634399414,\n",
              "  0.6426116824150085,\n",
              "  0.6340206265449524,\n",
              "  0.6288659572601318,\n",
              "  0.6597937941551208,\n",
              "  0.6477663516998291,\n",
              "  0.6529209613800049,\n",
              "  0.6580756306648254,\n",
              "  0.6649484634399414,\n",
              "  0.6477663516998291,\n",
              "  0.6391752362251282,\n",
              "  0.6408934593200684,\n",
              "  0.6151202917098999,\n",
              "  0.6374570727348328,\n",
              "  0.6718212962150574,\n",
              "  0.6048110127449036,\n",
              "  0.6426116824150085,\n",
              "  0.623711347579956,\n",
              "  0.6580756306648254,\n",
              "  0.6649484634399414,\n",
              "  0.5859106779098511,\n",
              "  0.6426116824150085,\n",
              "  0.6202749013900757,\n",
              "  0.6666666865348816,\n",
              "  0.6838487982749939,\n",
              "  0.6408934593200684,\n",
              "  0.6288659572601318,\n",
              "  0.6769759654998779,\n",
              "  0.6323024034500122,\n",
              "  0.6134020686149597,\n",
              "  0.661512017250061,\n",
              "  0.6735395193099976,\n",
              "  0.6701030731201172,\n",
              "  0.6340206265449524,\n",
              "  0.6786941289901733,\n",
              "  0.6546391844749451,\n",
              "  0.6357388496398926,\n",
              "  0.6283783912658691,\n",
              "  0.6587837934494019,\n",
              "  0.6529209613800049,\n",
              "  0.6408934593200684,\n",
              "  0.6443299055099487,\n",
              "  0.6254295706748962,\n",
              "  0.6408934593200684,\n",
              "  0.6288659572601318,\n",
              "  0.6666666865348816,\n",
              "  0.6597937941551208,\n",
              "  0.6254295706748962,\n",
              "  0.6477663516998291,\n",
              "  0.6151202917098999,\n",
              "  0.650337815284729,\n",
              "  0.6477663516998291,\n",
              "  0.6391752362251282,\n",
              "  0.6357388496398926,\n",
              "  0.6838487982749939,\n",
              "  0.6529209613800049,\n",
              "  0.6563574075698853,\n",
              "  0.6374570727348328,\n",
              "  0.6529209613800049,\n",
              "  0.6512027382850647,\n",
              "  0.6718212962150574,\n",
              "  0.6151202917098999,\n",
              "  0.6718212962150574,\n",
              "  0.6460481286048889,\n",
              "  0.6391752362251282,\n",
              "  0.6735395193099976,\n",
              "  0.6786941289901733,\n",
              "  0.668384850025177,\n",
              "  0.6632302403450012,\n",
              "  0.6666666865348816,\n",
              "  0.6374570727348328,\n",
              "  0.6735395193099976,\n",
              "  0.6408934593200684,\n",
              "  0.6546391844749451,\n",
              "  0.6872852444648743,\n",
              "  0.6907216310501099,\n",
              "  0.6786941289901733,\n",
              "  0.6718212962150574,\n",
              "  0.6391752362251282,\n",
              "  0.6752577424049377,\n",
              "  0.6735395193099976,\n",
              "  0.6408934593200684,\n",
              "  0.6351351141929626,\n",
              "  0.6580756306648254,\n",
              "  0.6460481286048889,\n",
              "  0.6701030731201172,\n",
              "  0.6546391844749451,\n",
              "  0.699312686920166,\n",
              "  0.661512017250061,\n",
              "  0.6649484634399414,\n",
              "  0.6580756306648254,\n",
              "  0.6426116824150085,\n",
              "  0.6769759654998779,\n",
              "  0.6408934593200684,\n",
              "  0.6786941289901733,\n",
              "  0.6769759654998779,\n",
              "  0.6391752362251282,\n",
              "  0.6701030731201172,\n",
              "  0.6718212962150574,\n",
              "  0.6065292358398438,\n",
              "  0.6855670213699341,\n",
              "  0.6512027382850647,\n",
              "  0.6804123520851135,\n",
              "  0.6254295706748962,\n",
              "  0.6666666865348816,\n",
              "  0.6804123520851135,\n",
              "  0.6426116824150085,\n",
              "  0.6323024034500122,\n",
              "  0.7199312448501587,\n",
              "  0.6477663516998291,\n",
              "  0.6563574075698853,\n",
              "  0.6804123520851135,\n",
              "  0.6460481286048889,\n",
              "  0.6512027382850647,\n",
              "  0.6649484634399414,\n",
              "  0.6529209613800049,\n",
              "  0.6907216310501099,\n",
              "  0.6975945234298706,\n",
              "  0.6408934593200684,\n",
              "  0.6666666865348816,\n",
              "  0.6494845151901245,\n",
              "  0.6804123520851135,\n",
              "  0.6735395193099976,\n",
              "  0.6666666865348816,\n",
              "  0.6494845151901245,\n",
              "  0.6494845151901245,\n",
              "  0.6821305751800537,\n",
              "  0.6735395193099976,\n",
              "  0.6838487982749939,\n",
              "  0.6786941289901733,\n",
              "  0.6563574075698853,\n",
              "  0.6769759654998779,\n",
              "  0.6391752362251282,\n",
              "  0.6872852444648743,\n",
              "  0.6958763003349304,\n",
              "  0.6666666865348816,\n",
              "  0.6460481286048889,\n",
              "  0.6752577424049377,\n",
              "  0.6786941289901733,\n",
              "  0.6855670213699341,\n",
              "  0.6632302403450012,\n",
              "  0.69243985414505,\n",
              "  0.6718212962150574,\n",
              "  0.7216494679450989,\n",
              "  0.6666666865348816,\n",
              "  0.6838487982749939,\n",
              "  0.6649484634399414,\n",
              "  0.6718212962150574,\n",
              "  0.6890034079551697,\n",
              "  0.6804123520851135,\n",
              "  0.6958763003349304,\n",
              "  0.6838487982749939,\n",
              "  0.6649484634399414,\n",
              "  0.6649484634399414,\n",
              "  0.69243985414505,\n",
              "  0.6580756306648254,\n",
              "  0.69243985414505,\n",
              "  0.6443299055099487,\n",
              "  0.6494845151901245,\n",
              "  0.6408934593200684,\n",
              "  0.6494845151901245,\n",
              "  0.6649484634399414,\n",
              "  0.6786941289901733,\n",
              "  0.7061855792999268,\n",
              "  0.6838487982749939,\n",
              "  0.6563574075698853,\n",
              "  0.6855670213699341,\n",
              "  0.661512017250061,\n",
              "  0.6907216310501099,\n",
              "  0.6821305751800537,\n",
              "  0.6546391844749451,\n",
              "  0.6323024034500122,\n",
              "  0.6838487982749939,\n",
              "  0.6890034079551697,\n",
              "  0.6890034079551697,\n",
              "  0.6477663516998291,\n",
              "  0.69243985414505,\n",
              "  0.6426116824150085,\n",
              "  0.6546391844749451,\n",
              "  0.6769759654998779,\n",
              "  0.6666666865348816,\n",
              "  0.668384850025177,\n",
              "  0.69243985414505,\n",
              "  0.6838487982749939,\n",
              "  0.6786941289901733,\n",
              "  0.6632302403450012,\n",
              "  0.6512027382850647,\n",
              "  0.6958763003349304,\n",
              "  0.6632302403450012,\n",
              "  0.6821305751800537,\n",
              "  0.6735395193099976,\n",
              "  0.6838487982749939,\n",
              "  0.6632302403450012,\n",
              "  0.6872852444648743,\n",
              "  0.6632302403450012,\n",
              "  0.6872852444648743,\n",
              "  0.6769759654998779,\n",
              "  0.6580756306648254,\n",
              "  0.6872852444648743,\n",
              "  0.6537162065505981,\n",
              "  0.6890034079551697,\n",
              "  0.6958763003349304,\n",
              "  0.7079038023948669,\n",
              "  0.6821305751800537,\n",
              "  0.6357388496398926,\n",
              "  0.6769759654998779,\n",
              "  0.668384850025177,\n",
              "  0.6494845151901245,\n",
              "  0.7061855792999268,\n",
              "  0.6804123520851135,\n",
              "  0.6701030731201172,\n",
              "  0.7182130813598633,\n",
              "  0.6769759654998779,\n",
              "  0.6804123520851135,\n",
              "  0.6821305751800537,\n",
              "  0.7010309100151062,\n",
              "  0.7216494679450989,\n",
              "  0.6786941289901733,\n",
              "  0.6718212962150574,\n",
              "  0.6941580772399902,\n",
              "  0.6769759654998779,\n",
              "  0.6804123520851135,\n",
              "  0.6821305751800537,\n",
              "  0.7096219658851624,\n",
              "  0.6769759654998779,\n",
              "  0.6838487982749939,\n",
              "  0.6443299055099487,\n",
              "  0.6855670213699341,\n",
              "  0.6666666865348816,\n",
              "  0.668384850025177,\n",
              "  0.6701030731201172,\n",
              "  0.6701030731201172,\n",
              "  0.6632302403450012,\n",
              "  0.6580756306648254,\n",
              "  0.6872852444648743,\n",
              "  0.6821305751800537,\n",
              "  0.6786941289901733,\n",
              "  0.6773648858070374,\n",
              "  0.6890034079551697,\n",
              "  0.6649484634399414,\n",
              "  0.6718212962150574,\n",
              "  0.7027491331100464,\n",
              "  0.6804123520851135,\n",
              "  0.6907216310501099,\n",
              "  0.6958763003349304,\n",
              "  0.6804123520851135,\n",
              "  0.6804123520851135,\n",
              "  0.6752577424049377,\n",
              "  0.6821305751800537,\n",
              "  0.6821305751800537,\n",
              "  0.6701030731201172,\n",
              "  0.6975945234298706,\n",
              "  0.6597937941551208,\n",
              "  0.661512017250061,\n",
              "  0.6872852444648743,\n",
              "  0.6872852444648743,\n",
              "  0.6890034079551697,\n",
              "  0.7094594836235046,\n",
              "  0.6477663516998291,\n",
              "  0.7010309100151062,\n",
              "  0.6546391844749451,\n",
              "  0.6786941289901733,\n",
              "  0.69243985414505,\n",
              "  0.6821305751800537,\n",
              "  0.6666666865348816,\n",
              "  0.6752577424049377,\n",
              "  0.7182130813598633,\n",
              "  0.7061855792999268,\n",
              "  0.7010309100151062,\n",
              "  0.6872852444648743,\n",
              "  0.6739864945411682,\n",
              "  0.6701030731201172,\n",
              "  0.69243985414505,\n",
              "  0.668384850025177,\n",
              "  0.7096219658851624,\n",
              "  0.7010309100151062,\n",
              "  0.69243985414505,\n",
              "  0.6838487982749939,\n",
              "  0.6718212962150574,\n",
              "  0.699312686920166,\n",
              "  0.7319587469100952,\n",
              "  0.6735395193099976,\n",
              "  0.7233676910400391,\n",
              "  0.7113401889801025,\n",
              "  0.7130584120750427,\n",
              "  0.6838487982749939,\n",
              "  0.7353951930999756,\n",
              "  0.7319587469100952,\n",
              "  0.6529209613800049,\n",
              "  0.6941580772399902,\n",
              "  0.6875,\n",
              "  0.6890034079551697,\n",
              "  0.6855670213699341,\n",
              "  0.7216494679450989,\n",
              "  0.6907216310501099,\n",
              "  0.6855670213699341,\n",
              "  0.6632302403450012,\n",
              "  0.7079038023948669,\n",
              "  0.6975945234298706,\n",
              "  0.7113401889801025,\n",
              "  0.6941580772399902,\n",
              "  0.7079038023948669,\n",
              "  0.6872852444648743,\n",
              "  0.6786941289901733,\n",
              "  0.6855670213699341,\n",
              "  0.6804123520851135,\n",
              "  0.6907216310501099,\n",
              "  0.6821305751800537,\n",
              "  0.7079038023948669,\n",
              "  0.6821305751800537,\n",
              "  0.6718212962150574,\n",
              "  0.7044673562049866,\n",
              "  0.6580756306648254,\n",
              "  0.7130584120750427,\n",
              "  0.6786941289901733,\n",
              "  0.7027491331100464,\n",
              "  0.7182130813598633,\n",
              "  0.6872852444648743,\n",
              "  0.7096219658851624,\n",
              "  0.6875,\n",
              "  0.668384850025177,\n",
              "  0.7199312448501587,\n",
              "  0.6872852444648743,\n",
              "  0.7044673562049866,\n",
              "  0.6958763003349304,\n",
              "  0.7044673562049866,\n",
              "  0.6718212962150574,\n",
              "  0.7113401889801025,\n",
              "  0.69243985414505,\n",
              "  0.7079038023948669,\n",
              "  0.699312686920166,\n",
              "  0.6975945234298706,\n",
              "  0.7044673562049866,\n",
              "  0.7010309100151062,\n",
              "  0.6890034079551697,\n",
              "  0.661512017250061,\n",
              "  0.6975945234298706,\n",
              "  0.7027491331100464,\n",
              "  0.7079038023948669,\n",
              "  0.7130584120750427,\n",
              "  0.6958763003349304,\n",
              "  0.7010309100151062,\n",
              "  0.7027491331100464,\n",
              "  0.6907216310501099,\n",
              "  0.6872852444648743,\n",
              "  0.7010309100151062,\n",
              "  0.6752577424049377,\n",
              "  0.7233676910400391,\n",
              "  0.7130584120750427,\n",
              "  0.7182130813598633,\n",
              "  0.7319587469100952,\n",
              "  0.7250859141349792,\n",
              "  0.7061855792999268,\n",
              "  0.69243985414505,\n",
              "  0.6907216310501099,\n",
              "  0.6752577424049377,\n",
              "  0.738831639289856,\n",
              "  0.6975945234298706,\n",
              "  0.6821305751800537,\n",
              "  0.6941580772399902,\n",
              "  0.7079038023948669,\n",
              "  0.699312686920166,\n",
              "  0.7250859141349792,\n",
              "  0.7096219658851624,\n",
              "  0.7079038023948669,\n",
              "  0.7044673562049866,\n",
              "  0.7130584120750427,\n",
              "  0.69243985414505,\n",
              "  0.6958763003349304,\n",
              "  0.7405498027801514,\n",
              "  0.6769759654998779,\n",
              "  0.6838487982749939,\n",
              "  0.7491409182548523,\n",
              "  0.7096219658851624,\n",
              "  0.7079038023948669,\n",
              "  0.7044673562049866,\n",
              "  0.7162162065505981,\n",
              "  0.7285223603248596,\n",
              "  0.7147766351699829,\n",
              "  0.699312686920166,\n",
              "  0.7319587469100952,\n",
              "  0.7250859141349792,\n",
              "  0.6907216310501099,\n",
              "  0.6632302403450012,\n",
              "  0.6821305751800537,\n",
              "  0.7147766351699829,\n",
              "  0.7113401889801025,\n",
              "  0.7199312448501587,\n",
              "  0.7164948582649231,\n",
              "  0.7285223603248596,\n",
              "  0.7422680258750916,\n",
              "  0.7079038023948669,\n",
              "  0.6769759654998779,\n",
              "  0.661512017250061,\n",
              "  0.7096219658851624,\n",
              "  0.6804123520851135,\n",
              "  0.6786941289901733,\n",
              "  0.7199312448501587,\n",
              "  0.6872852444648743,\n",
              "  0.6890034079551697,\n",
              "  0.7199312448501587,\n",
              "  0.7164948582649231,\n",
              "  0.6958763003349304,\n",
              "  0.69243985414505,\n",
              "  0.7182130813598633,\n",
              "  0.69243985414505,\n",
              "  0.7130584120750427,\n",
              "  0.6838487982749939,\n",
              "  0.6890034079551697,\n",
              "  0.7044673562049866,\n",
              "  0.7044673562049866,\n",
              "  0.6958763003349304,\n",
              "  0.699312686920166,\n",
              "  0.7044673562049866,\n",
              "  0.7079038023948669,\n",
              "  0.7250859141349792,\n",
              "  0.730240523815155,\n",
              "  0.7043918967247009,\n",
              "  0.6804123520851135,\n",
              "  0.6907216310501099,\n",
              "  0.7199312448501587,\n",
              "  0.6872852444648743,\n",
              "  0.6838487982749939,\n",
              "  0.7199312448501587,\n",
              "  0.7162162065505981,\n",
              "  0.7079038023948669,\n",
              "  0.6821305751800537,\n",
              "  0.7147766351699829,\n",
              "  0.6804123520851135,\n",
              "  0.6786941289901733,\n",
              "  0.7130584120750427,\n",
              "  0.7147766351699829,\n",
              "  0.6907216310501099,\n",
              "  0.6907216310501099,\n",
              "  0.7199312448501587,\n",
              "  0.7079038023948669,\n",
              "  0.7233676910400391,\n",
              "  0.6872852444648743,\n",
              "  0.7130584120750427,\n",
              "  0.7405498027801514,\n",
              "  0.6941580772399902,\n",
              "  0.7044673562049866,\n",
              "  0.6975945234298706,\n",
              "  0.7199312448501587,\n",
              "  0.6975945234298706,\n",
              "  0.7594501972198486,\n",
              "  0.6975945234298706,\n",
              "  0.7422680258750916,\n",
              "  0.7130584120750427,\n",
              "  0.7010309100151062,\n",
              "  0.7439862489700317,\n",
              "  0.6975945234298706,\n",
              "  0.7164948582649231,\n",
              "  0.7285223603248596,\n",
              "  0.7113401889801025,\n",
              "  0.7216494679450989,\n",
              "  0.6666666865348816,\n",
              "  0.7164948582649231,\n",
              "  0.6975945234298706,\n",
              "  0.7250859141349792,\n",
              "  0.7027491331100464,\n",
              "  0.7079038023948669,\n",
              "  0.7027491331100464,\n",
              "  0.6907216310501099,\n",
              "  0.7212837934494019,\n",
              "  0.7319587469100952,\n",
              "  0.699312686920166,\n",
              "  0.7250859141349792,\n",
              "  0.69243985414505,\n",
              "  0.7336769700050354,\n",
              "  0.6891891956329346,\n",
              "  0.7268041372299194,\n",
              "  0.7027491331100464,\n",
              "  0.7199312448501587,\n",
              "  0.6975945234298706,\n",
              "  0.6838487982749939,\n",
              "  0.6890034079551697,\n",
              "  0.6907216310501099,\n",
              "  0.7371134161949158,\n",
              "  0.7353951930999756,\n",
              "  0.738831639289856,\n",
              "  0.7182130813598633,\n",
              "  0.7130584120750427,\n",
              "  0.7250859141349792,\n",
              "  0.7371134161949158,\n",
              "  0.7130584120750427,\n",
              "  0.7216494679450989,\n",
              "  0.7164948582649231,\n",
              "  0.7199312448501587,\n",
              "  0.7147766351699829,\n",
              "  0.7353951930999756,\n",
              "  0.6786941289901733,\n",
              "  0.6975945234298706,\n",
              "  0.6786941289901733,\n",
              "  0.7061855792999268,\n",
              "  0.7199312448501587,\n",
              "  0.7079038023948669,\n",
              "  0.7164948582649231,\n",
              "  0.7096219658851624,\n",
              "  0.7061855792999268,\n",
              "  0.7199312448501587,\n",
              "  0.7233676910400391,\n",
              "  0.7079038023948669,\n",
              "  0.7130584120750427,\n",
              "  0.7113401889801025,\n",
              "  0.7027491331100464,\n",
              "  0.7182130813598633,\n",
              "  0.7061855792999268,\n",
              "  0.7268041372299194,\n",
              "  0.7130584120750427,\n",
              "  0.7216494679450989,\n",
              "  0.6941580772399902,\n",
              "  0.7405498027801514,\n",
              "  0.7199312448501587,\n",
              "  0.7027491331100464,\n",
              "  0.7164948582649231,\n",
              "  0.69243985414505,\n",
              "  0.7079038023948669,\n",
              "  0.7285223603248596,\n",
              "  0.7096219658851624,\n",
              "  0.6907216310501099,\n",
              "  0.730240523815155,\n",
              "  0.7422680258750916,\n",
              "  0.7079038023948669,\n",
              "  0.7147766351699829,\n",
              "  0.7268041372299194,\n",
              "  0.7336769700050354,\n",
              "  0.6872852444648743,\n",
              "  0.7336769700050354,\n",
              "  0.7268041372299194,\n",
              "  0.7079038023948669,\n",
              "  0.7182130813598633,\n",
              "  0.7010309100151062,\n",
              "  0.7079038023948669,\n",
              "  0.7182130813598633,\n",
              "  0.7079038023948669,\n",
              "  0.7010309100151062,\n",
              "  0.7319587469100952,\n",
              "  0.6958763003349304,\n",
              "  0.7164948582649231,\n",
              "  0.7130584120750427,\n",
              "  0.7147766351699829,\n",
              "  0.7216494679450989,\n",
              "  0.7113401889801025,\n",
              "  0.7336769700050354,\n",
              "  0.699312686920166,\n",
              "  0.7371134161949158,\n",
              "  0.7147766351699829,\n",
              "  0.6907216310501099,\n",
              "  0.7061855792999268,\n",
              "  0.6958763003349304,\n",
              "  0.6907216310501099,\n",
              "  0.7268041372299194,\n",
              "  0.7319587469100952,\n",
              "  0.6872852444648743,\n",
              "  0.7233676910400391,\n",
              "  0.7199312448501587,\n",
              "  0.7268041372299194,\n",
              "  0.7353951930999756,\n",
              "  0.7044673562049866,\n",
              "  0.7182130813598633,\n",
              "  0.7285223603248596,\n",
              "  0.7457044720649719,\n",
              "  0.7061855792999268,\n",
              "  0.7233676910400391,\n",
              "  0.6976351141929626,\n",
              "  0.7216494679450989,\n",
              "  0.7371134161949158,\n",
              "  0.7027491331100464,\n",
              "  0.7319587469100952,\n",
              "  0.7130584120750427,\n",
              "  0.699312686920166,\n",
              "  0.7268041372299194,\n",
              "  0.7405498027801514,\n",
              "  0.7439862489700317,\n",
              "  0.7010309100151062,\n",
              "  0.7216494679450989,\n",
              "  0.6975945234298706,\n",
              "  0.7285223603248596,\n",
              "  0.7130584120750427,\n",
              "  0.7285223603248596,\n",
              "  0.6958763003349304,\n",
              "  0.7061855792999268,\n",
              "  0.7233676910400391,\n",
              "  0.7439862489700317,\n",
              "  0.6975945234298706,\n",
              "  0.7353951930999756,\n",
              "  0.7336769700050354,\n",
              "  0.7250859141349792,\n",
              "  0.7233676910400391,\n",
              "  0.7147766351699829,\n",
              "  0.7508590817451477,\n",
              "  0.7199312448501587,\n",
              "  0.7096219658851624,\n",
              "  0.7027491331100464,\n",
              "  0.699312686920166,\n",
              "  0.6958763003349304,\n",
              "  0.7182130813598633,\n",
              "  0.7353951930999756,\n",
              "  0.7096219658851624,\n",
              "  0.7130584120750427,\n",
              "  0.7044673562049866,\n",
              "  0.7268041372299194,\n",
              "  0.730240523815155,\n",
              "  0.7113401889801025,\n",
              "  0.7061855792999268,\n",
              "  0.7216494679450989,\n",
              "  0.7422680258750916,\n",
              "  0.7250859141349792,\n",
              "  0.7199312448501587,\n",
              "  0.7096219658851624,\n",
              "  0.7285223603248596,\n",
              "  0.7491409182548523,\n",
              "  0.730240523815155,\n",
              "  0.7250859141349792,\n",
              "  0.7319587469100952,\n",
              "  0.6958763003349304,\n",
              "  0.7285223603248596,\n",
              "  0.730240523815155,\n",
              "  0.7216494679450989,\n",
              "  0.7079038023948669,\n",
              "  0.7182130813598633,\n",
              "  0.730240523815155,\n",
              "  0.7491409182548523,\n",
              "  0.7457044720649719,\n",
              "  0.7216494679450989,\n",
              "  0.69243985414505,\n",
              "  0.7353951930999756,\n",
              "  0.7010309100151062,\n",
              "  0.7182130813598633,\n",
              "  0.7096219658851624,\n",
              "  0.7268041372299194,\n",
              "  0.7663230299949646,\n",
              "  0.6838487982749939,\n",
              "  0.7179054021835327,\n",
              "  0.7164948582649231,\n",
              "  0.7044673562049866,\n",
              "  0.6872852444648743,\n",
              "  0.7113401889801025,\n",
              "  0.7285223603248596,\n",
              "  0.7336769700050354,\n",
              "  0.6752577424049377,\n",
              "  0.7164948582649231,\n",
              "  0.7250859141349792,\n",
              "  0.6941580772399902,\n",
              "  0.7268041372299194,\n",
              "  0.7113401889801025,\n",
              "  0.7319587469100952,\n",
              "  0.7508590817451477,\n",
              "  0.6890034079551697,\n",
              "  0.7164948582649231,\n",
              "  0.7233676910400391,\n",
              "  0.7250859141349792,\n",
              "  0.7061855792999268,\n",
              "  0.738831639289856,\n",
              "  0.7285223603248596,\n",
              "  0.7113401889801025,\n",
              "  0.7594501972198486,\n",
              "  0.7096219658851624,\n",
              "  0.7233676910400391,\n",
              "  0.7336769700050354,\n",
              "  0.7216494679450989,\n",
              "  0.7199312448501587,\n",
              "  0.7212837934494019,\n",
              "  0.7491409182548523,\n",
              "  0.7336769700050354,\n",
              "  0.7525773048400879,\n",
              "  0.7285223603248596,\n",
              "  0.7199312448501587,\n",
              "  0.7250859141349792,\n",
              "  0.7525773048400879,\n",
              "  0.761168360710144,\n",
              "  0.7164948582649231,\n",
              "  0.7233676910400391,\n",
              "  0.7250859141349792,\n",
              "  0.7130584120750427,\n",
              "  0.7199312448501587,\n",
              "  0.730240523815155,\n",
              "  0.7353951930999756,\n",
              "  0.7319587469100952,\n",
              "  0.7130584120750427,\n",
              "  0.7628865838050842,\n",
              "  0.7164948582649231,\n",
              "  0.7336769700050354,\n",
              "  0.7216494679450989,\n",
              "  0.7353951930999756,\n",
              "  0.7130584120750427,\n",
              "  0.738831639289856,\n",
              "  0.7027491331100464],\n",
              " 'val_loss': [1.052996277809143,\n",
              "  1.0235886573791504,\n",
              "  1.0256913900375366,\n",
              "  1.0180200338363647,\n",
              "  1.06071138381958,\n",
              "  1.0438873767852783,\n",
              "  1.0388067960739136,\n",
              "  1.0293468236923218,\n",
              "  1.04568612575531,\n",
              "  1.0542281866073608,\n",
              "  1.072161078453064,\n",
              "  1.0321062803268433,\n",
              "  1.064915657043457,\n",
              "  1.0566858053207397,\n",
              "  1.06234872341156,\n",
              "  1.064076542854309,\n",
              "  1.0783227682113647,\n",
              "  1.0769222974777222,\n",
              "  1.0514919757843018,\n",
              "  1.0568755865097046,\n",
              "  1.078579306602478,\n",
              "  1.05777108669281,\n",
              "  1.077440857887268,\n",
              "  1.0705865621566772,\n",
              "  1.0493718385696411,\n",
              "  1.0702362060546875,\n",
              "  1.0700870752334595,\n",
              "  1.0712167024612427,\n",
              "  1.0907660722732544,\n",
              "  1.0789936780929565,\n",
              "  1.081058382987976,\n",
              "  1.0807219743728638,\n",
              "  1.0907387733459473,\n",
              "  1.0855761766433716,\n",
              "  1.095781683921814,\n",
              "  1.0782219171524048,\n",
              "  1.0717047452926636,\n",
              "  1.0795494318008423,\n",
              "  1.065171241760254,\n",
              "  1.107599139213562,\n",
              "  1.0606598854064941,\n",
              "  1.0743364095687866,\n",
              "  1.0780292749404907,\n",
              "  1.082269549369812,\n",
              "  1.088998794555664,\n",
              "  1.0886038541793823,\n",
              "  1.078658103942871,\n",
              "  1.0791547298431396,\n",
              "  1.0844427347183228,\n",
              "  1.0776013135910034,\n",
              "  1.0691341161727905,\n",
              "  1.1084305047988892,\n",
              "  1.0877643823623657,\n",
              "  1.0837539434432983,\n",
              "  1.0987834930419922,\n",
              "  1.0878386497497559,\n",
              "  1.0971177816390991,\n",
              "  1.1057826280593872,\n",
              "  1.1096466779708862,\n",
              "  1.1059883832931519,\n",
              "  1.1082810163497925,\n",
              "  1.093358039855957,\n",
              "  1.0997451543807983,\n",
              "  1.0843541622161865,\n",
              "  1.0924869775772095,\n",
              "  1.1026420593261719,\n",
              "  1.0860682725906372,\n",
              "  1.1049778461456299,\n",
              "  1.0864088535308838,\n",
              "  1.0872873067855835,\n",
              "  1.0748928785324097,\n",
              "  1.0896365642547607,\n",
              "  1.0759990215301514,\n",
              "  1.0708907842636108,\n",
              "  1.0784063339233398,\n",
              "  1.0671454668045044,\n",
              "  1.0697957277297974,\n",
              "  1.0678963661193848,\n",
              "  1.0866183042526245,\n",
              "  1.0773462057113647,\n",
              "  1.076880931854248,\n",
              "  1.077722191810608,\n",
              "  1.0963411331176758,\n",
              "  1.0767087936401367,\n",
              "  1.0830987691879272,\n",
              "  1.1105022430419922,\n",
              "  1.0944209098815918,\n",
              "  1.0763214826583862,\n",
              "  1.0719887018203735,\n",
              "  1.0750044584274292,\n",
              "  1.0468471050262451,\n",
              "  1.0612043142318726,\n",
              "  1.0932669639587402,\n",
              "  1.0681488513946533,\n",
              "  1.0576471090316772,\n",
              "  1.0616308450698853,\n",
              "  1.081586241722107,\n",
              "  1.0788143873214722,\n",
              "  1.0663400888442993,\n",
              "  1.0614243745803833,\n",
              "  1.0790683031082153,\n",
              "  1.1013038158416748,\n",
              "  1.0883750915527344,\n",
              "  1.09152352809906,\n",
              "  1.0792787075042725,\n",
              "  1.0486479997634888,\n",
              "  1.0686252117156982,\n",
              "  1.086371660232544,\n",
              "  1.0679255723953247,\n",
              "  1.0849138498306274,\n",
              "  1.0697718858718872,\n",
              "  1.0673540830612183,\n",
              "  1.0737181901931763,\n",
              "  1.0730992555618286,\n",
              "  1.076860785484314,\n",
              "  1.0649816989898682,\n",
              "  1.0560423135757446,\n",
              "  1.0901511907577515,\n",
              "  1.0725971460342407,\n",
              "  1.062427043914795,\n",
              "  1.0870082378387451,\n",
              "  1.064587950706482,\n",
              "  1.058455467224121,\n",
              "  1.0494571924209595,\n",
              "  1.06094229221344,\n",
              "  1.0694447755813599,\n",
              "  1.042518973350525,\n",
              "  1.0700185298919678,\n",
              "  1.0652053356170654,\n",
              "  1.0426584482192993,\n",
              "  1.0524004697799683,\n",
              "  1.0268545150756836,\n",
              "  1.0673179626464844,\n",
              "  1.0517010688781738,\n",
              "  1.0382436513900757,\n",
              "  1.0602022409439087,\n",
              "  1.0388309955596924,\n",
              "  1.0451041460037231,\n",
              "  1.032887578010559,\n",
              "  1.048110008239746,\n",
              "  1.0279330015182495,\n",
              "  1.0541878938674927,\n",
              "  1.0550435781478882,\n",
              "  1.04355788230896,\n",
              "  1.0248643159866333,\n",
              "  1.0401264429092407,\n",
              "  1.0235621929168701,\n",
              "  1.0487948656082153,\n",
              "  1.043645977973938,\n",
              "  1.0100148916244507,\n",
              "  1.0369411706924438,\n",
              "  1.0427511930465698,\n",
              "  1.047057032585144,\n",
              "  1.0305802822113037,\n",
              "  1.0295275449752808,\n",
              "  1.04067063331604,\n",
              "  1.0226086378097534,\n",
              "  1.0410946607589722,\n",
              "  1.0229989290237427,\n",
              "  1.0222657918930054,\n",
              "  1.0449843406677246,\n",
              "  1.026743769645691,\n",
              "  1.0222368240356445,\n",
              "  1.0403891801834106,\n",
              "  1.0223180055618286,\n",
              "  1.0144673585891724,\n",
              "  1.0314496755599976,\n",
              "  1.0405417680740356,\n",
              "  1.0151073932647705,\n",
              "  1.0009689331054688,\n",
              "  1.0009924173355103,\n",
              "  1.0083229541778564,\n",
              "  0.9947083592414856,\n",
              "  1.005785346031189,\n",
              "  1.0107600688934326,\n",
              "  1.0000145435333252,\n",
              "  1.0088549852371216,\n",
              "  1.0256311893463135,\n",
              "  1.0062403678894043,\n",
              "  1.0024508237838745,\n",
              "  1.011060118675232,\n",
              "  1.0154269933700562,\n",
              "  0.9858132004737854,\n",
              "  1.0062675476074219,\n",
              "  0.9707109332084656,\n",
              "  1.0109262466430664,\n",
              "  1.0149370431900024,\n",
              "  1.0042529106140137,\n",
              "  1.0168181657791138,\n",
              "  1.0087658166885376,\n",
              "  1.010806918144226,\n",
              "  1.0117288827896118,\n",
              "  1.0076731443405151,\n",
              "  1.0019360780715942,\n",
              "  0.981174647808075,\n",
              "  1.0021694898605347,\n",
              "  0.9776861071586609,\n",
              "  0.9866716265678406,\n",
              "  0.9782931804656982,\n",
              "  0.9964203834533691,\n",
              "  0.9839346408843994,\n",
              "  1.0018097162246704,\n",
              "  1.007887363433838,\n",
              "  0.9913327097892761,\n",
              "  0.9803085327148438,\n",
              "  0.9977283477783203,\n",
              "  0.982048749923706,\n",
              "  0.9967173933982849,\n",
              "  0.9845533967018127,\n",
              "  0.9971889853477478,\n",
              "  0.9762665629386902,\n",
              "  0.9566087126731873,\n",
              "  0.9648036956787109,\n",
              "  0.9673234820365906,\n",
              "  0.9778976440429688,\n",
              "  0.9736392498016357,\n",
              "  0.9943129420280457,\n",
              "  0.9751367568969727,\n",
              "  0.9578297734260559,\n",
              "  0.9764613509178162,\n",
              "  0.9849992394447327,\n",
              "  0.9658689498901367,\n",
              "  0.9487293362617493,\n",
              "  0.9499251246452332,\n",
              "  0.9867057800292969,\n",
              "  0.9661858677864075,\n",
              "  0.9855794906616211,\n",
              "  0.9737794995307922,\n",
              "  0.9541258811950684,\n",
              "  0.9794440269470215,\n",
              "  0.9603182673454285,\n",
              "  0.9802853465080261,\n",
              "  0.9698763489723206,\n",
              "  0.9725611209869385,\n",
              "  0.9639663696289062,\n",
              "  0.9657260775566101,\n",
              "  0.9653937816619873,\n",
              "  0.9852420687675476,\n",
              "  0.9918068051338196,\n",
              "  0.9690473079681396,\n",
              "  0.97527676820755,\n",
              "  0.9827132225036621,\n",
              "  0.9668664336204529,\n",
              "  0.9703556895256042,\n",
              "  0.9700254797935486,\n",
              "  0.971879780292511,\n",
              "  0.9658252596855164,\n",
              "  0.9756762981414795,\n",
              "  0.9476261138916016,\n",
              "  0.9548377990722656,\n",
              "  0.9659853577613831,\n",
              "  0.9672359824180603,\n",
              "  0.9651332497596741,\n",
              "  0.9528524279594421,\n",
              "  0.9535039067268372,\n",
              "  0.9429122805595398,\n",
              "  0.9579488635063171,\n",
              "  0.9590107798576355,\n",
              "  0.9523658752441406,\n",
              "  0.949451208114624,\n",
              "  0.9705666899681091,\n",
              "  0.9426131248474121,\n",
              "  0.9639378190040588,\n",
              "  0.9533204436302185,\n",
              "  0.9604495167732239,\n",
              "  0.9470846056938171,\n",
              "  0.9575284123420715,\n",
              "  0.9656994342803955,\n",
              "  0.9516363739967346,\n",
              "  0.960608959197998,\n",
              "  0.9347074627876282,\n",
              "  0.9275168776512146,\n",
              "  0.9303838610649109,\n",
              "  0.9520018100738525,\n",
              "  0.9394876956939697,\n",
              "  0.9349431991577148,\n",
              "  0.9400789737701416,\n",
              "  0.9398736953735352,\n",
              "  0.9466304183006287,\n",
              "  0.9396969676017761,\n",
              "  0.9475923180580139,\n",
              "  0.9393094182014465,\n",
              "  0.9157013893127441,\n",
              "  0.9336546063423157,\n",
              "  0.9307144284248352,\n",
              "  0.9390951991081238,\n",
              "  0.9358358979225159,\n",
              "  0.9318515658378601,\n",
              "  0.9179279804229736,\n",
              "  0.9245266914367676,\n",
              "  0.9261150360107422,\n",
              "  0.9332015514373779,\n",
              "  0.9307913184165955,\n",
              "  0.92293381690979,\n",
              "  0.9143336415290833,\n",
              "  0.9150266051292419,\n",
              "  0.9281730651855469,\n",
              "  0.9242985844612122,\n",
              "  0.9179100394248962,\n",
              "  0.9102241396903992,\n",
              "  0.9070839285850525,\n",
              "  0.925098180770874,\n",
              "  0.9181988835334778,\n",
              "  0.9326775670051575,\n",
              "  0.9242475628852844,\n",
              "  0.9250803589820862,\n",
              "  0.9298970699310303,\n",
              "  0.9271923899650574,\n",
              "  0.8956648707389832,\n",
              "  0.9094635844230652,\n",
              "  0.921302855014801,\n",
              "  0.9100727438926697,\n",
              "  0.923778772354126,\n",
              "  0.9037994742393494,\n",
              "  0.9196111559867859,\n",
              "  0.9082673192024231,\n",
              "  0.9157143235206604,\n",
              "  0.9141230583190918,\n",
              "  0.9112899899482727,\n",
              "  0.9131376147270203,\n",
              "  0.885934591293335,\n",
              "  0.9250194430351257,\n",
              "  0.9124905467033386,\n",
              "  0.9198456406593323,\n",
              "  0.8906920552253723,\n",
              "  0.8925995826721191,\n",
              "  0.8956937789916992,\n",
              "  0.9251602292060852,\n",
              "  0.8794403672218323,\n",
              "  0.9043652415275574,\n",
              "  0.8986645340919495,\n",
              "  0.8982880115509033,\n",
              "  0.8784403800964355,\n",
              "  0.9066030383110046,\n",
              "  0.8861856460571289,\n",
              "  0.894673764705658,\n",
              "  0.8865797519683838,\n",
              "  0.8817472457885742,\n",
              "  0.883261501789093,\n",
              "  0.8841790556907654,\n",
              "  0.8866467475891113,\n",
              "  0.9064436554908752,\n",
              "  0.8756301999092102,\n",
              "  0.8992705941200256,\n",
              "  0.8920879364013672,\n",
              "  0.8867760300636292,\n",
              "  0.9024332165718079,\n",
              "  0.8783949017524719,\n",
              "  0.8763487935066223,\n",
              "  0.888716995716095,\n",
              "  0.8784624934196472,\n",
              "  0.8791126608848572,\n",
              "  0.8817961812019348,\n",
              "  0.8881542086601257,\n",
              "  0.892010509967804,\n",
              "  0.8727342486381531,\n",
              "  0.8817302584648132,\n",
              "  0.876802921295166,\n",
              "  0.8948615193367004,\n",
              "  0.8790559768676758,\n",
              "  0.8820837140083313,\n",
              "  0.862324059009552,\n",
              "  0.8946145176887512,\n",
              "  0.8881210684776306,\n",
              "  0.8885278701782227,\n",
              "  0.8852875232696533,\n",
              "  0.8849830627441406,\n",
              "  0.888180673122406,\n",
              "  0.8906919360160828,\n",
              "  0.8732585906982422,\n",
              "  0.8663923144340515,\n",
              "  0.878619372844696,\n",
              "  0.882206916809082,\n",
              "  0.8921439051628113,\n",
              "  0.8973040580749512,\n",
              "  0.88145512342453,\n",
              "  0.8981350064277649,\n",
              "  0.8652012944221497,\n",
              "  0.8787897229194641,\n",
              "  0.8811003565788269,\n",
              "  0.8880906105041504,\n",
              "  0.8835893273353577,\n",
              "  0.8768327832221985,\n",
              "  0.8582879900932312,\n",
              "  0.8781723380088806,\n",
              "  0.8570089340209961,\n",
              "  0.8784778118133545,\n",
              "  0.8799893260002136,\n",
              "  0.880364716053009,\n",
              "  0.8661031723022461,\n",
              "  0.8646721243858337,\n",
              "  0.8511891961097717,\n",
              "  0.8728538155555725,\n",
              "  0.8423672318458557,\n",
              "  0.8693427443504333,\n",
              "  0.8701910972595215,\n",
              "  0.8594085574150085,\n",
              "  0.8747783303260803,\n",
              "  0.8548262715339661,\n",
              "  0.8842952251434326,\n",
              "  0.8737867474555969,\n",
              "  0.8622552752494812,\n",
              "  0.862180769443512,\n",
              "  0.880774974822998,\n",
              "  0.8665489554405212,\n",
              "  0.8737382888793945,\n",
              "  0.8813729286193848,\n",
              "  0.8509617447853088,\n",
              "  0.8585793375968933,\n",
              "  0.8591731190681458,\n",
              "  0.8335252404212952,\n",
              "  0.8701848387718201,\n",
              "  0.8503611087799072,\n",
              "  0.8439791798591614,\n",
              "  0.8350762724876404,\n",
              "  0.8693742156028748,\n",
              "  0.8713273406028748,\n",
              "  0.8590518832206726,\n",
              "  0.8751264214515686,\n",
              "  0.8587848544120789,\n",
              "  0.8574683666229248,\n",
              "  0.8503105044364929,\n",
              "  0.8588852286338806,\n",
              "  0.8681424260139465,\n",
              "  0.8795342445373535,\n",
              "  0.8615615963935852,\n",
              "  0.8727984428405762,\n",
              "  0.8573570251464844,\n",
              "  0.8556177020072937,\n",
              "  0.8757732510566711,\n",
              "  0.8635094165802002,\n",
              "  0.853581964969635,\n",
              "  0.8493902683258057,\n",
              "  0.8496072888374329,\n",
              "  0.8637720942497253,\n",
              "  0.8457737565040588,\n",
              "  0.8637240529060364,\n",
              "  0.8544918894767761,\n",
              "  0.8615553975105286,\n",
              "  0.840642511844635,\n",
              "  0.8348882794380188,\n",
              "  0.859025239944458,\n",
              "  0.8439038395881653,\n",
              "  0.84596186876297,\n",
              "  0.843559980392456,\n",
              "  0.8542852401733398,\n",
              "  0.8533796668052673,\n",
              "  0.838644802570343,\n",
              "  0.8551658987998962,\n",
              "  0.854154646396637,\n",
              "  0.8519346714019775,\n",
              "  0.8450462222099304,\n",
              "  0.8327780365943909,\n",
              "  0.830021321773529,\n",
              "  0.8557133078575134,\n",
              "  0.8353239893913269,\n",
              "  0.8272613883018494,\n",
              "  0.845244824886322,\n",
              "  0.829692542552948,\n",
              "  0.8522126078605652,\n",
              "  0.8270687460899353,\n",
              "  0.8219535946846008,\n",
              "  0.8425949215888977,\n",
              "  0.810356855392456,\n",
              "  0.8514528274536133,\n",
              "  0.8518106341362,\n",
              "  0.8462634086608887,\n",
              "  0.83308345079422,\n",
              "  0.8295273780822754,\n",
              "  0.8237283825874329,\n",
              "  0.831852376461029,\n",
              "  0.8170437812805176,\n",
              "  0.8322262763977051,\n",
              "  0.8413951992988586,\n",
              "  0.8078451156616211,\n",
              "  0.8174024224281311,\n",
              "  0.8324598670005798,\n",
              "  0.8297755122184753,\n",
              "  0.8473260402679443,\n",
              "  0.835212767124176,\n",
              "  0.8271457552909851,\n",
              "  0.837996780872345,\n",
              "  0.8297508358955383,\n",
              "  0.838117778301239,\n",
              "  0.8434599041938782,\n",
              "  0.833625316619873,\n",
              "  0.8278884291648865,\n",
              "  0.8261398673057556,\n",
              "  0.8063104152679443,\n",
              "  0.8304386138916016,\n",
              "  0.8331224322319031,\n",
              "  0.8477768898010254,\n",
              "  0.8306178450584412,\n",
              "  0.8320283889770508,\n",
              "  0.8204011917114258,\n",
              "  0.8314085006713867,\n",
              "  0.8138375282287598,\n",
              "  0.8294073939323425,\n",
              "  0.8112781047821045,\n",
              "  0.8199090957641602,\n",
              "  0.8478887677192688,\n",
              "  0.8143327832221985,\n",
              "  0.8346862196922302,\n",
              "  0.8459782004356384,\n",
              "  0.8222193717956543,\n",
              "  0.8046736717224121,\n",
              "  0.8203033804893494,\n",
              "  0.8241159915924072,\n",
              "  0.8135318756103516,\n",
              "  0.825253963470459,\n",
              "  0.8150967955589294,\n",
              "  0.8295685648918152,\n",
              "  0.8334862589836121,\n",
              "  0.8275812268257141,\n",
              "  0.8117594122886658,\n",
              "  0.8177342414855957,\n",
              "  0.8307264447212219,\n",
              "  0.8221541047096252,\n",
              "  0.8230457901954651,\n",
              "  0.8264162540435791,\n",
              "  0.8211213946342468,\n",
              "  0.817696750164032,\n",
              "  0.8275812268257141,\n",
              "  0.8113400340080261,\n",
              "  0.8219786286354065,\n",
              "  0.8200633525848389,\n",
              "  0.8077287077903748,\n",
              "  0.8153298497200012,\n",
              "  0.8330926895141602,\n",
              "  0.8209323883056641,\n",
              "  0.8239712715148926,\n",
              "  0.8000097274780273,\n",
              "  0.8131317496299744,\n",
              "  0.8198750615119934,\n",
              "  0.7851387858390808,\n",
              "  0.8175172209739685,\n",
              "  0.8099444508552551,\n",
              "  0.8065324425697327,\n",
              "  0.8152483105659485,\n",
              "  0.7933640480041504,\n",
              "  0.7828805446624756,\n",
              "  0.8050715327262878,\n",
              "  0.8123416304588318,\n",
              "  0.7874458432197571,\n",
              "  0.8114089965820312,\n",
              "  0.8052156567573547,\n",
              "  0.7980281710624695,\n",
              "  0.8055452704429626,\n",
              "  0.8200104236602783,\n",
              "  0.8063890337944031,\n",
              "  0.799898624420166,\n",
              "  0.8004493713378906,\n",
              "  0.8114665150642395,\n",
              "  0.7989326119422913,\n",
              "  0.8104701638221741,\n",
              "  0.8014950156211853,\n",
              "  0.8068730235099792,\n",
              "  0.7926194071769714,\n",
              "  0.8040059208869934,\n",
              "  0.7854321599006653,\n",
              "  0.7977846264839172,\n",
              "  0.8018543124198914,\n",
              "  0.8118093013763428,\n",
              "  0.7868300080299377,\n",
              "  0.794428288936615,\n",
              "  0.786710262298584,\n",
              "  0.7941746115684509,\n",
              "  0.7881894111633301,\n",
              "  0.7688438296318054,\n",
              "  0.7988694310188293,\n",
              "  0.7982707023620605,\n",
              "  0.7903333306312561,\n",
              "  0.7846012115478516,\n",
              "  0.7987846732139587,\n",
              "  0.7996838688850403,\n",
              "  0.7846347689628601,\n",
              "  0.7929460406303406,\n",
              "  0.7948706150054932,\n",
              "  0.8107364773750305,\n",
              "  0.8112960457801819,\n",
              "  0.7879374027252197,\n",
              "  0.7977981567382812,\n",
              "  0.8015082478523254,\n",
              "  0.7898790836334229,\n",
              "  0.8038465976715088,\n",
              "  0.7821378111839294,\n",
              "  0.7939176559448242,\n",
              "  0.7987886071205139,\n",
              "  0.8090982437133789,\n",
              "  0.7913861870765686,\n",
              "  0.804126501083374,\n",
              "  0.7993955612182617,\n",
              "  0.7813428044319153,\n",
              "  0.7839198112487793,\n",
              "  0.8040189743041992,\n",
              "  0.8115162253379822,\n",
              "  0.7966022491455078,\n",
              "  0.7943302989006042,\n",
              "  0.7911327481269836,\n",
              "  0.795874834060669,\n",
              "  0.784827470779419,\n",
              "  0.7936699390411377,\n",
              "  0.790743350982666,\n",
              "  0.7809816002845764,\n",
              "  0.7691017985343933,\n",
              "  0.7757658958435059,\n",
              "  0.7835426926612854,\n",
              "  0.7969956994056702,\n",
              "  0.7730532288551331,\n",
              "  0.7538933753967285,\n",
              "  0.7822193503379822,\n",
              "  0.7682971954345703,\n",
              "  0.7928875088691711,\n",
              "  0.7614176869392395,\n",
              "  0.7809979319572449,\n",
              "  0.7850456237792969,\n",
              "  0.7851831912994385,\n",
              "  0.7892935276031494,\n",
              "  0.7819421291351318,\n",
              "  0.783768892288208,\n",
              "  0.7738504409790039,\n",
              "  0.7862390875816345,\n",
              "  0.7776218056678772,\n",
              "  0.7739365696907043,\n",
              "  0.7737615704536438,\n",
              "  0.7677660584449768,\n",
              "  0.7702698707580566,\n",
              "  0.7789180874824524,\n",
              "  0.7824887633323669,\n",
              "  0.7621166110038757,\n",
              "  0.7709290981292725,\n",
              "  0.761967122554779,\n",
              "  0.756074845790863,\n",
              "  0.7827090620994568,\n",
              "  0.7774061560630798,\n",
              "  0.7751355171203613,\n",
              "  0.7672107219696045,\n",
              "  0.7906668782234192,\n",
              "  0.7736583352088928,\n",
              "  0.7827484607696533,\n",
              "  0.7645142674446106,\n",
              "  0.7781922221183777,\n",
              "  0.7863469123840332,\n",
              "  0.7737154960632324,\n",
              "  0.785172700881958,\n",
              "  0.7647753357887268,\n",
              "  0.7475650310516357,\n",
              "  0.7733548283576965,\n",
              "  0.7641146779060364,\n",
              "  0.7617781162261963,\n",
              "  0.7601591944694519,\n",
              "  0.7697954177856445,\n",
              "  0.777308464050293,\n",
              "  0.7608036994934082,\n",
              "  0.7692503929138184,\n",
              "  0.7686693072319031,\n",
              "  0.7722897529602051,\n",
              "  0.7679861187934875,\n",
              "  0.778754711151123,\n",
              "  0.7650619149208069,\n",
              "  0.7710702419281006,\n",
              "  0.7605265974998474,\n",
              "  0.7734496593475342,\n",
              "  0.7469317317008972,\n",
              "  0.7606959939002991,\n",
              "  0.7605820298194885,\n",
              "  0.7775974273681641,\n",
              "  0.7673861980438232,\n",
              "  0.762584924697876,\n",
              "  0.775035560131073,\n",
              "  0.7674354910850525,\n",
              "  0.7792630195617676,\n",
              "  0.7720471024513245,\n",
              "  0.7748929858207703,\n",
              "  0.7756474614143372,\n",
              "  0.7826319336891174,\n",
              "  0.755265474319458,\n",
              "  0.7777689099311829,\n",
              "  0.7751521468162537,\n",
              "  0.7648358345031738,\n",
              "  0.7714870572090149,\n",
              "  0.757439136505127,\n",
              "  0.7723572254180908,\n",
              "  0.766890823841095,\n",
              "  0.7542456984519958,\n",
              "  0.7757351398468018,\n",
              "  0.7651040554046631,\n",
              "  0.7582077980041504,\n",
              "  0.7748228907585144,\n",
              "  0.7718427777290344,\n",
              "  0.7757067084312439,\n",
              "  0.7728365063667297,\n",
              "  0.7779585719108582,\n",
              "  0.7723445892333984,\n",
              "  0.7806391716003418,\n",
              "  0.7610416412353516,\n",
              "  0.7656958699226379,\n",
              "  0.7655675411224365,\n",
              "  0.7784772515296936,\n",
              "  0.7801622748374939,\n",
              "  0.7791709899902344,\n",
              "  0.7574393153190613,\n",
              "  0.7779703140258789,\n",
              "  0.7763721942901611,\n",
              "  0.751392662525177,\n",
              "  0.7564980983734131,\n",
              "  0.7667300701141357,\n",
              "  0.7641162872314453,\n",
              "  0.7564340233802795,\n",
              "  0.7539737224578857,\n",
              "  0.7712655663490295,\n",
              "  0.7558247447013855,\n",
              "  0.7737686038017273,\n",
              "  0.7692141532897949,\n",
              "  0.7511206269264221,\n",
              "  0.7579954266548157,\n",
              "  0.7567276954650879,\n",
              "  0.7652011513710022,\n",
              "  0.7706639170646667,\n",
              "  0.7726048827171326,\n",
              "  0.7574194073677063,\n",
              "  0.7710778117179871,\n",
              "  0.7787430286407471,\n",
              "  0.7623913884162903,\n",
              "  0.7698626518249512,\n",
              "  0.7615202069282532,\n",
              "  0.7668388485908508,\n",
              "  0.7629847526550293,\n",
              "  0.754343569278717,\n",
              "  0.7668140530586243,\n",
              "  0.7672553062438965,\n",
              "  0.7456355690956116,\n",
              "  0.7530342936515808,\n",
              "  0.7433280944824219,\n",
              "  0.7591186165809631,\n",
              "  0.7606884837150574,\n",
              "  0.7409317493438721,\n",
              "  0.7582200169563293,\n",
              "  0.7432662844657898,\n",
              "  0.754284143447876,\n",
              "  0.7464151978492737,\n",
              "  0.7535340189933777,\n",
              "  0.7380101680755615,\n",
              "  0.7367229461669922,\n",
              "  0.7455687522888184,\n",
              "  0.7509310841560364,\n",
              "  0.7452059388160706,\n",
              "  0.7579860091209412,\n",
              "  0.7451375126838684,\n",
              "  0.7252526879310608,\n",
              "  0.7433741092681885,\n",
              "  0.7239324450492859,\n",
              "  0.7394556999206543,\n",
              "  0.7289392352104187,\n",
              "  0.7474362254142761,\n",
              "  0.7364117503166199,\n",
              "  0.7410114407539368,\n",
              "  0.7444607615470886,\n",
              "  0.7372997403144836,\n",
              "  0.7251472473144531,\n",
              "  0.7375635504722595,\n",
              "  0.7404654622077942,\n",
              "  0.730442225933075,\n",
              "  0.7492769360542297,\n",
              "  0.7359674572944641,\n",
              "  0.7642390727996826,\n",
              "  0.7590433955192566,\n",
              "  0.7517045140266418,\n",
              "  0.7354879379272461,\n",
              "  0.748077929019928,\n",
              "  0.7543771862983704,\n",
              "  0.7569904327392578,\n",
              "  0.7539542317390442,\n",
              "  0.7598111629486084,\n",
              "  0.7577643990516663,\n",
              "  0.7313728332519531,\n",
              "  0.7411121726036072,\n",
              "  0.7395280003547668,\n",
              "  0.7519583106040955,\n",
              "  0.7331449389457703,\n",
              "  0.7436012625694275,\n",
              "  0.7520881295204163,\n",
              "  0.7310685515403748,\n",
              "  0.7454743385314941,\n",
              "  0.7445980906486511,\n",
              "  0.7545046806335449,\n",
              "  0.736886739730835,\n",
              "  0.7384039759635925,\n",
              "  0.7268388271331787,\n",
              "  0.7251833081245422,\n",
              "  0.732539176940918,\n",
              "  0.753777265548706,\n",
              "  0.7501227855682373,\n",
              "  0.7359979748725891,\n",
              "  0.7525221705436707,\n",
              "  0.7560389637947083,\n",
              "  0.743953287601471,\n",
              "  0.7269724011421204,\n",
              "  0.7513197064399719,\n",
              "  0.7457540035247803,\n",
              "  0.7382262349128723,\n",
              "  0.7385387420654297,\n",
              "  0.7351271510124207,\n",
              "  0.737044632434845,\n",
              "  0.7285268902778625,\n",
              "  0.720980167388916,\n",
              "  0.7506939768791199,\n",
              "  0.7472071647644043,\n",
              "  0.7294425964355469,\n",
              "  0.7362678647041321,\n",
              "  0.7302486300468445,\n",
              "  0.7279793620109558,\n",
              "  0.7318213582038879,\n",
              "  0.7297211289405823,\n",
              "  0.745344877243042,\n",
              "  0.7316309809684753,\n",
              "  0.730449914932251,\n",
              "  0.7457688450813293,\n",
              "  0.7282805442810059,\n",
              "  0.7179903984069824,\n",
              "  0.7464141845703125,\n",
              "  0.7417978644371033,\n",
              "  0.7437906265258789,\n",
              "  0.7416204810142517,\n",
              "  0.7478938102722168,\n",
              "  0.7396063804626465,\n",
              "  0.728922426700592,\n",
              "  0.743046760559082,\n",
              "  0.7358881831169128,\n",
              "  0.734790027141571,\n",
              "  0.736029326915741,\n",
              "  0.7444806098937988,\n",
              "  0.7394905090332031,\n",
              "  0.738900899887085,\n",
              "  0.7287194132804871,\n",
              "  0.7384061813354492,\n",
              "  0.7537224888801575,\n",
              "  0.7397362589836121,\n",
              "  0.7501260638237,\n",
              "  0.7507030367851257,\n",
              "  0.738124430179596,\n",
              "  0.7297084331512451,\n",
              "  0.7517154812812805,\n",
              "  0.728339433670044,\n",
              "  0.7408227324485779,\n",
              "  0.7255141735076904,\n",
              "  0.734622061252594,\n",
              "  0.7373831868171692,\n",
              "  0.737884521484375,\n",
              "  0.7479822039604187,\n",
              "  0.7459221482276917,\n",
              "  0.7306827902793884,\n",
              "  0.7366666793823242,\n",
              "  0.7223143577575684,\n",
              "  0.7471320629119873,\n",
              "  0.7295714020729065,\n",
              "  0.7204396724700928,\n",
              "  0.7364910244941711,\n",
              "  0.7369512915611267,\n",
              "  0.7267723679542542,\n",
              "  0.7372040152549744,\n",
              "  0.73923259973526,\n",
              "  0.7419483065605164,\n",
              "  0.7289897799491882,\n",
              "  0.7293921113014221,\n",
              "  0.7232663631439209,\n",
              "  0.7315866351127625,\n",
              "  0.734466552734375,\n",
              "  0.7336554527282715,\n",
              "  0.7355009913444519,\n",
              "  0.7480562329292297,\n",
              "  0.7395203113555908,\n",
              "  0.7613760828971863,\n",
              "  0.7335121631622314,\n",
              "  0.7330366969108582,\n",
              "  0.7074916958808899,\n",
              "  0.7377598881721497,\n",
              "  0.7190520763397217,\n",
              "  0.7282586097717285,\n",
              "  0.7249810695648193,\n",
              "  0.7274495959281921,\n",
              "  0.7266198992729187,\n",
              "  0.7351231575012207,\n",
              "  0.7281470894813538,\n",
              "  0.7310542464256287,\n",
              "  0.7133061289787292,\n",
              "  0.714735209941864,\n",
              "  0.7320236563682556,\n",
              "  0.7172020077705383,\n",
              "  0.7184829115867615,\n",
              "  0.745518147945404,\n",
              "  0.7171054482460022,\n",
              "  0.716635525226593,\n",
              "  0.740389883518219,\n",
              "  0.7382893562316895,\n",
              "  0.7375244498252869,\n",
              "  0.7308425307273865,\n",
              "  0.7198197245597839,\n",
              "  0.731433093547821,\n",
              "  0.7175590395927429,\n",
              "  0.7244955897331238,\n",
              "  0.7297422289848328,\n",
              "  0.7384836077690125,\n",
              "  0.6983594298362732,\n",
              "  0.6981573104858398,\n",
              "  0.7203412055969238,\n",
              "  0.7186340689659119,\n",
              "  0.7184391021728516,\n",
              "  0.7285042405128479,\n",
              "  0.7337875366210938,\n",
              "  0.7316680550575256,\n",
              "  0.7220730781555176,\n",
              "  0.7170073986053467,\n",
              "  0.7245437502861023,\n",
              "  0.7257630825042725,\n",
              "  0.6982057094573975,\n",
              "  0.7128207683563232,\n",
              "  0.7146692872047424,\n",
              "  0.7192843556404114,\n",
              "  0.6979489922523499,\n",
              "  0.7276909947395325,\n",
              "  0.7078582644462585,\n",
              "  0.7134392261505127,\n",
              "  0.7176602482795715,\n",
              "  0.7096427083015442,\n",
              "  0.7140149474143982,\n",
              "  0.7232471108436584,\n",
              "  0.7097297310829163,\n",
              "  0.711414635181427,\n",
              "  0.7254295945167542,\n",
              "  0.7047303318977356,\n",
              "  0.7201763987541199,\n",
              "  0.7062451243400574,\n",
              "  0.711956799030304,\n",
              "  0.7148855328559875,\n",
              "  0.7195019721984863,\n",
              "  0.7108554840087891,\n",
              "  0.6984798312187195,\n",
              "  0.7238901257514954,\n",
              "  0.693730354309082,\n",
              "  0.7040180563926697,\n",
              "  0.7104327082633972,\n",
              "  0.7137060165405273,\n",
              "  0.7131319046020508,\n",
              "  0.7152963280677795,\n",
              "  0.7186210751533508,\n",
              "  0.7170476913452148,\n",
              "  0.7184140086174011,\n",
              "  0.7030046582221985,\n",
              "  0.7140603065490723,\n",
              "  0.7322831153869629,\n",
              "  0.7251529693603516,\n",
              "  0.722928524017334,\n",
              "  0.7039119601249695,\n",
              "  0.7187142372131348,\n",
              "  0.710519015789032,\n",
              "  0.7089187502861023,\n",
              "  0.7349184155464172,\n",
              "  0.7340700626373291,\n",
              "  0.7114720940589905,\n",
              "  0.7132146954536438,\n",
              "  0.7138633131980896,\n",
              "  0.7175018191337585,\n",
              "  0.7372866272926331,\n",
              "  0.7155542969703674,\n",
              "  0.7025101780891418,\n",
              "  0.7255554795265198,\n",
              "  0.704091489315033,\n",
              "  0.71405029296875,\n",
              "  0.7212510108947754,\n",
              "  0.7259463667869568,\n",
              "  0.7068580985069275,\n",
              "  0.7267751693725586,\n",
              "  0.7191457748413086,\n",
              "  0.7084827423095703,\n",
              "  0.6937898993492126,\n",
              "  0.704156219959259,\n",
              "  0.7246236801147461,\n",
              "  0.73291015625,\n",
              "  0.7253051400184631,\n",
              "  0.733925998210907,\n",
              "  0.7052345275878906,\n",
              "  0.7189013957977295,\n",
              "  0.713839590549469,\n",
              "  0.7199208736419678,\n",
              "  0.713539183139801,\n",
              "  0.7300052642822266,\n",
              "  0.7234571576118469,\n",
              "  0.7242187857627869,\n",
              "  0.718754231929779,\n",
              "  0.7252900004386902,\n",
              "  0.7138712406158447,\n",
              "  0.7024975419044495,\n",
              "  0.7199293971061707,\n",
              "  0.7162263989448547,\n",
              "  0.7023648619651794,\n",
              "  0.712902307510376,\n",
              "  0.6947093605995178,\n",
              "  0.7010364532470703,\n",
              "  0.7028040885925293],\n",
              " 'val_acc': [0.40625,\n",
              "  0.375,\n",
              "  0.3333333432674408,\n",
              "  0.34375,\n",
              "  0.3020833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3229166567325592,\n",
              "  0.3125,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3020833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.3020833432674408,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.2708333432674408,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.2708333432674408,\n",
              "  0.28125,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.28125,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.25,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2604166567325592,\n",
              "  0.2395833283662796,\n",
              "  0.2708333432674408,\n",
              "  0.2291666716337204,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2291666716337204,\n",
              "  0.25,\n",
              "  0.2291666716337204,\n",
              "  0.21875,\n",
              "  0.2291666716337204,\n",
              "  0.21875,\n",
              "  0.2395833283662796,\n",
              "  0.2604166567325592,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2604166567325592,\n",
              "  0.21875,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2291666716337204,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2291666716337204,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.2291666716337204,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2291666716337204,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.1979166716337204,\n",
              "  0.2083333283662796,\n",
              "  0.2291666716337204,\n",
              "  0.2291666716337204,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.2291666716337204,\n",
              "  0.2291666716337204,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2291666716337204,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2708333432674408,\n",
              "  0.2291666716337204,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2291666716337204,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2291666716337204,\n",
              "  0.2708333432674408,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2395833283662796,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2604166567325592,\n",
              "  0.2604166567325592,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2708333432674408,\n",
              "  0.25,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.28125,\n",
              "  0.2604166567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2604166567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2916666567325592,\n",
              "  0.2916666567325592,\n",
              "  0.2395833283662796,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.2708333432674408,\n",
              "  0.2916666567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.2916666567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2708333432674408,\n",
              "  0.28125,\n",
              "  0.2604166567325592,\n",
              "  0.2604166567325592,\n",
              "  0.2708333432674408,\n",
              "  0.2604166567325592,\n",
              "  0.25,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.28125,\n",
              "  0.3020833432674408,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.2916666567325592,\n",
              "  0.2916666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.2916666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3020833432674408,\n",
              "  0.3229166567325592,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.3125,\n",
              "  0.3020833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3229166567325592,\n",
              "  0.3020833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.28125,\n",
              "  0.2916666567325592,\n",
              "  0.2916666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.2916666567325592,\n",
              "  0.3020833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3229166567325592,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.3229166567325592,\n",
              "  0.3333333432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.3333333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3020833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3125,\n",
              "  0.3229166567325592,\n",
              "  0.3020833432674408,\n",
              "  0.34375,\n",
              "  0.3229166567325592,\n",
              "  0.2916666567325592,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3020833432674408,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.3333333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3333333432674408,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3541666567325592,\n",
              "  0.375,\n",
              "  0.3229166567325592,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.3333333432674408,\n",
              "  0.3229166567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3333333432674408,\n",
              "  0.3541666567325592,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3229166567325592,\n",
              "  0.3229166567325592,\n",
              "  0.3333333432674408,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.3333333432674408,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.34375,\n",
              "  0.3333333432674408,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3229166567325592,\n",
              "  0.3333333432674408,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.375,\n",
              "  0.3229166567325592,\n",
              "  0.375,\n",
              "  0.3229166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3229166567325592,\n",
              "  0.3333333432674408,\n",
              "  0.3125,\n",
              "  0.3229166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3958333432674408,\n",
              "  0.3333333432674408,\n",
              "  0.3229166567325592,\n",
              "  0.3020833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.34375,\n",
              "  0.3333333432674408,\n",
              "  0.3229166567325592,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3541666567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3333333432674408,\n",
              "  0.34375,\n",
              "  0.3645833432674408,\n",
              "  0.3333333432674408,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.3541666567325592,\n",
              "  0.3645833432674408,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.3645833432674408,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3645833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3854166567325592,\n",
              "  0.3645833432674408,\n",
              "  0.3645833432674408,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3541666567325592,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.3854166567325592,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.375,\n",
              "  0.3958333432674408,\n",
              "  0.3854166567325592,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.3854166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.3958333432674408,\n",
              "  0.40625,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.3958333432674408,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.3854166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.40625,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4895833432674408,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4166666567325592,\n",
              "  0.3958333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.4791666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4166666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.4166666567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.40625,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.5,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.46875,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4479166567325592,\n",
              "  0.4479166567325592,\n",
              "  0.46875,\n",
              "  0.4270833432674408,\n",
              "  0.4375,\n",
              "  0.4375,\n",
              "  0.46875,\n",
              "  0.4479166567325592,\n",
              "  0.4270833432674408,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.4583333432674408,\n",
              "  0.4375,\n",
              "  0.4479166567325592,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.4583333432674408,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.4583333432674408,\n",
              "  0.4479166567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.46875,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.4270833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.5208333134651184,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4583333432674408,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.4895833432674408,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.4791666567325592,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5520833134651184,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.4791666567325592,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.4791666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5729166865348816,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5625,\n",
              "  0.5104166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184]}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "6afb3408-57b1-4b23-ad87-b5e882d54ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "6b1e3f64-fc01-4780-c150-a4dd65ceff89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c1b0343-fbc7-4694-b18a-7f011503e046\", \"2Class.h5\", 16605968)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}