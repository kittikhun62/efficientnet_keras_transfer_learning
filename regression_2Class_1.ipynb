{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNagRIcVEZ5rchflo3CXIHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "4d32f103-80c2-445f-968c-c8e525532e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "f3e44bc9-f7d6-4795-e016-f4f4a3864e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d0315a6-4277-46a6-b246-756605bf0661\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d0315a6-4277-46a6-b246-756605bf0661')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d0315a6-4277-46a6-b246-756605bf0661 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d0315a6-4277-46a6-b246-756605bf0661');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "8d28e42c-1f5f-43c2-9d73-cfa374280c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "2ef78529-71e1-45a4-c424-fb9143b8ef68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "5240372a-1fcc-40de-d6c4-0c4ea39097ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "44bd9069-f521-4434-b798-2220672f684c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "eaace5a9-8afb-4521-e6d5-49c2e1087bee"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "ac06a42c-b0fd-41bb-97e7-bb316c49f4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "8517947b-1ae5-4de1-f210-f2a42280adc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 75, 75, 32)   864         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 75, 75, 32)  128         ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_49 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_16 (Depthwise  (None, 75, 75, 32)  288         ['swish_49[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 75, 75, 32)  128         ['depthwise_conv2d_16[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_50 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_16 (Lambda)             (None, 1, 1, 32)     0           ['swish_50[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 1, 1, 8)      264         ['lambda_16[0][0]']              \n",
            "                                                                                                  \n",
            " swish_51 (Swish)               (None, 1, 1, 8)      0           ['conv2d_66[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 1, 1, 32)     288         ['swish_51[0][0]']               \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 1, 1, 32)     0           ['conv2d_67[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_16 (Multiply)         (None, 75, 75, 32)   0           ['activation_16[0][0]',          \n",
            "                                                                  'swish_50[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 75, 75, 16)   512         ['multiply_16[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 75, 75, 16)  64          ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 75, 75, 96)   1536        ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 75, 75, 96)  384         ['conv2d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_52 (Swish)               (None, 75, 75, 96)   0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_17 (Depthwise  (None, 38, 38, 96)  864         ['swish_52[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 38, 38, 96)  384         ['depthwise_conv2d_17[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_53 (Swish)               (None, 38, 38, 96)   0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_17 (Lambda)             (None, 1, 1, 96)     0           ['swish_53[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 1, 1, 4)      388         ['lambda_17[0][0]']              \n",
            "                                                                                                  \n",
            " swish_54 (Swish)               (None, 1, 1, 4)      0           ['conv2d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 1, 1, 96)     480         ['swish_54[0][0]']               \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 1, 1, 96)     0           ['conv2d_71[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_17 (Multiply)         (None, 38, 38, 96)   0           ['activation_17[0][0]',          \n",
            "                                                                  'swish_53[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 38, 38, 24)   2304        ['multiply_17[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 38, 38, 24)  96          ['conv2d_72[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 38, 38, 144)  3456        ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 38, 38, 144)  576        ['conv2d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_55 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_18 (Depthwise  (None, 38, 38, 144)  1296       ['swish_55[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 38, 38, 144)  576        ['depthwise_conv2d_18[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_56 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_18 (Lambda)             (None, 1, 1, 144)    0           ['swish_56[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_18[0][0]']              \n",
            "                                                                                                  \n",
            " swish_57 (Swish)               (None, 1, 1, 6)      0           ['conv2d_74[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_57[0][0]']               \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 1, 1, 144)    0           ['conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_18 (Multiply)         (None, 38, 38, 144)  0           ['activation_18[0][0]',          \n",
            "                                                                  'swish_56[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_18[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 38, 38, 24)  96          ['conv2d_76[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_9 (DropConnect)   (None, 38, 38, 24)   0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 38, 38, 24)   0           ['drop_connect_9[0][0]',         \n",
            "                                                                  'batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 38, 38, 144)  3456        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 38, 38, 144)  576        ['conv2d_77[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_58 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_19 (Depthwise  (None, 19, 19, 144)  3600       ['swish_58[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_19[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_59 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_19 (Lambda)             (None, 1, 1, 144)    0           ['swish_59[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_19[0][0]']              \n",
            "                                                                                                  \n",
            " swish_60 (Swish)               (None, 1, 1, 6)      0           ['conv2d_78[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_60[0][0]']               \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 1, 1, 144)    0           ['conv2d_79[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_19 (Multiply)         (None, 19, 19, 144)  0           ['activation_19[0][0]',          \n",
            "                                                                  'swish_59[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_19[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 19, 19, 40)  160         ['conv2d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 19, 19, 240)  960        ['conv2d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_61 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_20 (Depthwise  (None, 19, 19, 240)  6000       ['swish_61[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_20[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_62 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_20 (Lambda)             (None, 1, 1, 240)    0           ['swish_62[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_20[0][0]']              \n",
            "                                                                                                  \n",
            " swish_63 (Swish)               (None, 1, 1, 10)     0           ['conv2d_82[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_63[0][0]']               \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 1, 1, 240)    0           ['conv2d_83[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_20 (Multiply)         (None, 19, 19, 240)  0           ['activation_20[0][0]',          \n",
            "                                                                  'swish_62[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_20[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 19, 19, 40)  160         ['conv2d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_10 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_10[0][0]',        \n",
            "                                                                  'batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 19, 19, 240)  9600        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 19, 19, 240)  960        ['conv2d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_64 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_21 (Depthwise  (None, 10, 10, 240)  2160       ['swish_64[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_21[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_65 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_21 (Lambda)             (None, 1, 1, 240)    0           ['swish_65[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_21[0][0]']              \n",
            "                                                                                                  \n",
            " swish_66 (Swish)               (None, 1, 1, 10)     0           ['conv2d_86[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_66[0][0]']               \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 1, 1, 240)    0           ['conv2d_87[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_21 (Multiply)         (None, 10, 10, 240)  0           ['activation_21[0][0]',          \n",
            "                                                                  'swish_65[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_21[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 10, 10, 80)  320         ['conv2d_88[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_89[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_67 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_22 (Depthwise  (None, 10, 10, 480)  4320       ['swish_67[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_22[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_68 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_22 (Lambda)             (None, 1, 1, 480)    0           ['swish_68[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_22[0][0]']              \n",
            "                                                                                                  \n",
            " swish_69 (Swish)               (None, 1, 1, 20)     0           ['conv2d_90[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_69[0][0]']               \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 1, 1, 480)    0           ['conv2d_91[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_22 (Multiply)         (None, 10, 10, 480)  0           ['activation_22[0][0]',          \n",
            "                                                                  'swish_68[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_22[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 10, 10, 80)  320         ['conv2d_92[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_11 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_11[0][0]',        \n",
            "                                                                  'batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 10, 10, 480)  38400       ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_93[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_70 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_23 (Depthwise  (None, 10, 10, 480)  4320       ['swish_70[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_23[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_71 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_23 (Lambda)             (None, 1, 1, 480)    0           ['swish_71[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_23[0][0]']              \n",
            "                                                                                                  \n",
            " swish_72 (Swish)               (None, 1, 1, 20)     0           ['conv2d_94[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_72[0][0]']               \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 1, 1, 480)    0           ['conv2d_95[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_23 (Multiply)         (None, 10, 10, 480)  0           ['activation_23[0][0]',          \n",
            "                                                                  'swish_71[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_96 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_23[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 10, 10, 80)  320         ['conv2d_96[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_12 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_12[0][0]',        \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_97 (Conv2D)             (None, 10, 10, 480)  38400       ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_97[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_73 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_24 (Depthwise  (None, 10, 10, 480)  12000      ['swish_73[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_24[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_74 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_24 (Lambda)             (None, 1, 1, 480)    0           ['swish_74[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_98 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_24[0][0]']              \n",
            "                                                                                                  \n",
            " swish_75 (Swish)               (None, 1, 1, 20)     0           ['conv2d_98[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_75[0][0]']               \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 1, 1, 480)    0           ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_24 (Multiply)         (None, 10, 10, 480)  0           ['activation_24[0][0]',          \n",
            "                                                                  'swish_74[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_24[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 10, 10, 112)  448        ['conv2d_100[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_101[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_76 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_25 (Depthwise  (None, 10, 10, 672)  16800      ['swish_76[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_25[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_77 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_25 (Lambda)             (None, 1, 1, 672)    0           ['swish_77[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_25[0][0]']              \n",
            "                                                                                                  \n",
            " swish_78 (Swish)               (None, 1, 1, 28)     0           ['conv2d_102[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_78[0][0]']               \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 1, 1, 672)    0           ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_25 (Multiply)         (None, 10, 10, 672)  0           ['activation_25[0][0]',          \n",
            "                                                                  'swish_77[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_25[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 10, 10, 112)  448        ['conv2d_104[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_13 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_13[0][0]',        \n",
            "                                                                  'batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 10, 10, 672)  75264       ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_105[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_79 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_26 (Depthwise  (None, 10, 10, 672)  16800      ['swish_79[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_26[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_80 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_26 (Lambda)             (None, 1, 1, 672)    0           ['swish_80[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_26[0][0]']              \n",
            "                                                                                                  \n",
            " swish_81 (Swish)               (None, 1, 1, 28)     0           ['conv2d_106[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_81[0][0]']               \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 1, 1, 672)    0           ['conv2d_107[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_26 (Multiply)         (None, 10, 10, 672)  0           ['activation_26[0][0]',          \n",
            "                                                                  'swish_80[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_26[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 10, 10, 112)  448        ['conv2d_108[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_14 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_14[0][0]',        \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_109 (Conv2D)            (None, 10, 10, 672)  75264       ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_109[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_82 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_27 (Depthwise  (None, 5, 5, 672)   16800       ['swish_82[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_27[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_83 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_27 (Lambda)             (None, 1, 1, 672)    0           ['swish_83[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_110 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_27[0][0]']              \n",
            "                                                                                                  \n",
            " swish_84 (Swish)               (None, 1, 1, 28)     0           ['conv2d_110[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_111 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_84[0][0]']               \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 1, 1, 672)    0           ['conv2d_111[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_27 (Multiply)         (None, 5, 5, 672)    0           ['activation_27[0][0]',          \n",
            "                                                                  'swish_83[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_112 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_27[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 5, 5, 192)   768         ['conv2d_112[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_113 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_113[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_85 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_28 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_85[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_28[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_86 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_28 (Lambda)             (None, 1, 1, 1152)   0           ['swish_86[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_28[0][0]']              \n",
            "                                                                                                  \n",
            " swish_87 (Swish)               (None, 1, 1, 48)     0           ['conv2d_114[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_87[0][0]']               \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_115[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_28 (Multiply)         (None, 5, 5, 1152)   0           ['activation_28[0][0]',          \n",
            "                                                                  'swish_86[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_28[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 5, 5, 192)   768         ['conv2d_116[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_15 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_15[0][0]',        \n",
            "                                                                  'batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_117 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_117[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_88 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_29 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_88[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_29[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_89 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_29 (Lambda)             (None, 1, 1, 1152)   0           ['swish_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_118 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_29[0][0]']              \n",
            "                                                                                                  \n",
            " swish_90 (Swish)               (None, 1, 1, 48)     0           ['conv2d_118[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_119 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_90[0][0]']               \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_119[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_29 (Multiply)         (None, 5, 5, 1152)   0           ['activation_29[0][0]',          \n",
            "                                                                  'swish_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_120 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_29[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 5, 5, 192)   768         ['conv2d_120[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_16 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_16[0][0]',        \n",
            "                                                                  'add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_121 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_121[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_91 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_30 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_91[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_30[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_92 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_30 (Lambda)             (None, 1, 1, 1152)   0           ['swish_92[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_122 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_30[0][0]']              \n",
            "                                                                                                  \n",
            " swish_93 (Swish)               (None, 1, 1, 48)     0           ['conv2d_122[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_93[0][0]']               \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_123[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_30 (Multiply)         (None, 5, 5, 1152)   0           ['activation_30[0][0]',          \n",
            "                                                                  'swish_92[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_30[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 5, 5, 192)   768         ['conv2d_124[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_17 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_17[0][0]',        \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_94 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_125[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_94 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_31 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_94[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_95 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_31[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_95 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_31 (Lambda)             (None, 1, 1, 1152)   0           ['swish_95[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_31[0][0]']              \n",
            "                                                                                                  \n",
            " swish_96 (Swish)               (None, 1, 1, 48)     0           ['conv2d_126[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_96[0][0]']               \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_127[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_31 (Multiply)         (None, 5, 5, 1152)   0           ['activation_31[0][0]',          \n",
            "                                                                  'swish_95[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_31[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_128[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_129[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_97 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "b86fe1ae-883a-4aa0-c464-8b018f8e4a07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "aafb172d-420c-4505-d6da-90310da3e894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "b71f7655-26f8-495d-9ebc-f30490c49e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(lr=2e-5),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced7ca7d-70b3-4b61-bd21-c74b6f090cb1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "<ipython-input-61-5a61ec28a7f4>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 19s 131ms/step - loss: 0.5071 - mse: 0.5071 - val_loss: 0.3562 - val_mse: 0.3562\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4476 - mse: 0.4476 - val_loss: 0.2022 - val_mse: 0.2022\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.3705 - mse: 0.3705 - val_loss: 0.0845 - val_mse: 0.0845\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.3540 - mse: 0.3540 - val_loss: 0.2977 - val_mse: 0.2977\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.3297 - mse: 0.3297 - val_loss: 0.2201 - val_mse: 0.2201\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.3063 - mse: 0.3063 - val_loss: 0.2249 - val_mse: 0.2249\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2862 - mse: 0.2862 - val_loss: 0.2567 - val_mse: 0.2567\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2805 - mse: 0.2805 - val_loss: 0.2637 - val_mse: 0.2637\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2599 - mse: 0.2599 - val_loss: 0.1737 - val_mse: 0.1737\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.1943 - val_mse: 0.1943\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2349 - mse: 0.2349 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.1757 - val_mse: 0.1757\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2491 - mse: 0.2491 - val_loss: 0.2418 - val_mse: 0.2418\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2350 - mse: 0.2350 - val_loss: 0.2183 - val_mse: 0.2183\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2223 - mse: 0.2223 - val_loss: 0.2068 - val_mse: 0.2068\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2347 - mse: 0.2347 - val_loss: 0.1984 - val_mse: 0.1984\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2331 - mse: 0.2331 - val_loss: 0.1812 - val_mse: 0.1812\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2388 - mse: 0.2388 - val_loss: 0.1671 - val_mse: 0.1671\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2262 - mse: 0.2262 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2305 - mse: 0.2305 - val_loss: 0.2407 - val_mse: 0.2407\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2359 - val_mse: 0.2359\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2349 - mse: 0.2349 - val_loss: 0.2199 - val_mse: 0.2199\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2328 - mse: 0.2328 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2280 - mse: 0.2280 - val_loss: 0.1959 - val_mse: 0.1959\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.2287 - mse: 0.2287 - val_loss: 0.2165 - val_mse: 0.2165\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2344 - mse: 0.2344 - val_loss: 0.1925 - val_mse: 0.1925\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2152 - mse: 0.2152 - val_loss: 0.1962 - val_mse: 0.1962\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2260 - mse: 0.2260 - val_loss: 0.1647 - val_mse: 0.1647\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.1901 - val_mse: 0.1901\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.2233 - mse: 0.2233 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2186 - mse: 0.2186 - val_loss: 0.1656 - val_mse: 0.1656\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2311 - mse: 0.2311 - val_loss: 0.1807 - val_mse: 0.1807\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2233 - mse: 0.2233 - val_loss: 0.2009 - val_mse: 0.2009\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2173 - mse: 0.2173 - val_loss: 0.1637 - val_mse: 0.1637\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2174 - mse: 0.2174 - val_loss: 0.1148 - val_mse: 0.1148\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2172 - mse: 0.2172 - val_loss: 0.1797 - val_mse: 0.1797\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2179 - mse: 0.2179 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.2041 - mse: 0.2041 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2134 - mse: 0.2134 - val_loss: 0.1748 - val_mse: 0.1748\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2038 - mse: 0.2038 - val_loss: 0.1427 - val_mse: 0.1427\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2257 - mse: 0.2257 - val_loss: 0.2131 - val_mse: 0.2131\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2035 - mse: 0.2035 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2190 - mse: 0.2190 - val_loss: 0.1837 - val_mse: 0.1837\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2166 - mse: 0.2166 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2047 - mse: 0.2047 - val_loss: 0.1335 - val_mse: 0.1335\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2123 - mse: 0.2123 - val_loss: 0.1160 - val_mse: 0.1160\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2131 - mse: 0.2131 - val_loss: 0.1147 - val_mse: 0.1147\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.2026 - mse: 0.2026 - val_loss: 0.1767 - val_mse: 0.1767\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2143 - mse: 0.2143 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2051 - mse: 0.2051 - val_loss: 0.1091 - val_mse: 0.1091\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2144 - mse: 0.2144 - val_loss: 0.1920 - val_mse: 0.1920\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2046 - mse: 0.2046 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2013 - mse: 0.2013 - val_loss: 0.1302 - val_mse: 0.1302\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.1351 - val_mse: 0.1351\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.0835 - val_mse: 0.0835\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2013 - mse: 0.2013 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2061 - mse: 0.2061 - val_loss: 0.1582 - val_mse: 0.1582\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.1985 - mse: 0.1985 - val_loss: 0.0908 - val_mse: 0.0908\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.1041 - val_mse: 0.1041\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1915 - mse: 0.1915 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2071 - mse: 0.2071 - val_loss: 0.1302 - val_mse: 0.1302\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.1300 - val_mse: 0.1300\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1998 - mse: 0.1998 - val_loss: 0.1656 - val_mse: 0.1656\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.1658 - val_mse: 0.1658\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1942 - mse: 0.1942 - val_loss: 0.1322 - val_mse: 0.1322\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2015 - mse: 0.2015 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2008 - mse: 0.2008 - val_loss: 0.1211 - val_mse: 0.1211\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2062 - mse: 0.2062 - val_loss: 0.1134 - val_mse: 0.1134\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1951 - mse: 0.1951 - val_loss: 0.1173 - val_mse: 0.1173\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1766 - mse: 0.1766 - val_loss: 0.1217 - val_mse: 0.1217\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1992 - mse: 0.1992 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1907 - mse: 0.1907 - val_loss: 0.1321 - val_mse: 0.1321\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1926 - mse: 0.1926 - val_loss: 0.1021 - val_mse: 0.1021\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1341 - val_mse: 0.1341\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.1322 - val_mse: 0.1322\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1962 - mse: 0.1962 - val_loss: 0.1289 - val_mse: 0.1289\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2013 - mse: 0.2013 - val_loss: 0.1113 - val_mse: 0.1113\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2031 - mse: 0.2031 - val_loss: 0.1351 - val_mse: 0.1351\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.1074 - val_mse: 0.1074\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2051 - mse: 0.2051 - val_loss: 0.1646 - val_mse: 0.1646\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1954 - mse: 0.1954 - val_loss: 0.1070 - val_mse: 0.1070\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.1016 - val_mse: 0.1016\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1989 - mse: 0.1989 - val_loss: 0.1074 - val_mse: 0.1074\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.1129 - val_mse: 0.1129\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1786 - mse: 0.1786 - val_loss: 0.1005 - val_mse: 0.1005\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2061 - mse: 0.2061 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1849 - mse: 0.1849 - val_loss: 0.1352 - val_mse: 0.1352\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2027 - mse: 0.2027 - val_loss: 0.1923 - val_mse: 0.1923\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2003 - mse: 0.2003 - val_loss: 0.1157 - val_mse: 0.1157\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.1252 - val_mse: 0.1252\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 0.2009 - mse: 0.2009 - val_loss: 0.1215 - val_mse: 0.1215\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1847 - mse: 0.1847 - val_loss: 0.0746 - val_mse: 0.0746\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2127 - mse: 0.2127 - val_loss: 0.0694 - val_mse: 0.0694\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.1133 - val_mse: 0.1133\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2032 - mse: 0.2032 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1194 - val_mse: 0.1194\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1820 - mse: 0.1820 - val_loss: 0.1693 - val_mse: 0.1693\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1831 - mse: 0.1831 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.1707 - val_mse: 0.1707\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1913 - mse: 0.1913 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.0998 - val_mse: 0.0998\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1903 - mse: 0.1903 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.0657 - val_mse: 0.0657\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1968 - mse: 0.1968 - val_loss: 0.0757 - val_mse: 0.0757\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1838 - mse: 0.1838 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1952 - mse: 0.1952 - val_loss: 0.0635 - val_mse: 0.0635\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2014 - mse: 0.2014 - val_loss: 0.0681 - val_mse: 0.0681\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.0873 - val_mse: 0.0873\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1855 - mse: 0.1855 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1930 - mse: 0.1930 - val_loss: 0.0947 - val_mse: 0.0947\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.1577 - val_mse: 0.1577\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1812 - mse: 0.1812 - val_loss: 0.0853 - val_mse: 0.0853\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.1370 - val_mse: 0.1370\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.0834 - val_mse: 0.0834\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.1537 - val_mse: 0.1537\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1146 - val_mse: 0.1146\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.0834 - val_mse: 0.0834\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1734 - mse: 0.1734 - val_loss: 0.1249 - val_mse: 0.1249\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.1178 - val_mse: 0.1178\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1772 - mse: 0.1772 - val_loss: 0.1395 - val_mse: 0.1395\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1824 - mse: 0.1824 - val_loss: 0.1534 - val_mse: 0.1534\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1831 - mse: 0.1831 - val_loss: 0.1421 - val_mse: 0.1421\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1978 - mse: 0.1978 - val_loss: 0.1230 - val_mse: 0.1230\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.0636 - val_mse: 0.0636\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1758 - mse: 0.1758 - val_loss: 0.1361 - val_mse: 0.1361\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0747 - val_mse: 0.0747\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.0740 - val_mse: 0.0740\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.1299 - val_mse: 0.1299\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 0.0987 - val_mse: 0.0987\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.1871 - val_mse: 0.1871\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2108 - mse: 0.2108 - val_loss: 0.1662 - val_mse: 0.1662\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.2068 - val_mse: 0.2068\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1993 - mse: 0.1993 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1965 - mse: 0.1965 - val_loss: 0.0841 - val_mse: 0.0841\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1563 - val_mse: 0.1563\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1881 - mse: 0.1881 - val_loss: 0.0972 - val_mse: 0.0972\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1974 - mse: 0.1974 - val_loss: 0.1024 - val_mse: 0.1024\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.1008 - val_mse: 0.1008\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.1361 - val_mse: 0.1361\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1994 - mse: 0.1994 - val_loss: 0.0909 - val_mse: 0.0909\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1926 - mse: 0.1926 - val_loss: 0.1434 - val_mse: 0.1434\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.1816 - mse: 0.1816 - val_loss: 0.0954 - val_mse: 0.0954\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1822 - mse: 0.1822 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.1760 - mse: 0.1760 - val_loss: 0.1077 - val_mse: 0.1077\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.1259 - val_mse: 0.1259\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1766 - mse: 0.1766 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1931 - mse: 0.1931 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1843 - mse: 0.1843 - val_loss: 0.0862 - val_mse: 0.0862\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1901 - mse: 0.1901 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1768 - mse: 0.1768 - val_loss: 0.1244 - val_mse: 0.1244\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1932 - mse: 0.1932 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1765 - mse: 0.1765 - val_loss: 0.1249 - val_mse: 0.1249\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2019 - mse: 0.2019 - val_loss: 0.1009 - val_mse: 0.1009\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.0855 - val_mse: 0.0855\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1999 - mse: 0.1999 - val_loss: 0.1168 - val_mse: 0.1168\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2148 - mse: 0.2148 - val_loss: 0.1261 - val_mse: 0.1261\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1804 - mse: 0.1804 - val_loss: 0.1153 - val_mse: 0.1153\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1841 - mse: 0.1841 - val_loss: 0.1322 - val_mse: 0.1322\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2022 - mse: 0.2022 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1827 - mse: 0.1827 - val_loss: 0.1079 - val_mse: 0.1079\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1949 - mse: 0.1949 - val_loss: 0.0886 - val_mse: 0.0886\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2132 - mse: 0.2132 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1362 - val_mse: 0.1362\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1707 - mse: 0.1707 - val_loss: 0.1017 - val_mse: 0.1017\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2047 - mse: 0.2047 - val_loss: 0.1161 - val_mse: 0.1161\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.0868 - val_mse: 0.0868\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1849 - mse: 0.1849 - val_loss: 0.0866 - val_mse: 0.0866\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.0890 - val_mse: 0.0890\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.1124 - val_mse: 0.1124\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1729 - mse: 0.1729 - val_loss: 0.0891 - val_mse: 0.0891\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.0844 - val_mse: 0.0844\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1753 - mse: 0.1753 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0949 - val_mse: 0.0949\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1768 - mse: 0.1768 - val_loss: 0.1044 - val_mse: 0.1044\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1800 - mse: 0.1800 - val_loss: 0.0876 - val_mse: 0.0876\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1752 - mse: 0.1752 - val_loss: 0.1145 - val_mse: 0.1145\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1866 - mse: 0.1866 - val_loss: 0.1233 - val_mse: 0.1233\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2045 - mse: 0.2045 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1733 - mse: 0.1733 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1238 - val_mse: 0.1238\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1778 - mse: 0.1778 - val_loss: 0.1274 - val_mse: 0.1274\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1926 - mse: 0.1926 - val_loss: 0.1260 - val_mse: 0.1260\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2036 - mse: 0.2036 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1909 - mse: 0.1909 - val_loss: 0.0891 - val_mse: 0.0891\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.0932 - val_mse: 0.0932\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1775 - mse: 0.1775 - val_loss: 0.0936 - val_mse: 0.0936\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1688 - mse: 0.1688 - val_loss: 0.1407 - val_mse: 0.1407\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.2024 - val_mse: 0.2024\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.1735 - val_mse: 0.1735\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1214 - val_mse: 0.1214\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.1964 - mse: 0.1964 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1728 - mse: 0.1728 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.1877 - mse: 0.1877 - val_loss: 0.0968 - val_mse: 0.0968\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 0.1041 - val_mse: 0.1041\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1775 - mse: 0.1775 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.1418 - val_mse: 0.1418\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.0985 - val_mse: 0.0985\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1943 - mse: 0.1943 - val_loss: 0.1091 - val_mse: 0.1091\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1807 - mse: 0.1807 - val_loss: 0.1292 - val_mse: 0.1292\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1816 - mse: 0.1816 - val_loss: 0.0866 - val_mse: 0.0866\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1116 - val_mse: 0.1116\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2013 - mse: 0.2013 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.0927 - val_mse: 0.0927\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1959 - mse: 0.1959 - val_loss: 0.0849 - val_mse: 0.0849\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.0839 - val_mse: 0.0839\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1800 - mse: 0.1800 - val_loss: 0.1286 - val_mse: 0.1286\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1944 - mse: 0.1944 - val_loss: 0.0922 - val_mse: 0.0922\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1943 - mse: 0.1943 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1941 - mse: 0.1941 - val_loss: 0.0944 - val_mse: 0.0944\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1786 - mse: 0.1786 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1755 - mse: 0.1755 - val_loss: 0.1236 - val_mse: 0.1236\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.0736 - val_mse: 0.0736\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1752 - mse: 0.1752 - val_loss: 0.1093 - val_mse: 0.1093\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.0908 - val_mse: 0.0908\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.1165 - val_mse: 0.1165\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.1116 - val_mse: 0.1116\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.1232 - val_mse: 0.1232\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1724 - mse: 0.1724 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1952 - mse: 0.1952 - val_loss: 0.2295 - val_mse: 0.2295\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2099 - mse: 0.2099 - val_loss: 0.1363 - val_mse: 0.1363\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 9s 213ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1355 - val_mse: 0.1355\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.1436 - val_mse: 0.1436\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.1369 - val_mse: 0.1369\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1777 - mse: 0.1777 - val_loss: 0.1288 - val_mse: 0.1288\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1334 - val_mse: 0.1334\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1945 - mse: 0.1945 - val_loss: 0.1017 - val_mse: 0.1017\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1670 - mse: 0.1670 - val_loss: 0.1319 - val_mse: 0.1319\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1881 - mse: 0.1881 - val_loss: 0.1118 - val_mse: 0.1118\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1774 - mse: 0.1774 - val_loss: 0.0986 - val_mse: 0.0986\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.1091 - val_mse: 0.1091\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1996 - mse: 0.1996 - val_loss: 0.1336 - val_mse: 0.1336\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1781 - mse: 0.1781 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1848 - mse: 0.1848 - val_loss: 0.1067 - val_mse: 0.1067\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1932 - mse: 0.1932 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.0899 - val_mse: 0.0899\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1820 - mse: 0.1820 - val_loss: 0.1310 - val_mse: 0.1310\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1733 - mse: 0.1733 - val_loss: 0.0885 - val_mse: 0.0885\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.0973 - val_mse: 0.0973\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.1275 - val_mse: 0.1275\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.1278 - val_mse: 0.1278\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1909 - mse: 0.1909 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.1330 - val_mse: 0.1330\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.0970 - val_mse: 0.0970\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.1248 - val_mse: 0.1248\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1864 - mse: 0.1864 - val_loss: 0.1088 - val_mse: 0.1088\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.0943 - val_mse: 0.0943\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.0910 - val_mse: 0.0910\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.1172 - val_mse: 0.1172\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.1025 - val_mse: 0.1025\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1777 - mse: 0.1777 - val_loss: 0.1247 - val_mse: 0.1247\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1774 - mse: 0.1774 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.0967 - val_mse: 0.0967\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.0903 - val_mse: 0.0903\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1855 - mse: 0.1855 - val_loss: 0.1221 - val_mse: 0.1221\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.1133 - val_mse: 0.1133\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.1144 - val_mse: 0.1144\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 0.1916 - mse: 0.1916 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1955 - mse: 0.1955 - val_loss: 0.1218 - val_mse: 0.1218\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.1786 - mse: 0.1786 - val_loss: 0.1255 - val_mse: 0.1255\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.1413 - val_mse: 0.1413\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1111 - val_mse: 0.1111\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1957 - mse: 0.1957 - val_loss: 0.1426 - val_mse: 0.1426\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1708 - mse: 0.1708 - val_loss: 0.1282 - val_mse: 0.1282\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1941 - mse: 0.1941 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.1274 - val_mse: 0.1274\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1953 - mse: 0.1953 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1881 - mse: 0.1881 - val_loss: 0.1151 - val_mse: 0.1151\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.1847 - mse: 0.1847 - val_loss: 0.1153 - val_mse: 0.1153\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1915 - mse: 0.1915 - val_loss: 0.1111 - val_mse: 0.1111\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1745 - mse: 0.1745 - val_loss: 0.1177 - val_mse: 0.1177\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1955 - mse: 0.1955 - val_loss: 0.1285 - val_mse: 0.1285\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1967 - mse: 0.1967 - val_loss: 0.1144 - val_mse: 0.1144\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1914 - mse: 0.1914 - val_loss: 0.1185 - val_mse: 0.1185\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1785 - mse: 0.1785 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.1213 - val_mse: 0.1213\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1832 - mse: 0.1832 - val_loss: 0.1439 - val_mse: 0.1439\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1850 - mse: 0.1850 - val_loss: 0.1657 - val_mse: 0.1657\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.1573 - val_mse: 0.1573\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1094 - val_mse: 0.1094\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 0.1384 - val_mse: 0.1384\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1938 - mse: 0.1938 - val_loss: 0.1277 - val_mse: 0.1277\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.1074 - val_mse: 0.1074\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1999 - mse: 0.1999 - val_loss: 0.1180 - val_mse: 0.1180\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2007 - mse: 0.2007 - val_loss: 0.0834 - val_mse: 0.0834\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1928 - mse: 0.1928 - val_loss: 0.0924 - val_mse: 0.0924\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.1944 - mse: 0.1944 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2004 - mse: 0.2004 - val_loss: 0.0945 - val_mse: 0.0945\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.1773 - mse: 0.1773 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.0846 - val_mse: 0.0846\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1280 - val_mse: 0.1280\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1029 - val_mse: 0.1029\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.1218 - val_mse: 0.1218\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1864 - mse: 0.1864 - val_loss: 0.1079 - val_mse: 0.1079\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.1084 - val_mse: 0.1084\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.1004 - val_mse: 0.1004\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.0941 - val_mse: 0.0941\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1832 - mse: 0.1832 - val_loss: 0.0961 - val_mse: 0.0961\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1179 - val_mse: 0.1179\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.1431 - val_mse: 0.1431\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1844 - mse: 0.1844 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.1126 - val_mse: 0.1126\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1802 - mse: 0.1802 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1943 - mse: 0.1943 - val_loss: 0.1243 - val_mse: 0.1243\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.1278 - val_mse: 0.1278\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1774 - mse: 0.1774 - val_loss: 0.1263 - val_mse: 0.1263\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 0.2054 - mse: 0.2054 - val_loss: 0.0880 - val_mse: 0.0880\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.0831 - val_mse: 0.0831\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1830 - mse: 0.1830 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1884 - mse: 0.1884 - val_loss: 0.1065 - val_mse: 0.1065\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1913 - mse: 0.1913 - val_loss: 0.1168 - val_mse: 0.1168\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1912 - mse: 0.1912 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.0963 - val_mse: 0.0963\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.1894 - mse: 0.1894 - val_loss: 0.0909 - val_mse: 0.0909\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1962 - mse: 0.1962 - val_loss: 0.0992 - val_mse: 0.0992\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 0.1895 - mse: 0.1895 - val_loss: 0.1659 - val_mse: 0.1659\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1148 - val_mse: 0.1148\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.0898 - val_mse: 0.0898\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.1070 - val_mse: 0.1070\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.0971 - val_mse: 0.0971\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1831 - mse: 0.1831 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.1110 - val_mse: 0.1110\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1681 - mse: 0.1681 - val_loss: 0.1303 - val_mse: 0.1303\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.0984 - val_mse: 0.0984\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1017 - val_mse: 0.1017\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1275 - val_mse: 0.1275\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1899 - mse: 0.1899 - val_loss: 0.1099 - val_mse: 0.1099\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0997 - val_mse: 0.0997\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1629 - mse: 0.1629 - val_loss: 0.0744 - val_mse: 0.0744\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1929 - mse: 0.1929 - val_loss: 0.0887 - val_mse: 0.0887\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1778 - mse: 0.1778 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2049 - mse: 0.2049 - val_loss: 0.1076 - val_mse: 0.1076\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.1153 - val_mse: 0.1153\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1606 - mse: 0.1606 - val_loss: 0.0941 - val_mse: 0.0941\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1261 - val_mse: 0.1261\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.0751 - val_mse: 0.0751\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.0831 - val_mse: 0.0831\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.1364 - val_mse: 0.1364\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1765 - mse: 0.1765 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 0.1987 - mse: 0.1987 - val_loss: 0.1079 - val_mse: 0.1079\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2004 - mse: 0.2004 - val_loss: 0.0632 - val_mse: 0.0632\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.1907 - mse: 0.1907 - val_loss: 0.0751 - val_mse: 0.0751\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.0848 - val_mse: 0.0848\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1896 - mse: 0.1896 - val_loss: 0.0908 - val_mse: 0.0908\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.1198 - val_mse: 0.1198\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1742 - mse: 0.1742 - val_loss: 0.0968 - val_mse: 0.0968\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.0749 - val_mse: 0.0749\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.0886 - val_mse: 0.0886\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 0.1658 - mse: 0.1658 - val_loss: 0.0834 - val_mse: 0.0834\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.0920 - val_mse: 0.0920\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1885 - mse: 0.1885 - val_loss: 0.1053 - val_mse: 0.1053\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1943 - mse: 0.1943 - val_loss: 0.0898 - val_mse: 0.0898\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1812 - mse: 0.1812 - val_loss: 0.0935 - val_mse: 0.0935\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.1297 - val_mse: 0.1297\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1898 - mse: 0.1898 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1934 - mse: 0.1934 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 0.1702 - mse: 0.1702 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.0892 - val_mse: 0.0892\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1728 - mse: 0.1728 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1813 - mse: 0.1813 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.0914 - val_mse: 0.0914\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.1797 - mse: 0.1797 - val_loss: 0.0869 - val_mse: 0.0869\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.0919 - val_mse: 0.0919\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.0927 - val_mse: 0.0927\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1951 - mse: 0.1951 - val_loss: 0.0893 - val_mse: 0.0893\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.0932 - val_mse: 0.0932\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.0913 - val_mse: 0.0913\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.0835 - val_mse: 0.0835\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.0998 - val_mse: 0.0998\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.1122 - val_mse: 0.1122\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1340 - val_mse: 0.1340\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.0842 - val_mse: 0.0842\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.1394 - val_mse: 0.1394\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 0.1898 - mse: 0.1898 - val_loss: 0.1709 - val_mse: 0.1709\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.1540 - val_mse: 0.1540\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2017 - mse: 0.2017 - val_loss: 0.0840 - val_mse: 0.0840\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1909 - mse: 0.1909 - val_loss: 0.0963 - val_mse: 0.0963\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.1286 - val_mse: 0.1286\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1783 - mse: 0.1783 - val_loss: 0.1105 - val_mse: 0.1105\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1723 - mse: 0.1723 - val_loss: 0.1359 - val_mse: 0.1359\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.0925 - val_mse: 0.0925\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.0892 - val_mse: 0.0892\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.1960 - mse: 0.1960 - val_loss: 0.0925 - val_mse: 0.0925\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1899 - mse: 0.1899 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1807 - mse: 0.1807 - val_loss: 0.0839 - val_mse: 0.0839\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.1815 - val_mse: 0.1815\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1907 - mse: 0.1907 - val_loss: 0.0871 - val_mse: 0.0871\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1989 - mse: 0.1989 - val_loss: 0.1080 - val_mse: 0.1080\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1960 - mse: 0.1960 - val_loss: 0.1047 - val_mse: 0.1047\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.1058 - val_mse: 0.1058\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.1003 - val_mse: 0.1003\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1767 - mse: 0.1767 - val_loss: 0.0879 - val_mse: 0.0879\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1977 - mse: 0.1977 - val_loss: 0.1206 - val_mse: 0.1206\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.1408 - val_mse: 0.1408\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1707 - mse: 0.1707 - val_loss: 0.1102 - val_mse: 0.1102\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1810 - mse: 0.1810 - val_loss: 0.1068 - val_mse: 0.1068\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.1161 - val_mse: 0.1161\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1753 - mse: 0.1753 - val_loss: 0.1314 - val_mse: 0.1314\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.1310 - val_mse: 0.1310\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1790 - mse: 0.1790 - val_loss: 0.1117 - val_mse: 0.1117\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1839 - mse: 0.1839 - val_loss: 0.0916 - val_mse: 0.0916\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1839 - mse: 0.1839 - val_loss: 0.1275 - val_mse: 0.1275\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.0723 - val_mse: 0.0723\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1739 - mse: 0.1739 - val_loss: 0.0931 - val_mse: 0.0931\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1901 - mse: 0.1901 - val_loss: 0.0652 - val_mse: 0.0652\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.0872 - val_mse: 0.0872\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.1014 - val_mse: 0.1014\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.0921 - val_mse: 0.0921\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.0980 - val_mse: 0.0980\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.1641 - mse: 0.1641 - val_loss: 0.0866 - val_mse: 0.0866\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1850 - mse: 0.1850 - val_loss: 0.1084 - val_mse: 0.1084\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.2034 - mse: 0.2034 - val_loss: 0.0925 - val_mse: 0.0925\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1941 - mse: 0.1941 - val_loss: 0.1011 - val_mse: 0.1011\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1755 - mse: 0.1755 - val_loss: 0.0879 - val_mse: 0.0879\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1931 - mse: 0.1931 - val_loss: 0.0750 - val_mse: 0.0750\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1821 - mse: 0.1821 - val_loss: 0.0681 - val_mse: 0.0681\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0960 - val_mse: 0.0960\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.0925 - val_mse: 0.0925\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1841 - mse: 0.1841 - val_loss: 0.0893 - val_mse: 0.0893\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1135 - val_mse: 0.1135\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.0880 - val_mse: 0.0880\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.1248 - val_mse: 0.1248\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1829 - mse: 0.1829 - val_loss: 0.0938 - val_mse: 0.0938\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.1132 - val_mse: 0.1132\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.1786 - mse: 0.1786 - val_loss: 0.0999 - val_mse: 0.0999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "db9f878c-e191-4bcf-eed1-f2e010b43f88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.566998302936554,\n",
              "  0.45793208479881287,\n",
              "  0.39791613817214966,\n",
              "  0.3487909138202667,\n",
              "  0.3102872669696808,\n",
              "  0.28137311339378357,\n",
              "  0.26253193616867065,\n",
              "  0.25269633531570435,\n",
              "  0.2501084804534912,\n",
              "  0.25,\n",
              "  0.2500007748603821,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500010132789612,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000089406967163,\n",
              "  0.2500010132789612,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164],\n",
              " 'mse': [0.566998302936554,\n",
              "  0.45793208479881287,\n",
              "  0.39791613817214966,\n",
              "  0.3487909138202667,\n",
              "  0.3102872669696808,\n",
              "  0.28137311339378357,\n",
              "  0.26253193616867065,\n",
              "  0.25269633531570435,\n",
              "  0.2501084804534912,\n",
              "  0.25,\n",
              "  0.2500007748603821,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500010132789612,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000089406967163,\n",
              "  0.2500010132789612,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164],\n",
              " 'val_loss': [0.490416020154953,\n",
              "  0.4254283607006073,\n",
              "  0.37116917967796326,\n",
              "  0.3273015320301056,\n",
              "  0.29368075728416443,\n",
              "  0.2700752913951874,\n",
              "  0.25604769587516785,\n",
              "  0.2505247890949249,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2500004470348358,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500029504299164,\n",
              "  0.2500007748603821,\n",
              "  0.250000923871994,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500007450580597,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000086426734924,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500007152557373,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500007450580597,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500011622905731,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388],\n",
              " 'val_mse': [0.490416020154953,\n",
              "  0.4254283607006073,\n",
              "  0.37116917967796326,\n",
              "  0.3273015320301056,\n",
              "  0.29368075728416443,\n",
              "  0.2700752913951874,\n",
              "  0.25604769587516785,\n",
              "  0.2505247890949249,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2500004470348358,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500029504299164,\n",
              "  0.2500007748603821,\n",
              "  0.250000923871994,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500007450580597,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000086426734924,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500007152557373,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500007450580597,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500011622905731,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "8f77b0b3-1c5b-449c-b0dc-009365d152e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5881a4a-d2ae-47c3-86ba-7dec8d40f5c3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf1597f1-be25-4213-aa90-a59d98586489\", \"2Class_regression.h5\", 16623752)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}