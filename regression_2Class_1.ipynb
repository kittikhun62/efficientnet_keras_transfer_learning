{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMpb+2pwH34pOfJrSQxc0HL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "44dfb220-48a6-4778-fa8f-8325a280f56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "dbae6512-46cd-420b-d5b3-55587151b33d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0c56984-f9dc-43b0-bf1f-bb34e28d7185\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0c56984-f9dc-43b0-bf1f-bb34e28d7185')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0c56984-f9dc-43b0-bf1f-bb34e28d7185 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0c56984-f9dc-43b0-bf1f-bb34e28d7185');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "2134a392-db89-4fd8-9363-3fefec38be3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "59c3614c-d0c2-4768-a8dc-ecd78f6a3f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "aa9b7283-b092-4a3b-ab2e-202b0b6116b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "2d24e491-a220-475c-9e7b-6be034e06ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]"
      ],
      "metadata": {
        "id": "fjMOfH15xYfP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/new project\"\n",
        "os.chdir(DATA_PATH)\n"
      ],
      "metadata": {
        "id": "CUyB_XVXxkNH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi0dToSmxkLK",
        "outputId": "4b26f29b-baaa-43da-ad71-af65591c6531"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new project/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO4FrhHdxkI9",
        "outputId": "011b7c33-b230-4863-e57b-095dc7d46f6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new project/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new project'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val = df[df['No'].between(599,699)]\n",
        "# train = df[df['No'].between(1,598)]\n",
        "# test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "f37e9e3e-3b3a-43f7-9fe4-da8c18f924b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d5f664-9c26-4d2a-c53d-0402002955eb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "c2cf6d5a-959c-4573-f669-93e4cc4a50b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "a045e795-282e-4ac9-9d51-88a47ecde727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "ffef119b-d1e0-479a-9076-8823c22bc8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "41eaa0e5-a03a-47bf-855e-e2f16cfcef9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2b710c-2f8e-46c6-c268-babddeff57a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n",
            "<ipython-input-37-a61b1be0b028>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 19s 134ms/step - loss: 0.5670 - mse: 0.5670 - val_loss: 0.4904 - val_mse: 0.4904\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.4579 - mse: 0.4579 - val_loss: 0.4254 - val_mse: 0.4254\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.3979 - mse: 0.3979 - val_loss: 0.3712 - val_mse: 0.3712\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.3488 - mse: 0.3488 - val_loss: 0.3273 - val_mse: 0.3273\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.3103 - mse: 0.3103 - val_loss: 0.2937 - val_mse: 0.2937\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2814 - mse: 0.2814 - val_loss: 0.2701 - val_mse: 0.2701\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2625 - mse: 0.2625 - val_loss: 0.2560 - val_mse: 0.2560\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2527 - mse: 0.2527 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "37/37 [==============================] - ETA: 0s - loss: 0.2500 - mse: 0.2500Epoch 55/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 5s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 5s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 6s 121ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 4s 111ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 9s 249ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "db9f878c-e191-4bcf-eed1-f2e010b43f88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.566998302936554,\n",
              "  0.45793208479881287,\n",
              "  0.39791613817214966,\n",
              "  0.3487909138202667,\n",
              "  0.3102872669696808,\n",
              "  0.28137311339378357,\n",
              "  0.26253193616867065,\n",
              "  0.25269633531570435,\n",
              "  0.2501084804534912,\n",
              "  0.25,\n",
              "  0.2500007748603821,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500010132789612,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000089406967163,\n",
              "  0.2500010132789612,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164],\n",
              " 'mse': [0.566998302936554,\n",
              "  0.45793208479881287,\n",
              "  0.39791613817214966,\n",
              "  0.3487909138202667,\n",
              "  0.3102872669696808,\n",
              "  0.28137311339378357,\n",
              "  0.26253193616867065,\n",
              "  0.25269633531570435,\n",
              "  0.2501084804534912,\n",
              "  0.25,\n",
              "  0.2500007748603821,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500010132789612,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000089406967163,\n",
              "  0.2500010132789612,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164],\n",
              " 'val_loss': [0.490416020154953,\n",
              "  0.4254283607006073,\n",
              "  0.37116917967796326,\n",
              "  0.3273015320301056,\n",
              "  0.29368075728416443,\n",
              "  0.2700752913951874,\n",
              "  0.25604769587516785,\n",
              "  0.2505247890949249,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2500004470348358,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500029504299164,\n",
              "  0.2500007748603821,\n",
              "  0.250000923871994,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500007450580597,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000086426734924,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500007152557373,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500007450580597,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500011622905731,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388],\n",
              " 'val_mse': [0.490416020154953,\n",
              "  0.4254283607006073,\n",
              "  0.37116917967796326,\n",
              "  0.3273015320301056,\n",
              "  0.29368075728416443,\n",
              "  0.2700752913951874,\n",
              "  0.25604769587516785,\n",
              "  0.2505247890949249,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.2500004470348358,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500029504299164,\n",
              "  0.2500007748603821,\n",
              "  0.250000923871994,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500007450580597,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000086426734924,\n",
              "  0.2500012218952179,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500011622905731,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500007152557373,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500007450580597,\n",
              "  0.25000107288360596,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.25000086426734924,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.25000104308128357,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.25000107288360596,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.2500011622905731,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009536743164,\n",
              "  0.250000923871994,\n",
              "  0.2500009536743164,\n",
              "  0.2500009536743164,\n",
              "  0.25000083446502686,\n",
              "  0.2500009834766388,\n",
              "  0.250000923871994,\n",
              "  0.250000923871994,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388,\n",
              "  0.2500009834766388]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'c', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'c', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "2d7b5c10-5cb6-40a4-e861-9d3f4bb7775f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "#model.save('./content/drive/My Drive/new/2Class.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#files.download(\"./content/drive/My Drive/new/2Class.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}