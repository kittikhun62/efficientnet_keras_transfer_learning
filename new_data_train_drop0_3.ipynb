{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFg79gezZXyZsxWhavk812",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train_drop0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "03a5389d-a1c8-4df9-affe-cbf79ccbd41b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data Remake.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "5228ce61-c2e8-4a3a-ac78-8e597512c433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-500  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-500  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-500  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-500  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-500  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-500  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-500  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-500  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-500  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-500  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2d11bb3-4906-41c6-9d67-84dcc7a31786\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2d11bb3-4906-41c6-9d67-84dcc7a31786')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2d11bb3-4906-41c6-9d67-84dcc7a31786 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2d11bb3-4906-41c6-9d67-84dcc7a31786');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "45e26d86-daa0-4535-84f5-a8fdf1a98ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "914e3826-f35a-4330-b7b0-aac1bf209545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "e54a1770-8662-4fae-8b65-e59e518be393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "4ae960e6-260d-453b-9ccf-e7a7bc92264b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)] \n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "8d5fa1f2-104c-4718-ed75-111b3b61d03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 197\n",
            "total training 2 images: 138\n",
            "total training 3 images: 315 \n",
            "\n",
            "total validation 1 images: 59\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 6 \n",
            "\n",
            "total test 1 images: 59\n",
            "total test 2 images: 3\n",
            "total test 3 images: 13 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 650  # จำนวนภาพ Train\n",
        "NUM_TEST = 75 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "b5ff4d2c-22be-4e2b-b2e5-b0c8eaa0af8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "03a556d4-7667-4691-e297-b31baa0794c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "ba82cb7f-837a-479b-f8cb-ccde80ad2a95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "453c5be2-ce5b-4e5e-d264-290b4f930c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 650 images belonging to 3 classes.\n",
            "Found 75 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31980772-ebe3-403f-af36-43b33f613c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 33s 2s/step - loss: 1.7313 - acc: 0.3823 - val_loss: 2.1807 - val_acc: 0.1406\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.6631 - acc: 0.4403 - val_loss: 2.0511 - val_acc: 0.1562\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.7021 - acc: 0.3938 - val_loss: 2.0564 - val_acc: 0.1406\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6197 - acc: 0.3976 - val_loss: 1.9088 - val_acc: 0.1562\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5871 - acc: 0.4096 - val_loss: 1.8782 - val_acc: 0.1406\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6447 - acc: 0.4010 - val_loss: 1.8659 - val_acc: 0.1406\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6291 - acc: 0.4130 - val_loss: 1.8565 - val_acc: 0.1406\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6205 - acc: 0.4164 - val_loss: 1.8214 - val_acc: 0.1250\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6604 - acc: 0.4096 - val_loss: 1.8576 - val_acc: 0.1406\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6615 - acc: 0.3959 - val_loss: 1.8431 - val_acc: 0.1562\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5549 - acc: 0.4172 - val_loss: 1.6952 - val_acc: 0.1875\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4690 - acc: 0.4198 - val_loss: 1.8130 - val_acc: 0.2031\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5776 - acc: 0.3908 - val_loss: 1.7019 - val_acc: 0.1562\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5644 - acc: 0.4334 - val_loss: 1.8049 - val_acc: 0.1719\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6345 - acc: 0.3823 - val_loss: 1.7327 - val_acc: 0.1875\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5533 - acc: 0.4061 - val_loss: 1.7732 - val_acc: 0.2031\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4832 - acc: 0.4198 - val_loss: 1.7462 - val_acc: 0.1562\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.5336 - acc: 0.4031 - val_loss: 1.7047 - val_acc: 0.2188\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5035 - acc: 0.4334 - val_loss: 1.7998 - val_acc: 0.1719\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5378 - acc: 0.4078 - val_loss: 1.7882 - val_acc: 0.1562\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5390 - acc: 0.4096 - val_loss: 1.7365 - val_acc: 0.1719\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.6464 - acc: 0.3805 - val_loss: 1.7647 - val_acc: 0.1719\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5767 - acc: 0.3840 - val_loss: 1.7432 - val_acc: 0.1875\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6129 - acc: 0.3720 - val_loss: 1.7720 - val_acc: 0.1250\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6314 - acc: 0.3976 - val_loss: 1.8889 - val_acc: 0.1406\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4370 - acc: 0.4403 - val_loss: 1.8285 - val_acc: 0.1406\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.5802 - acc: 0.3925 - val_loss: 1.8655 - val_acc: 0.1250\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4398 - acc: 0.4531 - val_loss: 1.8394 - val_acc: 0.1094\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5706 - acc: 0.4147 - val_loss: 1.7842 - val_acc: 0.1562\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5723 - acc: 0.4198 - val_loss: 1.6912 - val_acc: 0.1250\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4878 - acc: 0.4198 - val_loss: 1.8484 - val_acc: 0.1562\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4917 - acc: 0.4181 - val_loss: 1.8076 - val_acc: 0.1250\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5159 - acc: 0.4181 - val_loss: 1.8413 - val_acc: 0.1406\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4160 - acc: 0.4453 - val_loss: 1.7670 - val_acc: 0.1719\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5032 - acc: 0.4130 - val_loss: 1.7557 - val_acc: 0.1719\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5279 - acc: 0.4266 - val_loss: 1.8373 - val_acc: 0.1406\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4770 - acc: 0.4352 - val_loss: 1.7437 - val_acc: 0.1719\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5719 - acc: 0.4317 - val_loss: 1.8067 - val_acc: 0.1406\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5337 - acc: 0.4283 - val_loss: 1.7365 - val_acc: 0.1562\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5221 - acc: 0.4215 - val_loss: 1.6915 - val_acc: 0.1406\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4373 - acc: 0.4164 - val_loss: 1.8441 - val_acc: 0.1562\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4342 - acc: 0.4590 - val_loss: 1.8451 - val_acc: 0.1562\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4835 - acc: 0.4369 - val_loss: 1.7888 - val_acc: 0.1562\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5950 - acc: 0.4113 - val_loss: 1.8725 - val_acc: 0.0938\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4903 - acc: 0.4147 - val_loss: 1.7487 - val_acc: 0.1562\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3843 - acc: 0.4625 - val_loss: 1.7527 - val_acc: 0.1719\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4783 - acc: 0.4539 - val_loss: 1.8042 - val_acc: 0.1250\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4807 - acc: 0.4352 - val_loss: 1.7962 - val_acc: 0.1094\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.5057 - acc: 0.4125 - val_loss: 1.8511 - val_acc: 0.1094\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4732 - acc: 0.4215 - val_loss: 1.7527 - val_acc: 0.1406\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5175 - acc: 0.4386 - val_loss: 1.8041 - val_acc: 0.1562\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4652 - acc: 0.4522 - val_loss: 1.7501 - val_acc: 0.1406\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5323 - acc: 0.4061 - val_loss: 1.7282 - val_acc: 0.1719\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3966 - acc: 0.4437 - val_loss: 1.8038 - val_acc: 0.1250\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4547 - acc: 0.4044 - val_loss: 1.7814 - val_acc: 0.1719\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.5006 - acc: 0.4344 - val_loss: 1.7607 - val_acc: 0.1562\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4427 - acc: 0.4403 - val_loss: 1.7421 - val_acc: 0.1562\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3961 - acc: 0.4539 - val_loss: 1.7762 - val_acc: 0.1719\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4207 - acc: 0.4181 - val_loss: 1.7868 - val_acc: 0.1719\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4580 - acc: 0.4181 - val_loss: 1.8909 - val_acc: 0.0938\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4751 - acc: 0.4300 - val_loss: 1.9564 - val_acc: 0.0938\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4821 - acc: 0.4403 - val_loss: 1.8045 - val_acc: 0.1250\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4666 - acc: 0.4403 - val_loss: 1.8687 - val_acc: 0.1250\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4399 - acc: 0.4437 - val_loss: 1.8551 - val_acc: 0.1250\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4839 - acc: 0.4266 - val_loss: 1.8173 - val_acc: 0.1406\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4039 - acc: 0.4488 - val_loss: 1.7058 - val_acc: 0.1406\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4561 - acc: 0.4539 - val_loss: 1.8219 - val_acc: 0.1250\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3217 - acc: 0.4608 - val_loss: 1.7295 - val_acc: 0.1562\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4345 - acc: 0.4328 - val_loss: 1.8119 - val_acc: 0.1406\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4752 - acc: 0.4369 - val_loss: 1.8951 - val_acc: 0.1250\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.4187 - acc: 0.4516 - val_loss: 1.8545 - val_acc: 0.1094\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3133 - acc: 0.4829 - val_loss: 1.7499 - val_acc: 0.1094\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4931 - acc: 0.4300 - val_loss: 1.7953 - val_acc: 0.1562\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3725 - acc: 0.4266 - val_loss: 1.8713 - val_acc: 0.1406\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3835 - acc: 0.4352 - val_loss: 1.8490 - val_acc: 0.1094\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3086 - acc: 0.4881 - val_loss: 1.7496 - val_acc: 0.1250\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4046 - acc: 0.4164 - val_loss: 1.8062 - val_acc: 0.1094\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4836 - acc: 0.4181 - val_loss: 1.8336 - val_acc: 0.1250\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4800 - acc: 0.4283 - val_loss: 1.8419 - val_acc: 0.0938\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4477 - acc: 0.4369 - val_loss: 1.8805 - val_acc: 0.1094\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3577 - acc: 0.4642 - val_loss: 1.8979 - val_acc: 0.1094\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4251 - acc: 0.4334 - val_loss: 1.8493 - val_acc: 0.1250\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.5232 - acc: 0.4249 - val_loss: 1.7867 - val_acc: 0.1094\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4078 - acc: 0.4266 - val_loss: 1.8153 - val_acc: 0.0938\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3801 - acc: 0.4727 - val_loss: 1.8344 - val_acc: 0.1250\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3520 - acc: 0.4608 - val_loss: 1.8634 - val_acc: 0.1094\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.4327 - acc: 0.4522 - val_loss: 1.8116 - val_acc: 0.1094\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4174 - acc: 0.4317 - val_loss: 1.8309 - val_acc: 0.0938\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4885 - acc: 0.4147 - val_loss: 1.8359 - val_acc: 0.1406\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3911 - acc: 0.4471 - val_loss: 1.9134 - val_acc: 0.0781\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4511 - acc: 0.4505 - val_loss: 1.8346 - val_acc: 0.1250\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4450 - acc: 0.4522 - val_loss: 1.7783 - val_acc: 0.1250\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3948 - acc: 0.4556 - val_loss: 1.8951 - val_acc: 0.0781\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3477 - acc: 0.4795 - val_loss: 1.7553 - val_acc: 0.1094\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3446 - acc: 0.4573 - val_loss: 1.8458 - val_acc: 0.1094\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3892 - acc: 0.4573 - val_loss: 1.7999 - val_acc: 0.1406\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2868 - acc: 0.4949 - val_loss: 1.8030 - val_acc: 0.1250\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3573 - acc: 0.4750 - val_loss: 1.8048 - val_acc: 0.1250\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3221 - acc: 0.4778 - val_loss: 1.9298 - val_acc: 0.0938\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3831 - acc: 0.4710 - val_loss: 1.7829 - val_acc: 0.1250\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3030 - acc: 0.4642 - val_loss: 1.7753 - val_acc: 0.1250\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3134 - acc: 0.4778 - val_loss: 1.7897 - val_acc: 0.1094\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4676 - acc: 0.4181 - val_loss: 1.7764 - val_acc: 0.1406\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3362 - acc: 0.4403 - val_loss: 1.8421 - val_acc: 0.1094\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3547 - acc: 0.4744 - val_loss: 1.8889 - val_acc: 0.0781\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4921 - acc: 0.4369 - val_loss: 1.9040 - val_acc: 0.1094\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3919 - acc: 0.4406 - val_loss: 1.9889 - val_acc: 0.0625\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3106 - acc: 0.4659 - val_loss: 1.8417 - val_acc: 0.0938\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3615 - acc: 0.4420 - val_loss: 1.9737 - val_acc: 0.0938\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2759 - acc: 0.4966 - val_loss: 1.7803 - val_acc: 0.1094\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3083 - acc: 0.4744 - val_loss: 1.9639 - val_acc: 0.0781\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3007 - acc: 0.4795 - val_loss: 1.9109 - val_acc: 0.0938\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3512 - acc: 0.4453 - val_loss: 1.8898 - val_acc: 0.0938\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3566 - acc: 0.4437 - val_loss: 1.8007 - val_acc: 0.1094\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3618 - acc: 0.4795 - val_loss: 1.8961 - val_acc: 0.0938\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4032 - acc: 0.4471 - val_loss: 1.8798 - val_acc: 0.0938\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3357 - acc: 0.4522 - val_loss: 1.9428 - val_acc: 0.0781\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2741 - acc: 0.5085 - val_loss: 1.8619 - val_acc: 0.0938\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3920 - acc: 0.4488 - val_loss: 1.8368 - val_acc: 0.1094\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3337 - acc: 0.4334 - val_loss: 1.8636 - val_acc: 0.1094\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3552 - acc: 0.4539 - val_loss: 1.8105 - val_acc: 0.1094\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3020 - acc: 0.4915 - val_loss: 1.9146 - val_acc: 0.1094\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3830 - acc: 0.4556 - val_loss: 1.9246 - val_acc: 0.1094\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3091 - acc: 0.4795 - val_loss: 1.9254 - val_acc: 0.0938\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3719 - acc: 0.4608 - val_loss: 1.8002 - val_acc: 0.1094\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3227 - acc: 0.4778 - val_loss: 1.7248 - val_acc: 0.1406\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2846 - acc: 0.4795 - val_loss: 1.8415 - val_acc: 0.0938\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3161 - acc: 0.4563 - val_loss: 1.8820 - val_acc: 0.0938\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3861 - acc: 0.4488 - val_loss: 1.8490 - val_acc: 0.1094\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3770 - acc: 0.4531 - val_loss: 1.8228 - val_acc: 0.1094\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3003 - acc: 0.4863 - val_loss: 1.7251 - val_acc: 0.1250\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3027 - acc: 0.4898 - val_loss: 1.7520 - val_acc: 0.1719\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3442 - acc: 0.4734 - val_loss: 1.8125 - val_acc: 0.1250\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3115 - acc: 0.4659 - val_loss: 1.7831 - val_acc: 0.1250\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3345 - acc: 0.4625 - val_loss: 1.7520 - val_acc: 0.1094\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2579 - acc: 0.4676 - val_loss: 1.8689 - val_acc: 0.1094\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2789 - acc: 0.4949 - val_loss: 1.7982 - val_acc: 0.0938\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2371 - acc: 0.4829 - val_loss: 1.7547 - val_acc: 0.1562\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.2907 - acc: 0.4642 - val_loss: 1.7366 - val_acc: 0.1250\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3728 - acc: 0.4642 - val_loss: 1.9298 - val_acc: 0.1250\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3850 - acc: 0.4625 - val_loss: 1.9441 - val_acc: 0.0938\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2872 - acc: 0.4556 - val_loss: 1.7987 - val_acc: 0.1250\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2105 - acc: 0.5102 - val_loss: 1.8070 - val_acc: 0.1406\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2330 - acc: 0.5017 - val_loss: 1.8265 - val_acc: 0.1094\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3881 - acc: 0.4672 - val_loss: 1.8910 - val_acc: 0.1250\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2494 - acc: 0.4915 - val_loss: 1.9154 - val_acc: 0.1094\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3596 - acc: 0.4573 - val_loss: 1.7614 - val_acc: 0.0938\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3312 - acc: 0.4761 - val_loss: 1.8559 - val_acc: 0.1094\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2969 - acc: 0.4573 - val_loss: 1.8323 - val_acc: 0.1406\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3290 - acc: 0.4875 - val_loss: 1.9247 - val_acc: 0.0938\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2415 - acc: 0.4983 - val_loss: 1.8965 - val_acc: 0.1094\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.4155 - acc: 0.4369 - val_loss: 1.9283 - val_acc: 0.1094\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3256 - acc: 0.4573 - val_loss: 1.8485 - val_acc: 0.1094\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2604 - acc: 0.4949 - val_loss: 1.8970 - val_acc: 0.1094\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2826 - acc: 0.4812 - val_loss: 1.8435 - val_acc: 0.1406\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2966 - acc: 0.4812 - val_loss: 1.8236 - val_acc: 0.1250\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3554 - acc: 0.4863 - val_loss: 1.7476 - val_acc: 0.1562\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3416 - acc: 0.4642 - val_loss: 1.7445 - val_acc: 0.1719\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.3399 - acc: 0.4352 - val_loss: 1.7395 - val_acc: 0.1562\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3268 - acc: 0.4590 - val_loss: 1.8215 - val_acc: 0.1562\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3889 - acc: 0.4656 - val_loss: 1.8858 - val_acc: 0.1562\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3641 - acc: 0.4590 - val_loss: 1.9047 - val_acc: 0.1250\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3099 - acc: 0.4812 - val_loss: 1.6992 - val_acc: 0.1719\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3197 - acc: 0.4778 - val_loss: 1.8189 - val_acc: 0.1719\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3078 - acc: 0.4829 - val_loss: 1.8210 - val_acc: 0.1250\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2802 - acc: 0.4625 - val_loss: 1.8748 - val_acc: 0.1250\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2858 - acc: 0.4744 - val_loss: 1.9470 - val_acc: 0.0938\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3012 - acc: 0.4812 - val_loss: 1.8199 - val_acc: 0.1562\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3289 - acc: 0.4556 - val_loss: 1.8064 - val_acc: 0.1562\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2379 - acc: 0.5000 - val_loss: 1.8284 - val_acc: 0.1562\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3257 - acc: 0.4744 - val_loss: 1.8748 - val_acc: 0.1406\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3023 - acc: 0.4761 - val_loss: 1.7598 - val_acc: 0.1406\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2970 - acc: 0.4727 - val_loss: 1.8833 - val_acc: 0.1250\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2711 - acc: 0.5172 - val_loss: 1.8602 - val_acc: 0.1250\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2756 - acc: 0.4625 - val_loss: 1.9234 - val_acc: 0.1094\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.2926 - acc: 0.5000 - val_loss: 1.8381 - val_acc: 0.1406\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2863 - acc: 0.4898 - val_loss: 1.9629 - val_acc: 0.0938\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2442 - acc: 0.5034 - val_loss: 1.9070 - val_acc: 0.1562\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2811 - acc: 0.4761 - val_loss: 1.8372 - val_acc: 0.1250\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2452 - acc: 0.5102 - val_loss: 1.7457 - val_acc: 0.1719\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1981 - acc: 0.4932 - val_loss: 1.8992 - val_acc: 0.1406\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2710 - acc: 0.4659 - val_loss: 1.8648 - val_acc: 0.1406\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2717 - acc: 0.4590 - val_loss: 1.8287 - val_acc: 0.1406\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1694 - acc: 0.5171 - val_loss: 1.8091 - val_acc: 0.1562\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2987 - acc: 0.4672 - val_loss: 1.7980 - val_acc: 0.1562\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2626 - acc: 0.4693 - val_loss: 1.7605 - val_acc: 0.1562\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2915 - acc: 0.4846 - val_loss: 1.7774 - val_acc: 0.1250\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1943 - acc: 0.5017 - val_loss: 1.8357 - val_acc: 0.1250\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3044 - acc: 0.4898 - val_loss: 1.8707 - val_acc: 0.1250\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2597 - acc: 0.4744 - val_loss: 1.8165 - val_acc: 0.1406\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3150 - acc: 0.4797 - val_loss: 1.8428 - val_acc: 0.1406\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2562 - acc: 0.5137 - val_loss: 1.7446 - val_acc: 0.1562\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2595 - acc: 0.4608 - val_loss: 1.7190 - val_acc: 0.1406\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3093 - acc: 0.4693 - val_loss: 1.8602 - val_acc: 0.1406\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3096 - acc: 0.4812 - val_loss: 1.6894 - val_acc: 0.1562\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3063 - acc: 0.4744 - val_loss: 1.8196 - val_acc: 0.1250\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3086 - acc: 0.4761 - val_loss: 1.7559 - val_acc: 0.1406\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2665 - acc: 0.4829 - val_loss: 1.7930 - val_acc: 0.1094\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2215 - acc: 0.5017 - val_loss: 1.7287 - val_acc: 0.1719\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2864 - acc: 0.4734 - val_loss: 1.8117 - val_acc: 0.1562\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3069 - acc: 0.4437 - val_loss: 1.8452 - val_acc: 0.1562\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2542 - acc: 0.5171 - val_loss: 1.8264 - val_acc: 0.1562\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3274 - acc: 0.4744 - val_loss: 1.9198 - val_acc: 0.1094\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2811 - acc: 0.4829 - val_loss: 1.7285 - val_acc: 0.1406\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2800 - acc: 0.4915 - val_loss: 1.7436 - val_acc: 0.1719\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2664 - acc: 0.5119 - val_loss: 1.8412 - val_acc: 0.1406\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2072 - acc: 0.5000 - val_loss: 1.7885 - val_acc: 0.1406\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1802 - acc: 0.5137 - val_loss: 1.8280 - val_acc: 0.1406\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2288 - acc: 0.5051 - val_loss: 1.7247 - val_acc: 0.1562\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1736 - acc: 0.5017 - val_loss: 1.8459 - val_acc: 0.1406\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3365 - acc: 0.4471 - val_loss: 1.7937 - val_acc: 0.1406\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2289 - acc: 0.4846 - val_loss: 1.8747 - val_acc: 0.1562\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3078 - acc: 0.4181 - val_loss: 1.8563 - val_acc: 0.1406\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1990 - acc: 0.5205 - val_loss: 1.7876 - val_acc: 0.1562\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2578 - acc: 0.4983 - val_loss: 1.7421 - val_acc: 0.1562\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1997 - acc: 0.5256 - val_loss: 1.8125 - val_acc: 0.1406\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2655 - acc: 0.5102 - val_loss: 1.8117 - val_acc: 0.1562\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.3909 - acc: 0.4676 - val_loss: 1.7520 - val_acc: 0.1719\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1507 - acc: 0.5290 - val_loss: 1.7028 - val_acc: 0.1562\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2299 - acc: 0.4949 - val_loss: 1.8933 - val_acc: 0.1406\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2656 - acc: 0.4983 - val_loss: 1.7908 - val_acc: 0.1562\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2320 - acc: 0.4915 - val_loss: 1.8231 - val_acc: 0.1094\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1630 - acc: 0.5256 - val_loss: 1.8328 - val_acc: 0.1250\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2399 - acc: 0.4966 - val_loss: 1.8691 - val_acc: 0.1719\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2308 - acc: 0.4983 - val_loss: 1.8408 - val_acc: 0.1250\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1938 - acc: 0.4966 - val_loss: 1.8470 - val_acc: 0.1406\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2999 - acc: 0.4761 - val_loss: 1.8282 - val_acc: 0.1719\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2284 - acc: 0.5141 - val_loss: 1.8198 - val_acc: 0.1562\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2525 - acc: 0.5017 - val_loss: 1.7914 - val_acc: 0.1562\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2015 - acc: 0.5125 - val_loss: 1.8990 - val_acc: 0.1094\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3095 - acc: 0.4693 - val_loss: 1.7873 - val_acc: 0.1562\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2334 - acc: 0.4829 - val_loss: 1.8285 - val_acc: 0.1406\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2803 - acc: 0.4881 - val_loss: 1.7628 - val_acc: 0.1406\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1536 - acc: 0.5119 - val_loss: 1.7342 - val_acc: 0.1562\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1587 - acc: 0.5281 - val_loss: 1.8079 - val_acc: 0.1250\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2320 - acc: 0.5256 - val_loss: 1.8208 - val_acc: 0.1094\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2267 - acc: 0.4846 - val_loss: 1.8160 - val_acc: 0.1250\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2521 - acc: 0.4881 - val_loss: 1.8716 - val_acc: 0.1250\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1984 - acc: 0.4881 - val_loss: 1.8102 - val_acc: 0.1250\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2300 - acc: 0.4922 - val_loss: 1.7817 - val_acc: 0.1406\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3142 - acc: 0.4881 - val_loss: 1.6855 - val_acc: 0.1719\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2410 - acc: 0.4846 - val_loss: 1.7706 - val_acc: 0.1719\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1621 - acc: 0.5171 - val_loss: 1.7586 - val_acc: 0.1719\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1686 - acc: 0.5119 - val_loss: 1.7538 - val_acc: 0.1719\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2242 - acc: 0.5119 - val_loss: 1.7409 - val_acc: 0.1562\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1930 - acc: 0.4983 - val_loss: 1.7795 - val_acc: 0.1562\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2827 - acc: 0.5068 - val_loss: 1.8077 - val_acc: 0.1562\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2367 - acc: 0.5016 - val_loss: 1.8041 - val_acc: 0.1719\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2188 - acc: 0.4881 - val_loss: 1.8808 - val_acc: 0.1406\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2128 - acc: 0.5307 - val_loss: 1.8213 - val_acc: 0.1719\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2510 - acc: 0.4744 - val_loss: 1.9419 - val_acc: 0.1094\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1629 - acc: 0.5000 - val_loss: 1.8807 - val_acc: 0.1250\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2145 - acc: 0.4932 - val_loss: 1.7920 - val_acc: 0.1719\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2350 - acc: 0.4795 - val_loss: 1.6965 - val_acc: 0.1719\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2077 - acc: 0.5205 - val_loss: 1.8644 - val_acc: 0.1094\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1880 - acc: 0.5051 - val_loss: 1.8391 - val_acc: 0.1562\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2192 - acc: 0.4915 - val_loss: 1.7725 - val_acc: 0.1562\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1673 - acc: 0.5392 - val_loss: 1.7905 - val_acc: 0.1562\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1431 - acc: 0.5324 - val_loss: 1.8311 - val_acc: 0.1406\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2770 - acc: 0.4761 - val_loss: 1.8689 - val_acc: 0.1406\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1535 - acc: 0.5017 - val_loss: 1.7952 - val_acc: 0.1719\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2203 - acc: 0.4983 - val_loss: 1.7068 - val_acc: 0.1562\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1305 - acc: 0.5085 - val_loss: 1.8643 - val_acc: 0.1406\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2413 - acc: 0.4915 - val_loss: 1.8316 - val_acc: 0.1406\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1939 - acc: 0.4983 - val_loss: 1.7261 - val_acc: 0.1719\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1710 - acc: 0.5154 - val_loss: 1.8734 - val_acc: 0.1406\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1432 - acc: 0.5205 - val_loss: 1.7534 - val_acc: 0.1406\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1491 - acc: 0.5495 - val_loss: 1.8518 - val_acc: 0.1719\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1674 - acc: 0.5188 - val_loss: 1.7540 - val_acc: 0.1406\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2085 - acc: 0.5068 - val_loss: 1.7359 - val_acc: 0.1562\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1671 - acc: 0.5119 - val_loss: 1.7497 - val_acc: 0.1719\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1400 - acc: 0.5375 - val_loss: 1.8499 - val_acc: 0.1406\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2344 - acc: 0.4915 - val_loss: 1.8017 - val_acc: 0.1719\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1704 - acc: 0.5205 - val_loss: 1.8042 - val_acc: 0.1719\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1531 - acc: 0.5290 - val_loss: 1.7772 - val_acc: 0.1719\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1955 - acc: 0.5281 - val_loss: 1.8143 - val_acc: 0.1250\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2094 - acc: 0.4932 - val_loss: 1.7203 - val_acc: 0.1562\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1408 - acc: 0.5307 - val_loss: 1.7781 - val_acc: 0.1562\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1354 - acc: 0.5444 - val_loss: 1.9099 - val_acc: 0.1094\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0908 - acc: 0.5359 - val_loss: 1.8964 - val_acc: 0.1250\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2233 - acc: 0.4795 - val_loss: 1.8047 - val_acc: 0.1406\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1126 - acc: 0.5324 - val_loss: 1.8560 - val_acc: 0.1562\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1835 - acc: 0.5222 - val_loss: 1.7628 - val_acc: 0.1406\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2095 - acc: 0.4966 - val_loss: 1.8670 - val_acc: 0.1250\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0959 - acc: 0.5410 - val_loss: 1.7368 - val_acc: 0.1875\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1383 - acc: 0.5239 - val_loss: 1.8185 - val_acc: 0.1562\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1223 - acc: 0.5154 - val_loss: 1.7972 - val_acc: 0.1719\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1851 - acc: 0.5266 - val_loss: 1.9311 - val_acc: 0.1094\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2124 - acc: 0.5102 - val_loss: 1.8083 - val_acc: 0.1562\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2397 - acc: 0.4922 - val_loss: 1.7898 - val_acc: 0.1719\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1087 - acc: 0.5469 - val_loss: 1.8235 - val_acc: 0.1250\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2091 - acc: 0.5222 - val_loss: 1.7724 - val_acc: 0.1719\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2054 - acc: 0.5358 - val_loss: 1.7917 - val_acc: 0.1406\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1099 - acc: 0.5461 - val_loss: 1.7342 - val_acc: 0.1406\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1863 - acc: 0.5017 - val_loss: 1.8649 - val_acc: 0.1562\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2092 - acc: 0.5068 - val_loss: 1.7743 - val_acc: 0.1406\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1821 - acc: 0.5102 - val_loss: 1.8495 - val_acc: 0.1094\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1975 - acc: 0.4812 - val_loss: 1.8791 - val_acc: 0.0938\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1007 - acc: 0.5307 - val_loss: 1.8971 - val_acc: 0.1406\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2099 - acc: 0.4983 - val_loss: 1.8182 - val_acc: 0.1250\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1508 - acc: 0.4966 - val_loss: 1.8244 - val_acc: 0.0938\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1797 - acc: 0.4766 - val_loss: 1.8919 - val_acc: 0.1094\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1687 - acc: 0.5256 - val_loss: 1.9061 - val_acc: 0.1406\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1955 - acc: 0.5125 - val_loss: 1.7532 - val_acc: 0.1562\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1431 - acc: 0.5324 - val_loss: 1.8504 - val_acc: 0.1250\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1712 - acc: 0.5017 - val_loss: 1.8633 - val_acc: 0.1406\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1646 - acc: 0.5307 - val_loss: 1.7798 - val_acc: 0.1406\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1831 - acc: 0.5119 - val_loss: 1.8172 - val_acc: 0.1406\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1553 - acc: 0.5137 - val_loss: 1.7934 - val_acc: 0.1562\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1834 - acc: 0.5051 - val_loss: 1.7953 - val_acc: 0.1719\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1453 - acc: 0.5239 - val_loss: 1.7073 - val_acc: 0.1562\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1177 - acc: 0.5512 - val_loss: 1.7347 - val_acc: 0.1719\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1347 - acc: 0.5392 - val_loss: 1.7693 - val_acc: 0.1406\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1366 - acc: 0.5222 - val_loss: 1.7594 - val_acc: 0.1719\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0868 - acc: 0.5392 - val_loss: 1.7326 - val_acc: 0.1562\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2215 - acc: 0.4898 - val_loss: 1.8089 - val_acc: 0.1562\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0743 - acc: 0.5578 - val_loss: 1.8151 - val_acc: 0.1406\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1499 - acc: 0.5495 - val_loss: 1.7530 - val_acc: 0.1562\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1598 - acc: 0.5102 - val_loss: 1.6680 - val_acc: 0.1875\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2008 - acc: 0.5256 - val_loss: 1.7048 - val_acc: 0.1406\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1664 - acc: 0.4915 - val_loss: 1.8567 - val_acc: 0.1406\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1827 - acc: 0.5495 - val_loss: 1.7740 - val_acc: 0.1562\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0993 - acc: 0.5290 - val_loss: 1.8297 - val_acc: 0.1562\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2113 - acc: 0.5290 - val_loss: 1.7943 - val_acc: 0.1406\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2092 - acc: 0.5102 - val_loss: 1.8233 - val_acc: 0.1562\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2000 - acc: 0.5085 - val_loss: 1.8291 - val_acc: 0.1250\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1571 - acc: 0.5392 - val_loss: 1.8011 - val_acc: 0.1562\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1765 - acc: 0.5034 - val_loss: 1.7766 - val_acc: 0.1406\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1226 - acc: 0.5375 - val_loss: 1.7525 - val_acc: 0.1719\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2574 - acc: 0.5017 - val_loss: 1.7672 - val_acc: 0.1562\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0996 - acc: 0.5495 - val_loss: 1.8104 - val_acc: 0.1562\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0741 - acc: 0.5444 - val_loss: 1.7758 - val_acc: 0.1719\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1593 - acc: 0.5239 - val_loss: 1.7633 - val_acc: 0.1875\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1706 - acc: 0.5273 - val_loss: 1.7421 - val_acc: 0.1875\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1736 - acc: 0.5312 - val_loss: 1.8757 - val_acc: 0.1406\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0818 - acc: 0.5444 - val_loss: 1.8265 - val_acc: 0.1406\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1702 - acc: 0.5392 - val_loss: 1.7055 - val_acc: 0.1719\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0918 - acc: 0.5683 - val_loss: 1.7380 - val_acc: 0.1719\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2190 - acc: 0.5119 - val_loss: 1.7906 - val_acc: 0.1562\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1609 - acc: 0.5154 - val_loss: 1.6849 - val_acc: 0.1719\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1280 - acc: 0.5495 - val_loss: 1.6924 - val_acc: 0.1406\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0728 - acc: 0.5512 - val_loss: 1.6743 - val_acc: 0.1875\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1319 - acc: 0.5256 - val_loss: 1.7110 - val_acc: 0.1562\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2310 - acc: 0.5068 - val_loss: 1.6666 - val_acc: 0.1719\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1093 - acc: 0.5359 - val_loss: 1.7065 - val_acc: 0.1875\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1401 - acc: 0.5444 - val_loss: 1.7125 - val_acc: 0.1719\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0884 - acc: 0.5427 - val_loss: 1.7148 - val_acc: 0.1562\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1411 - acc: 0.5119 - val_loss: 1.6331 - val_acc: 0.1875\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0661 - acc: 0.5375 - val_loss: 1.6955 - val_acc: 0.1719\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1510 - acc: 0.5290 - val_loss: 1.7528 - val_acc: 0.1562\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1727 - acc: 0.4966 - val_loss: 1.7625 - val_acc: 0.1562\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1278 - acc: 0.5324 - val_loss: 1.6995 - val_acc: 0.1875\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0777 - acc: 0.5529 - val_loss: 1.8059 - val_acc: 0.1562\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1594 - acc: 0.5273 - val_loss: 1.7882 - val_acc: 0.1719\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1734 - acc: 0.5256 - val_loss: 1.7743 - val_acc: 0.1562\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1449 - acc: 0.5188 - val_loss: 1.8318 - val_acc: 0.1562\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0362 - acc: 0.5444 - val_loss: 1.7746 - val_acc: 0.1562\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1116 - acc: 0.5563 - val_loss: 1.7374 - val_acc: 0.1406\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0881 - acc: 0.5358 - val_loss: 1.7437 - val_acc: 0.1719\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1312 - acc: 0.5256 - val_loss: 1.7604 - val_acc: 0.1406\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1340 - acc: 0.5290 - val_loss: 1.7778 - val_acc: 0.1562\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1813 - acc: 0.5034 - val_loss: 1.6509 - val_acc: 0.1562\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1117 - acc: 0.5102 - val_loss: 1.7216 - val_acc: 0.1719\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.2263 - acc: 0.5017 - val_loss: 1.7701 - val_acc: 0.1406\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0627 - acc: 0.5597 - val_loss: 1.7854 - val_acc: 0.1562\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0642 - acc: 0.5683 - val_loss: 1.6400 - val_acc: 0.1562\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1224 - acc: 0.5068 - val_loss: 1.7456 - val_acc: 0.1719\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1495 - acc: 0.5205 - val_loss: 1.7760 - val_acc: 0.1719\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1732 - acc: 0.5068 - val_loss: 1.6965 - val_acc: 0.1875\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1171 - acc: 0.5478 - val_loss: 1.8156 - val_acc: 0.1406\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0848 - acc: 0.5444 - val_loss: 1.7907 - val_acc: 0.1250\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0953 - acc: 0.5444 - val_loss: 1.8007 - val_acc: 0.1406\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1659 - acc: 0.5188 - val_loss: 1.7573 - val_acc: 0.1406\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1440 - acc: 0.5290 - val_loss: 1.6914 - val_acc: 0.1875\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0909 - acc: 0.5700 - val_loss: 1.8154 - val_acc: 0.1406\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1756 - acc: 0.5137 - val_loss: 1.7604 - val_acc: 0.1875\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1152 - acc: 0.5375 - val_loss: 1.8903 - val_acc: 0.1094\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1172 - acc: 0.5410 - val_loss: 1.8221 - val_acc: 0.1406\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1362 - acc: 0.5358 - val_loss: 1.6693 - val_acc: 0.1875\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1971 - acc: 0.5017 - val_loss: 1.6810 - val_acc: 0.1562\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1613 - acc: 0.5290 - val_loss: 1.7273 - val_acc: 0.1719\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0974 - acc: 0.5392 - val_loss: 1.7790 - val_acc: 0.1719\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0742 - acc: 0.5375 - val_loss: 1.7947 - val_acc: 0.1406\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1112 - acc: 0.4898 - val_loss: 1.6807 - val_acc: 0.1719\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1167 - acc: 0.5461 - val_loss: 1.7923 - val_acc: 0.1406\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0691 - acc: 0.5546 - val_loss: 1.6897 - val_acc: 0.1875\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0629 - acc: 0.5614 - val_loss: 1.7124 - val_acc: 0.1250\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1325 - acc: 0.5290 - val_loss: 1.6883 - val_acc: 0.1719\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1117 - acc: 0.5222 - val_loss: 1.6463 - val_acc: 0.1875\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1590 - acc: 0.5102 - val_loss: 1.7543 - val_acc: 0.1562\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0881 - acc: 0.5290 - val_loss: 1.8124 - val_acc: 0.1250\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0768 - acc: 0.5495 - val_loss: 1.8911 - val_acc: 0.0938\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0958 - acc: 0.5478 - val_loss: 1.7392 - val_acc: 0.1719\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1295 - acc: 0.5290 - val_loss: 1.7666 - val_acc: 0.1719\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0582 - acc: 0.5444 - val_loss: 1.8061 - val_acc: 0.1562\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1201 - acc: 0.5358 - val_loss: 1.7381 - val_acc: 0.1875\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1045 - acc: 0.5444 - val_loss: 1.7650 - val_acc: 0.1562\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0845 - acc: 0.5580 - val_loss: 1.7874 - val_acc: 0.1562\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1205 - acc: 0.5578 - val_loss: 1.6758 - val_acc: 0.1875\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1035 - acc: 0.5546 - val_loss: 1.7349 - val_acc: 0.1719\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1309 - acc: 0.5307 - val_loss: 1.7012 - val_acc: 0.1719\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0935 - acc: 0.5546 - val_loss: 1.7450 - val_acc: 0.1562\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1125 - acc: 0.5341 - val_loss: 1.6671 - val_acc: 0.1719\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1438 - acc: 0.5188 - val_loss: 1.7666 - val_acc: 0.1406\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1814 - acc: 0.5375 - val_loss: 1.6553 - val_acc: 0.1875\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.1505 - acc: 0.5205 - val_loss: 1.6743 - val_acc: 0.1875\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0454 - acc: 0.5597 - val_loss: 1.6258 - val_acc: 0.2031\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1593 - acc: 0.5256 - val_loss: 1.6853 - val_acc: 0.1875\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0844 - acc: 0.5546 - val_loss: 1.7209 - val_acc: 0.1875\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0749 - acc: 0.5547 - val_loss: 1.7291 - val_acc: 0.1719\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0292 - acc: 0.5614 - val_loss: 1.6908 - val_acc: 0.2031\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1112 - acc: 0.5234 - val_loss: 1.6729 - val_acc: 0.1875\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0935 - acc: 0.5290 - val_loss: 1.7278 - val_acc: 0.1719\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1263 - acc: 0.5172 - val_loss: 1.6518 - val_acc: 0.1719\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1341 - acc: 0.5290 - val_loss: 1.6647 - val_acc: 0.1875\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0388 - acc: 0.5768 - val_loss: 1.7048 - val_acc: 0.1719\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1686 - acc: 0.4966 - val_loss: 1.8017 - val_acc: 0.1562\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0774 - acc: 0.5594 - val_loss: 1.7204 - val_acc: 0.1719\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0953 - acc: 0.5281 - val_loss: 1.7989 - val_acc: 0.1562\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0699 - acc: 0.5444 - val_loss: 1.6872 - val_acc: 0.1719\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0532 - acc: 0.5614 - val_loss: 1.7255 - val_acc: 0.1875\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0719 - acc: 0.5546 - val_loss: 1.8354 - val_acc: 0.1406\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1399 - acc: 0.5102 - val_loss: 1.7272 - val_acc: 0.1719\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1101 - acc: 0.5546 - val_loss: 1.7883 - val_acc: 0.1562\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0779 - acc: 0.5529 - val_loss: 1.7505 - val_acc: 0.1562\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0704 - acc: 0.5512 - val_loss: 1.7776 - val_acc: 0.1406\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0422 - acc: 0.5631 - val_loss: 1.6761 - val_acc: 0.1719\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0177 - acc: 0.5648 - val_loss: 1.6647 - val_acc: 0.2031\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1175 - acc: 0.5119 - val_loss: 1.6592 - val_acc: 0.1875\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0587 - acc: 0.5597 - val_loss: 1.7774 - val_acc: 0.1562\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0950 - acc: 0.5410 - val_loss: 1.7350 - val_acc: 0.1719\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1534 - acc: 0.5205 - val_loss: 1.7374 - val_acc: 0.1562\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1361 - acc: 0.5256 - val_loss: 1.7663 - val_acc: 0.1562\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9883 - acc: 0.5717 - val_loss: 1.7097 - val_acc: 0.1875\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0807 - acc: 0.5410 - val_loss: 1.6917 - val_acc: 0.1719\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0922 - acc: 0.5290 - val_loss: 1.7136 - val_acc: 0.1719\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9995 - acc: 0.5836 - val_loss: 1.7049 - val_acc: 0.1719\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.0534 - acc: 0.5594 - val_loss: 1.7916 - val_acc: 0.1562\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0547 - acc: 0.5802 - val_loss: 1.7261 - val_acc: 0.1875\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0154 - acc: 0.5563 - val_loss: 1.8202 - val_acc: 0.1094\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0569 - acc: 0.5546 - val_loss: 1.8313 - val_acc: 0.1250\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1081 - acc: 0.5495 - val_loss: 1.6761 - val_acc: 0.2031\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1446 - acc: 0.5290 - val_loss: 1.6984 - val_acc: 0.1719\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9662 - acc: 0.6024 - val_loss: 1.7862 - val_acc: 0.1562\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9957 - acc: 0.5666 - val_loss: 1.7410 - val_acc: 0.1562\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0609 - acc: 0.5392 - val_loss: 1.7205 - val_acc: 0.1719\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0363 - acc: 0.5768 - val_loss: 1.7404 - val_acc: 0.1719\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0370 - acc: 0.5819 - val_loss: 1.6424 - val_acc: 0.2188\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0885 - acc: 0.5392 - val_loss: 1.6996 - val_acc: 0.1875\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1173 - acc: 0.5461 - val_loss: 1.6098 - val_acc: 0.2031\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0072 - acc: 0.5836 - val_loss: 1.6814 - val_acc: 0.1875\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0407 - acc: 0.5597 - val_loss: 1.7242 - val_acc: 0.1875\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0549 - acc: 0.5358 - val_loss: 1.6735 - val_acc: 0.2031\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0790 - acc: 0.5461 - val_loss: 1.6997 - val_acc: 0.1406\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0324 - acc: 0.5563 - val_loss: 1.7074 - val_acc: 0.2031\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0035 - acc: 0.5853 - val_loss: 1.6727 - val_acc: 0.2188\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0017 - acc: 0.5853 - val_loss: 1.8004 - val_acc: 0.1406\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0525 - acc: 0.5512 - val_loss: 1.7666 - val_acc: 0.1562\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0762 - acc: 0.5614 - val_loss: 1.6623 - val_acc: 0.2031\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1496 - acc: 0.5358 - val_loss: 1.6734 - val_acc: 0.2031\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1469 - acc: 0.5000 - val_loss: 1.7539 - val_acc: 0.1719\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0339 - acc: 0.5904 - val_loss: 1.7331 - val_acc: 0.1875\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0103 - acc: 0.5887 - val_loss: 1.7388 - val_acc: 0.1562\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1368 - acc: 0.5392 - val_loss: 1.6558 - val_acc: 0.1875\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0755 - acc: 0.5597 - val_loss: 1.7501 - val_acc: 0.1875\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0368 - acc: 0.5700 - val_loss: 1.7546 - val_acc: 0.1562\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0866 - acc: 0.5461 - val_loss: 1.7151 - val_acc: 0.1719\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0358 - acc: 0.5734 - val_loss: 1.7996 - val_acc: 0.1562\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0494 - acc: 0.5563 - val_loss: 1.6865 - val_acc: 0.1719\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0725 - acc: 0.5563 - val_loss: 1.6542 - val_acc: 0.2031\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0739 - acc: 0.5392 - val_loss: 1.7475 - val_acc: 0.1719\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1387 - acc: 0.5000 - val_loss: 1.8018 - val_acc: 0.1406\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1281 - acc: 0.5358 - val_loss: 1.7831 - val_acc: 0.1562\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0253 - acc: 0.5751 - val_loss: 1.8092 - val_acc: 0.1562\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0274 - acc: 0.5461 - val_loss: 1.6672 - val_acc: 0.2031\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0325 - acc: 0.5683 - val_loss: 1.6726 - val_acc: 0.1719\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1013 - acc: 0.5461 - val_loss: 1.7432 - val_acc: 0.1875\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0561 - acc: 0.5461 - val_loss: 1.7375 - val_acc: 0.1719\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0818 - acc: 0.5375 - val_loss: 1.7132 - val_acc: 0.2031\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0401 - acc: 0.5478 - val_loss: 1.7642 - val_acc: 0.1562\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0777 - acc: 0.5461 - val_loss: 1.6535 - val_acc: 0.2031\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0900 - acc: 0.5312 - val_loss: 1.7184 - val_acc: 0.1875\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1225 - acc: 0.5410 - val_loss: 1.7544 - val_acc: 0.1719\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0778 - acc: 0.5785 - val_loss: 1.7064 - val_acc: 0.2031\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9953 - acc: 0.5870 - val_loss: 1.7304 - val_acc: 0.1406\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0873 - acc: 0.5324 - val_loss: 1.7033 - val_acc: 0.1875\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0400 - acc: 0.5444 - val_loss: 1.8053 - val_acc: 0.1562\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0234 - acc: 0.5688 - val_loss: 1.7846 - val_acc: 0.1719\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0060 - acc: 0.5973 - val_loss: 1.6936 - val_acc: 0.1562\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0523 - acc: 0.5631 - val_loss: 1.7022 - val_acc: 0.1562\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0465 - acc: 0.5546 - val_loss: 1.6427 - val_acc: 0.1719\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0397 - acc: 0.5427 - val_loss: 1.6478 - val_acc: 0.1719\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0554 - acc: 0.5580 - val_loss: 1.7212 - val_acc: 0.1875\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0967 - acc: 0.5392 - val_loss: 1.7247 - val_acc: 0.1875\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0642 - acc: 0.5453 - val_loss: 1.7470 - val_acc: 0.1875\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1203 - acc: 0.5546 - val_loss: 1.7463 - val_acc: 0.1719\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0820 - acc: 0.5597 - val_loss: 1.6158 - val_acc: 0.2031\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0815 - acc: 0.5478 - val_loss: 1.7931 - val_acc: 0.1406\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0140 - acc: 0.5797 - val_loss: 1.8525 - val_acc: 0.1250\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0293 - acc: 0.5597 - val_loss: 1.7296 - val_acc: 0.1562\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1098 - acc: 0.5512 - val_loss: 1.7414 - val_acc: 0.1719\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.9550 - acc: 0.5939 - val_loss: 1.7425 - val_acc: 0.1875\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.9848 - acc: 0.5939 - val_loss: 1.6070 - val_acc: 0.2031\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1181 - acc: 0.5461 - val_loss: 1.6845 - val_acc: 0.1719\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9852 - acc: 0.5683 - val_loss: 1.6966 - val_acc: 0.2031\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0767 - acc: 0.5597 - val_loss: 1.6845 - val_acc: 0.2031\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0478 - acc: 0.5529 - val_loss: 1.6764 - val_acc: 0.2031\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1132 - acc: 0.5427 - val_loss: 1.6901 - val_acc: 0.1562\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0173 - acc: 0.5819 - val_loss: 1.6105 - val_acc: 0.1875\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0539 - acc: 0.5648 - val_loss: 1.7583 - val_acc: 0.1406\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0094 - acc: 0.5614 - val_loss: 1.7329 - val_acc: 0.1719\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0719 - acc: 0.5239 - val_loss: 1.7276 - val_acc: 0.1562\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0711 - acc: 0.5546 - val_loss: 1.6862 - val_acc: 0.2031\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0582 - acc: 0.5546 - val_loss: 1.6933 - val_acc: 0.1875\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0576 - acc: 0.5703 - val_loss: 1.6991 - val_acc: 0.1406\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0148 - acc: 0.5666 - val_loss: 1.7460 - val_acc: 0.1875\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0579 - acc: 0.5427 - val_loss: 1.6579 - val_acc: 0.2031\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0086 - acc: 0.5609 - val_loss: 1.6556 - val_acc: 0.2031\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0811 - acc: 0.5461 - val_loss: 1.7037 - val_acc: 0.2031\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0764 - acc: 0.5307 - val_loss: 1.6800 - val_acc: 0.1719\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9669 - acc: 0.6024 - val_loss: 1.7133 - val_acc: 0.1719\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0467 - acc: 0.5785 - val_loss: 1.8891 - val_acc: 0.0938\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0599 - acc: 0.5768 - val_loss: 1.6834 - val_acc: 0.2031\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0857 - acc: 0.5563 - val_loss: 1.6912 - val_acc: 0.1719\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0695 - acc: 0.5256 - val_loss: 1.7477 - val_acc: 0.1719\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0565 - acc: 0.5734 - val_loss: 1.6064 - val_acc: 0.1875\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0315 - acc: 0.5870 - val_loss: 1.7022 - val_acc: 0.1875\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0557 - acc: 0.5614 - val_loss: 1.6247 - val_acc: 0.1875\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0330 - acc: 0.5563 - val_loss: 1.7900 - val_acc: 0.1719\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0386 - acc: 0.5751 - val_loss: 1.7406 - val_acc: 0.1875\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0248 - acc: 0.5717 - val_loss: 1.6927 - val_acc: 0.2031\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0259 - acc: 0.5427 - val_loss: 1.7163 - val_acc: 0.1719\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0181 - acc: 0.5529 - val_loss: 1.7105 - val_acc: 0.1875\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0440 - acc: 0.5751 - val_loss: 1.6483 - val_acc: 0.2188\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0560 - acc: 0.5614 - val_loss: 1.5798 - val_acc: 0.2031\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0580 - acc: 0.5256 - val_loss: 1.7011 - val_acc: 0.2031\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0479 - acc: 0.5546 - val_loss: 1.6505 - val_acc: 0.2031\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0784 - acc: 0.5648 - val_loss: 1.6823 - val_acc: 0.1875\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0350 - acc: 0.5614 - val_loss: 1.6929 - val_acc: 0.2188\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0117 - acc: 0.5478 - val_loss: 1.7329 - val_acc: 0.1875\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0022 - acc: 0.5768 - val_loss: 1.7278 - val_acc: 0.1875\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0537 - acc: 0.5580 - val_loss: 1.7306 - val_acc: 0.1562\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9768 - acc: 0.5751 - val_loss: 1.6627 - val_acc: 0.2031\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0371 - acc: 0.5751 - val_loss: 1.6617 - val_acc: 0.1875\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0607 - acc: 0.5614 - val_loss: 1.6335 - val_acc: 0.2031\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0375 - acc: 0.5546 - val_loss: 1.7612 - val_acc: 0.1719\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1421 - acc: 0.5427 - val_loss: 1.6874 - val_acc: 0.2031\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0405 - acc: 0.5410 - val_loss: 1.7215 - val_acc: 0.1719\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0042 - acc: 0.5734 - val_loss: 1.6944 - val_acc: 0.1719\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0662 - acc: 0.5785 - val_loss: 1.6958 - val_acc: 0.1719\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0777 - acc: 0.5358 - val_loss: 1.6860 - val_acc: 0.2031\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0210 - acc: 0.5700 - val_loss: 1.7739 - val_acc: 0.1406\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0022 - acc: 0.5828 - val_loss: 1.7377 - val_acc: 0.1719\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0330 - acc: 0.5656 - val_loss: 1.7413 - val_acc: 0.1875\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0446 - acc: 0.5672 - val_loss: 1.6633 - val_acc: 0.2031\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0347 - acc: 0.5641 - val_loss: 1.8021 - val_acc: 0.1406\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0068 - acc: 0.5751 - val_loss: 1.7323 - val_acc: 0.1562\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9710 - acc: 0.5717 - val_loss: 1.7039 - val_acc: 0.1875\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0260 - acc: 0.5529 - val_loss: 1.6757 - val_acc: 0.1875\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9964 - acc: 0.5734 - val_loss: 1.8204 - val_acc: 0.1406\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0015 - acc: 0.5648 - val_loss: 1.7591 - val_acc: 0.1562\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0283 - acc: 0.5580 - val_loss: 1.6594 - val_acc: 0.1719\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0690 - acc: 0.5631 - val_loss: 1.7570 - val_acc: 0.1719\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9474 - acc: 0.6041 - val_loss: 1.7414 - val_acc: 0.1406\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0599 - acc: 0.5597 - val_loss: 1.6699 - val_acc: 0.2031\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9888 - acc: 0.5956 - val_loss: 1.6883 - val_acc: 0.1875\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0088 - acc: 0.5683 - val_loss: 1.5803 - val_acc: 0.2500\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0220 - acc: 0.5410 - val_loss: 1.6476 - val_acc: 0.1719\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0655 - acc: 0.5512 - val_loss: 1.6527 - val_acc: 0.1875\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0021 - acc: 0.5648 - val_loss: 1.6656 - val_acc: 0.2188\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0290 - acc: 0.5580 - val_loss: 1.7200 - val_acc: 0.1562\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0040 - acc: 0.5819 - val_loss: 1.6990 - val_acc: 0.2031\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0786 - acc: 0.5495 - val_loss: 1.6166 - val_acc: 0.2031\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9376 - acc: 0.5887 - val_loss: 1.6605 - val_acc: 0.2031\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9625 - acc: 0.5700 - val_loss: 1.6365 - val_acc: 0.2031\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9633 - acc: 0.5887 - val_loss: 1.6954 - val_acc: 0.2188\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0326 - acc: 0.5802 - val_loss: 1.7738 - val_acc: 0.1562\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0019 - acc: 0.5563 - val_loss: 1.7055 - val_acc: 0.1875\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0323 - acc: 0.5734 - val_loss: 1.6963 - val_acc: 0.1875\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9830 - acc: 0.5751 - val_loss: 1.6659 - val_acc: 0.2188\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9979 - acc: 0.5922 - val_loss: 1.6901 - val_acc: 0.2500\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9050 - acc: 0.6007 - val_loss: 1.6734 - val_acc: 0.2031\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0271 - acc: 0.5819 - val_loss: 1.6306 - val_acc: 0.2188\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9985 - acc: 0.5631 - val_loss: 1.6583 - val_acc: 0.2188\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0198 - acc: 0.5580 - val_loss: 1.6032 - val_acc: 0.2031\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9877 - acc: 0.5922 - val_loss: 1.6484 - val_acc: 0.2188\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0313 - acc: 0.5672 - val_loss: 1.5704 - val_acc: 0.2031\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0054 - acc: 0.5512 - val_loss: 1.6463 - val_acc: 0.2344\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0154 - acc: 0.5614 - val_loss: 1.6116 - val_acc: 0.2188\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0464 - acc: 0.5751 - val_loss: 1.6856 - val_acc: 0.1875\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9562 - acc: 0.5597 - val_loss: 1.6573 - val_acc: 0.2031\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9983 - acc: 0.5648 - val_loss: 1.7273 - val_acc: 0.2031\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9487 - acc: 0.5887 - val_loss: 1.6576 - val_acc: 0.2344\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9477 - acc: 0.5819 - val_loss: 1.6294 - val_acc: 0.2188\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9814 - acc: 0.5990 - val_loss: 1.6603 - val_acc: 0.2031\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9890 - acc: 0.5785 - val_loss: 1.6806 - val_acc: 0.2188\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0702 - acc: 0.5666 - val_loss: 1.7710 - val_acc: 0.1719\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9949 - acc: 0.5904 - val_loss: 1.6688 - val_acc: 0.2344\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9805 - acc: 0.6092 - val_loss: 1.6934 - val_acc: 0.2188\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0026 - acc: 0.5836 - val_loss: 1.7501 - val_acc: 0.1562\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9513 - acc: 0.6092 - val_loss: 1.6813 - val_acc: 0.2344\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 1.0540 - acc: 0.5751 - val_loss: 1.6275 - val_acc: 0.2188\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0141 - acc: 0.5700 - val_loss: 1.6069 - val_acc: 0.2344\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0315 - acc: 0.5563 - val_loss: 1.7181 - val_acc: 0.2344\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0407 - acc: 0.5700 - val_loss: 1.7303 - val_acc: 0.1719\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0097 - acc: 0.5700 - val_loss: 1.7518 - val_acc: 0.1875\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0609 - acc: 0.5597 - val_loss: 1.5740 - val_acc: 0.2344\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0285 - acc: 0.5648 - val_loss: 1.8069 - val_acc: 0.1406\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9019 - acc: 0.6229 - val_loss: 1.7136 - val_acc: 0.2031\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0151 - acc: 0.5683 - val_loss: 1.7038 - val_acc: 0.2031\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0407 - acc: 0.5666 - val_loss: 1.7526 - val_acc: 0.1719\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9293 - acc: 0.6007 - val_loss: 1.6611 - val_acc: 0.2188\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9790 - acc: 0.5853 - val_loss: 1.6782 - val_acc: 0.2188\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9678 - acc: 0.5875 - val_loss: 1.6728 - val_acc: 0.2344\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9441 - acc: 0.6024 - val_loss: 1.6020 - val_acc: 0.2500\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0056 - acc: 0.5683 - val_loss: 1.6634 - val_acc: 0.2188\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0144 - acc: 0.5836 - val_loss: 1.6629 - val_acc: 0.2031\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9620 - acc: 0.6075 - val_loss: 1.6839 - val_acc: 0.2031\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9922 - acc: 0.5956 - val_loss: 1.6285 - val_acc: 0.2188\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9658 - acc: 0.5887 - val_loss: 1.7162 - val_acc: 0.2031\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0194 - acc: 0.5478 - val_loss: 1.6519 - val_acc: 0.2031\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9951 - acc: 0.5734 - val_loss: 1.5924 - val_acc: 0.2500\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9746 - acc: 0.5819 - val_loss: 1.6591 - val_acc: 0.2344\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0082 - acc: 0.5751 - val_loss: 1.6710 - val_acc: 0.2031\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0076 - acc: 0.5410 - val_loss: 1.6377 - val_acc: 0.2500\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9805 - acc: 0.6092 - val_loss: 1.6819 - val_acc: 0.2188\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0367 - acc: 0.5666 - val_loss: 1.6344 - val_acc: 0.2344\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9666 - acc: 0.5478 - val_loss: 1.6432 - val_acc: 0.2188\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0251 - acc: 0.5512 - val_loss: 1.6906 - val_acc: 0.2031\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9664 - acc: 0.5973 - val_loss: 1.6385 - val_acc: 0.2188\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0288 - acc: 0.5597 - val_loss: 1.6181 - val_acc: 0.2188\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9924 - acc: 0.5717 - val_loss: 1.7275 - val_acc: 0.1875\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9702 - acc: 0.6143 - val_loss: 1.7434 - val_acc: 0.1875\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0233 - acc: 0.5580 - val_loss: 1.6932 - val_acc: 0.2031\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0135 - acc: 0.5683 - val_loss: 1.6394 - val_acc: 0.2031\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9507 - acc: 0.5887 - val_loss: 1.6074 - val_acc: 0.2188\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9392 - acc: 0.5956 - val_loss: 1.7370 - val_acc: 0.2031\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9705 - acc: 0.5683 - val_loss: 1.7526 - val_acc: 0.1875\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9652 - acc: 0.5751 - val_loss: 1.6714 - val_acc: 0.2031\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0038 - acc: 0.5427 - val_loss: 1.6579 - val_acc: 0.2031\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9898 - acc: 0.5717 - val_loss: 1.6707 - val_acc: 0.2031\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9372 - acc: 0.6007 - val_loss: 1.7011 - val_acc: 0.2031\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0470 - acc: 0.5461 - val_loss: 1.7434 - val_acc: 0.1562\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9772 - acc: 0.5597 - val_loss: 1.7133 - val_acc: 0.2031\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9887 - acc: 0.5700 - val_loss: 1.5922 - val_acc: 0.2031\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0447 - acc: 0.5427 - val_loss: 1.6316 - val_acc: 0.2344\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9674 - acc: 0.5836 - val_loss: 1.6411 - val_acc: 0.2344\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0206 - acc: 0.5631 - val_loss: 1.6565 - val_acc: 0.2188\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0085 - acc: 0.5785 - val_loss: 1.5658 - val_acc: 0.2500\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0487 - acc: 0.5580 - val_loss: 1.6656 - val_acc: 0.2031\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9797 - acc: 0.5785 - val_loss: 1.6085 - val_acc: 0.2188\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9785 - acc: 0.5785 - val_loss: 1.6582 - val_acc: 0.2188\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9772 - acc: 0.6092 - val_loss: 1.5524 - val_acc: 0.2500\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0219 - acc: 0.5529 - val_loss: 1.6481 - val_acc: 0.1719\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9592 - acc: 0.5785 - val_loss: 1.6250 - val_acc: 0.2500\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9768 - acc: 0.5672 - val_loss: 1.5903 - val_acc: 0.2500\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0055 - acc: 0.5717 - val_loss: 1.6927 - val_acc: 0.2031\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8970 - acc: 0.6007 - val_loss: 1.6204 - val_acc: 0.2500\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9741 - acc: 0.5939 - val_loss: 1.6141 - val_acc: 0.2500\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9639 - acc: 0.5717 - val_loss: 1.6120 - val_acc: 0.1875\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9890 - acc: 0.5785 - val_loss: 1.6509 - val_acc: 0.2188\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0125 - acc: 0.5461 - val_loss: 1.6437 - val_acc: 0.2344\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9952 - acc: 0.5768 - val_loss: 1.6672 - val_acc: 0.2188\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0121 - acc: 0.5648 - val_loss: 1.6331 - val_acc: 0.2188\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9767 - acc: 0.5785 - val_loss: 1.6263 - val_acc: 0.2500\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9996 - acc: 0.5836 - val_loss: 1.6168 - val_acc: 0.2344\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9739 - acc: 0.5785 - val_loss: 1.6397 - val_acc: 0.2188\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0069 - acc: 0.5717 - val_loss: 1.6660 - val_acc: 0.2188\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0112 - acc: 0.5580 - val_loss: 1.6900 - val_acc: 0.2188\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9773 - acc: 0.5734 - val_loss: 1.7053 - val_acc: 0.2031\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0111 - acc: 0.5802 - val_loss: 1.7326 - val_acc: 0.1875\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9893 - acc: 0.5597 - val_loss: 1.6106 - val_acc: 0.2500\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9115 - acc: 0.6126 - val_loss: 1.6421 - val_acc: 0.2188\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9978 - acc: 0.5768 - val_loss: 1.6628 - val_acc: 0.2031\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9394 - acc: 0.6058 - val_loss: 1.5617 - val_acc: 0.2500\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9885 - acc: 0.5939 - val_loss: 1.6677 - val_acc: 0.2344\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9381 - acc: 0.6160 - val_loss: 1.5730 - val_acc: 0.2344\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9616 - acc: 0.6058 - val_loss: 1.7598 - val_acc: 0.1719\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0027 - acc: 0.5819 - val_loss: 1.6554 - val_acc: 0.2344\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9900 - acc: 0.6007 - val_loss: 1.5736 - val_acc: 0.2188\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0374 - acc: 0.5495 - val_loss: 1.5840 - val_acc: 0.2344\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9465 - acc: 0.5922 - val_loss: 1.5953 - val_acc: 0.2500\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9334 - acc: 0.6041 - val_loss: 1.5865 - val_acc: 0.2344\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9907 - acc: 0.5683 - val_loss: 1.5451 - val_acc: 0.2656\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0123 - acc: 0.5666 - val_loss: 1.5856 - val_acc: 0.2188\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9202 - acc: 0.5887 - val_loss: 1.5758 - val_acc: 0.2344\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9692 - acc: 0.5785 - val_loss: 1.5978 - val_acc: 0.2344\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9778 - acc: 0.5819 - val_loss: 1.6484 - val_acc: 0.2031\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9419 - acc: 0.5768 - val_loss: 1.6561 - val_acc: 0.2344\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9122 - acc: 0.6109 - val_loss: 1.5891 - val_acc: 0.2500\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9542 - acc: 0.6024 - val_loss: 1.6621 - val_acc: 0.1875\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9969 - acc: 0.5717 - val_loss: 1.6190 - val_acc: 0.2344\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9658 - acc: 0.6075 - val_loss: 1.6132 - val_acc: 0.2344\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9473 - acc: 0.6007 - val_loss: 1.6189 - val_acc: 0.2344\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9678 - acc: 0.5734 - val_loss: 1.5718 - val_acc: 0.2500\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0345 - acc: 0.5700 - val_loss: 1.6521 - val_acc: 0.2344\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9346 - acc: 0.5887 - val_loss: 1.5862 - val_acc: 0.2344\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9002 - acc: 0.6195 - val_loss: 1.5696 - val_acc: 0.2188\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9709 - acc: 0.5836 - val_loss: 1.5752 - val_acc: 0.2344\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9577 - acc: 0.5870 - val_loss: 1.5569 - val_acc: 0.2344\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9665 - acc: 0.5751 - val_loss: 1.6183 - val_acc: 0.2188\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9818 - acc: 0.5969 - val_loss: 1.5838 - val_acc: 0.2344\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9209 - acc: 0.5990 - val_loss: 1.5796 - val_acc: 0.2344\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0157 - acc: 0.5580 - val_loss: 1.6120 - val_acc: 0.2188\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9814 - acc: 0.5597 - val_loss: 1.6709 - val_acc: 0.2188\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9535 - acc: 0.5887 - val_loss: 1.7017 - val_acc: 0.2031\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9027 - acc: 0.6212 - val_loss: 1.6948 - val_acc: 0.2188\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9090 - acc: 0.6160 - val_loss: 1.5794 - val_acc: 0.2344\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9102 - acc: 0.5734 - val_loss: 1.5478 - val_acc: 0.2344\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9936 - acc: 0.5700 - val_loss: 1.6187 - val_acc: 0.2031\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0366 - acc: 0.5495 - val_loss: 1.5680 - val_acc: 0.2344\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9978 - acc: 0.5887 - val_loss: 1.6961 - val_acc: 0.1875\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9344 - acc: 0.5956 - val_loss: 1.6792 - val_acc: 0.1719\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9240 - acc: 0.5836 - val_loss: 1.6723 - val_acc: 0.2188\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9924 - acc: 0.5906 - val_loss: 1.7065 - val_acc: 0.2188\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9718 - acc: 0.5546 - val_loss: 1.6933 - val_acc: 0.2031\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.9506 - acc: 0.5875 - val_loss: 1.7095 - val_acc: 0.2188\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9682 - acc: 0.5922 - val_loss: 1.6180 - val_acc: 0.2344\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9117 - acc: 0.6177 - val_loss: 1.5879 - val_acc: 0.2656\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9141 - acc: 0.5956 - val_loss: 1.7264 - val_acc: 0.1875\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9203 - acc: 0.6075 - val_loss: 1.6567 - val_acc: 0.2031\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9372 - acc: 0.5973 - val_loss: 1.5973 - val_acc: 0.2500\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9942 - acc: 0.5768 - val_loss: 1.6621 - val_acc: 0.2031\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9332 - acc: 0.6024 - val_loss: 1.7315 - val_acc: 0.1719\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8860 - acc: 0.6263 - val_loss: 1.6778 - val_acc: 0.2031\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9284 - acc: 0.6092 - val_loss: 1.6664 - val_acc: 0.2344\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9403 - acc: 0.5956 - val_loss: 1.6893 - val_acc: 0.1875\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9850 - acc: 0.5717 - val_loss: 1.5799 - val_acc: 0.2500\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9545 - acc: 0.5939 - val_loss: 1.6218 - val_acc: 0.2188\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9632 - acc: 0.5700 - val_loss: 1.6504 - val_acc: 0.2031\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9539 - acc: 0.5990 - val_loss: 1.6859 - val_acc: 0.1875\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9512 - acc: 0.6177 - val_loss: 1.6276 - val_acc: 0.2344\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9460 - acc: 0.5734 - val_loss: 1.6682 - val_acc: 0.2188\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9708 - acc: 0.6024 - val_loss: 1.6697 - val_acc: 0.1875\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9594 - acc: 0.5973 - val_loss: 1.5588 - val_acc: 0.2500\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9329 - acc: 0.5973 - val_loss: 1.6383 - val_acc: 0.2188\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9882 - acc: 0.5768 - val_loss: 1.6793 - val_acc: 0.1719\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8780 - acc: 0.6246 - val_loss: 1.7350 - val_acc: 0.1875\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9134 - acc: 0.6092 - val_loss: 1.5240 - val_acc: 0.2500\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9585 - acc: 0.5802 - val_loss: 1.6311 - val_acc: 0.2344\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9671 - acc: 0.6024 - val_loss: 1.5855 - val_acc: 0.2344\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0114 - acc: 0.5683 - val_loss: 1.6376 - val_acc: 0.2344\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9475 - acc: 0.5990 - val_loss: 1.6401 - val_acc: 0.2344\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9395 - acc: 0.5990 - val_loss: 1.6615 - val_acc: 0.2031\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9348 - acc: 0.6007 - val_loss: 1.6712 - val_acc: 0.2188\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9744 - acc: 0.5887 - val_loss: 1.6420 - val_acc: 0.2031\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9448 - acc: 0.6246 - val_loss: 1.5848 - val_acc: 0.2344\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9158 - acc: 0.6075 - val_loss: 1.5649 - val_acc: 0.2188\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8845 - acc: 0.6041 - val_loss: 1.6248 - val_acc: 0.2031\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0087 - acc: 0.5802 - val_loss: 1.6008 - val_acc: 0.2188\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9900 - acc: 0.5853 - val_loss: 1.6244 - val_acc: 0.1875\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9889 - acc: 0.5813 - val_loss: 1.6561 - val_acc: 0.1875\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9627 - acc: 0.5990 - val_loss: 1.5938 - val_acc: 0.2344\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9908 - acc: 0.5700 - val_loss: 1.6250 - val_acc: 0.1875\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9218 - acc: 0.5973 - val_loss: 1.6304 - val_acc: 0.2031\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9035 - acc: 0.5956 - val_loss: 1.5655 - val_acc: 0.2031\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9386 - acc: 0.6007 - val_loss: 1.6385 - val_acc: 0.2188\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8850 - acc: 0.6092 - val_loss: 1.6174 - val_acc: 0.2344\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9121 - acc: 0.6246 - val_loss: 1.6480 - val_acc: 0.2188\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9694 - acc: 0.5922 - val_loss: 1.5904 - val_acc: 0.2031\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9785 - acc: 0.6092 - val_loss: 1.5823 - val_acc: 0.2344\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9722 - acc: 0.5844 - val_loss: 1.5742 - val_acc: 0.2188\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8926 - acc: 0.6058 - val_loss: 1.5801 - val_acc: 0.2188\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9137 - acc: 0.6092 - val_loss: 1.6479 - val_acc: 0.2031\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9189 - acc: 0.6314 - val_loss: 1.6911 - val_acc: 0.1875\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9107 - acc: 0.6212 - val_loss: 1.6868 - val_acc: 0.2188\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9217 - acc: 0.6007 - val_loss: 1.6855 - val_acc: 0.2031\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9782 - acc: 0.5768 - val_loss: 1.5677 - val_acc: 0.2344\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9408 - acc: 0.6058 - val_loss: 1.5521 - val_acc: 0.2656\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8778 - acc: 0.6177 - val_loss: 1.5443 - val_acc: 0.2344\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9671 - acc: 0.5563 - val_loss: 1.5450 - val_acc: 0.2500\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8689 - acc: 0.6109 - val_loss: 1.6052 - val_acc: 0.2500\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9627 - acc: 0.5922 - val_loss: 1.6960 - val_acc: 0.1875\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9216 - acc: 0.5859 - val_loss: 1.5572 - val_acc: 0.2188\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9329 - acc: 0.6314 - val_loss: 1.5281 - val_acc: 0.2344\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9134 - acc: 0.5870 - val_loss: 1.6767 - val_acc: 0.2188\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9553 - acc: 0.5887 - val_loss: 1.6274 - val_acc: 0.2188\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8845 - acc: 0.6297 - val_loss: 1.6172 - val_acc: 0.2188\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8845 - acc: 0.6416 - val_loss: 1.5486 - val_acc: 0.2344\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9138 - acc: 0.6058 - val_loss: 1.6099 - val_acc: 0.2188\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9259 - acc: 0.6041 - val_loss: 1.6124 - val_acc: 0.2188\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9432 - acc: 0.5802 - val_loss: 1.6665 - val_acc: 0.2031\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9085 - acc: 0.6041 - val_loss: 1.6350 - val_acc: 0.2031\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9236 - acc: 0.6058 - val_loss: 1.6692 - val_acc: 0.2188\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9568 - acc: 0.5717 - val_loss: 1.5971 - val_acc: 0.2188\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9911 - acc: 0.5859 - val_loss: 1.6442 - val_acc: 0.2344\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8593 - acc: 0.6246 - val_loss: 1.6120 - val_acc: 0.2188\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0121 - acc: 0.5802 - val_loss: 1.5993 - val_acc: 0.2344\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9325 - acc: 0.5953 - val_loss: 1.6278 - val_acc: 0.2188\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9717 - acc: 0.6143 - val_loss: 1.5930 - val_acc: 0.2344\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9882 - acc: 0.5683 - val_loss: 1.5945 - val_acc: 0.2188\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9127 - acc: 0.6314 - val_loss: 1.6167 - val_acc: 0.2188\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0034 - acc: 0.5870 - val_loss: 1.6030 - val_acc: 0.2031\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9115 - acc: 0.6007 - val_loss: 1.6183 - val_acc: 0.2031\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8924 - acc: 0.6092 - val_loss: 1.5306 - val_acc: 0.2344\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8813 - acc: 0.6246 - val_loss: 1.5338 - val_acc: 0.2500\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8947 - acc: 0.6229 - val_loss: 1.5688 - val_acc: 0.2656\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9098 - acc: 0.6125 - val_loss: 1.5854 - val_acc: 0.2188\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9205 - acc: 0.6109 - val_loss: 1.6379 - val_acc: 0.1875\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0084 - acc: 0.5700 - val_loss: 1.4407 - val_acc: 0.2500\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9279 - acc: 0.5870 - val_loss: 1.5283 - val_acc: 0.2031\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9583 - acc: 0.5802 - val_loss: 1.5739 - val_acc: 0.2344\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9286 - acc: 0.6280 - val_loss: 1.5980 - val_acc: 0.2500\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9008 - acc: 0.5956 - val_loss: 1.5828 - val_acc: 0.2031\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9758 - acc: 0.5819 - val_loss: 1.6240 - val_acc: 0.2031\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9291 - acc: 0.6075 - val_loss: 1.6701 - val_acc: 0.2031\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9024 - acc: 0.6007 - val_loss: 1.5526 - val_acc: 0.2188\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9454 - acc: 0.5922 - val_loss: 1.5439 - val_acc: 0.2344\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9069 - acc: 0.5922 - val_loss: 1.6869 - val_acc: 0.1875\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9345 - acc: 0.6041 - val_loss: 1.6941 - val_acc: 0.1875\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9257 - acc: 0.6297 - val_loss: 1.6270 - val_acc: 0.2188\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8815 - acc: 0.6391 - val_loss: 1.6423 - val_acc: 0.2188\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9339 - acc: 0.5819 - val_loss: 1.6186 - val_acc: 0.2344\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9257 - acc: 0.5904 - val_loss: 1.6242 - val_acc: 0.2188\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9277 - acc: 0.5973 - val_loss: 1.6121 - val_acc: 0.2188\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8580 - acc: 0.6246 - val_loss: 1.6114 - val_acc: 0.2031\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8974 - acc: 0.6348 - val_loss: 1.6139 - val_acc: 0.2344\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 0.8833 - acc: 0.6212 - val_loss: 1.7066 - val_acc: 0.1875\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9051 - acc: 0.6263 - val_loss: 1.5520 - val_acc: 0.2344\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9481 - acc: 0.6007 - val_loss: 1.5735 - val_acc: 0.2656\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9049 - acc: 0.5956 - val_loss: 1.6928 - val_acc: 0.2188\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8968 - acc: 0.5870 - val_loss: 1.6147 - val_acc: 0.2188\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8978 - acc: 0.6280 - val_loss: 1.7082 - val_acc: 0.2031\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9069 - acc: 0.6041 - val_loss: 1.6485 - val_acc: 0.2031\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9111 - acc: 0.5973 - val_loss: 1.5548 - val_acc: 0.2031\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8949 - acc: 0.6195 - val_loss: 1.6418 - val_acc: 0.2188\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8699 - acc: 0.6280 - val_loss: 1.6672 - val_acc: 0.2031\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9094 - acc: 0.6297 - val_loss: 1.6233 - val_acc: 0.2344\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9146 - acc: 0.6177 - val_loss: 1.6612 - val_acc: 0.2188\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9278 - acc: 0.6177 - val_loss: 1.5353 - val_acc: 0.2344\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9023 - acc: 0.5853 - val_loss: 1.6773 - val_acc: 0.2031\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9804 - acc: 0.5819 - val_loss: 1.6372 - val_acc: 0.2188\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9233 - acc: 0.6075 - val_loss: 1.6048 - val_acc: 0.1875\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9422 - acc: 0.5785 - val_loss: 1.6816 - val_acc: 0.1562\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9240 - acc: 0.6075 - val_loss: 1.5651 - val_acc: 0.2344\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9033 - acc: 0.6229 - val_loss: 1.5538 - val_acc: 0.2656\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8894 - acc: 0.6246 - val_loss: 1.6197 - val_acc: 0.1875\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9023 - acc: 0.6092 - val_loss: 1.6400 - val_acc: 0.2188\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8985 - acc: 0.6229 - val_loss: 1.6112 - val_acc: 0.2188\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9677 - acc: 0.5672 - val_loss: 1.6279 - val_acc: 0.2031\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9050 - acc: 0.6143 - val_loss: 1.6109 - val_acc: 0.2188\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9472 - acc: 0.5802 - val_loss: 1.6734 - val_acc: 0.1875\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9480 - acc: 0.5819 - val_loss: 1.6083 - val_acc: 0.2500\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9232 - acc: 0.6058 - val_loss: 1.6013 - val_acc: 0.1875\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9301 - acc: 0.6075 - val_loss: 1.5768 - val_acc: 0.2344\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9314 - acc: 0.6007 - val_loss: 1.5635 - val_acc: 0.2500\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9097 - acc: 0.6041 - val_loss: 1.6370 - val_acc: 0.2031\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9056 - acc: 0.6331 - val_loss: 1.6230 - val_acc: 0.1875\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9032 - acc: 0.6109 - val_loss: 1.7077 - val_acc: 0.1250\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9115 - acc: 0.5836 - val_loss: 1.5705 - val_acc: 0.2031\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8614 - acc: 0.6416 - val_loss: 1.6029 - val_acc: 0.1875\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9310 - acc: 0.6092 - val_loss: 1.6769 - val_acc: 0.1875\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8713 - acc: 0.6281 - val_loss: 1.7280 - val_acc: 0.1719\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9584 - acc: 0.5870 - val_loss: 1.6376 - val_acc: 0.2188\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9502 - acc: 0.5802 - val_loss: 1.6154 - val_acc: 0.2188\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9481 - acc: 0.5887 - val_loss: 1.5884 - val_acc: 0.2500\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8660 - acc: 0.6160 - val_loss: 1.5829 - val_acc: 0.2344\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8898 - acc: 0.5853 - val_loss: 1.5861 - val_acc: 0.2188\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9929 - acc: 0.5751 - val_loss: 1.5921 - val_acc: 0.2031\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9431 - acc: 0.5870 - val_loss: 1.5701 - val_acc: 0.2344\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9924 - acc: 0.5973 - val_loss: 1.5587 - val_acc: 0.2188\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9334 - acc: 0.5853 - val_loss: 1.6298 - val_acc: 0.1875\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9274 - acc: 0.5870 - val_loss: 1.5239 - val_acc: 0.2344\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9742 - acc: 0.5717 - val_loss: 1.5687 - val_acc: 0.2344\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8900 - acc: 0.6126 - val_loss: 1.6266 - val_acc: 0.2031\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9145 - acc: 0.5887 - val_loss: 1.6171 - val_acc: 0.2344\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8911 - acc: 0.6075 - val_loss: 1.5857 - val_acc: 0.2344\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.8893 - acc: 0.6126 - val_loss: 1.5503 - val_acc: 0.2344\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9422 - acc: 0.5939 - val_loss: 1.5873 - val_acc: 0.2188\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.9407 - acc: 0.5922 - val_loss: 1.4887 - val_acc: 0.2500\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9171 - acc: 0.5956 - val_loss: 1.5998 - val_acc: 0.2188\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9095 - acc: 0.6280 - val_loss: 1.6018 - val_acc: 0.2031\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8527 - acc: 0.6109 - val_loss: 1.5747 - val_acc: 0.2344\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8948 - acc: 0.6126 - val_loss: 1.5835 - val_acc: 0.2344\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9364 - acc: 0.6041 - val_loss: 1.5882 - val_acc: 0.2344\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9084 - acc: 0.6024 - val_loss: 1.5090 - val_acc: 0.2344\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8437 - acc: 0.6314 - val_loss: 1.5987 - val_acc: 0.2344\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8930 - acc: 0.6160 - val_loss: 1.5232 - val_acc: 0.2031\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8897 - acc: 0.6212 - val_loss: 1.5658 - val_acc: 0.2344\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9125 - acc: 0.6078 - val_loss: 1.6118 - val_acc: 0.2031\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8558 - acc: 0.6328 - val_loss: 1.6170 - val_acc: 0.2031\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9071 - acc: 0.6024 - val_loss: 1.5702 - val_acc: 0.2344\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9439 - acc: 0.5906 - val_loss: 1.5325 - val_acc: 0.2500\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8854 - acc: 0.6263 - val_loss: 1.6512 - val_acc: 0.1875\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8877 - acc: 0.6109 - val_loss: 1.7075 - val_acc: 0.1406\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8676 - acc: 0.5785 - val_loss: 1.5904 - val_acc: 0.2031\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9593 - acc: 0.5819 - val_loss: 1.6258 - val_acc: 0.2344\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8965 - acc: 0.6109 - val_loss: 1.5216 - val_acc: 0.2500\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9275 - acc: 0.6041 - val_loss: 1.6836 - val_acc: 0.1562\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8994 - acc: 0.6092 - val_loss: 1.5891 - val_acc: 0.2188\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8686 - acc: 0.6075 - val_loss: 1.6245 - val_acc: 0.2188\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9110 - acc: 0.5990 - val_loss: 1.5945 - val_acc: 0.2188\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8723 - acc: 0.6331 - val_loss: 1.6133 - val_acc: 0.2188\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8883 - acc: 0.5904 - val_loss: 1.5797 - val_acc: 0.2344\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8836 - acc: 0.5973 - val_loss: 1.5713 - val_acc: 0.2188\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9333 - acc: 0.6263 - val_loss: 1.6456 - val_acc: 0.2031\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 24s 3s/step - loss: 0.8697 - acc: 0.5922 - val_loss: 1.6042 - val_acc: 0.2188\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9010 - acc: 0.6109 - val_loss: 1.6230 - val_acc: 0.2188\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8384 - acc: 0.6416 - val_loss: 1.5551 - val_acc: 0.2188\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8402 - acc: 0.6331 - val_loss: 1.5229 - val_acc: 0.2344\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8322 - acc: 0.6519 - val_loss: 1.5292 - val_acc: 0.2500\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8749 - acc: 0.6314 - val_loss: 1.6057 - val_acc: 0.2188\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9040 - acc: 0.6041 - val_loss: 1.5825 - val_acc: 0.2031\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8765 - acc: 0.6280 - val_loss: 1.6528 - val_acc: 0.1875\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8922 - acc: 0.5922 - val_loss: 1.6409 - val_acc: 0.2031\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9015 - acc: 0.6280 - val_loss: 1.6055 - val_acc: 0.2188\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8596 - acc: 0.6041 - val_loss: 1.5704 - val_acc: 0.2344\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8381 - acc: 0.6160 - val_loss: 1.6609 - val_acc: 0.2188\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9434 - acc: 0.5802 - val_loss: 1.6978 - val_acc: 0.1719\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9002 - acc: 0.6177 - val_loss: 1.6023 - val_acc: 0.2188\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9092 - acc: 0.6109 - val_loss: 1.6318 - val_acc: 0.1719\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9317 - acc: 0.5768 - val_loss: 1.4540 - val_acc: 0.2500\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9109 - acc: 0.6212 - val_loss: 1.5657 - val_acc: 0.2188\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8911 - acc: 0.6212 - val_loss: 1.6044 - val_acc: 0.2188\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8567 - acc: 0.6519 - val_loss: 1.6147 - val_acc: 0.2031\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9055 - acc: 0.6126 - val_loss: 1.5668 - val_acc: 0.2188\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8780 - acc: 0.6229 - val_loss: 1.6145 - val_acc: 0.2188\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8435 - acc: 0.6382 - val_loss: 1.6799 - val_acc: 0.1875\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8664 - acc: 0.5922 - val_loss: 1.6159 - val_acc: 0.2031\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8989 - acc: 0.6212 - val_loss: 1.5962 - val_acc: 0.2188\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8934 - acc: 0.6280 - val_loss: 1.5669 - val_acc: 0.2344\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8690 - acc: 0.6348 - val_loss: 1.5363 - val_acc: 0.2188\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8594 - acc: 0.6281 - val_loss: 1.5520 - val_acc: 0.2031\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9097 - acc: 0.6126 - val_loss: 1.6110 - val_acc: 0.2031\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8969 - acc: 0.6092 - val_loss: 1.6023 - val_acc: 0.2188\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8381 - acc: 0.6246 - val_loss: 1.5464 - val_acc: 0.2188\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9510 - acc: 0.5887 - val_loss: 1.5261 - val_acc: 0.2188\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8869 - acc: 0.6007 - val_loss: 1.5855 - val_acc: 0.2188\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8434 - acc: 0.6348 - val_loss: 1.6176 - val_acc: 0.2188\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9166 - acc: 0.6024 - val_loss: 1.5969 - val_acc: 0.2188\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9248 - acc: 0.5973 - val_loss: 1.5705 - val_acc: 0.2188\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8503 - acc: 0.6177 - val_loss: 1.5859 - val_acc: 0.2344\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8586 - acc: 0.6177 - val_loss: 1.6224 - val_acc: 0.1875\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8682 - acc: 0.6229 - val_loss: 1.5503 - val_acc: 0.2188\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8988 - acc: 0.6007 - val_loss: 1.6391 - val_acc: 0.1719\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8474 - acc: 0.6092 - val_loss: 1.5604 - val_acc: 0.2188\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8702 - acc: 0.6416 - val_loss: 1.5927 - val_acc: 0.2188\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9102 - acc: 0.6160 - val_loss: 1.5888 - val_acc: 0.2344\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8836 - acc: 0.6297 - val_loss: 1.6407 - val_acc: 0.2031\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8915 - acc: 0.6125 - val_loss: 1.6447 - val_acc: 0.2188\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8819 - acc: 0.6160 - val_loss: 1.6110 - val_acc: 0.2344\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9017 - acc: 0.6007 - val_loss: 1.5653 - val_acc: 0.2344\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8921 - acc: 0.6359 - val_loss: 1.6370 - val_acc: 0.1875\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8530 - acc: 0.6468 - val_loss: 1.5305 - val_acc: 0.2344\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8930 - acc: 0.6177 - val_loss: 1.6473 - val_acc: 0.2188\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8666 - acc: 0.6246 - val_loss: 1.5455 - val_acc: 0.2344\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8372 - acc: 0.6280 - val_loss: 1.6252 - val_acc: 0.2031\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9663 - acc: 0.5836 - val_loss: 1.5717 - val_acc: 0.2031\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9291 - acc: 0.6047 - val_loss: 1.6196 - val_acc: 0.2031\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9041 - acc: 0.6058 - val_loss: 1.5526 - val_acc: 0.2344\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9104 - acc: 0.5973 - val_loss: 1.5847 - val_acc: 0.2188\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9342 - acc: 0.6024 - val_loss: 1.5108 - val_acc: 0.2188\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9388 - acc: 0.6092 - val_loss: 1.5902 - val_acc: 0.2344\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8794 - acc: 0.6160 - val_loss: 1.6496 - val_acc: 0.2031\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8573 - acc: 0.6246 - val_loss: 1.5600 - val_acc: 0.1875\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8345 - acc: 0.6229 - val_loss: 1.5769 - val_acc: 0.2188\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9004 - acc: 0.6263 - val_loss: 1.5468 - val_acc: 0.2344\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9389 - acc: 0.5597 - val_loss: 1.5703 - val_acc: 0.2500\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8610 - acc: 0.6156 - val_loss: 1.5615 - val_acc: 0.2188\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9014 - acc: 0.6160 - val_loss: 1.5381 - val_acc: 0.2500\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8781 - acc: 0.5887 - val_loss: 1.5978 - val_acc: 0.2031\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9768 - acc: 0.5648 - val_loss: 1.5570 - val_acc: 0.2188\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8662 - acc: 0.6177 - val_loss: 1.6750 - val_acc: 0.2031\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8650 - acc: 0.6331 - val_loss: 1.6591 - val_acc: 0.2188\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8580 - acc: 0.6263 - val_loss: 1.7159 - val_acc: 0.1719\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8507 - acc: 0.6348 - val_loss: 1.6265 - val_acc: 0.2188\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8763 - acc: 0.6109 - val_loss: 1.6161 - val_acc: 0.2031\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8506 - acc: 0.6126 - val_loss: 1.5569 - val_acc: 0.2188\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8532 - acc: 0.6365 - val_loss: 1.6421 - val_acc: 0.2031\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8593 - acc: 0.6297 - val_loss: 1.6161 - val_acc: 0.2344\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8866 - acc: 0.6348 - val_loss: 1.5531 - val_acc: 0.2500\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8990 - acc: 0.6203 - val_loss: 1.5444 - val_acc: 0.2344\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8723 - acc: 0.6109 - val_loss: 1.5624 - val_acc: 0.2188\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8434 - acc: 0.6399 - val_loss: 1.5726 - val_acc: 0.2188\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9437 - acc: 0.6007 - val_loss: 1.5570 - val_acc: 0.2188\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8628 - acc: 0.6280 - val_loss: 1.5095 - val_acc: 0.2500\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8713 - acc: 0.6007 - val_loss: 1.5881 - val_acc: 0.2344\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8559 - acc: 0.6229 - val_loss: 1.6042 - val_acc: 0.2031\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8900 - acc: 0.5990 - val_loss: 1.6155 - val_acc: 0.2188\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8610 - acc: 0.6297 - val_loss: 1.5936 - val_acc: 0.2188\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8871 - acc: 0.6143 - val_loss: 1.5812 - val_acc: 0.2344\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8773 - acc: 0.6092 - val_loss: 1.5794 - val_acc: 0.2031\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8747 - acc: 0.6092 - val_loss: 1.5922 - val_acc: 0.2344\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8544 - acc: 0.6246 - val_loss: 1.5637 - val_acc: 0.2500\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8499 - acc: 0.6382 - val_loss: 1.5136 - val_acc: 0.2500\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8521 - acc: 0.6314 - val_loss: 1.5348 - val_acc: 0.2500\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8582 - acc: 0.6433 - val_loss: 1.4961 - val_acc: 0.2344\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8898 - acc: 0.6075 - val_loss: 1.5633 - val_acc: 0.2188\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8706 - acc: 0.5956 - val_loss: 1.5341 - val_acc: 0.2188\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8852 - acc: 0.6212 - val_loss: 1.5164 - val_acc: 0.2344\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8374 - acc: 0.6280 - val_loss: 1.5404 - val_acc: 0.2344\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8460 - acc: 0.6331 - val_loss: 1.6055 - val_acc: 0.2344\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 0.9045 - acc: 0.6212 - val_loss: 1.5195 - val_acc: 0.2188\n",
            "Epoch 994/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1_drop0.3.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1_drop0.3.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}