{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNFg79gezZXyZsxWhavk812",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train_drop0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "661b47fe-1fed-49e3-9fa7-fc631535c7e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data Remake.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "4f5ff722-5bed-4558-c6ed-cb44d3ad5172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-500  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-500  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-500  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-500  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-500  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-500  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-500  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-500  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-500  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-500  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b03a08db-5b86-41eb-b3ad-29c55e5cd2ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b03a08db-5b86-41eb-b3ad-29c55e5cd2ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b03a08db-5b86-41eb-b3ad-29c55e5cd2ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b03a08db-5b86-41eb-b3ad-29c55e5cd2ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "b6f2ae55-9ae1-4c71-cb2f-21db3e7cd028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "144beec8-ccb3-47e1-a24d-45d749492805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "2f6412e7-fe89-47c7-eb15-06c06fdd1c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "60dc345d-e2e4-4850-b65c-30008301a3ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)] \n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "d73c0a78-1465-46c3-fdc1-727328858f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 98\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 71\n",
            "total test 2 images: 6\n",
            "total test 3 images: 46 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 650  # จำนวนภาพ Train\n",
        "NUM_TEST = 75 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "6d8fb66d-6b66-479d-9b9c-112b892b8032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "f2a7c3c1-3f86-445f-a201-2479825310bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "3aef81e5-4e7c-4509-89ad-24be8a3e4da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "a0c72727-3031-4925-da04-6e86db3ee9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "f2dd897b-ff10-4317-e030-ee19f4dccbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 120 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2297a20-7b2e-43e6-826f-79cf83c3b931"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-35-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 19s 634ms/step - loss: 1.9462 - acc: 0.3955 - val_loss: 0.7033 - val_acc: 0.7344\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 3s 220ms/step - loss: 1.9386 - acc: 0.4003 - val_loss: 0.8687 - val_acc: 0.5938\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.8771 - acc: 0.3955 - val_loss: 0.8428 - val_acc: 0.5938\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.8431 - acc: 0.3906 - val_loss: 0.8621 - val_acc: 0.5781\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.7963 - acc: 0.4036 - val_loss: 1.0941 - val_acc: 0.4844\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.8468 - acc: 0.3890 - val_loss: 1.0294 - val_acc: 0.4844\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.7866 - acc: 0.3890 - val_loss: 1.1308 - val_acc: 0.4688\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6873 - acc: 0.4425 - val_loss: 1.0872 - val_acc: 0.4531\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.7134 - acc: 0.4052 - val_loss: 1.2034 - val_acc: 0.4062\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.7586 - acc: 0.3857 - val_loss: 1.2925 - val_acc: 0.3594\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.7155 - acc: 0.4019 - val_loss: 1.3518 - val_acc: 0.2656\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 1.7808 - acc: 0.3793 - val_loss: 1.2352 - val_acc: 0.3281\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.7462 - acc: 0.4182 - val_loss: 1.3485 - val_acc: 0.2656\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.7147 - acc: 0.3793 - val_loss: 1.3570 - val_acc: 0.3125\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.7798 - acc: 0.3582 - val_loss: 1.2019 - val_acc: 0.3594\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6753 - acc: 0.4133 - val_loss: 1.3046 - val_acc: 0.3438\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.6053 - acc: 0.4036 - val_loss: 1.3731 - val_acc: 0.3281\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.6531 - acc: 0.4375 - val_loss: 1.2456 - val_acc: 0.4062\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.6444 - acc: 0.4230 - val_loss: 1.3672 - val_acc: 0.3125\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.6675 - acc: 0.3744 - val_loss: 1.1754 - val_acc: 0.3750\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.7617 - acc: 0.3776 - val_loss: 1.4094 - val_acc: 0.2188\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.6227 - acc: 0.4117 - val_loss: 1.2558 - val_acc: 0.3438\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.6873 - acc: 0.4100 - val_loss: 1.3626 - val_acc: 0.3281\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.7514 - acc: 0.3841 - val_loss: 1.1616 - val_acc: 0.3594\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.6566 - acc: 0.3906 - val_loss: 1.3086 - val_acc: 0.3438\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.6771 - acc: 0.3793 - val_loss: 1.3668 - val_acc: 0.3438\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.5838 - acc: 0.4198 - val_loss: 1.2108 - val_acc: 0.3438\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.7361 - acc: 0.3922 - val_loss: 1.3185 - val_acc: 0.3281\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.6046 - acc: 0.4100 - val_loss: 1.2965 - val_acc: 0.3125\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6271 - acc: 0.4019 - val_loss: 1.3833 - val_acc: 0.2969\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.6527 - acc: 0.4165 - val_loss: 1.3586 - val_acc: 0.2969\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5658 - acc: 0.4457 - val_loss: 1.3120 - val_acc: 0.3438\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.5182 - acc: 0.4360 - val_loss: 1.3367 - val_acc: 0.2969\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6849 - acc: 0.4000 - val_loss: 1.3144 - val_acc: 0.3906\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.6701 - acc: 0.3874 - val_loss: 1.3312 - val_acc: 0.3125\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.5698 - acc: 0.3906 - val_loss: 1.3121 - val_acc: 0.2969\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.6108 - acc: 0.4003 - val_loss: 1.4040 - val_acc: 0.2656\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.5957 - acc: 0.4198 - val_loss: 1.3632 - val_acc: 0.3438\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6313 - acc: 0.3971 - val_loss: 1.2164 - val_acc: 0.4062\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.7558 - acc: 0.4003 - val_loss: 1.3529 - val_acc: 0.2812\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.6923 - acc: 0.3844 - val_loss: 1.3453 - val_acc: 0.2656\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6470 - acc: 0.4234 - val_loss: 1.3062 - val_acc: 0.2344\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.6696 - acc: 0.4068 - val_loss: 1.3502 - val_acc: 0.2812\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.6247 - acc: 0.4036 - val_loss: 1.2898 - val_acc: 0.2656\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.6397 - acc: 0.4187 - val_loss: 1.3124 - val_acc: 0.3281\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.6190 - acc: 0.3922 - val_loss: 1.3484 - val_acc: 0.2812\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.5731 - acc: 0.4156 - val_loss: 1.3292 - val_acc: 0.2500\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5672 - acc: 0.4360 - val_loss: 1.2159 - val_acc: 0.3281\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.6448 - acc: 0.3728 - val_loss: 1.1600 - val_acc: 0.3750\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.5387 - acc: 0.4376 - val_loss: 1.4792 - val_acc: 0.2500\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4986 - acc: 0.4279 - val_loss: 1.2476 - val_acc: 0.3438\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5281 - acc: 0.4203 - val_loss: 1.3420 - val_acc: 0.3438\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.5657 - acc: 0.3938 - val_loss: 1.3278 - val_acc: 0.2812\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5879 - acc: 0.4117 - val_loss: 1.3796 - val_acc: 0.2344\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5341 - acc: 0.4117 - val_loss: 1.3022 - val_acc: 0.3125\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.5424 - acc: 0.4344 - val_loss: 1.2512 - val_acc: 0.2812\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.5450 - acc: 0.4052 - val_loss: 1.3859 - val_acc: 0.3438\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.5255 - acc: 0.4408 - val_loss: 1.3802 - val_acc: 0.2656\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.4903 - acc: 0.3938 - val_loss: 1.2596 - val_acc: 0.2969\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.5186 - acc: 0.4425 - val_loss: 1.3051 - val_acc: 0.2812\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.5402 - acc: 0.4279 - val_loss: 1.4243 - val_acc: 0.3125\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.5511 - acc: 0.4198 - val_loss: 1.3968 - val_acc: 0.3281\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.5294 - acc: 0.4311 - val_loss: 1.4410 - val_acc: 0.2656\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5515 - acc: 0.4052 - val_loss: 1.3966 - val_acc: 0.2500\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.6410 - acc: 0.4019 - val_loss: 1.4213 - val_acc: 0.2500\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4963 - acc: 0.4019 - val_loss: 1.2316 - val_acc: 0.2812\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5390 - acc: 0.4230 - val_loss: 1.3145 - val_acc: 0.2969\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.6082 - acc: 0.3712 - val_loss: 1.4574 - val_acc: 0.2500\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.5442 - acc: 0.4019 - val_loss: 1.3775 - val_acc: 0.2500\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.6432 - acc: 0.4068 - val_loss: 1.5080 - val_acc: 0.2500\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4375 - acc: 0.4182 - val_loss: 1.4210 - val_acc: 0.2500\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.5220 - acc: 0.4182 - val_loss: 1.3476 - val_acc: 0.2656\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.5127 - acc: 0.4133 - val_loss: 1.3481 - val_acc: 0.3438\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5496 - acc: 0.4117 - val_loss: 1.2903 - val_acc: 0.3125\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.5042 - acc: 0.4165 - val_loss: 1.2967 - val_acc: 0.3281\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.5221 - acc: 0.4457 - val_loss: 1.2171 - val_acc: 0.3125\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 2s 151ms/step - loss: 1.5021 - acc: 0.4311 - val_loss: 1.3559 - val_acc: 0.3438\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.5049 - acc: 0.4473 - val_loss: 1.2925 - val_acc: 0.3125\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.6182 - acc: 0.3971 - val_loss: 1.3424 - val_acc: 0.2969\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.4722 - acc: 0.4182 - val_loss: 1.3557 - val_acc: 0.2656\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.4216 - acc: 0.4133 - val_loss: 1.3160 - val_acc: 0.2656\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.4999 - acc: 0.4281 - val_loss: 1.3184 - val_acc: 0.2969\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5359 - acc: 0.4016 - val_loss: 1.4230 - val_acc: 0.2969\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.5130 - acc: 0.4263 - val_loss: 1.4498 - val_acc: 0.1875\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.5281 - acc: 0.4133 - val_loss: 1.4174 - val_acc: 0.2812\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.4587 - acc: 0.4441 - val_loss: 1.3739 - val_acc: 0.2812\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.4924 - acc: 0.4441 - val_loss: 1.4303 - val_acc: 0.2344\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.4483 - acc: 0.4489 - val_loss: 1.3775 - val_acc: 0.2812\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4408 - acc: 0.4506 - val_loss: 1.3466 - val_acc: 0.2344\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4735 - acc: 0.4327 - val_loss: 1.4171 - val_acc: 0.2812\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4654 - acc: 0.4441 - val_loss: 1.3366 - val_acc: 0.2656\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5072 - acc: 0.4100 - val_loss: 1.3450 - val_acc: 0.2812\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.5071 - acc: 0.4230 - val_loss: 1.3135 - val_acc: 0.2969\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.4491 - acc: 0.4360 - val_loss: 1.4589 - val_acc: 0.2500\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.4544 - acc: 0.4344 - val_loss: 1.4006 - val_acc: 0.2969\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3412 - acc: 0.4587 - val_loss: 1.3479 - val_acc: 0.2812\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.4663 - acc: 0.4182 - val_loss: 1.4367 - val_acc: 0.2656\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5326 - acc: 0.4214 - val_loss: 1.3379 - val_acc: 0.2656\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3674 - acc: 0.4635 - val_loss: 1.4548 - val_acc: 0.2656\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3144 - acc: 0.4814 - val_loss: 1.4259 - val_acc: 0.3125\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.3705 - acc: 0.4506 - val_loss: 1.4215 - val_acc: 0.2656\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4754 - acc: 0.4133 - val_loss: 1.4986 - val_acc: 0.1875\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.3665 - acc: 0.4375 - val_loss: 1.6235 - val_acc: 0.1719\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4834 - acc: 0.4538 - val_loss: 1.4660 - val_acc: 0.2344\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.3731 - acc: 0.4781 - val_loss: 1.4563 - val_acc: 0.2188\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.4938 - acc: 0.4344 - val_loss: 1.3598 - val_acc: 0.2812\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.4952 - acc: 0.4214 - val_loss: 1.3778 - val_acc: 0.2188\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3784 - acc: 0.4668 - val_loss: 1.4515 - val_acc: 0.2188\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3348 - acc: 0.4684 - val_loss: 1.4814 - val_acc: 0.2188\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.4823 - acc: 0.4214 - val_loss: 1.5080 - val_acc: 0.2188\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3771 - acc: 0.4619 - val_loss: 1.4506 - val_acc: 0.1875\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3736 - acc: 0.4635 - val_loss: 1.5263 - val_acc: 0.2344\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3372 - acc: 0.4571 - val_loss: 1.5812 - val_acc: 0.2344\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.5630 - acc: 0.4198 - val_loss: 1.4892 - val_acc: 0.2188\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.3928 - acc: 0.4668 - val_loss: 1.3488 - val_acc: 0.2969\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.3920 - acc: 0.4473 - val_loss: 1.4917 - val_acc: 0.2656\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.4474 - acc: 0.4506 - val_loss: 1.4856 - val_acc: 0.2656\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3905 - acc: 0.4571 - val_loss: 1.3533 - val_acc: 0.2656\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.4409 - acc: 0.4392 - val_loss: 1.3523 - val_acc: 0.2500\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.4365 - acc: 0.4392 - val_loss: 1.4644 - val_acc: 0.2031\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4479 - acc: 0.4327 - val_loss: 1.3539 - val_acc: 0.2500\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3813 - acc: 0.4457 - val_loss: 1.4640 - val_acc: 0.2656\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.4266 - acc: 0.4425 - val_loss: 1.3837 - val_acc: 0.2969\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4321 - acc: 0.4279 - val_loss: 1.4638 - val_acc: 0.2344\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3455 - acc: 0.4571 - val_loss: 1.4646 - val_acc: 0.2188\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3490 - acc: 0.4619 - val_loss: 1.3749 - val_acc: 0.2344\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.3944 - acc: 0.4538 - val_loss: 1.4161 - val_acc: 0.2344\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.4098 - acc: 0.4263 - val_loss: 1.4552 - val_acc: 0.2500\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.3730 - acc: 0.4538 - val_loss: 1.4330 - val_acc: 0.2656\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3861 - acc: 0.4766 - val_loss: 1.2837 - val_acc: 0.3281\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3430 - acc: 0.4797 - val_loss: 1.3711 - val_acc: 0.2500\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3530 - acc: 0.4830 - val_loss: 1.4713 - val_acc: 0.2656\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.3712 - acc: 0.4360 - val_loss: 1.4315 - val_acc: 0.2344\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 1.3806 - acc: 0.4457 - val_loss: 1.4889 - val_acc: 0.1719\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3401 - acc: 0.4619 - val_loss: 1.3539 - val_acc: 0.2188\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2952 - acc: 0.4700 - val_loss: 1.5101 - val_acc: 0.2031\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2886 - acc: 0.4927 - val_loss: 1.4921 - val_acc: 0.2031\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.3820 - acc: 0.4422 - val_loss: 1.4152 - val_acc: 0.2969\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3544 - acc: 0.4814 - val_loss: 1.4871 - val_acc: 0.2031\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3342 - acc: 0.4571 - val_loss: 1.4357 - val_acc: 0.2500\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3211 - acc: 0.4594 - val_loss: 1.3820 - val_acc: 0.2188\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3583 - acc: 0.4700 - val_loss: 1.4022 - val_acc: 0.2188\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3472 - acc: 0.4619 - val_loss: 1.3934 - val_acc: 0.2031\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4165 - acc: 0.4392 - val_loss: 1.4039 - val_acc: 0.2656\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4121 - acc: 0.4441 - val_loss: 1.3609 - val_acc: 0.2188\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.3652 - acc: 0.4587 - val_loss: 1.4114 - val_acc: 0.2344\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3469 - acc: 0.4506 - val_loss: 1.3992 - val_acc: 0.2344\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3617 - acc: 0.4441 - val_loss: 1.3977 - val_acc: 0.2344\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3208 - acc: 0.4765 - val_loss: 1.5026 - val_acc: 0.2500\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.4183 - acc: 0.4554 - val_loss: 1.4320 - val_acc: 0.2500\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 2s 191ms/step - loss: 1.2400 - acc: 0.5008 - val_loss: 1.3917 - val_acc: 0.2344\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.3382 - acc: 0.4765 - val_loss: 1.4735 - val_acc: 0.2500\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3779 - acc: 0.4441 - val_loss: 1.4083 - val_acc: 0.2188\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.3332 - acc: 0.4457 - val_loss: 1.5058 - val_acc: 0.2188\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3353 - acc: 0.4408 - val_loss: 1.4687 - val_acc: 0.2500\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3893 - acc: 0.4441 - val_loss: 1.4206 - val_acc: 0.2344\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3206 - acc: 0.4749 - val_loss: 1.3963 - val_acc: 0.2812\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3177 - acc: 0.4878 - val_loss: 1.4249 - val_acc: 0.2344\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3156 - acc: 0.4619 - val_loss: 1.5093 - val_acc: 0.2344\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2541 - acc: 0.4976 - val_loss: 1.4175 - val_acc: 0.2500\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3090 - acc: 0.4506 - val_loss: 1.3109 - val_acc: 0.2812\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.3794 - acc: 0.4765 - val_loss: 1.4605 - val_acc: 0.2812\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3066 - acc: 0.4976 - val_loss: 1.4413 - val_acc: 0.2188\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3131 - acc: 0.4688 - val_loss: 1.3128 - val_acc: 0.3125\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.2910 - acc: 0.4733 - val_loss: 1.4378 - val_acc: 0.2500\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3166 - acc: 0.4749 - val_loss: 1.4149 - val_acc: 0.2656\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3749 - acc: 0.4425 - val_loss: 1.4428 - val_acc: 0.2344\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4065 - acc: 0.4700 - val_loss: 1.4687 - val_acc: 0.1562\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3582 - acc: 0.4522 - val_loss: 1.3186 - val_acc: 0.2656\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4049 - acc: 0.4327 - val_loss: 1.4302 - val_acc: 0.2812\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3202 - acc: 0.4749 - val_loss: 1.3796 - val_acc: 0.2500\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2823 - acc: 0.4672 - val_loss: 1.5884 - val_acc: 0.2344\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.3318 - acc: 0.4716 - val_loss: 1.5212 - val_acc: 0.2500\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3316 - acc: 0.4506 - val_loss: 1.5024 - val_acc: 0.2500\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.3333 - acc: 0.4749 - val_loss: 1.5049 - val_acc: 0.2500\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3017 - acc: 0.4750 - val_loss: 1.4535 - val_acc: 0.2031\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3181 - acc: 0.4554 - val_loss: 1.3832 - val_acc: 0.2812\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2539 - acc: 0.4992 - val_loss: 1.4765 - val_acc: 0.2031\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2639 - acc: 0.4700 - val_loss: 1.4168 - val_acc: 0.2812\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.3130 - acc: 0.4635 - val_loss: 1.4658 - val_acc: 0.2500\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2876 - acc: 0.4635 - val_loss: 1.4193 - val_acc: 0.2188\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3133 - acc: 0.5105 - val_loss: 1.4681 - val_acc: 0.2188\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3399 - acc: 0.4797 - val_loss: 1.3263 - val_acc: 0.2969\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.3113 - acc: 0.4749 - val_loss: 1.3573 - val_acc: 0.2969\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 1.2366 - acc: 0.5089 - val_loss: 1.4299 - val_acc: 0.2500\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.3306 - acc: 0.4684 - val_loss: 1.5396 - val_acc: 0.1562\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.2851 - acc: 0.4878 - val_loss: 1.3949 - val_acc: 0.2812\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3334 - acc: 0.4862 - val_loss: 1.5038 - val_acc: 0.1875\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3451 - acc: 0.4656 - val_loss: 1.2771 - val_acc: 0.2969\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2575 - acc: 0.4700 - val_loss: 1.4507 - val_acc: 0.2344\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3127 - acc: 0.4603 - val_loss: 1.5061 - val_acc: 0.2188\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2505 - acc: 0.4878 - val_loss: 1.3216 - val_acc: 0.2812\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2875 - acc: 0.4875 - val_loss: 1.5007 - val_acc: 0.2031\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2398 - acc: 0.4943 - val_loss: 1.5071 - val_acc: 0.2188\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 1.2607 - acc: 0.4943 - val_loss: 1.3716 - val_acc: 0.2344\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2429 - acc: 0.4749 - val_loss: 1.3869 - val_acc: 0.2344\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2867 - acc: 0.4797 - val_loss: 1.3026 - val_acc: 0.3438\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2590 - acc: 0.4733 - val_loss: 1.3691 - val_acc: 0.2500\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2630 - acc: 0.4859 - val_loss: 1.3279 - val_acc: 0.2500\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.2179 - acc: 0.4781 - val_loss: 1.4621 - val_acc: 0.2188\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2862 - acc: 0.4700 - val_loss: 1.4328 - val_acc: 0.2656\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2254 - acc: 0.4943 - val_loss: 1.4683 - val_acc: 0.1719\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2883 - acc: 0.4765 - val_loss: 1.5962 - val_acc: 0.2656\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.2457 - acc: 0.5063 - val_loss: 1.3798 - val_acc: 0.2812\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2786 - acc: 0.5105 - val_loss: 1.4583 - val_acc: 0.2031\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2928 - acc: 0.4781 - val_loss: 1.4872 - val_acc: 0.2031\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2596 - acc: 0.4781 - val_loss: 1.4398 - val_acc: 0.2344\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3423 - acc: 0.4571 - val_loss: 1.5091 - val_acc: 0.2188\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.2622 - acc: 0.4862 - val_loss: 1.4956 - val_acc: 0.2344\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2719 - acc: 0.4927 - val_loss: 1.3499 - val_acc: 0.2969\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.2927 - acc: 0.5057 - val_loss: 1.4538 - val_acc: 0.2188\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2670 - acc: 0.4781 - val_loss: 1.3840 - val_acc: 0.2031\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.2324 - acc: 0.4716 - val_loss: 1.4661 - val_acc: 0.2188\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1987 - acc: 0.5186 - val_loss: 1.4034 - val_acc: 0.2500\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3503 - acc: 0.4668 - val_loss: 1.4562 - val_acc: 0.2031\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2534 - acc: 0.4797 - val_loss: 1.4066 - val_acc: 0.2344\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2641 - acc: 0.4814 - val_loss: 1.5229 - val_acc: 0.2500\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2094 - acc: 0.5024 - val_loss: 1.3695 - val_acc: 0.2500\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2955 - acc: 0.4609 - val_loss: 1.2529 - val_acc: 0.3281\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2189 - acc: 0.5008 - val_loss: 1.3521 - val_acc: 0.2656\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2495 - acc: 0.4959 - val_loss: 1.4430 - val_acc: 0.2188\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2536 - acc: 0.4749 - val_loss: 1.5243 - val_acc: 0.2188\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.2644 - acc: 0.4814 - val_loss: 1.3731 - val_acc: 0.2969\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2116 - acc: 0.5047 - val_loss: 1.4436 - val_acc: 0.2031\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2363 - acc: 0.4700 - val_loss: 1.3557 - val_acc: 0.2656\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2120 - acc: 0.5024 - val_loss: 1.4145 - val_acc: 0.2344\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3325 - acc: 0.4652 - val_loss: 1.3982 - val_acc: 0.2812\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1952 - acc: 0.4911 - val_loss: 1.3724 - val_acc: 0.2031\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.2019 - acc: 0.4992 - val_loss: 1.5600 - val_acc: 0.1875\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2468 - acc: 0.5125 - val_loss: 1.4191 - val_acc: 0.2500\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2919 - acc: 0.4684 - val_loss: 1.3513 - val_acc: 0.2812\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2221 - acc: 0.5251 - val_loss: 1.4332 - val_acc: 0.2344\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 1.2271 - acc: 0.4700 - val_loss: 1.4350 - val_acc: 0.2031\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2239 - acc: 0.4927 - val_loss: 1.5097 - val_acc: 0.1875\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.2618 - acc: 0.4781 - val_loss: 1.4221 - val_acc: 0.2344\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.2583 - acc: 0.4927 - val_loss: 1.2732 - val_acc: 0.3281\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3076 - acc: 0.4895 - val_loss: 1.4901 - val_acc: 0.1875\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3120 - acc: 0.4408 - val_loss: 1.3751 - val_acc: 0.3125\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2617 - acc: 0.4830 - val_loss: 1.3799 - val_acc: 0.3125\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 1.2537 - acc: 0.5041 - val_loss: 1.4300 - val_acc: 0.2656\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.2185 - acc: 0.5008 - val_loss: 1.3633 - val_acc: 0.2500\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2811 - acc: 0.4635 - val_loss: 1.4135 - val_acc: 0.3125\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2798 - acc: 0.4927 - val_loss: 1.5255 - val_acc: 0.2031\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1891 - acc: 0.5186 - val_loss: 1.3862 - val_acc: 0.2656\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1811 - acc: 0.5122 - val_loss: 1.4148 - val_acc: 0.2031\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2381 - acc: 0.4781 - val_loss: 1.4370 - val_acc: 0.3125\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2536 - acc: 0.4875 - val_loss: 1.3922 - val_acc: 0.2969\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1602 - acc: 0.5332 - val_loss: 1.3823 - val_acc: 0.2969\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.1661 - acc: 0.5024 - val_loss: 1.2932 - val_acc: 0.2812\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1696 - acc: 0.5267 - val_loss: 1.4472 - val_acc: 0.3125\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2647 - acc: 0.4927 - val_loss: 1.4726 - val_acc: 0.2344\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2010 - acc: 0.5154 - val_loss: 1.5376 - val_acc: 0.2188\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.2402 - acc: 0.4976 - val_loss: 1.6298 - val_acc: 0.1719\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2165 - acc: 0.5057 - val_loss: 1.4157 - val_acc: 0.2188\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1352 - acc: 0.5332 - val_loss: 1.5278 - val_acc: 0.1875\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2485 - acc: 0.4814 - val_loss: 1.5198 - val_acc: 0.2188\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2060 - acc: 0.4862 - val_loss: 1.5593 - val_acc: 0.1719\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.2381 - acc: 0.5047 - val_loss: 1.4644 - val_acc: 0.2656\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2466 - acc: 0.4716 - val_loss: 1.5698 - val_acc: 0.2344\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2413 - acc: 0.4878 - val_loss: 1.3393 - val_acc: 0.3125\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1316 - acc: 0.5312 - val_loss: 1.4179 - val_acc: 0.2812\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2159 - acc: 0.4895 - val_loss: 1.5046 - val_acc: 0.1719\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2033 - acc: 0.4992 - val_loss: 1.4500 - val_acc: 0.2344\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2349 - acc: 0.4862 - val_loss: 1.3381 - val_acc: 0.2500\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1141 - acc: 0.5348 - val_loss: 1.4401 - val_acc: 0.2500\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.2475 - acc: 0.4862 - val_loss: 1.5513 - val_acc: 0.2344\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1921 - acc: 0.4976 - val_loss: 1.4180 - val_acc: 0.3125\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2428 - acc: 0.5186 - val_loss: 1.4550 - val_acc: 0.2344\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0935 - acc: 0.5348 - val_loss: 1.5005 - val_acc: 0.2344\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2855 - acc: 0.4765 - val_loss: 1.4469 - val_acc: 0.2656\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.2058 - acc: 0.5122 - val_loss: 1.4900 - val_acc: 0.3125\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1963 - acc: 0.5186 - val_loss: 1.3645 - val_acc: 0.2031\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1224 - acc: 0.5413 - val_loss: 1.5432 - val_acc: 0.2344\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1890 - acc: 0.5235 - val_loss: 1.5185 - val_acc: 0.2656\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2250 - acc: 0.4911 - val_loss: 1.4716 - val_acc: 0.2344\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1533 - acc: 0.5300 - val_loss: 1.4333 - val_acc: 0.2344\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1014 - acc: 0.5267 - val_loss: 1.4741 - val_acc: 0.2344\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2060 - acc: 0.5219 - val_loss: 1.4876 - val_acc: 0.2344\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1789 - acc: 0.4976 - val_loss: 1.5372 - val_acc: 0.2031\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1795 - acc: 0.5203 - val_loss: 1.6306 - val_acc: 0.1562\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1689 - acc: 0.5031 - val_loss: 1.4299 - val_acc: 0.2188\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.2428 - acc: 0.4878 - val_loss: 1.6435 - val_acc: 0.1875\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1098 - acc: 0.5381 - val_loss: 1.4405 - val_acc: 0.2656\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2164 - acc: 0.4943 - val_loss: 1.3063 - val_acc: 0.2969\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2066 - acc: 0.5008 - val_loss: 1.5051 - val_acc: 0.2812\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.1913 - acc: 0.5235 - val_loss: 1.3753 - val_acc: 0.2812\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1865 - acc: 0.4781 - val_loss: 1.5516 - val_acc: 0.2031\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.2181 - acc: 0.4878 - val_loss: 1.4104 - val_acc: 0.2656\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2697 - acc: 0.4619 - val_loss: 1.5172 - val_acc: 0.2344\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1561 - acc: 0.5063 - val_loss: 1.3602 - val_acc: 0.3125\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1901 - acc: 0.5063 - val_loss: 1.4227 - val_acc: 0.2500\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0912 - acc: 0.5527 - val_loss: 1.4488 - val_acc: 0.3125\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1534 - acc: 0.5138 - val_loss: 1.5334 - val_acc: 0.1875\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1089 - acc: 0.5478 - val_loss: 1.3761 - val_acc: 0.2344\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1944 - acc: 0.4959 - val_loss: 1.5171 - val_acc: 0.2344\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2364 - acc: 0.5105 - val_loss: 1.4166 - val_acc: 0.2344\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.2257 - acc: 0.4878 - val_loss: 1.5227 - val_acc: 0.2188\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1357 - acc: 0.5284 - val_loss: 1.4418 - val_acc: 0.2656\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1640 - acc: 0.5008 - val_loss: 1.5116 - val_acc: 0.2500\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1625 - acc: 0.5008 - val_loss: 1.5062 - val_acc: 0.2500\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1739 - acc: 0.4891 - val_loss: 1.5290 - val_acc: 0.2344\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1156 - acc: 0.5328 - val_loss: 1.4378 - val_acc: 0.2656\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2855 - acc: 0.4781 - val_loss: 1.5586 - val_acc: 0.2188\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.2084 - acc: 0.5138 - val_loss: 1.4249 - val_acc: 0.2812\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1093 - acc: 0.5348 - val_loss: 1.3673 - val_acc: 0.2656\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.1339 - acc: 0.5284 - val_loss: 1.4552 - val_acc: 0.2344\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1090 - acc: 0.5219 - val_loss: 1.4541 - val_acc: 0.2812\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1913 - acc: 0.5251 - val_loss: 1.4756 - val_acc: 0.2188\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.1505 - acc: 0.5016 - val_loss: 1.4030 - val_acc: 0.2656\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0936 - acc: 0.5511 - val_loss: 1.4274 - val_acc: 0.2344\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2209 - acc: 0.5122 - val_loss: 1.4989 - val_acc: 0.2344\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1464 - acc: 0.5138 - val_loss: 1.4079 - val_acc: 0.2656\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1831 - acc: 0.4911 - val_loss: 1.5652 - val_acc: 0.2188\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0882 - acc: 0.5332 - val_loss: 1.4151 - val_acc: 0.2188\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.1750 - acc: 0.4862 - val_loss: 1.5204 - val_acc: 0.2031\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2204 - acc: 0.5154 - val_loss: 1.4850 - val_acc: 0.1875\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1719 - acc: 0.4992 - val_loss: 1.5057 - val_acc: 0.2500\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.1745 - acc: 0.5235 - val_loss: 1.4717 - val_acc: 0.2344\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1718 - acc: 0.4846 - val_loss: 1.4557 - val_acc: 0.2812\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.1660 - acc: 0.4969 - val_loss: 1.3336 - val_acc: 0.2656\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1266 - acc: 0.5154 - val_loss: 1.4372 - val_acc: 0.2812\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2508 - acc: 0.4862 - val_loss: 1.4815 - val_acc: 0.2500\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0857 - acc: 0.5250 - val_loss: 1.4762 - val_acc: 0.2500\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1693 - acc: 0.5008 - val_loss: 1.5027 - val_acc: 0.1719\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1236 - acc: 0.5154 - val_loss: 1.5543 - val_acc: 0.1875\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0835 - acc: 0.5348 - val_loss: 1.4105 - val_acc: 0.2969\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.1982 - acc: 0.5138 - val_loss: 1.5347 - val_acc: 0.2188\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0697 - acc: 0.5429 - val_loss: 1.5165 - val_acc: 0.2656\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1280 - acc: 0.5172 - val_loss: 1.4642 - val_acc: 0.2812\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0979 - acc: 0.5365 - val_loss: 1.5267 - val_acc: 0.2344\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1408 - acc: 0.5397 - val_loss: 1.4861 - val_acc: 0.2344\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1272 - acc: 0.4992 - val_loss: 1.5245 - val_acc: 0.1875\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1895 - acc: 0.5089 - val_loss: 1.5918 - val_acc: 0.2188\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1495 - acc: 0.4878 - val_loss: 1.4618 - val_acc: 0.2812\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1735 - acc: 0.5462 - val_loss: 1.4577 - val_acc: 0.2344\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0629 - acc: 0.5462 - val_loss: 1.6114 - val_acc: 0.1719\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1543 - acc: 0.5332 - val_loss: 1.5168 - val_acc: 0.2500\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1134 - acc: 0.5608 - val_loss: 1.5205 - val_acc: 0.2344\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.0388 - acc: 0.5559 - val_loss: 1.4921 - val_acc: 0.2031\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0741 - acc: 0.5413 - val_loss: 1.5251 - val_acc: 0.2500\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2126 - acc: 0.5008 - val_loss: 1.4735 - val_acc: 0.2500\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.1164 - acc: 0.5219 - val_loss: 1.5112 - val_acc: 0.2656\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1098 - acc: 0.5041 - val_loss: 1.4816 - val_acc: 0.2344\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1504 - acc: 0.5251 - val_loss: 1.4092 - val_acc: 0.2812\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.1144 - acc: 0.5186 - val_loss: 1.4606 - val_acc: 0.2812\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1344 - acc: 0.5397 - val_loss: 1.4441 - val_acc: 0.2031\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1037 - acc: 0.5429 - val_loss: 1.5102 - val_acc: 0.2188\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0914 - acc: 0.5500 - val_loss: 1.4005 - val_acc: 0.2812\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0718 - acc: 0.5462 - val_loss: 1.5416 - val_acc: 0.2031\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1144 - acc: 0.5203 - val_loss: 1.3663 - val_acc: 0.2656\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1935 - acc: 0.4927 - val_loss: 1.5053 - val_acc: 0.1719\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1061 - acc: 0.5284 - val_loss: 1.3764 - val_acc: 0.2344\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1346 - acc: 0.5138 - val_loss: 1.3766 - val_acc: 0.2500\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1173 - acc: 0.5170 - val_loss: 1.4135 - val_acc: 0.2344\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2165 - acc: 0.4668 - val_loss: 1.4950 - val_acc: 0.2344\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1187 - acc: 0.5057 - val_loss: 1.5154 - val_acc: 0.2188\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1478 - acc: 0.5219 - val_loss: 1.6570 - val_acc: 0.1875\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1105 - acc: 0.5186 - val_loss: 1.4691 - val_acc: 0.2344\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1598 - acc: 0.5138 - val_loss: 1.4657 - val_acc: 0.2500\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1585 - acc: 0.4992 - val_loss: 1.4001 - val_acc: 0.2969\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1127 - acc: 0.5316 - val_loss: 1.4785 - val_acc: 0.2500\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1267 - acc: 0.5203 - val_loss: 1.5646 - val_acc: 0.2344\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0822 - acc: 0.5547 - val_loss: 1.5491 - val_acc: 0.2812\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1820 - acc: 0.5008 - val_loss: 1.3800 - val_acc: 0.2812\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1423 - acc: 0.5543 - val_loss: 1.3885 - val_acc: 0.2500\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1403 - acc: 0.5397 - val_loss: 1.4218 - val_acc: 0.2812\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1394 - acc: 0.5381 - val_loss: 1.4442 - val_acc: 0.2656\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.1061 - acc: 0.5316 - val_loss: 1.5889 - val_acc: 0.2031\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0933 - acc: 0.5281 - val_loss: 1.5605 - val_acc: 0.1719\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0866 - acc: 0.5397 - val_loss: 1.5744 - val_acc: 0.2344\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1162 - acc: 0.5186 - val_loss: 1.4883 - val_acc: 0.2500\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1358 - acc: 0.5219 - val_loss: 1.4521 - val_acc: 0.2500\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1135 - acc: 0.5186 - val_loss: 1.4576 - val_acc: 0.2500\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0883 - acc: 0.5575 - val_loss: 1.3927 - val_acc: 0.2188\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1313 - acc: 0.5316 - val_loss: 1.5144 - val_acc: 0.2188\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1007 - acc: 0.5235 - val_loss: 1.4942 - val_acc: 0.2344\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1440 - acc: 0.5478 - val_loss: 1.4107 - val_acc: 0.2812\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0929 - acc: 0.5527 - val_loss: 1.5052 - val_acc: 0.2031\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.0838 - acc: 0.5494 - val_loss: 1.4772 - val_acc: 0.2188\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1046 - acc: 0.5381 - val_loss: 1.4095 - val_acc: 0.2500\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0452 - acc: 0.5624 - val_loss: 1.4422 - val_acc: 0.2812\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0535 - acc: 0.5462 - val_loss: 1.4025 - val_acc: 0.2656\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0599 - acc: 0.5406 - val_loss: 1.2317 - val_acc: 0.3438\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1761 - acc: 0.5170 - val_loss: 1.3855 - val_acc: 0.2656\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0235 - acc: 0.5413 - val_loss: 1.3262 - val_acc: 0.3125\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1356 - acc: 0.5073 - val_loss: 1.4647 - val_acc: 0.2812\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.0371 - acc: 0.5381 - val_loss: 1.4362 - val_acc: 0.2656\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0862 - acc: 0.5429 - val_loss: 1.4881 - val_acc: 0.2344\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0784 - acc: 0.5575 - val_loss: 1.4627 - val_acc: 0.2344\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1015 - acc: 0.5316 - val_loss: 1.3923 - val_acc: 0.2812\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0189 - acc: 0.5462 - val_loss: 1.4259 - val_acc: 0.2500\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0698 - acc: 0.5511 - val_loss: 1.4545 - val_acc: 0.2344\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1201 - acc: 0.5397 - val_loss: 1.6225 - val_acc: 0.1562\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0990 - acc: 0.5527 - val_loss: 1.5119 - val_acc: 0.2344\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0783 - acc: 0.5608 - val_loss: 1.5055 - val_acc: 0.2812\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0647 - acc: 0.5543 - val_loss: 1.4952 - val_acc: 0.2188\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1252 - acc: 0.5348 - val_loss: 1.4074 - val_acc: 0.2031\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1776 - acc: 0.4992 - val_loss: 1.3531 - val_acc: 0.3125\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0522 - acc: 0.5348 - val_loss: 1.5000 - val_acc: 0.2344\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0768 - acc: 0.5300 - val_loss: 1.4944 - val_acc: 0.2500\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 1.0606 - acc: 0.5446 - val_loss: 1.4436 - val_acc: 0.2812\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0269 - acc: 0.5721 - val_loss: 1.3568 - val_acc: 0.3125\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0595 - acc: 0.5344 - val_loss: 1.4256 - val_acc: 0.3125\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1525 - acc: 0.5073 - val_loss: 1.3655 - val_acc: 0.3125\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1441 - acc: 0.5332 - val_loss: 1.3666 - val_acc: 0.2812\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.0819 - acc: 0.5348 - val_loss: 1.5192 - val_acc: 0.2500\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1017 - acc: 0.5348 - val_loss: 1.4431 - val_acc: 0.2500\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0975 - acc: 0.5640 - val_loss: 1.4929 - val_acc: 0.2031\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1822 - acc: 0.4862 - val_loss: 1.3522 - val_acc: 0.2344\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0880 - acc: 0.5494 - val_loss: 1.3856 - val_acc: 0.2344\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0338 - acc: 0.5365 - val_loss: 1.4804 - val_acc: 0.2188\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 1.0483 - acc: 0.5266 - val_loss: 1.3918 - val_acc: 0.2500\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1118 - acc: 0.5397 - val_loss: 1.5082 - val_acc: 0.1875\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1134 - acc: 0.5284 - val_loss: 1.3614 - val_acc: 0.2344\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1392 - acc: 0.5031 - val_loss: 1.2830 - val_acc: 0.3125\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0996 - acc: 0.5429 - val_loss: 1.3837 - val_acc: 0.2969\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0752 - acc: 0.5446 - val_loss: 1.3890 - val_acc: 0.2969\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0758 - acc: 0.5527 - val_loss: 1.5210 - val_acc: 0.2344\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.0849 - acc: 0.5375 - val_loss: 1.3147 - val_acc: 0.2656\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0908 - acc: 0.5251 - val_loss: 1.4197 - val_acc: 0.2188\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1019 - acc: 0.5575 - val_loss: 1.3490 - val_acc: 0.2969\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0605 - acc: 0.5344 - val_loss: 1.4443 - val_acc: 0.2500\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 1.0852 - acc: 0.5462 - val_loss: 1.3559 - val_acc: 0.3281\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0631 - acc: 0.5316 - val_loss: 1.4620 - val_acc: 0.1875\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0936 - acc: 0.5429 - val_loss: 1.4150 - val_acc: 0.2969\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0157 - acc: 0.5316 - val_loss: 1.4817 - val_acc: 0.2188\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0563 - acc: 0.5770 - val_loss: 1.3254 - val_acc: 0.2500\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9970 - acc: 0.5900 - val_loss: 1.4066 - val_acc: 0.2500\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0851 - acc: 0.5332 - val_loss: 1.3954 - val_acc: 0.2656\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0469 - acc: 0.5656 - val_loss: 1.5021 - val_acc: 0.2188\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0811 - acc: 0.5266 - val_loss: 1.3979 - val_acc: 0.2344\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0457 - acc: 0.5186 - val_loss: 1.4103 - val_acc: 0.2188\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0127 - acc: 0.5462 - val_loss: 1.5344 - val_acc: 0.2031\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0203 - acc: 0.5543 - val_loss: 1.4574 - val_acc: 0.2500\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0615 - acc: 0.5365 - val_loss: 1.4653 - val_acc: 0.2500\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0898 - acc: 0.5608 - val_loss: 1.5647 - val_acc: 0.2500\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0566 - acc: 0.5673 - val_loss: 1.4478 - val_acc: 0.1875\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0961 - acc: 0.5381 - val_loss: 1.5006 - val_acc: 0.2031\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0162 - acc: 0.5770 - val_loss: 1.3983 - val_acc: 0.2656\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0841 - acc: 0.5348 - val_loss: 1.4706 - val_acc: 0.2656\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0768 - acc: 0.5446 - val_loss: 1.4809 - val_acc: 0.2500\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0271 - acc: 0.5494 - val_loss: 1.3973 - val_acc: 0.2812\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1068 - acc: 0.5316 - val_loss: 1.4126 - val_acc: 0.2812\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0819 - acc: 0.5608 - val_loss: 1.3671 - val_acc: 0.2656\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9686 - acc: 0.5786 - val_loss: 1.4856 - val_acc: 0.2031\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0454 - acc: 0.5592 - val_loss: 1.4272 - val_acc: 0.2969\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0677 - acc: 0.5348 - val_loss: 1.3973 - val_acc: 0.2188\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.0420 - acc: 0.5413 - val_loss: 1.3395 - val_acc: 0.2812\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1052 - acc: 0.5122 - val_loss: 1.3752 - val_acc: 0.2656\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0331 - acc: 0.5511 - val_loss: 1.3356 - val_acc: 0.2656\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0258 - acc: 0.5608 - val_loss: 1.3988 - val_acc: 0.2500\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0149 - acc: 0.5851 - val_loss: 1.5595 - val_acc: 0.1875\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0580 - acc: 0.5575 - val_loss: 1.3294 - val_acc: 0.2656\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0365 - acc: 0.5413 - val_loss: 1.2864 - val_acc: 0.3281\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1445 - acc: 0.5478 - val_loss: 1.5757 - val_acc: 0.1875\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.0335 - acc: 0.5543 - val_loss: 1.4080 - val_acc: 0.2344\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0792 - acc: 0.5478 - val_loss: 1.3956 - val_acc: 0.2969\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0577 - acc: 0.5429 - val_loss: 1.4893 - val_acc: 0.2188\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0421 - acc: 0.5543 - val_loss: 1.3397 - val_acc: 0.2500\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.0115 - acc: 0.5705 - val_loss: 1.3178 - val_acc: 0.2812\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.0162 - acc: 0.5640 - val_loss: 1.3418 - val_acc: 0.3125\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0144 - acc: 0.5511 - val_loss: 1.5246 - val_acc: 0.2188\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0727 - acc: 0.5397 - val_loss: 1.3643 - val_acc: 0.3125\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0850 - acc: 0.5494 - val_loss: 1.4173 - val_acc: 0.2656\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1095 - acc: 0.5348 - val_loss: 1.3901 - val_acc: 0.2188\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0142 - acc: 0.5786 - val_loss: 1.4105 - val_acc: 0.2812\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0535 - acc: 0.5527 - val_loss: 1.3607 - val_acc: 0.2656\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0575 - acc: 0.5429 - val_loss: 1.4728 - val_acc: 0.1562\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0601 - acc: 0.5737 - val_loss: 1.4183 - val_acc: 0.2500\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0483 - acc: 0.5624 - val_loss: 1.4235 - val_acc: 0.2031\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0896 - acc: 0.5592 - val_loss: 1.5136 - val_acc: 0.1719\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.0403 - acc: 0.5703 - val_loss: 1.3478 - val_acc: 0.2812\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0765 - acc: 0.5381 - val_loss: 1.2912 - val_acc: 0.2969\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0582 - acc: 0.5429 - val_loss: 1.2169 - val_acc: 0.3594\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0695 - acc: 0.5413 - val_loss: 1.5969 - val_acc: 0.2188\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9837 - acc: 0.5705 - val_loss: 1.3881 - val_acc: 0.2344\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0382 - acc: 0.5673 - val_loss: 1.4219 - val_acc: 0.2188\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0496 - acc: 0.5559 - val_loss: 1.3509 - val_acc: 0.3125\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0650 - acc: 0.5429 - val_loss: 1.2659 - val_acc: 0.3281\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0376 - acc: 0.5624 - val_loss: 1.3270 - val_acc: 0.2656\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0409 - acc: 0.5689 - val_loss: 1.4077 - val_acc: 0.2344\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0531 - acc: 0.5511 - val_loss: 1.4345 - val_acc: 0.2344\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0723 - acc: 0.5543 - val_loss: 1.3190 - val_acc: 0.2812\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9952 - acc: 0.5818 - val_loss: 1.4504 - val_acc: 0.2500\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0841 - acc: 0.5594 - val_loss: 1.2523 - val_acc: 0.3438\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0517 - acc: 0.5703 - val_loss: 1.4940 - val_acc: 0.1562\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0628 - acc: 0.4984 - val_loss: 1.5630 - val_acc: 0.1719\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.0239 - acc: 0.5592 - val_loss: 1.4163 - val_acc: 0.2500\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9666 - acc: 0.5592 - val_loss: 1.4467 - val_acc: 0.2344\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0078 - acc: 0.5608 - val_loss: 1.4145 - val_acc: 0.2656\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0294 - acc: 0.5543 - val_loss: 1.5081 - val_acc: 0.2031\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0047 - acc: 0.5705 - val_loss: 1.4062 - val_acc: 0.2812\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0282 - acc: 0.5559 - val_loss: 1.5048 - val_acc: 0.1875\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0122 - acc: 0.5446 - val_loss: 1.3196 - val_acc: 0.2656\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0580 - acc: 0.5494 - val_loss: 1.3004 - val_acc: 0.3125\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0337 - acc: 0.5916 - val_loss: 1.4306 - val_acc: 0.2500\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0438 - acc: 0.5511 - val_loss: 1.4144 - val_acc: 0.2500\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0148 - acc: 0.5527 - val_loss: 1.3253 - val_acc: 0.2500\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0499 - acc: 0.5592 - val_loss: 1.3681 - val_acc: 0.2344\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9934 - acc: 0.5689 - val_loss: 1.4647 - val_acc: 0.2344\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0743 - acc: 0.5559 - val_loss: 1.4202 - val_acc: 0.2812\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0739 - acc: 0.5478 - val_loss: 1.3319 - val_acc: 0.2656\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0079 - acc: 0.5703 - val_loss: 1.3615 - val_acc: 0.2656\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 1.1099 - acc: 0.5608 - val_loss: 1.3384 - val_acc: 0.2969\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0550 - acc: 0.5429 - val_loss: 1.4219 - val_acc: 0.2188\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9997 - acc: 0.5592 - val_loss: 1.4217 - val_acc: 0.2500\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0443 - acc: 0.5673 - val_loss: 1.2765 - val_acc: 0.3125\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0223 - acc: 0.5867 - val_loss: 1.4225 - val_acc: 0.2969\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0075 - acc: 0.5689 - val_loss: 1.4997 - val_acc: 0.2500\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0045 - acc: 0.5705 - val_loss: 1.4599 - val_acc: 0.2500\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.0814 - acc: 0.5478 - val_loss: 1.4317 - val_acc: 0.2344\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0188 - acc: 0.5797 - val_loss: 1.3587 - val_acc: 0.2656\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9977 - acc: 0.5781 - val_loss: 1.4102 - val_acc: 0.2031\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9746 - acc: 0.5786 - val_loss: 1.4835 - val_acc: 0.2031\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.0604 - acc: 0.5462 - val_loss: 1.4628 - val_acc: 0.2344\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0378 - acc: 0.5543 - val_loss: 1.4460 - val_acc: 0.2031\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0117 - acc: 0.5672 - val_loss: 1.4306 - val_acc: 0.2031\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9716 - acc: 0.5781 - val_loss: 1.3708 - val_acc: 0.3125\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0544 - acc: 0.5446 - val_loss: 1.3987 - val_acc: 0.2656\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0872 - acc: 0.5267 - val_loss: 1.3900 - val_acc: 0.2656\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 1.0459 - acc: 0.5413 - val_loss: 1.5278 - val_acc: 0.1562\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0551 - acc: 0.5559 - val_loss: 1.3306 - val_acc: 0.2969\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 2s 208ms/step - loss: 0.9638 - acc: 0.5673 - val_loss: 1.2699 - val_acc: 0.3125\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.0277 - acc: 0.5737 - val_loss: 1.5709 - val_acc: 0.1875\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9775 - acc: 0.5867 - val_loss: 1.3809 - val_acc: 0.2812\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0458 - acc: 0.5381 - val_loss: 1.4013 - val_acc: 0.2188\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9937 - acc: 0.5575 - val_loss: 1.4382 - val_acc: 0.2500\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0921 - acc: 0.5397 - val_loss: 1.3232 - val_acc: 0.2969\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.0706 - acc: 0.5284 - val_loss: 1.4340 - val_acc: 0.2031\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0306 - acc: 0.5559 - val_loss: 1.3762 - val_acc: 0.2188\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0734 - acc: 0.5478 - val_loss: 1.5246 - val_acc: 0.2031\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 1.0028 - acc: 0.5689 - val_loss: 1.4653 - val_acc: 0.1875\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0039 - acc: 0.5478 - val_loss: 1.3978 - val_acc: 0.2969\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 3s 184ms/step - loss: 1.0922 - acc: 0.5281 - val_loss: 1.3090 - val_acc: 0.3281\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0798 - acc: 0.5770 - val_loss: 1.3536 - val_acc: 0.2500\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0353 - acc: 0.5559 - val_loss: 1.3779 - val_acc: 0.3125\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9763 - acc: 0.5786 - val_loss: 1.5310 - val_acc: 0.1875\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9876 - acc: 0.5770 - val_loss: 1.4424 - val_acc: 0.1875\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0423 - acc: 0.5429 - val_loss: 1.4016 - val_acc: 0.2656\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0535 - acc: 0.5543 - val_loss: 1.3750 - val_acc: 0.2812\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9936 - acc: 0.5770 - val_loss: 1.3332 - val_acc: 0.3125\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9725 - acc: 0.5705 - val_loss: 1.4176 - val_acc: 0.3125\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9773 - acc: 0.5543 - val_loss: 1.4714 - val_acc: 0.2344\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 1.0196 - acc: 0.5705 - val_loss: 1.2645 - val_acc: 0.2656\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9651 - acc: 0.5770 - val_loss: 1.3535 - val_acc: 0.2656\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0207 - acc: 0.5381 - val_loss: 1.3555 - val_acc: 0.2969\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0520 - acc: 0.5478 - val_loss: 1.3870 - val_acc: 0.2031\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0276 - acc: 0.5592 - val_loss: 1.3378 - val_acc: 0.2969\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0171 - acc: 0.5437 - val_loss: 1.3509 - val_acc: 0.2812\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0151 - acc: 0.5656 - val_loss: 1.4027 - val_acc: 0.2500\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9779 - acc: 0.5964 - val_loss: 1.3880 - val_acc: 0.2188\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0694 - acc: 0.5527 - val_loss: 1.3706 - val_acc: 0.2812\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0463 - acc: 0.5478 - val_loss: 1.3075 - val_acc: 0.3125\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9647 - acc: 0.5719 - val_loss: 1.3881 - val_acc: 0.2500\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9935 - acc: 0.5624 - val_loss: 1.2667 - val_acc: 0.2656\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0260 - acc: 0.5543 - val_loss: 1.2928 - val_acc: 0.2969\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0288 - acc: 0.5543 - val_loss: 1.2674 - val_acc: 0.2812\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9967 - acc: 0.5478 - val_loss: 1.2523 - val_acc: 0.3594\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.0058 - acc: 0.5608 - val_loss: 1.3585 - val_acc: 0.2500\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0262 - acc: 0.5624 - val_loss: 1.4302 - val_acc: 0.2344\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9563 - acc: 0.5786 - val_loss: 1.2272 - val_acc: 0.2969\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0071 - acc: 0.5719 - val_loss: 1.3292 - val_acc: 0.2812\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9957 - acc: 0.5754 - val_loss: 1.2156 - val_acc: 0.3594\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0335 - acc: 0.5543 - val_loss: 1.3370 - val_acc: 0.2812\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0137 - acc: 0.5737 - val_loss: 1.3302 - val_acc: 0.2812\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9965 - acc: 0.5656 - val_loss: 1.4783 - val_acc: 0.2344\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9863 - acc: 0.5948 - val_loss: 1.3882 - val_acc: 0.2656\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9721 - acc: 0.5981 - val_loss: 1.3716 - val_acc: 0.2031\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.0420 - acc: 0.5640 - val_loss: 1.3947 - val_acc: 0.2500\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0329 - acc: 0.5170 - val_loss: 1.3896 - val_acc: 0.2344\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0290 - acc: 0.5527 - val_loss: 1.3092 - val_acc: 0.3125\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9929 - acc: 0.5721 - val_loss: 1.3914 - val_acc: 0.2500\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0279 - acc: 0.5511 - val_loss: 1.2887 - val_acc: 0.3281\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0296 - acc: 0.5703 - val_loss: 1.4167 - val_acc: 0.2656\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9829 - acc: 0.5770 - val_loss: 1.4160 - val_acc: 0.2344\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9956 - acc: 0.5964 - val_loss: 1.4654 - val_acc: 0.2344\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9978 - acc: 0.5559 - val_loss: 1.3870 - val_acc: 0.2344\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9804 - acc: 0.6016 - val_loss: 1.2851 - val_acc: 0.2812\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.9782 - acc: 0.5859 - val_loss: 1.3369 - val_acc: 0.2656\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9443 - acc: 0.5721 - val_loss: 1.4775 - val_acc: 0.2188\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0015 - acc: 0.5737 - val_loss: 1.4398 - val_acc: 0.2031\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9943 - acc: 0.5900 - val_loss: 1.3359 - val_acc: 0.2812\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9939 - acc: 0.5802 - val_loss: 1.4985 - val_acc: 0.2500\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.0024 - acc: 0.5494 - val_loss: 1.3878 - val_acc: 0.1875\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0208 - acc: 0.5689 - val_loss: 1.3561 - val_acc: 0.2344\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9480 - acc: 0.5964 - val_loss: 1.3202 - val_acc: 0.2500\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 1.0076 - acc: 0.5559 - val_loss: 1.5205 - val_acc: 0.2188\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9307 - acc: 0.6013 - val_loss: 1.3872 - val_acc: 0.2500\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0864 - acc: 0.5332 - val_loss: 1.3763 - val_acc: 0.2812\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9662 - acc: 0.5770 - val_loss: 1.2590 - val_acc: 0.3281\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9702 - acc: 0.5705 - val_loss: 1.4406 - val_acc: 0.2031\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8736 - acc: 0.6305 - val_loss: 1.4064 - val_acc: 0.2344\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9809 - acc: 0.5867 - val_loss: 1.3799 - val_acc: 0.2969\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9952 - acc: 0.5835 - val_loss: 1.4235 - val_acc: 0.2500\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9873 - acc: 0.5689 - val_loss: 1.3870 - val_acc: 0.2812\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9960 - acc: 0.5559 - val_loss: 1.4618 - val_acc: 0.2188\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9528 - acc: 0.6045 - val_loss: 1.3236 - val_acc: 0.2969\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9909 - acc: 0.5705 - val_loss: 1.3083 - val_acc: 0.2812\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9641 - acc: 0.5656 - val_loss: 1.4969 - val_acc: 0.1562\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0383 - acc: 0.5673 - val_loss: 1.3705 - val_acc: 0.2500\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9510 - acc: 0.5705 - val_loss: 1.2620 - val_acc: 0.2812\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9562 - acc: 0.5948 - val_loss: 1.3578 - val_acc: 0.2812\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9830 - acc: 0.5640 - val_loss: 1.4383 - val_acc: 0.1719\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9442 - acc: 0.5737 - val_loss: 1.3326 - val_acc: 0.2969\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9825 - acc: 0.5754 - val_loss: 1.3571 - val_acc: 0.2344\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 0.9498 - acc: 0.5770 - val_loss: 1.5056 - val_acc: 0.1875\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9922 - acc: 0.5640 - val_loss: 1.3028 - val_acc: 0.2812\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9600 - acc: 0.5867 - val_loss: 1.3158 - val_acc: 0.2969\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0090 - acc: 0.5754 - val_loss: 1.2952 - val_acc: 0.2812\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0536 - acc: 0.5203 - val_loss: 1.4495 - val_acc: 0.2812\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0318 - acc: 0.5511 - val_loss: 1.2437 - val_acc: 0.3281\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9042 - acc: 0.5813 - val_loss: 1.2991 - val_acc: 0.2500\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9032 - acc: 0.5721 - val_loss: 1.4292 - val_acc: 0.2656\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9902 - acc: 0.5592 - val_loss: 1.4068 - val_acc: 0.2188\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9426 - acc: 0.5867 - val_loss: 1.2368 - val_acc: 0.3438\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0103 - acc: 0.5547 - val_loss: 1.3343 - val_acc: 0.2812\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0320 - acc: 0.5594 - val_loss: 1.3259 - val_acc: 0.2656\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9766 - acc: 0.5964 - val_loss: 1.4829 - val_acc: 0.2344\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.0402 - acc: 0.5543 - val_loss: 1.3794 - val_acc: 0.2188\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9378 - acc: 0.5818 - val_loss: 1.2686 - val_acc: 0.2656\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9703 - acc: 0.5673 - val_loss: 1.3882 - val_acc: 0.2500\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9608 - acc: 0.5818 - val_loss: 1.3281 - val_acc: 0.2500\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 0.9990 - acc: 0.5575 - val_loss: 1.4142 - val_acc: 0.2656\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9376 - acc: 0.5835 - val_loss: 1.3892 - val_acc: 0.2656\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9504 - acc: 0.6045 - val_loss: 1.4625 - val_acc: 0.2344\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9862 - acc: 0.5851 - val_loss: 1.3983 - val_acc: 0.2812\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9613 - acc: 0.5867 - val_loss: 1.3973 - val_acc: 0.2188\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9979 - acc: 0.5770 - val_loss: 1.3783 - val_acc: 0.2500\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.9462 - acc: 0.5844 - val_loss: 1.3170 - val_acc: 0.2812\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9354 - acc: 0.6045 - val_loss: 1.4273 - val_acc: 0.2188\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9712 - acc: 0.5818 - val_loss: 1.4602 - val_acc: 0.1875\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9303 - acc: 0.5867 - val_loss: 1.4719 - val_acc: 0.2812\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9663 - acc: 0.5867 - val_loss: 1.3905 - val_acc: 0.2344\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9602 - acc: 0.5835 - val_loss: 1.4040 - val_acc: 0.2656\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9979 - acc: 0.5624 - val_loss: 1.4755 - val_acc: 0.1875\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9240 - acc: 0.6126 - val_loss: 1.3869 - val_acc: 0.2344\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9444 - acc: 0.5754 - val_loss: 1.3970 - val_acc: 0.3125\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9718 - acc: 0.5851 - val_loss: 1.4615 - val_acc: 0.2344\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9860 - acc: 0.5624 - val_loss: 1.3266 - val_acc: 0.2969\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9533 - acc: 0.5867 - val_loss: 1.3563 - val_acc: 0.3125\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9932 - acc: 0.5640 - val_loss: 1.4998 - val_acc: 0.2500\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9673 - acc: 0.5624 - val_loss: 1.4019 - val_acc: 0.3125\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9723 - acc: 0.5770 - val_loss: 1.4315 - val_acc: 0.2656\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9480 - acc: 0.6175 - val_loss: 1.4739 - val_acc: 0.1875\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9968 - acc: 0.5750 - val_loss: 1.3256 - val_acc: 0.2812\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9899 - acc: 0.5802 - val_loss: 1.3613 - val_acc: 0.2656\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9924 - acc: 0.5786 - val_loss: 1.4021 - val_acc: 0.2969\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9158 - acc: 0.6029 - val_loss: 1.3206 - val_acc: 0.3125\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9338 - acc: 0.5900 - val_loss: 1.4609 - val_acc: 0.2500\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9702 - acc: 0.5818 - val_loss: 1.2727 - val_acc: 0.2344\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9584 - acc: 0.5656 - val_loss: 1.3681 - val_acc: 0.2188\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0071 - acc: 0.5624 - val_loss: 1.3859 - val_acc: 0.2500\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9308 - acc: 0.5997 - val_loss: 1.4345 - val_acc: 0.2188\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9511 - acc: 0.5900 - val_loss: 1.4233 - val_acc: 0.2500\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9787 - acc: 0.5754 - val_loss: 1.1615 - val_acc: 0.3594\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0077 - acc: 0.5797 - val_loss: 1.3063 - val_acc: 0.2500\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9870 - acc: 0.5867 - val_loss: 1.3582 - val_acc: 0.3125\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9405 - acc: 0.5997 - val_loss: 1.2894 - val_acc: 0.3125\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9586 - acc: 0.5900 - val_loss: 1.5371 - val_acc: 0.1562\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9575 - acc: 0.5964 - val_loss: 1.2831 - val_acc: 0.3281\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9429 - acc: 0.5981 - val_loss: 1.2480 - val_acc: 0.2969\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.0090 - acc: 0.5437 - val_loss: 1.2600 - val_acc: 0.3125\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 0.9531 - acc: 0.5900 - val_loss: 1.3592 - val_acc: 0.3125\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9370 - acc: 0.5867 - val_loss: 1.3725 - val_acc: 0.2969\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9353 - acc: 0.5964 - val_loss: 1.3025 - val_acc: 0.2969\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0117 - acc: 0.5543 - val_loss: 1.3222 - val_acc: 0.2969\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9448 - acc: 0.5916 - val_loss: 1.2769 - val_acc: 0.2969\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9224 - acc: 0.6305 - val_loss: 1.2801 - val_acc: 0.2969\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9867 - acc: 0.5559 - val_loss: 1.3717 - val_acc: 0.2188\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9550 - acc: 0.5802 - val_loss: 1.3685 - val_acc: 0.2812\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9648 - acc: 0.5721 - val_loss: 1.3935 - val_acc: 0.2188\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0370 - acc: 0.5531 - val_loss: 1.2937 - val_acc: 0.2656\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9883 - acc: 0.5689 - val_loss: 1.3270 - val_acc: 0.2344\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9534 - acc: 0.5543 - val_loss: 1.4207 - val_acc: 0.2656\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9614 - acc: 0.5883 - val_loss: 1.3523 - val_acc: 0.3125\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9662 - acc: 0.5624 - val_loss: 1.4444 - val_acc: 0.2656\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0252 - acc: 0.5365 - val_loss: 1.5216 - val_acc: 0.2500\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0072 - acc: 0.5883 - val_loss: 1.2715 - val_acc: 0.3594\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9382 - acc: 0.6013 - val_loss: 1.3166 - val_acc: 0.2656\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8687 - acc: 0.6272 - val_loss: 1.2417 - val_acc: 0.3125\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9330 - acc: 0.6143 - val_loss: 1.4736 - val_acc: 0.2344\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9473 - acc: 0.5883 - val_loss: 1.2930 - val_acc: 0.2969\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9570 - acc: 0.5883 - val_loss: 1.2757 - val_acc: 0.2969\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9478 - acc: 0.5883 - val_loss: 1.3564 - val_acc: 0.2812\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9069 - acc: 0.5844 - val_loss: 1.3554 - val_acc: 0.3281\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9079 - acc: 0.5997 - val_loss: 1.4788 - val_acc: 0.2031\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9181 - acc: 0.6110 - val_loss: 1.3345 - val_acc: 0.2500\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9901 - acc: 0.5786 - val_loss: 1.3069 - val_acc: 0.2812\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9124 - acc: 0.5932 - val_loss: 1.3587 - val_acc: 0.2500\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0100 - acc: 0.5656 - val_loss: 1.3989 - val_acc: 0.2812\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9043 - acc: 0.6045 - val_loss: 1.3522 - val_acc: 0.2812\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9337 - acc: 0.5835 - val_loss: 1.4649 - val_acc: 0.2031\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9446 - acc: 0.5964 - val_loss: 1.3225 - val_acc: 0.2812\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9613 - acc: 0.5754 - val_loss: 1.4194 - val_acc: 0.2500\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9470 - acc: 0.5705 - val_loss: 1.4321 - val_acc: 0.2188\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9019 - acc: 0.5900 - val_loss: 1.3729 - val_acc: 0.3125\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9864 - acc: 0.5786 - val_loss: 1.4042 - val_acc: 0.2500\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9506 - acc: 0.5964 - val_loss: 1.3306 - val_acc: 0.3125\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9713 - acc: 0.5802 - val_loss: 1.3581 - val_acc: 0.2812\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9691 - acc: 0.5786 - val_loss: 1.3243 - val_acc: 0.2812\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9876 - acc: 0.5640 - val_loss: 1.3905 - val_acc: 0.2656\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9434 - acc: 0.5802 - val_loss: 1.3001 - val_acc: 0.2188\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9540 - acc: 0.5754 - val_loss: 1.2914 - val_acc: 0.3438\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9858 - acc: 0.5705 - val_loss: 1.3344 - val_acc: 0.2656\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9236 - acc: 0.5835 - val_loss: 1.3368 - val_acc: 0.2969\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9382 - acc: 0.6013 - val_loss: 1.2738 - val_acc: 0.2812\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9713 - acc: 0.5656 - val_loss: 1.3560 - val_acc: 0.2500\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0035 - acc: 0.5624 - val_loss: 1.4240 - val_acc: 0.2344\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9500 - acc: 0.5851 - val_loss: 1.2896 - val_acc: 0.2656\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9391 - acc: 0.5575 - val_loss: 1.4343 - val_acc: 0.2344\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9478 - acc: 0.5883 - val_loss: 1.2585 - val_acc: 0.3125\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9338 - acc: 0.5867 - val_loss: 1.3867 - val_acc: 0.2656\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9472 - acc: 0.5705 - val_loss: 1.3085 - val_acc: 0.2969\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9297 - acc: 0.5883 - val_loss: 1.4308 - val_acc: 0.2500\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.9038 - acc: 0.6013 - val_loss: 1.3653 - val_acc: 0.2812\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9248 - acc: 0.6045 - val_loss: 1.2973 - val_acc: 0.3281\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9265 - acc: 0.5932 - val_loss: 1.3669 - val_acc: 0.2812\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9614 - acc: 0.5705 - val_loss: 1.3657 - val_acc: 0.2812\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9008 - acc: 0.5851 - val_loss: 1.3767 - val_acc: 0.2656\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9894 - acc: 0.5575 - val_loss: 1.3257 - val_acc: 0.2969\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0192 - acc: 0.5737 - val_loss: 1.4088 - val_acc: 0.2812\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8947 - acc: 0.6207 - val_loss: 1.3577 - val_acc: 0.2969\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9033 - acc: 0.5997 - val_loss: 1.3111 - val_acc: 0.3125\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9107 - acc: 0.6110 - val_loss: 1.3345 - val_acc: 0.2500\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9483 - acc: 0.5786 - val_loss: 1.2904 - val_acc: 0.3125\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9084 - acc: 0.6062 - val_loss: 1.2776 - val_acc: 0.2500\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9757 - acc: 0.6062 - val_loss: 1.3395 - val_acc: 0.2812\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0142 - acc: 0.5527 - val_loss: 1.3904 - val_acc: 0.3438\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9640 - acc: 0.5754 - val_loss: 1.3238 - val_acc: 0.3125\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9399 - acc: 0.5835 - val_loss: 1.3447 - val_acc: 0.2969\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9371 - acc: 0.5932 - val_loss: 1.3646 - val_acc: 0.3281\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8862 - acc: 0.6240 - val_loss: 1.2645 - val_acc: 0.2969\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9443 - acc: 0.6029 - val_loss: 1.2592 - val_acc: 0.2969\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9563 - acc: 0.5818 - val_loss: 1.2975 - val_acc: 0.2812\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9221 - acc: 0.5916 - val_loss: 1.2644 - val_acc: 0.3125\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9210 - acc: 0.6110 - val_loss: 1.3050 - val_acc: 0.3125\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9085 - acc: 0.5981 - val_loss: 1.3430 - val_acc: 0.2031\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9183 - acc: 0.6029 - val_loss: 1.3707 - val_acc: 0.2812\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9209 - acc: 0.5948 - val_loss: 1.3686 - val_acc: 0.2969\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9531 - acc: 0.5754 - val_loss: 1.3626 - val_acc: 0.2812\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.8997 - acc: 0.6078 - val_loss: 1.3286 - val_acc: 0.2812\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8863 - acc: 0.6175 - val_loss: 1.2774 - val_acc: 0.3438\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9368 - acc: 0.6094 - val_loss: 1.3409 - val_acc: 0.2969\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9205 - acc: 0.6013 - val_loss: 1.3602 - val_acc: 0.2969\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8874 - acc: 0.5964 - val_loss: 1.2951 - val_acc: 0.2812\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9256 - acc: 0.5964 - val_loss: 1.3773 - val_acc: 0.2969\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9192 - acc: 0.5984 - val_loss: 1.3844 - val_acc: 0.2500\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9102 - acc: 0.5867 - val_loss: 1.4478 - val_acc: 0.2344\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9313 - acc: 0.5938 - val_loss: 1.4009 - val_acc: 0.2812\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8876 - acc: 0.6110 - val_loss: 1.3621 - val_acc: 0.3281\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9311 - acc: 0.6191 - val_loss: 1.3594 - val_acc: 0.3125\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.9499 - acc: 0.5673 - val_loss: 1.2791 - val_acc: 0.2812\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9270 - acc: 0.6175 - val_loss: 1.3540 - val_acc: 0.2812\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9346 - acc: 0.5932 - val_loss: 1.3616 - val_acc: 0.2188\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9485 - acc: 0.5608 - val_loss: 1.3260 - val_acc: 0.2812\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8978 - acc: 0.5770 - val_loss: 1.3060 - val_acc: 0.2500\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9598 - acc: 0.5705 - val_loss: 1.3944 - val_acc: 0.2969\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9424 - acc: 0.5835 - val_loss: 1.3583 - val_acc: 0.2969\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8722 - acc: 0.6288 - val_loss: 1.3484 - val_acc: 0.2656\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9011 - acc: 0.5883 - val_loss: 1.3705 - val_acc: 0.2656\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9559 - acc: 0.5786 - val_loss: 1.4341 - val_acc: 0.1719\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8848 - acc: 0.6281 - val_loss: 1.3310 - val_acc: 0.3281\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9218 - acc: 0.5835 - val_loss: 1.2361 - val_acc: 0.2656\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9401 - acc: 0.5964 - val_loss: 1.3012 - val_acc: 0.3125\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9408 - acc: 0.5859 - val_loss: 1.2838 - val_acc: 0.2969\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9318 - acc: 0.5640 - val_loss: 1.2132 - val_acc: 0.3281\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9714 - acc: 0.5673 - val_loss: 1.2553 - val_acc: 0.3438\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9040 - acc: 0.6062 - val_loss: 1.4284 - val_acc: 0.2500\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 2s 207ms/step - loss: 0.8844 - acc: 0.5981 - val_loss: 1.2738 - val_acc: 0.2969\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9437 - acc: 0.5883 - val_loss: 1.2753 - val_acc: 0.3750\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9525 - acc: 0.5689 - val_loss: 1.4103 - val_acc: 0.2500\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9166 - acc: 0.6013 - val_loss: 1.3162 - val_acc: 0.2656\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8522 - acc: 0.6337 - val_loss: 1.2579 - val_acc: 0.3594\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9221 - acc: 0.6045 - val_loss: 1.4496 - val_acc: 0.2500\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9644 - acc: 0.5883 - val_loss: 1.3372 - val_acc: 0.2656\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8822 - acc: 0.6143 - val_loss: 1.2566 - val_acc: 0.2500\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8938 - acc: 0.5770 - val_loss: 1.3160 - val_acc: 0.2812\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9041 - acc: 0.6219 - val_loss: 1.2943 - val_acc: 0.2500\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9112 - acc: 0.5851 - val_loss: 1.2774 - val_acc: 0.3594\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8757 - acc: 0.6240 - val_loss: 1.3266 - val_acc: 0.3125\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9535 - acc: 0.5608 - val_loss: 1.4056 - val_acc: 0.2500\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9006 - acc: 0.6045 - val_loss: 1.2682 - val_acc: 0.3438\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9435 - acc: 0.5835 - val_loss: 1.3592 - val_acc: 0.3281\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8970 - acc: 0.5786 - val_loss: 1.3216 - val_acc: 0.2344\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8940 - acc: 0.6143 - val_loss: 1.2119 - val_acc: 0.3125\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9402 - acc: 0.5900 - val_loss: 1.2199 - val_acc: 0.3438\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9105 - acc: 0.6045 - val_loss: 1.3148 - val_acc: 0.3125\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9394 - acc: 0.6013 - val_loss: 1.1909 - val_acc: 0.3750\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8930 - acc: 0.6250 - val_loss: 1.3105 - val_acc: 0.3281\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9873 - acc: 0.5786 - val_loss: 1.2908 - val_acc: 0.2812\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9264 - acc: 0.6029 - val_loss: 1.2730 - val_acc: 0.3438\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8952 - acc: 0.6029 - val_loss: 1.3987 - val_acc: 0.2812\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9625 - acc: 0.5703 - val_loss: 1.1906 - val_acc: 0.3281\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9232 - acc: 0.5900 - val_loss: 1.4436 - val_acc: 0.2500\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8466 - acc: 0.6256 - val_loss: 1.3068 - val_acc: 0.2812\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9229 - acc: 0.6013 - val_loss: 1.3617 - val_acc: 0.3125\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8843 - acc: 0.5883 - val_loss: 1.3564 - val_acc: 0.2969\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8943 - acc: 0.6256 - val_loss: 1.2442 - val_acc: 0.2656\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9543 - acc: 0.5883 - val_loss: 1.2938 - val_acc: 0.2969\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8958 - acc: 0.5948 - val_loss: 1.3526 - val_acc: 0.2812\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9177 - acc: 0.5818 - val_loss: 1.2271 - val_acc: 0.3438\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8789 - acc: 0.6126 - val_loss: 1.3175 - val_acc: 0.3125\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8883 - acc: 0.5981 - val_loss: 1.4190 - val_acc: 0.3125\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.9298 - acc: 0.6224 - val_loss: 1.2779 - val_acc: 0.3594\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9783 - acc: 0.5754 - val_loss: 1.3125 - val_acc: 0.3125\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8935 - acc: 0.6110 - val_loss: 1.2532 - val_acc: 0.3125\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9101 - acc: 0.6013 - val_loss: 1.3327 - val_acc: 0.3281\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8955 - acc: 0.5867 - val_loss: 1.3071 - val_acc: 0.3438\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8998 - acc: 0.6094 - val_loss: 1.3093 - val_acc: 0.2656\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8953 - acc: 0.5948 - val_loss: 1.3223 - val_acc: 0.2812\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8673 - acc: 0.6159 - val_loss: 1.3287 - val_acc: 0.2656\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9242 - acc: 0.5851 - val_loss: 1.3478 - val_acc: 0.2656\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.8995 - acc: 0.6062 - val_loss: 1.4030 - val_acc: 0.2812\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8372 - acc: 0.6288 - val_loss: 1.1625 - val_acc: 0.3750\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8447 - acc: 0.6402 - val_loss: 1.2676 - val_acc: 0.3281\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9353 - acc: 0.6126 - val_loss: 1.2976 - val_acc: 0.3281\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8859 - acc: 0.6143 - val_loss: 1.3017 - val_acc: 0.3125\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8476 - acc: 0.6234 - val_loss: 1.3576 - val_acc: 0.3281\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8142 - acc: 0.6272 - val_loss: 1.1306 - val_acc: 0.3906\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9422 - acc: 0.5770 - val_loss: 1.3278 - val_acc: 0.2656\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8884 - acc: 0.6143 - val_loss: 1.3955 - val_acc: 0.2969\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9252 - acc: 0.5705 - val_loss: 1.2742 - val_acc: 0.3281\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8591 - acc: 0.6207 - val_loss: 1.3976 - val_acc: 0.2969\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9614 - acc: 0.5932 - val_loss: 1.2676 - val_acc: 0.3750\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9089 - acc: 0.6013 - val_loss: 1.4029 - val_acc: 0.2812\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.8522 - acc: 0.6224 - val_loss: 1.2538 - val_acc: 0.3438\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8576 - acc: 0.6126 - val_loss: 1.3485 - val_acc: 0.3125\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9050 - acc: 0.5916 - val_loss: 1.2139 - val_acc: 0.3594\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9159 - acc: 0.6159 - val_loss: 1.3621 - val_acc: 0.2812\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9469 - acc: 0.6143 - val_loss: 1.2615 - val_acc: 0.3125\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8975 - acc: 0.5964 - val_loss: 1.3187 - val_acc: 0.2812\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9203 - acc: 0.5883 - val_loss: 1.1871 - val_acc: 0.3438\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8651 - acc: 0.6359 - val_loss: 1.3310 - val_acc: 0.2812\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8674 - acc: 0.6256 - val_loss: 1.3444 - val_acc: 0.3125\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8538 - acc: 0.6175 - val_loss: 1.3035 - val_acc: 0.3125\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8940 - acc: 0.6110 - val_loss: 1.2191 - val_acc: 0.2969\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9492 - acc: 0.5922 - val_loss: 1.2728 - val_acc: 0.2969\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8599 - acc: 0.6110 - val_loss: 1.2809 - val_acc: 0.2969\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8683 - acc: 0.5818 - val_loss: 1.2736 - val_acc: 0.3281\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.8900 - acc: 0.6062 - val_loss: 1.3576 - val_acc: 0.3125\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8954 - acc: 0.5867 - val_loss: 1.2414 - val_acc: 0.3281\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9212 - acc: 0.5969 - val_loss: 1.2924 - val_acc: 0.3125\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8718 - acc: 0.5818 - val_loss: 1.3572 - val_acc: 0.3281\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8924 - acc: 0.6078 - val_loss: 1.2477 - val_acc: 0.3125\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8521 - acc: 0.6370 - val_loss: 1.2926 - val_acc: 0.2812\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8739 - acc: 0.6224 - val_loss: 1.3109 - val_acc: 0.3750\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.8479 - acc: 0.6256 - val_loss: 1.2884 - val_acc: 0.2812\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8705 - acc: 0.6207 - val_loss: 1.3669 - val_acc: 0.2969\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8169 - acc: 0.6564 - val_loss: 1.2850 - val_acc: 0.2969\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9128 - acc: 0.6078 - val_loss: 1.4276 - val_acc: 0.2656\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9232 - acc: 0.5883 - val_loss: 1.3365 - val_acc: 0.2969\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8673 - acc: 0.6175 - val_loss: 1.3492 - val_acc: 0.2656\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8446 - acc: 0.6013 - val_loss: 1.3370 - val_acc: 0.2656\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9600 - acc: 0.5818 - val_loss: 1.3915 - val_acc: 0.2344\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9212 - acc: 0.5948 - val_loss: 1.4240 - val_acc: 0.2656\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9031 - acc: 0.6191 - val_loss: 1.2925 - val_acc: 0.2969\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8601 - acc: 0.6143 - val_loss: 1.3452 - val_acc: 0.3750\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8646 - acc: 0.6402 - val_loss: 1.3603 - val_acc: 0.2656\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9101 - acc: 0.6013 - val_loss: 1.3414 - val_acc: 0.2969\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9142 - acc: 0.5964 - val_loss: 1.4107 - val_acc: 0.2500\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9348 - acc: 0.5818 - val_loss: 1.2678 - val_acc: 0.2656\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8392 - acc: 0.6305 - val_loss: 1.4456 - val_acc: 0.2812\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9260 - acc: 0.5851 - val_loss: 1.3360 - val_acc: 0.3125\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9319 - acc: 0.5916 - val_loss: 1.3712 - val_acc: 0.2812\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9110 - acc: 0.6305 - val_loss: 1.3657 - val_acc: 0.2500\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9304 - acc: 0.6159 - val_loss: 1.2692 - val_acc: 0.2969\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8783 - acc: 0.6143 - val_loss: 1.3922 - val_acc: 0.2969\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9273 - acc: 0.5906 - val_loss: 1.3286 - val_acc: 0.3281\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 2s 208ms/step - loss: 0.8779 - acc: 0.6078 - val_loss: 1.3545 - val_acc: 0.3125\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9307 - acc: 0.5766 - val_loss: 1.3382 - val_acc: 0.2188\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8763 - acc: 0.6240 - val_loss: 1.3208 - val_acc: 0.3125\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 2s 209ms/step - loss: 0.9255 - acc: 0.5932 - val_loss: 1.2410 - val_acc: 0.3750\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.8403 - acc: 0.6548 - val_loss: 1.4096 - val_acc: 0.2500\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.9189 - acc: 0.5802 - val_loss: 1.2290 - val_acc: 0.3125\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9131 - acc: 0.6256 - val_loss: 1.3077 - val_acc: 0.2969\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9428 - acc: 0.5835 - val_loss: 1.4174 - val_acc: 0.2656\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8837 - acc: 0.6013 - val_loss: 1.4164 - val_acc: 0.2188\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8504 - acc: 0.6240 - val_loss: 1.2445 - val_acc: 0.2969\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8473 - acc: 0.6207 - val_loss: 1.2886 - val_acc: 0.3125\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9034 - acc: 0.6062 - val_loss: 1.2859 - val_acc: 0.2969\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8439 - acc: 0.6467 - val_loss: 1.2331 - val_acc: 0.3281\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8844 - acc: 0.5754 - val_loss: 1.3330 - val_acc: 0.2812\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8819 - acc: 0.6207 - val_loss: 1.3429 - val_acc: 0.2500\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.8607 - acc: 0.6159 - val_loss: 1.3695 - val_acc: 0.2969\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8701 - acc: 0.6062 - val_loss: 1.2725 - val_acc: 0.3125\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9159 - acc: 0.5770 - val_loss: 1.2903 - val_acc: 0.3281\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8875 - acc: 0.6240 - val_loss: 1.4668 - val_acc: 0.2031\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8976 - acc: 0.6000 - val_loss: 1.2857 - val_acc: 0.2812\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8681 - acc: 0.6207 - val_loss: 1.3249 - val_acc: 0.2812\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8725 - acc: 0.6078 - val_loss: 1.3917 - val_acc: 0.2500\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8959 - acc: 0.6175 - val_loss: 1.3950 - val_acc: 0.2812\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 0.8696 - acc: 0.6159 - val_loss: 1.3284 - val_acc: 0.3281\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9090 - acc: 0.6126 - val_loss: 1.3333 - val_acc: 0.3281\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9413 - acc: 0.5900 - val_loss: 1.4131 - val_acc: 0.2188\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8846 - acc: 0.6078 - val_loss: 1.3151 - val_acc: 0.2969\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8160 - acc: 0.6288 - val_loss: 1.3090 - val_acc: 0.2812\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.8734 - acc: 0.6078 - val_loss: 1.2986 - val_acc: 0.2812\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8778 - acc: 0.6126 - val_loss: 1.2790 - val_acc: 0.2812\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9044 - acc: 0.5835 - val_loss: 1.3234 - val_acc: 0.2500\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8766 - acc: 0.6305 - val_loss: 1.4078 - val_acc: 0.2031\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8076 - acc: 0.6548 - val_loss: 1.3429 - val_acc: 0.2969\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8782 - acc: 0.6175 - val_loss: 1.4093 - val_acc: 0.2344\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9460 - acc: 0.6094 - val_loss: 1.3419 - val_acc: 0.2500\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8696 - acc: 0.6126 - val_loss: 1.2946 - val_acc: 0.2969\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8834 - acc: 0.5883 - val_loss: 1.2918 - val_acc: 0.3125\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9203 - acc: 0.6094 - val_loss: 1.2847 - val_acc: 0.3438\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8608 - acc: 0.6224 - val_loss: 1.4525 - val_acc: 0.2500\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9300 - acc: 0.5883 - val_loss: 1.2480 - val_acc: 0.3125\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9095 - acc: 0.5883 - val_loss: 1.1343 - val_acc: 0.3594\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8563 - acc: 0.6109 - val_loss: 1.3659 - val_acc: 0.2656\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8844 - acc: 0.6045 - val_loss: 1.2678 - val_acc: 0.2969\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8986 - acc: 0.5916 - val_loss: 1.3911 - val_acc: 0.2500\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8909 - acc: 0.6256 - val_loss: 1.2871 - val_acc: 0.2812\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8264 - acc: 0.6515 - val_loss: 1.3011 - val_acc: 0.3125\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8530 - acc: 0.6187 - val_loss: 1.3341 - val_acc: 0.3281\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9244 - acc: 0.5981 - val_loss: 1.2977 - val_acc: 0.2969\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 0.9138 - acc: 0.6045 - val_loss: 1.2511 - val_acc: 0.2969\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8408 - acc: 0.6337 - val_loss: 1.2675 - val_acc: 0.2812\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8586 - acc: 0.6191 - val_loss: 1.3392 - val_acc: 0.2500\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8597 - acc: 0.6159 - val_loss: 1.3763 - val_acc: 0.2656\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8950 - acc: 0.5948 - val_loss: 1.2772 - val_acc: 0.2969\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8871 - acc: 0.6272 - val_loss: 1.3468 - val_acc: 0.2812\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8095 - acc: 0.6305 - val_loss: 1.1359 - val_acc: 0.3750\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.8527 - acc: 0.6321 - val_loss: 1.1925 - val_acc: 0.3125\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9035 - acc: 0.5867 - val_loss: 1.2902 - val_acc: 0.3281\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8626 - acc: 0.6094 - val_loss: 1.3393 - val_acc: 0.2812\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8597 - acc: 0.6234 - val_loss: 1.3721 - val_acc: 0.2969\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8536 - acc: 0.6143 - val_loss: 1.3476 - val_acc: 0.3438\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8566 - acc: 0.6305 - val_loss: 1.3623 - val_acc: 0.2344\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.8873 - acc: 0.6013 - val_loss: 1.3451 - val_acc: 0.2656\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.8656 - acc: 0.6256 - val_loss: 1.2938 - val_acc: 0.3281\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8542 - acc: 0.6203 - val_loss: 1.4029 - val_acc: 0.2188\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8852 - acc: 0.6272 - val_loss: 1.3278 - val_acc: 0.2969\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8702 - acc: 0.6240 - val_loss: 1.2388 - val_acc: 0.2969\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8330 - acc: 0.6418 - val_loss: 1.3348 - val_acc: 0.2969\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.8727 - acc: 0.6337 - val_loss: 1.3042 - val_acc: 0.3281\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8171 - acc: 0.6321 - val_loss: 1.4543 - val_acc: 0.1719\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 2s 190ms/step - loss: 0.8904 - acc: 0.6305 - val_loss: 1.3776 - val_acc: 0.2031\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8911 - acc: 0.6272 - val_loss: 1.2624 - val_acc: 0.3281\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8612 - acc: 0.6143 - val_loss: 1.3992 - val_acc: 0.1875\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8278 - acc: 0.6434 - val_loss: 1.3349 - val_acc: 0.2656\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8247 - acc: 0.6467 - val_loss: 1.3406 - val_acc: 0.2812\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9229 - acc: 0.6126 - val_loss: 1.3914 - val_acc: 0.2969\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.9081 - acc: 0.6337 - val_loss: 1.1741 - val_acc: 0.3594\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8622 - acc: 0.6029 - val_loss: 1.2154 - val_acc: 0.3125\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.8265 - acc: 0.6272 - val_loss: 1.2766 - val_acc: 0.2812\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8942 - acc: 0.5964 - val_loss: 1.3191 - val_acc: 0.2344\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8764 - acc: 0.6062 - val_loss: 1.2234 - val_acc: 0.2812\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.8958 - acc: 0.6062 - val_loss: 1.2920 - val_acc: 0.2969\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8203 - acc: 0.6483 - val_loss: 1.2453 - val_acc: 0.2812\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.8696 - acc: 0.6434 - val_loss: 1.2478 - val_acc: 0.3125\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 2s 208ms/step - loss: 0.8529 - acc: 0.6094 - val_loss: 1.2066 - val_acc: 0.3750\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8035 - acc: 0.6578 - val_loss: 1.4117 - val_acc: 0.2812\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 2s 210ms/step - loss: 0.8238 - acc: 0.6434 - val_loss: 1.3574 - val_acc: 0.3281\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8337 - acc: 0.6175 - val_loss: 1.2613 - val_acc: 0.3438\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8257 - acc: 0.6321 - val_loss: 1.2843 - val_acc: 0.2812\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8587 - acc: 0.6159 - val_loss: 1.3956 - val_acc: 0.2500\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8562 - acc: 0.6305 - val_loss: 1.2774 - val_acc: 0.3281\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8654 - acc: 0.6143 - val_loss: 1.4030 - val_acc: 0.2500\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8719 - acc: 0.6029 - val_loss: 1.2919 - val_acc: 0.3281\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8392 - acc: 0.6175 - val_loss: 1.3128 - val_acc: 0.2969\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8615 - acc: 0.6272 - val_loss: 1.3759 - val_acc: 0.3281\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8591 - acc: 0.6305 - val_loss: 1.3594 - val_acc: 0.2656\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8709 - acc: 0.6240 - val_loss: 1.3471 - val_acc: 0.2656\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 2s 210ms/step - loss: 0.8491 - acc: 0.6305 - val_loss: 1.4739 - val_acc: 0.2344\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8462 - acc: 0.6305 - val_loss: 1.3389 - val_acc: 0.2656\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8762 - acc: 0.6110 - val_loss: 1.3699 - val_acc: 0.3125\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8382 - acc: 0.6451 - val_loss: 1.2166 - val_acc: 0.2969\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8378 - acc: 0.6499 - val_loss: 1.4023 - val_acc: 0.2500\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8305 - acc: 0.6175 - val_loss: 1.2480 - val_acc: 0.3281\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9075 - acc: 0.5984 - val_loss: 1.3380 - val_acc: 0.2656\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8620 - acc: 0.6159 - val_loss: 1.4238 - val_acc: 0.2656\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9004 - acc: 0.6159 - val_loss: 1.4483 - val_acc: 0.2500\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8619 - acc: 0.6143 - val_loss: 1.2547 - val_acc: 0.3125\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9079 - acc: 0.6013 - val_loss: 1.3337 - val_acc: 0.3125\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8463 - acc: 0.6240 - val_loss: 1.2545 - val_acc: 0.2969\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8369 - acc: 0.6386 - val_loss: 1.2732 - val_acc: 0.2344\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8382 - acc: 0.6467 - val_loss: 1.3159 - val_acc: 0.2656\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9136 - acc: 0.6062 - val_loss: 1.1738 - val_acc: 0.3281\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8270 - acc: 0.6451 - val_loss: 1.3334 - val_acc: 0.3281\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8866 - acc: 0.6143 - val_loss: 1.3195 - val_acc: 0.2500\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 0.8318 - acc: 0.6402 - val_loss: 1.2256 - val_acc: 0.2656\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8728 - acc: 0.6029 - val_loss: 1.2695 - val_acc: 0.3438\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8080 - acc: 0.6451 - val_loss: 1.3724 - val_acc: 0.2500\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8641 - acc: 0.6240 - val_loss: 1.2999 - val_acc: 0.2812\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8376 - acc: 0.6094 - val_loss: 1.3054 - val_acc: 0.3594\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8588 - acc: 0.6224 - val_loss: 1.4399 - val_acc: 0.3125\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8334 - acc: 0.6386 - val_loss: 1.3798 - val_acc: 0.3125\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8165 - acc: 0.6353 - val_loss: 1.4004 - val_acc: 0.2500\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8034 - acc: 0.6402 - val_loss: 1.3346 - val_acc: 0.2812\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8896 - acc: 0.6013 - val_loss: 1.2960 - val_acc: 0.2969\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8329 - acc: 0.6207 - val_loss: 1.2582 - val_acc: 0.3281\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8212 - acc: 0.6483 - val_loss: 1.3497 - val_acc: 0.3281\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8493 - acc: 0.6256 - val_loss: 1.2349 - val_acc: 0.3125\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8581 - acc: 0.6175 - val_loss: 1.3191 - val_acc: 0.2656\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8405 - acc: 0.6256 - val_loss: 1.1535 - val_acc: 0.3750\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8151 - acc: 0.6434 - val_loss: 1.2242 - val_acc: 0.3281\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8308 - acc: 0.6434 - val_loss: 1.3336 - val_acc: 0.2812\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9022 - acc: 0.5906 - val_loss: 1.2489 - val_acc: 0.3125\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8471 - acc: 0.6305 - val_loss: 1.3232 - val_acc: 0.2812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "10e5b26f-b289-42f9-fe49-6c02f8830fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.9462062120437622,\n",
              "  1.9386318922042847,\n",
              "  1.877082109451294,\n",
              "  1.843130111694336,\n",
              "  1.7962802648544312,\n",
              "  1.8467968702316284,\n",
              "  1.7866408824920654,\n",
              "  1.6872748136520386,\n",
              "  1.7133725881576538,\n",
              "  1.7586438655853271,\n",
              "  1.7154897451400757,\n",
              "  1.7807856798171997,\n",
              "  1.7461726665496826,\n",
              "  1.714652180671692,\n",
              "  1.7797616720199585,\n",
              "  1.6753214597702026,\n",
              "  1.6053097248077393,\n",
              "  1.6531274318695068,\n",
              "  1.6444368362426758,\n",
              "  1.667527198791504,\n",
              "  1.7617017030715942,\n",
              "  1.6226558685302734,\n",
              "  1.6873308420181274,\n",
              "  1.7513936758041382,\n",
              "  1.6566046476364136,\n",
              "  1.6770650148391724,\n",
              "  1.583794116973877,\n",
              "  1.7360543012619019,\n",
              "  1.6046332120895386,\n",
              "  1.627099871635437,\n",
              "  1.6527457237243652,\n",
              "  1.565783977508545,\n",
              "  1.5181725025177002,\n",
              "  1.6848682165145874,\n",
              "  1.670116901397705,\n",
              "  1.5698224306106567,\n",
              "  1.610750675201416,\n",
              "  1.5957201719284058,\n",
              "  1.6312943696975708,\n",
              "  1.7558239698410034,\n",
              "  1.6923032999038696,\n",
              "  1.6469990015029907,\n",
              "  1.6696017980575562,\n",
              "  1.624677062034607,\n",
              "  1.6396989822387695,\n",
              "  1.6189991235733032,\n",
              "  1.5731173753738403,\n",
              "  1.567242980003357,\n",
              "  1.644832968711853,\n",
              "  1.5387300252914429,\n",
              "  1.4985547065734863,\n",
              "  1.528092384338379,\n",
              "  1.5656776428222656,\n",
              "  1.587943434715271,\n",
              "  1.534051537513733,\n",
              "  1.5424443483352661,\n",
              "  1.5450235605239868,\n",
              "  1.5255194902420044,\n",
              "  1.4902968406677246,\n",
              "  1.5186192989349365,\n",
              "  1.5402348041534424,\n",
              "  1.5511482954025269,\n",
              "  1.5294189453125,\n",
              "  1.5514847040176392,\n",
              "  1.6410311460494995,\n",
              "  1.496329426765442,\n",
              "  1.5390021800994873,\n",
              "  1.6082183122634888,\n",
              "  1.5441651344299316,\n",
              "  1.6431797742843628,\n",
              "  1.4375293254852295,\n",
              "  1.5220248699188232,\n",
              "  1.51267409324646,\n",
              "  1.5496015548706055,\n",
              "  1.5041824579238892,\n",
              "  1.522094488143921,\n",
              "  1.5020549297332764,\n",
              "  1.5048929452896118,\n",
              "  1.6182477474212646,\n",
              "  1.472246766090393,\n",
              "  1.4216209650039673,\n",
              "  1.4998552799224854,\n",
              "  1.53590989112854,\n",
              "  1.5129668712615967,\n",
              "  1.5281484127044678,\n",
              "  1.4586620330810547,\n",
              "  1.4923861026763916,\n",
              "  1.4482645988464355,\n",
              "  1.4407904148101807,\n",
              "  1.4734976291656494,\n",
              "  1.4654439687728882,\n",
              "  1.5072457790374756,\n",
              "  1.5071099996566772,\n",
              "  1.4491382837295532,\n",
              "  1.4543873071670532,\n",
              "  1.341186285018921,\n",
              "  1.4662712812423706,\n",
              "  1.5326176881790161,\n",
              "  1.3674139976501465,\n",
              "  1.3143848180770874,\n",
              "  1.3704723119735718,\n",
              "  1.4753968715667725,\n",
              "  1.366522192955017,\n",
              "  1.4834028482437134,\n",
              "  1.3731387853622437,\n",
              "  1.493751883506775,\n",
              "  1.495168685913086,\n",
              "  1.3784176111221313,\n",
              "  1.3348472118377686,\n",
              "  1.4823243618011475,\n",
              "  1.377120852470398,\n",
              "  1.3735731840133667,\n",
              "  1.3371602296829224,\n",
              "  1.563049554824829,\n",
              "  1.3928357362747192,\n",
              "  1.3920063972473145,\n",
              "  1.4474225044250488,\n",
              "  1.3904820680618286,\n",
              "  1.4408628940582275,\n",
              "  1.4365378618240356,\n",
              "  1.4479098320007324,\n",
              "  1.3813377618789673,\n",
              "  1.4265655279159546,\n",
              "  1.4320787191390991,\n",
              "  1.3455013036727905,\n",
              "  1.3489885330200195,\n",
              "  1.394391417503357,\n",
              "  1.4097816944122314,\n",
              "  1.3729908466339111,\n",
              "  1.3860958814620972,\n",
              "  1.343015432357788,\n",
              "  1.35300874710083,\n",
              "  1.3712043762207031,\n",
              "  1.38059401512146,\n",
              "  1.3401219844818115,\n",
              "  1.2952114343643188,\n",
              "  1.2885864973068237,\n",
              "  1.3820230960845947,\n",
              "  1.3543980121612549,\n",
              "  1.3342288732528687,\n",
              "  1.3210872411727905,\n",
              "  1.3582606315612793,\n",
              "  1.3471927642822266,\n",
              "  1.4164708852767944,\n",
              "  1.4120680093765259,\n",
              "  1.3652293682098389,\n",
              "  1.3468974828720093,\n",
              "  1.3617273569107056,\n",
              "  1.320757508277893,\n",
              "  1.4183422327041626,\n",
              "  1.2399814128875732,\n",
              "  1.3382141590118408,\n",
              "  1.3779149055480957,\n",
              "  1.3331955671310425,\n",
              "  1.3353006839752197,\n",
              "  1.3892813920974731,\n",
              "  1.3205772638320923,\n",
              "  1.3176631927490234,\n",
              "  1.3156216144561768,\n",
              "  1.2540825605392456,\n",
              "  1.3089762926101685,\n",
              "  1.3793771266937256,\n",
              "  1.306626796722412,\n",
              "  1.3130676746368408,\n",
              "  1.2909578084945679,\n",
              "  1.316580057144165,\n",
              "  1.3749017715454102,\n",
              "  1.4064595699310303,\n",
              "  1.358154058456421,\n",
              "  1.40487539768219,\n",
              "  1.3202012777328491,\n",
              "  1.2822874784469604,\n",
              "  1.331756830215454,\n",
              "  1.3316290378570557,\n",
              "  1.3333314657211304,\n",
              "  1.301684856414795,\n",
              "  1.3180509805679321,\n",
              "  1.2539256811141968,\n",
              "  1.2638715505599976,\n",
              "  1.3129584789276123,\n",
              "  1.2875776290893555,\n",
              "  1.3133164644241333,\n",
              "  1.3398643732070923,\n",
              "  1.3112715482711792,\n",
              "  1.2365714311599731,\n",
              "  1.330574870109558,\n",
              "  1.2851260900497437,\n",
              "  1.3333730697631836,\n",
              "  1.3450621366500854,\n",
              "  1.257531762123108,\n",
              "  1.3127334117889404,\n",
              "  1.2504562139511108,\n",
              "  1.2875406742095947,\n",
              "  1.2397726774215698,\n",
              "  1.2607284784317017,\n",
              "  1.242931842803955,\n",
              "  1.2867319583892822,\n",
              "  1.2590340375900269,\n",
              "  1.263034701347351,\n",
              "  1.2178847789764404,\n",
              "  1.2862318754196167,\n",
              "  1.225449800491333,\n",
              "  1.2883007526397705,\n",
              "  1.2456698417663574,\n",
              "  1.278606653213501,\n",
              "  1.292752742767334,\n",
              "  1.2596032619476318,\n",
              "  1.342281460762024,\n",
              "  1.2622106075286865,\n",
              "  1.2719285488128662,\n",
              "  1.292682409286499,\n",
              "  1.267000675201416,\n",
              "  1.2324011325836182,\n",
              "  1.198688268661499,\n",
              "  1.3502812385559082,\n",
              "  1.2533749341964722,\n",
              "  1.2640751600265503,\n",
              "  1.2094194889068604,\n",
              "  1.295536994934082,\n",
              "  1.21889328956604,\n",
              "  1.2494654655456543,\n",
              "  1.2536473274230957,\n",
              "  1.2644479274749756,\n",
              "  1.2115501165390015,\n",
              "  1.2362654209136963,\n",
              "  1.2120331525802612,\n",
              "  1.332477331161499,\n",
              "  1.1951987743377686,\n",
              "  1.2019096612930298,\n",
              "  1.2467644214630127,\n",
              "  1.2919480800628662,\n",
              "  1.2221112251281738,\n",
              "  1.2271203994750977,\n",
              "  1.2238801717758179,\n",
              "  1.261817455291748,\n",
              "  1.2583003044128418,\n",
              "  1.3076103925704956,\n",
              "  1.311957597732544,\n",
              "  1.261658787727356,\n",
              "  1.2536733150482178,\n",
              "  1.218505859375,\n",
              "  1.2811253070831299,\n",
              "  1.2798088788986206,\n",
              "  1.189050555229187,\n",
              "  1.1811424493789673,\n",
              "  1.2380577325820923,\n",
              "  1.2536399364471436,\n",
              "  1.1601742506027222,\n",
              "  1.166085124015808,\n",
              "  1.1696302890777588,\n",
              "  1.2646690607070923,\n",
              "  1.2009915113449097,\n",
              "  1.2402009963989258,\n",
              "  1.2165286540985107,\n",
              "  1.1351851224899292,\n",
              "  1.248496413230896,\n",
              "  1.2060226202011108,\n",
              "  1.2381336688995361,\n",
              "  1.2466384172439575,\n",
              "  1.2413493394851685,\n",
              "  1.1316479444503784,\n",
              "  1.2158730030059814,\n",
              "  1.2032748460769653,\n",
              "  1.2349356412887573,\n",
              "  1.114101529121399,\n",
              "  1.2475426197052002,\n",
              "  1.1920663118362427,\n",
              "  1.242825984954834,\n",
              "  1.0934747457504272,\n",
              "  1.2854868173599243,\n",
              "  1.2058087587356567,\n",
              "  1.1963306665420532,\n",
              "  1.1224340200424194,\n",
              "  1.1890490055084229,\n",
              "  1.2250043153762817,\n",
              "  1.153274655342102,\n",
              "  1.101366400718689,\n",
              "  1.2059835195541382,\n",
              "  1.178877830505371,\n",
              "  1.1795402765274048,\n",
              "  1.168903112411499,\n",
              "  1.242814064025879,\n",
              "  1.1098369359970093,\n",
              "  1.2163677215576172,\n",
              "  1.2065809965133667,\n",
              "  1.1912763118743896,\n",
              "  1.1865308284759521,\n",
              "  1.2180542945861816,\n",
              "  1.26972496509552,\n",
              "  1.1560981273651123,\n",
              "  1.1901161670684814,\n",
              "  1.0911751985549927,\n",
              "  1.153400182723999,\n",
              "  1.1088844537734985,\n",
              "  1.194412112236023,\n",
              "  1.2363872528076172,\n",
              "  1.2256542444229126,\n",
              "  1.135656476020813,\n",
              "  1.1639701128005981,\n",
              "  1.1624797582626343,\n",
              "  1.1739333868026733,\n",
              "  1.1155555248260498,\n",
              "  1.285508394241333,\n",
              "  1.2084380388259888,\n",
              "  1.109277606010437,\n",
              "  1.1338586807250977,\n",
              "  1.1089751720428467,\n",
              "  1.1913403272628784,\n",
              "  1.1504900455474854,\n",
              "  1.0935723781585693,\n",
              "  1.2208718061447144,\n",
              "  1.1463873386383057,\n",
              "  1.1830999851226807,\n",
              "  1.0882031917572021,\n",
              "  1.1750216484069824,\n",
              "  1.2203648090362549,\n",
              "  1.1718710660934448,\n",
              "  1.1745189428329468,\n",
              "  1.1718038320541382,\n",
              "  1.1660064458847046,\n",
              "  1.1266237497329712,\n",
              "  1.250838279724121,\n",
              "  1.085697889328003,\n",
              "  1.1693135499954224,\n",
              "  1.1236326694488525,\n",
              "  1.0835027694702148,\n",
              "  1.198199987411499,\n",
              "  1.069673776626587,\n",
              "  1.1280468702316284,\n",
              "  1.0978790521621704,\n",
              "  1.1408339738845825,\n",
              "  1.1272311210632324,\n",
              "  1.1895405054092407,\n",
              "  1.1495318412780762,\n",
              "  1.1734920740127563,\n",
              "  1.0629416704177856,\n",
              "  1.154338002204895,\n",
              "  1.1134051084518433,\n",
              "  1.038816213607788,\n",
              "  1.074076533317566,\n",
              "  1.2125965356826782,\n",
              "  1.1164445877075195,\n",
              "  1.1097867488861084,\n",
              "  1.1503534317016602,\n",
              "  1.1143994331359863,\n",
              "  1.1344034671783447,\n",
              "  1.103727102279663,\n",
              "  1.0914499759674072,\n",
              "  1.0718201398849487,\n",
              "  1.1144204139709473,\n",
              "  1.193542718887329,\n",
              "  1.106087565422058,\n",
              "  1.1345655918121338,\n",
              "  1.1172999143600464,\n",
              "  1.2164945602416992,\n",
              "  1.1186877489089966,\n",
              "  1.1477749347686768,\n",
              "  1.1104865074157715,\n",
              "  1.1597741842269897,\n",
              "  1.1584656238555908,\n",
              "  1.1127443313598633,\n",
              "  1.1267011165618896,\n",
              "  1.0822185277938843,\n",
              "  1.1820056438446045,\n",
              "  1.142258882522583,\n",
              "  1.1403493881225586,\n",
              "  1.1394412517547607,\n",
              "  1.1060810089111328,\n",
              "  1.093277096748352,\n",
              "  1.086573600769043,\n",
              "  1.1161813735961914,\n",
              "  1.1357676982879639,\n",
              "  1.113489031791687,\n",
              "  1.088289737701416,\n",
              "  1.1313250064849854,\n",
              "  1.1006534099578857,\n",
              "  1.14395010471344,\n",
              "  1.0929006338119507,\n",
              "  1.0837737321853638,\n",
              "  1.1045808792114258,\n",
              "  1.045168161392212,\n",
              "  1.0535129308700562,\n",
              "  1.0599374771118164,\n",
              "  1.1761494874954224,\n",
              "  1.0235453844070435,\n",
              "  1.1356196403503418,\n",
              "  1.0370968580245972,\n",
              "  1.0862338542938232,\n",
              "  1.0784263610839844,\n",
              "  1.1014907360076904,\n",
              "  1.018918752670288,\n",
              "  1.0698291063308716,\n",
              "  1.1201436519622803,\n",
              "  1.0989614725112915,\n",
              "  1.0783380270004272,\n",
              "  1.0646827220916748,\n",
              "  1.12516188621521,\n",
              "  1.1775529384613037,\n",
              "  1.0522116422653198,\n",
              "  1.0768024921417236,\n",
              "  1.0605918169021606,\n",
              "  1.0269052982330322,\n",
              "  1.059471845626831,\n",
              "  1.152491807937622,\n",
              "  1.144118070602417,\n",
              "  1.0818822383880615,\n",
              "  1.101670265197754,\n",
              "  1.097517490386963,\n",
              "  1.1822060346603394,\n",
              "  1.087978720664978,\n",
              "  1.0338375568389893,\n",
              "  1.0483386516571045,\n",
              "  1.111750602722168,\n",
              "  1.1133990287780762,\n",
              "  1.139243483543396,\n",
              "  1.0996299982070923,\n",
              "  1.0752280950546265,\n",
              "  1.0757839679718018,\n",
              "  1.0849319696426392,\n",
              "  1.0908360481262207,\n",
              "  1.1019260883331299,\n",
              "  1.060481309890747,\n",
              "  1.0852066278457642,\n",
              "  1.0631266832351685,\n",
              "  1.0936485528945923,\n",
              "  1.015676498413086,\n",
              "  1.0563400983810425,\n",
              "  0.9969679117202759,\n",
              "  1.0851471424102783,\n",
              "  1.0468782186508179,\n",
              "  1.0811225175857544,\n",
              "  1.0457407236099243,\n",
              "  1.012661337852478,\n",
              "  1.020261287689209,\n",
              "  1.0614560842514038,\n",
              "  1.089809536933899,\n",
              "  1.0565712451934814,\n",
              "  1.0961109399795532,\n",
              "  1.0162056684494019,\n",
              "  1.0841196775436401,\n",
              "  1.0768314599990845,\n",
              "  1.0270618200302124,\n",
              "  1.106824517250061,\n",
              "  1.0819107294082642,\n",
              "  0.9686418771743774,\n",
              "  1.045427918434143,\n",
              "  1.0677469968795776,\n",
              "  1.04204261302948,\n",
              "  1.1051819324493408,\n",
              "  1.0331114530563354,\n",
              "  1.0258153676986694,\n",
              "  1.0149447917938232,\n",
              "  1.058032512664795,\n",
              "  1.036458969116211,\n",
              "  1.144469976425171,\n",
              "  1.03354012966156,\n",
              "  1.0791712999343872,\n",
              "  1.0577166080474854,\n",
              "  1.0421249866485596,\n",
              "  1.0114929676055908,\n",
              "  1.016179084777832,\n",
              "  1.014418125152588,\n",
              "  1.072726845741272,\n",
              "  1.0849642753601074,\n",
              "  1.1095271110534668,\n",
              "  1.0142221450805664,\n",
              "  1.0535385608673096,\n",
              "  1.0575065612792969,\n",
              "  1.060138463973999,\n",
              "  1.048256278038025,\n",
              "  1.0896053314208984,\n",
              "  1.0403398275375366,\n",
              "  1.0765002965927124,\n",
              "  1.0581570863723755,\n",
              "  1.0694578886032104,\n",
              "  0.9836548566818237,\n",
              "  1.0381629467010498,\n",
              "  1.0495692491531372,\n",
              "  1.064974308013916,\n",
              "  1.037636637687683,\n",
              "  1.0409200191497803,\n",
              "  1.0530825853347778,\n",
              "  1.0723340511322021,\n",
              "  0.9952404499053955,\n",
              "  1.0841115713119507,\n",
              "  1.0516712665557861,\n",
              "  1.0628347396850586,\n",
              "  1.0239392518997192,\n",
              "  0.9666167497634888,\n",
              "  1.0078076124191284,\n",
              "  1.0294421911239624,\n",
              "  1.004669189453125,\n",
              "  1.0281505584716797,\n",
              "  1.0122401714324951,\n",
              "  1.0579924583435059,\n",
              "  1.0337207317352295,\n",
              "  1.0438309907913208,\n",
              "  1.014843463897705,\n",
              "  1.04994797706604,\n",
              "  0.9933545589447021,\n",
              "  1.0743204355239868,\n",
              "  1.0738626718521118,\n",
              "  1.0079058408737183,\n",
              "  1.1098638772964478,\n",
              "  1.0550098419189453,\n",
              "  0.9997218251228333,\n",
              "  1.0442599058151245,\n",
              "  1.0222980976104736,\n",
              "  1.0074501037597656,\n",
              "  1.0045273303985596,\n",
              "  1.0813778638839722,\n",
              "  1.018791913986206,\n",
              "  0.997717559337616,\n",
              "  0.9746032357215881,\n",
              "  1.0604320764541626,\n",
              "  1.0378453731536865,\n",
              "  1.011678695678711,\n",
              "  0.9716311693191528,\n",
              "  1.054414987564087,\n",
              "  1.0871894359588623,\n",
              "  1.0459392070770264,\n",
              "  1.0551221370697021,\n",
              "  0.9637832045555115,\n",
              "  1.027700424194336,\n",
              "  0.977525532245636,\n",
              "  1.0458214282989502,\n",
              "  0.993728518486023,\n",
              "  1.0920888185501099,\n",
              "  1.0705710649490356,\n",
              "  1.0306062698364258,\n",
              "  1.0733623504638672,\n",
              "  1.0027998685836792,\n",
              "  1.0038686990737915,\n",
              "  1.092246413230896,\n",
              "  1.0798282623291016,\n",
              "  1.0352585315704346,\n",
              "  0.9763121008872986,\n",
              "  0.9876197576522827,\n",
              "  1.0423387289047241,\n",
              "  1.0534659624099731,\n",
              "  0.9935692548751831,\n",
              "  0.9725085496902466,\n",
              "  0.9772730469703674,\n",
              "  1.0196037292480469,\n",
              "  0.9650566577911377,\n",
              "  1.0206876993179321,\n",
              "  1.051986575126648,\n",
              "  1.0275799036026,\n",
              "  1.0170626640319824,\n",
              "  1.0150530338287354,\n",
              "  0.9778692722320557,\n",
              "  1.0694434642791748,\n",
              "  1.0462639331817627,\n",
              "  0.9646998643875122,\n",
              "  0.9935014843940735,\n",
              "  1.0260448455810547,\n",
              "  1.0288352966308594,\n",
              "  0.9966529607772827,\n",
              "  1.0058220624923706,\n",
              "  1.0261969566345215,\n",
              "  0.9563103318214417,\n",
              "  1.0070650577545166,\n",
              "  0.9956599473953247,\n",
              "  1.033508539199829,\n",
              "  1.0137367248535156,\n",
              "  0.9965336322784424,\n",
              "  0.986253559589386,\n",
              "  0.9721255302429199,\n",
              "  1.0420211553573608,\n",
              "  1.032909631729126,\n",
              "  1.0289924144744873,\n",
              "  0.9928744435310364,\n",
              "  1.0278507471084595,\n",
              "  1.0296213626861572,\n",
              "  0.9828840494155884,\n",
              "  0.9956234693527222,\n",
              "  0.9978094696998596,\n",
              "  0.9804331660270691,\n",
              "  0.9782357215881348,\n",
              "  0.9442519545555115,\n",
              "  1.0015308856964111,\n",
              "  0.994282603263855,\n",
              "  0.993930995464325,\n",
              "  1.0023982524871826,\n",
              "  1.0208258628845215,\n",
              "  0.9479625225067139,\n",
              "  1.007597804069519,\n",
              "  0.9307303428649902,\n",
              "  1.0864168405532837,\n",
              "  0.9661989808082581,\n",
              "  0.9702496528625488,\n",
              "  0.8735774755477905,\n",
              "  0.9808874726295471,\n",
              "  0.9952296614646912,\n",
              "  0.9873176217079163,\n",
              "  0.9959933161735535,\n",
              "  0.9527595043182373,\n",
              "  0.9908631443977356,\n",
              "  0.9641004204750061,\n",
              "  1.0383211374282837,\n",
              "  0.9509950280189514,\n",
              "  0.9561673402786255,\n",
              "  0.9829632043838501,\n",
              "  0.9442236423492432,\n",
              "  0.9825439453125,\n",
              "  0.9498247504234314,\n",
              "  0.992224395275116,\n",
              "  0.959958553314209,\n",
              "  1.0090274810791016,\n",
              "  1.0536364316940308,\n",
              "  1.0318360328674316,\n",
              "  0.9041574597358704,\n",
              "  0.9032407999038696,\n",
              "  0.9901913404464722,\n",
              "  0.9426078796386719,\n",
              "  1.0103328227996826,\n",
              "  1.032008409500122,\n",
              "  0.9765548706054688,\n",
              "  1.040223479270935,\n",
              "  0.9378228783607483,\n",
              "  0.9702812433242798,\n",
              "  0.9607812166213989,\n",
              "  0.999039888381958,\n",
              "  0.9376097321510315,\n",
              "  0.9504052400588989,\n",
              "  0.9862331748008728,\n",
              "  0.9612706899642944,\n",
              "  0.9979336261749268,\n",
              "  0.9461629986763,\n",
              "  0.9354471564292908,\n",
              "  0.9712395071983337,\n",
              "  0.9303151369094849,\n",
              "  0.9663198590278625,\n",
              "  0.9602323770523071,\n",
              "  0.9978935718536377,\n",
              "  0.9240337610244751,\n",
              "  0.9444372057914734,\n",
              "  0.9717684388160706,\n",
              "  0.9859805107116699,\n",
              "  0.9532890319824219,\n",
              "  0.993192732334137,\n",
              "  0.9672672748565674,\n",
              "  0.9722535610198975,\n",
              "  0.9480067491531372,\n",
              "  0.9967764019966125,\n",
              "  0.9898561239242554,\n",
              "  0.9923886060714722,\n",
              "  0.9158398509025574,\n",
              "  0.9338085055351257,\n",
              "  0.9702428579330444,\n",
              "  0.9583615660667419,\n",
              "  1.0071133375167847,\n",
              "  0.9307700991630554,\n",
              "  0.9510664939880371,\n",
              "  0.9787400960922241,\n",
              "  1.0077011585235596,\n",
              "  0.9870086908340454,\n",
              "  0.9405211806297302,\n",
              "  0.9586021304130554,\n",
              "  0.957461953163147,\n",
              "  0.9428654313087463,\n",
              "  1.0089757442474365,\n",
              "  0.9531248211860657,\n",
              "  0.9369857907295227,\n",
              "  0.9353331923484802,\n",
              "  1.0116844177246094,\n",
              "  0.9447656273841858,\n",
              "  0.9224016666412354,\n",
              "  0.9866767525672913,\n",
              "  0.9549733400344849,\n",
              "  0.9648415446281433,\n",
              "  1.0369501113891602,\n",
              "  0.9882886409759521,\n",
              "  0.9533994197845459,\n",
              "  0.9614360928535461,\n",
              "  0.9662250280380249,\n",
              "  1.0251702070236206,\n",
              "  1.0071758031845093,\n",
              "  0.9382402300834656,\n",
              "  0.8687002062797546,\n",
              "  0.9330246448516846,\n",
              "  0.9472715854644775,\n",
              "  0.957023024559021,\n",
              "  0.947787344455719,\n",
              "  0.9068979024887085,\n",
              "  0.9078829288482666,\n",
              "  0.9181138873100281,\n",
              "  0.990090548992157,\n",
              "  0.9123575687408447,\n",
              "  1.0100281238555908,\n",
              "  0.9042612910270691,\n",
              "  0.9337242245674133,\n",
              "  0.9445574283599854,\n",
              "  0.9613494277000427,\n",
              "  0.9470418691635132,\n",
              "  0.9018559455871582,\n",
              "  0.9863752126693726,\n",
              "  0.9506469964981079,\n",
              "  0.9713450074195862,\n",
              "  0.9690970182418823,\n",
              "  0.987601637840271,\n",
              "  0.9434223771095276,\n",
              "  0.9540098905563354,\n",
              "  0.9858325123786926,\n",
              "  0.9235697388648987,\n",
              "  0.9382163882255554,\n",
              "  0.9712614417076111,\n",
              "  1.0034773349761963,\n",
              "  0.9499622583389282,\n",
              "  0.939085841178894,\n",
              "  0.9477558135986328,\n",
              "  0.9337863326072693,\n",
              "  0.9471572041511536,\n",
              "  0.9296528697013855,\n",
              "  0.9037985801696777,\n",
              "  0.9247715473175049,\n",
              "  0.926458477973938,\n",
              "  0.9614434838294983,\n",
              "  0.9007951021194458,\n",
              "  0.9894457459449768,\n",
              "  1.0191802978515625,\n",
              "  0.8946655988693237,\n",
              "  0.9032717347145081,\n",
              "  0.9107475876808167,\n",
              "  0.9483492374420166,\n",
              "  0.9083808660507202,\n",
              "  0.9757357835769653,\n",
              "  1.0141702890396118,\n",
              "  0.9640064835548401,\n",
              "  0.9399159550666809,\n",
              "  0.9371093511581421,\n",
              "  0.8862265944480896,\n",
              "  0.9443430304527283,\n",
              "  0.9563444256782532,\n",
              "  0.9221069812774658,\n",
              "  0.9209858775138855,\n",
              "  0.9084811210632324,\n",
              "  0.9183245897293091,\n",
              "  0.9208779335021973,\n",
              "  0.9530693888664246,\n",
              "  0.8997299075126648,\n",
              "  0.8862792253494263,\n",
              "  0.9367897510528564,\n",
              "  0.9205003976821899,\n",
              "  0.8874349594116211,\n",
              "  0.9256116151809692,\n",
              "  0.9191786646842957,\n",
              "  0.9101670384407043,\n",
              "  0.9312928318977356,\n",
              "  0.8876289129257202,\n",
              "  0.9311226606369019,\n",
              "  0.9499143958091736,\n",
              "  0.9270169138908386,\n",
              "  0.9346250891685486,\n",
              "  0.948453962802887,\n",
              "  0.897798478603363,\n",
              "  0.9597913026809692,\n",
              "  0.942444920539856,\n",
              "  0.8721704483032227,\n",
              "  0.9010840654373169,\n",
              "  0.9559065103530884,\n",
              "  0.8847794532775879,\n",
              "  0.921772837638855,\n",
              "  0.9400622844696045,\n",
              "  0.9408344030380249,\n",
              "  0.9317803978919983,\n",
              "  0.9713883399963379,\n",
              "  0.9040267467498779,\n",
              "  0.8843762874603271,\n",
              "  0.9437130093574524,\n",
              "  0.9524624347686768,\n",
              "  0.9166267514228821,\n",
              "  0.8522182703018188,\n",
              "  0.922143280506134,\n",
              "  0.9643763303756714,\n",
              "  0.882198691368103,\n",
              "  0.893773078918457,\n",
              "  0.9040595889091492,\n",
              "  0.9111648797988892,\n",
              "  0.8757036328315735,\n",
              "  0.9535303115844727,\n",
              "  0.900550901889801,\n",
              "  0.9434528350830078,\n",
              "  0.8969792723655701,\n",
              "  0.8940183520317078,\n",
              "  0.9401735663414001,\n",
              "  0.9104900360107422,\n",
              "  0.9394401907920837,\n",
              "  0.8930419683456421,\n",
              "  0.9872843027114868,\n",
              "  0.9264228343963623,\n",
              "  0.8951738476753235,\n",
              "  0.9625040888786316,\n",
              "  0.9231972694396973,\n",
              "  0.846628725528717,\n",
              "  0.922947883605957,\n",
              "  0.8842870593070984,\n",
              "  0.8942523002624512,\n",
              "  0.9543211460113525,\n",
              "  0.8958176374435425,\n",
              "  0.9177212715148926,\n",
              "  0.8789293169975281,\n",
              "  0.8882606625556946,\n",
              "  0.929790198802948,\n",
              "  0.9782589077949524,\n",
              "  0.8935266137123108,\n",
              "  0.9101211428642273,\n",
              "  0.8955373764038086,\n",
              "  0.8998087644577026,\n",
              "  0.8952740430831909,\n",
              "  0.8672825694084167,\n",
              "  0.924217700958252,\n",
              "  0.8995090126991272,\n",
              "  0.8371663093566895,\n",
              "  0.8447211980819702,\n",
              "  0.9353238344192505,\n",
              "  0.88590008020401,\n",
              "  0.8476036787033081,\n",
              "  0.8142181038856506,\n",
              "  0.9421883225440979,\n",
              "  0.8883903622627258,\n",
              "  0.9252414107322693,\n",
              "  0.8591198921203613,\n",
              "  0.9614487290382385,\n",
              "  0.9088775515556335,\n",
              "  0.8522248864173889,\n",
              "  0.857550859451294,\n",
              "  0.9049562215805054,\n",
              "  0.9159247279167175,\n",
              "  0.9468867778778076,\n",
              "  0.8975073099136353,\n",
              "  0.9202972054481506,\n",
              "  0.8650967478752136,\n",
              "  0.8673824667930603,\n",
              "  0.853804349899292,\n",
              "  0.8940277099609375,\n",
              "  0.9491724967956543,\n",
              "  0.8598549962043762,\n",
              "  0.8683164119720459,\n",
              "  0.8899574279785156,\n",
              "  0.8954306244850159,\n",
              "  0.9211939573287964,\n",
              "  0.8718353509902954,\n",
              "  0.8924174904823303,\n",
              "  0.8521279692649841,\n",
              "  0.8739250898361206,\n",
              "  0.8479405641555786,\n",
              "  0.8704805374145508,\n",
              "  0.8169201016426086,\n",
              "  0.9128020405769348,\n",
              "  0.9232128262519836,\n",
              "  0.8673338294029236,\n",
              "  0.8446391224861145,\n",
              "  0.959991991519928,\n",
              "  0.921218752861023,\n",
              "  0.9031363129615784,\n",
              "  0.8600991368293762,\n",
              "  0.8646392822265625,\n",
              "  0.9100772142410278,\n",
              "  0.9141502380371094,\n",
              "  0.934761643409729,\n",
              "  0.8391568064689636,\n",
              "  0.9259937405586243,\n",
              "  0.9319053292274475,\n",
              "  0.9110150933265686,\n",
              "  0.9303921461105347,\n",
              "  0.8782973885536194,\n",
              "  0.9272669553756714,\n",
              "  0.8778984546661377,\n",
              "  0.9306872487068176,\n",
              "  0.8763068914413452,\n",
              "  0.9254509210586548,\n",
              "  0.8403441905975342,\n",
              "  0.918936014175415,\n",
              "  0.9130670428276062,\n",
              "  0.9427900910377502,\n",
              "  0.8837088346481323,\n",
              "  0.85039222240448,\n",
              "  0.8473014235496521,\n",
              "  0.9033604860305786,\n",
              "  0.8438597321510315,\n",
              "  0.8843689560890198,\n",
              "  0.8819272518157959,\n",
              "  0.8606519103050232,\n",
              "  0.8700594305992126,\n",
              "  0.9158856272697449,\n",
              "  0.887519896030426,\n",
              "  0.8976116180419922,\n",
              "  0.8680897951126099,\n",
              "  0.8724597692489624,\n",
              "  0.8958988785743713,\n",
              "  0.8696032762527466,\n",
              "  0.9090021252632141,\n",
              "  0.9412543177604675,\n",
              "  0.8845971822738647,\n",
              "  0.816001832485199,\n",
              "  0.873415470123291,\n",
              "  0.8777734041213989,\n",
              "  0.9044167399406433,\n",
              "  0.8765945434570312,\n",
              "  0.8076361417770386,\n",
              "  0.8781653046607971,\n",
              "  0.9460480809211731,\n",
              "  0.8695753812789917,\n",
              "  0.8833816051483154,\n",
              "  0.9202650189399719,\n",
              "  0.8607661724090576,\n",
              "  0.9300442934036255,\n",
              "  0.9095267057418823,\n",
              "  0.8563348650932312,\n",
              "  0.8844229578971863,\n",
              "  0.8985841870307922,\n",
              "  0.8909403681755066,\n",
              "  0.8263959884643555,\n",
              "  0.853013813495636,\n",
              "  0.9244434237480164,\n",
              "  0.913841724395752,\n",
              "  0.8408070802688599,\n",
              "  0.8585543632507324,\n",
              "  0.8597016334533691,\n",
              "  0.8950100541114807,\n",
              "  0.8871191740036011,\n",
              "  0.8094809055328369,\n",
              "  0.8527045845985413,\n",
              "  0.9034507870674133,\n",
              "  0.8625780940055847,\n",
              "  0.8596905469894409,\n",
              "  0.8535622358322144,\n",
              "  0.8566483855247498,\n",
              "  0.8872780799865723,\n",
              "  0.8656482696533203,\n",
              "  0.8541659116744995,\n",
              "  0.8852224349975586,\n",
              "  0.8701605200767517,\n",
              "  0.8329846858978271,\n",
              "  0.872678279876709,\n",
              "  0.817130982875824,\n",
              "  0.8903744220733643,\n",
              "  0.8910597562789917,\n",
              "  0.8611915111541748,\n",
              "  0.8278106451034546,\n",
              "  0.8246924877166748,\n",
              "  0.9228681921958923,\n",
              "  0.9081368446350098,\n",
              "  0.8621967434883118,\n",
              "  0.8265215754508972,\n",
              "  0.8941731452941895,\n",
              "  0.8764243721961975,\n",
              "  0.8957833051681519,\n",
              "  0.8202984929084778,\n",
              "  0.8696368932723999,\n",
              "  0.8529336452484131,\n",
              "  0.8035235404968262,\n",
              "  0.823846697807312,\n",
              "  0.8337354063987732,\n",
              "  0.8257210850715637,\n",
              "  0.8587157726287842,\n",
              "  0.856232225894928,\n",
              "  0.8653767108917236,\n",
              "  0.871871292591095,\n",
              "  0.8392022252082825,\n",
              "  0.8614680767059326,\n",
              "  0.8590719103813171,\n",
              "  0.8708671927452087,\n",
              "  0.8491450548171997,\n",
              "  0.8461862802505493,\n",
              "  0.8762235641479492,\n",
              "  0.8382313251495361,\n",
              "  0.8378247618675232,\n",
              "  0.830452561378479,\n",
              "  0.9075295329093933,\n",
              "  0.8619897961616516,\n",
              "  0.9003972411155701,\n",
              "  0.8618854284286499,\n",
              "  0.9078807234764099,\n",
              "  0.846268355846405,\n",
              "  0.8368629217147827,\n",
              "  0.8381856083869934,\n",
              "  0.9135631322860718,\n",
              "  0.8269944190979004,\n",
              "  0.8865572214126587,\n",
              "  0.8317875266075134,\n",
              "  0.872835099697113,\n",
              "  0.8080070614814758,\n",
              "  0.8640859127044678,\n",
              "  0.8376235365867615,\n",
              "  0.8588205575942993,\n",
              "  0.8333801627159119,\n",
              "  0.8165014386177063,\n",
              "  0.8033568859100342,\n",
              "  0.8896001577377319,\n",
              "  0.832912027835846,\n",
              "  0.8211930394172668,\n",
              "  0.8493033051490784,\n",
              "  0.8581349849700928,\n",
              "  0.8404563665390015,\n",
              "  0.8150979280471802,\n",
              "  0.830772340297699,\n",
              "  0.902248203754425,\n",
              "  0.8471012711524963],\n",
              " 'acc': [0.39546191692352295,\n",
              "  0.40032413601875305,\n",
              "  0.39546191692352295,\n",
              "  0.39059966802597046,\n",
              "  0.4035656452178955,\n",
              "  0.3889789283275604,\n",
              "  0.3889789283275604,\n",
              "  0.44246354699134827,\n",
              "  0.40518638491630554,\n",
              "  0.38573744893074036,\n",
              "  0.4019449055194855,\n",
              "  0.37925446033477783,\n",
              "  0.4181523621082306,\n",
              "  0.37925446033477783,\n",
              "  0.3581847548484802,\n",
              "  0.4132901132106781,\n",
              "  0.4035656452178955,\n",
              "  0.4375,\n",
              "  0.4230145812034607,\n",
              "  0.37439221143722534,\n",
              "  0.3776337206363678,\n",
              "  0.41166937351226807,\n",
              "  0.41004863381385803,\n",
              "  0.38411667943000793,\n",
              "  0.39059966802597046,\n",
              "  0.37925446033477783,\n",
              "  0.4197731018066406,\n",
              "  0.3922204077243805,\n",
              "  0.41004863381385803,\n",
              "  0.4019449055194855,\n",
              "  0.41653159260749817,\n",
              "  0.44570502638816833,\n",
              "  0.43598055839538574,\n",
              "  0.4000000059604645,\n",
              "  0.3873581886291504,\n",
              "  0.39059966802597046,\n",
              "  0.40032413601875305,\n",
              "  0.4197731018066406,\n",
              "  0.397082656621933,\n",
              "  0.40032413601875305,\n",
              "  0.3843750059604645,\n",
              "  0.4234375059604645,\n",
              "  0.4068071246147156,\n",
              "  0.4035656452178955,\n",
              "  0.41874998807907104,\n",
              "  0.3922204077243805,\n",
              "  0.4156250059604645,\n",
              "  0.43598055839538574,\n",
              "  0.3727714717388153,\n",
              "  0.4376012980937958,\n",
              "  0.4278768301010132,\n",
              "  0.4203124940395355,\n",
              "  0.3938411772251129,\n",
              "  0.41166937351226807,\n",
              "  0.41166937351226807,\n",
              "  0.4343598186969757,\n",
              "  0.40518638491630554,\n",
              "  0.44084277749061584,\n",
              "  0.3938411772251129,\n",
              "  0.44246354699134827,\n",
              "  0.4278768301010132,\n",
              "  0.4197731018066406,\n",
              "  0.43111830949783325,\n",
              "  0.40518638491630554,\n",
              "  0.4019449055194855,\n",
              "  0.4019449055194855,\n",
              "  0.4230145812034607,\n",
              "  0.3711507320404053,\n",
              "  0.4019449055194855,\n",
              "  0.4068071246147156,\n",
              "  0.4181523621082306,\n",
              "  0.4181523621082306,\n",
              "  0.4132901132106781,\n",
              "  0.41166937351226807,\n",
              "  0.41653159260749817,\n",
              "  0.44570502638816833,\n",
              "  0.43111830949783325,\n",
              "  0.44732576608657837,\n",
              "  0.397082656621933,\n",
              "  0.4181523621082306,\n",
              "  0.4132901132106781,\n",
              "  0.4281249940395355,\n",
              "  0.40156251192092896,\n",
              "  0.42625609040260315,\n",
              "  0.4132901132106781,\n",
              "  0.4440842866897583,\n",
              "  0.4440842866897583,\n",
              "  0.4489465057849884,\n",
              "  0.4505672752857208,\n",
              "  0.4327390491962433,\n",
              "  0.4440842866897583,\n",
              "  0.41004863381385803,\n",
              "  0.4230145812034607,\n",
              "  0.43598055839538574,\n",
              "  0.4343598186969757,\n",
              "  0.458670973777771,\n",
              "  0.4181523621082306,\n",
              "  0.42139384150505066,\n",
              "  0.4635332226753235,\n",
              "  0.48136141896247864,\n",
              "  0.4505672752857208,\n",
              "  0.4132901132106781,\n",
              "  0.4375,\n",
              "  0.4538087546825409,\n",
              "  0.47811993956565857,\n",
              "  0.4343598186969757,\n",
              "  0.42139384150505066,\n",
              "  0.46677470207214355,\n",
              "  0.468395471572876,\n",
              "  0.42139384150505066,\n",
              "  0.46191248297691345,\n",
              "  0.4635332226753235,\n",
              "  0.45705023407936096,\n",
              "  0.4197731018066406,\n",
              "  0.46677470207214355,\n",
              "  0.44732576608657837,\n",
              "  0.4505672752857208,\n",
              "  0.45705023407936096,\n",
              "  0.4392220377922058,\n",
              "  0.4392220377922058,\n",
              "  0.4327390491962433,\n",
              "  0.44570502638816833,\n",
              "  0.44246354699134827,\n",
              "  0.4278768301010132,\n",
              "  0.45705023407936096,\n",
              "  0.46191248297691345,\n",
              "  0.4538087546825409,\n",
              "  0.42625609040260315,\n",
              "  0.4538087546825409,\n",
              "  0.4765625,\n",
              "  0.4797406792640686,\n",
              "  0.48298215866088867,\n",
              "  0.43598055839538574,\n",
              "  0.44570502638816833,\n",
              "  0.46191248297691345,\n",
              "  0.470016211271286,\n",
              "  0.49270665645599365,\n",
              "  0.44218748807907104,\n",
              "  0.48136141896247864,\n",
              "  0.45705023407936096,\n",
              "  0.4593749940395355,\n",
              "  0.470016211271286,\n",
              "  0.46191248297691345,\n",
              "  0.4392220377922058,\n",
              "  0.4440842866897583,\n",
              "  0.458670973777771,\n",
              "  0.4505672752857208,\n",
              "  0.4440842866897583,\n",
              "  0.47649919986724854,\n",
              "  0.4554294943809509,\n",
              "  0.5008103847503662,\n",
              "  0.47649919986724854,\n",
              "  0.4440842866897583,\n",
              "  0.44570502638816833,\n",
              "  0.44084277749061584,\n",
              "  0.4440842866897583,\n",
              "  0.4748784303665161,\n",
              "  0.48784440755844116,\n",
              "  0.46191248297691345,\n",
              "  0.49756887555122375,\n",
              "  0.4505672752857208,\n",
              "  0.47649919986724854,\n",
              "  0.49756887555122375,\n",
              "  0.46875,\n",
              "  0.4732576906681061,\n",
              "  0.4748784303665161,\n",
              "  0.44246354699134827,\n",
              "  0.470016211271286,\n",
              "  0.45218801498413086,\n",
              "  0.4327390491962433,\n",
              "  0.4748784303665161,\n",
              "  0.4671874940395355,\n",
              "  0.47163695096969604,\n",
              "  0.4505672752857208,\n",
              "  0.4748784303665161,\n",
              "  0.4749999940395355,\n",
              "  0.4554294943809509,\n",
              "  0.4991896152496338,\n",
              "  0.470016211271286,\n",
              "  0.4635332226753235,\n",
              "  0.4635332226753235,\n",
              "  0.5105348229408264,\n",
              "  0.4797406792640686,\n",
              "  0.4748784303665161,\n",
              "  0.5089141130447388,\n",
              "  0.468395471572876,\n",
              "  0.48784440755844116,\n",
              "  0.48622366786003113,\n",
              "  0.46562498807907104,\n",
              "  0.470016211271286,\n",
              "  0.4602917432785034,\n",
              "  0.48784440755844116,\n",
              "  0.48750001192092896,\n",
              "  0.4943273961544037,\n",
              "  0.4943273961544037,\n",
              "  0.4748784303665161,\n",
              "  0.4797406792640686,\n",
              "  0.4732576906681061,\n",
              "  0.4859375059604645,\n",
              "  0.47811993956565857,\n",
              "  0.470016211271286,\n",
              "  0.4943273961544037,\n",
              "  0.47649919986724854,\n",
              "  0.5062500238418579,\n",
              "  0.5105348229408264,\n",
              "  0.4781250059604645,\n",
              "  0.4781250059604645,\n",
              "  0.45705023407936096,\n",
              "  0.48622366786003113,\n",
              "  0.49270665645599365,\n",
              "  0.5056726336479187,\n",
              "  0.47811993956565857,\n",
              "  0.47163695096969604,\n",
              "  0.518638551235199,\n",
              "  0.46677470207214355,\n",
              "  0.47968751192092896,\n",
              "  0.48136141896247864,\n",
              "  0.5024310946464539,\n",
              "  0.4609375,\n",
              "  0.5008103847503662,\n",
              "  0.4959481358528137,\n",
              "  0.4748784303665161,\n",
              "  0.48136141896247864,\n",
              "  0.504687488079071,\n",
              "  0.470016211271286,\n",
              "  0.5024310946464539,\n",
              "  0.4651539623737335,\n",
              "  0.49108588695526123,\n",
              "  0.4991896152496338,\n",
              "  0.512499988079071,\n",
              "  0.468395471572876,\n",
              "  0.5251215696334839,\n",
              "  0.470016211271286,\n",
              "  0.49270665645599365,\n",
              "  0.47811993956565857,\n",
              "  0.49270665645599365,\n",
              "  0.4894651472568512,\n",
              "  0.44084277749061584,\n",
              "  0.48298215866088867,\n",
              "  0.5040518641471863,\n",
              "  0.5008103847503662,\n",
              "  0.4635332226753235,\n",
              "  0.49270665645599365,\n",
              "  0.518638551235199,\n",
              "  0.5121555924415588,\n",
              "  0.47811993956565857,\n",
              "  0.48750001192092896,\n",
              "  0.5332252979278564,\n",
              "  0.5024310946464539,\n",
              "  0.5267422795295715,\n",
              "  0.49270665645599365,\n",
              "  0.5153970718383789,\n",
              "  0.49756887555122375,\n",
              "  0.5056726336479187,\n",
              "  0.5332252979278564,\n",
              "  0.48136141896247864,\n",
              "  0.48622366786003113,\n",
              "  0.504687488079071,\n",
              "  0.47163695096969604,\n",
              "  0.48784440755844116,\n",
              "  0.53125,\n",
              "  0.4894651472568512,\n",
              "  0.4991896152496338,\n",
              "  0.48622366786003113,\n",
              "  0.5348460078239441,\n",
              "  0.48622366786003113,\n",
              "  0.49756887555122375,\n",
              "  0.518638551235199,\n",
              "  0.5348460078239441,\n",
              "  0.47649919986724854,\n",
              "  0.5121555924415588,\n",
              "  0.518638551235199,\n",
              "  0.541329026222229,\n",
              "  0.5235008001327515,\n",
              "  0.49108588695526123,\n",
              "  0.5299838185310364,\n",
              "  0.5267422795295715,\n",
              "  0.5218800902366638,\n",
              "  0.49756887555122375,\n",
              "  0.5202593207359314,\n",
              "  0.503125011920929,\n",
              "  0.48784440755844116,\n",
              "  0.5380875468254089,\n",
              "  0.4943273961544037,\n",
              "  0.5008103847503662,\n",
              "  0.5235008001327515,\n",
              "  0.47811993956565857,\n",
              "  0.48784440755844116,\n",
              "  0.46191248297691345,\n",
              "  0.5062500238418579,\n",
              "  0.5062500238418579,\n",
              "  0.5526742339134216,\n",
              "  0.5137763619422913,\n",
              "  0.5478119850158691,\n",
              "  0.4959481358528137,\n",
              "  0.5105348229408264,\n",
              "  0.48784440755844116,\n",
              "  0.528363049030304,\n",
              "  0.5008103847503662,\n",
              "  0.5008103847503662,\n",
              "  0.48906248807907104,\n",
              "  0.5328124761581421,\n",
              "  0.47811993956565857,\n",
              "  0.5137763619422913,\n",
              "  0.5348460078239441,\n",
              "  0.528363049030304,\n",
              "  0.5218800902366638,\n",
              "  0.5251215696334839,\n",
              "  0.5015624761581421,\n",
              "  0.5510534644126892,\n",
              "  0.5121555924415588,\n",
              "  0.5137763619422913,\n",
              "  0.49108588695526123,\n",
              "  0.5332252979278564,\n",
              "  0.48622366786003113,\n",
              "  0.5153970718383789,\n",
              "  0.4991896152496338,\n",
              "  0.5235008001327515,\n",
              "  0.4846029281616211,\n",
              "  0.49687498807907104,\n",
              "  0.5153970718383789,\n",
              "  0.48622366786003113,\n",
              "  0.5249999761581421,\n",
              "  0.5008103847503662,\n",
              "  0.5153970718383789,\n",
              "  0.5348460078239441,\n",
              "  0.5137763619422913,\n",
              "  0.5429497361183167,\n",
              "  0.5171874761581421,\n",
              "  0.5364667773246765,\n",
              "  0.5397082567214966,\n",
              "  0.4991896152496338,\n",
              "  0.5089141130447388,\n",
              "  0.48784440755844116,\n",
              "  0.5461912751197815,\n",
              "  0.5461912751197815,\n",
              "  0.5332252979278564,\n",
              "  0.5607779622077942,\n",
              "  0.5559157133102417,\n",
              "  0.541329026222229,\n",
              "  0.5008103847503662,\n",
              "  0.5218800902366638,\n",
              "  0.5040518641471863,\n",
              "  0.5251215696334839,\n",
              "  0.518638551235199,\n",
              "  0.5397082567214966,\n",
              "  0.5429497361183167,\n",
              "  0.550000011920929,\n",
              "  0.5461912751197815,\n",
              "  0.5202593207359314,\n",
              "  0.49270665645599365,\n",
              "  0.528363049030304,\n",
              "  0.5137763619422913,\n",
              "  0.5170178413391113,\n",
              "  0.46677470207214355,\n",
              "  0.5056726336479187,\n",
              "  0.5218800902366638,\n",
              "  0.518638551235199,\n",
              "  0.5137763619422913,\n",
              "  0.4991896152496338,\n",
              "  0.531604528427124,\n",
              "  0.5202593207359314,\n",
              "  0.5546875,\n",
              "  0.5008103847503662,\n",
              "  0.554295003414154,\n",
              "  0.5397082567214966,\n",
              "  0.5380875468254089,\n",
              "  0.531604528427124,\n",
              "  0.528124988079071,\n",
              "  0.5397082567214966,\n",
              "  0.518638551235199,\n",
              "  0.5218800902366638,\n",
              "  0.518638551235199,\n",
              "  0.5575364828109741,\n",
              "  0.531604528427124,\n",
              "  0.5235008001327515,\n",
              "  0.5478119850158691,\n",
              "  0.5526742339134216,\n",
              "  0.5494327545166016,\n",
              "  0.5380875468254089,\n",
              "  0.5623987317085266,\n",
              "  0.5461912751197815,\n",
              "  0.5406249761581421,\n",
              "  0.5170178413391113,\n",
              "  0.541329026222229,\n",
              "  0.5072933435440063,\n",
              "  0.5380875468254089,\n",
              "  0.5429497361183167,\n",
              "  0.5575364828109741,\n",
              "  0.531604528427124,\n",
              "  0.5461912751197815,\n",
              "  0.5510534644126892,\n",
              "  0.5397082567214966,\n",
              "  0.5526742339134216,\n",
              "  0.5607779622077942,\n",
              "  0.554295003414154,\n",
              "  0.5348460078239441,\n",
              "  0.4991896152496338,\n",
              "  0.5348460078239441,\n",
              "  0.5299838185310364,\n",
              "  0.5445705056190491,\n",
              "  0.5721231698989868,\n",
              "  0.534375011920929,\n",
              "  0.5072933435440063,\n",
              "  0.5332252979278564,\n",
              "  0.5348460078239441,\n",
              "  0.5348460078239441,\n",
              "  0.5640194416046143,\n",
              "  0.48622366786003113,\n",
              "  0.5494327545166016,\n",
              "  0.5364667773246765,\n",
              "  0.526562511920929,\n",
              "  0.5397082567214966,\n",
              "  0.528363049030304,\n",
              "  0.503125011920929,\n",
              "  0.5429497361183167,\n",
              "  0.5445705056190491,\n",
              "  0.5526742339134216,\n",
              "  0.5375000238418579,\n",
              "  0.5251215696334839,\n",
              "  0.5575364828109741,\n",
              "  0.534375011920929,\n",
              "  0.5461912751197815,\n",
              "  0.531604528427124,\n",
              "  0.5429497361183167,\n",
              "  0.531604528427124,\n",
              "  0.5769854187965393,\n",
              "  0.5899513959884644,\n",
              "  0.5332252979278564,\n",
              "  0.5656402111053467,\n",
              "  0.526562511920929,\n",
              "  0.518638551235199,\n",
              "  0.5461912751197815,\n",
              "  0.554295003414154,\n",
              "  0.5364667773246765,\n",
              "  0.5607779622077942,\n",
              "  0.5672609210014343,\n",
              "  0.5380875468254089,\n",
              "  0.5769854187965393,\n",
              "  0.5348460078239441,\n",
              "  0.5445705056190491,\n",
              "  0.5494327545166016,\n",
              "  0.531604528427124,\n",
              "  0.5607779622077942,\n",
              "  0.5786061882972717,\n",
              "  0.5591571927070618,\n",
              "  0.5348460078239441,\n",
              "  0.541329026222229,\n",
              "  0.5121555924415588,\n",
              "  0.5510534644126892,\n",
              "  0.5607779622077942,\n",
              "  0.5850891470909119,\n",
              "  0.5575364828109741,\n",
              "  0.541329026222229,\n",
              "  0.5478119850158691,\n",
              "  0.554295003414154,\n",
              "  0.5478119850158691,\n",
              "  0.5429497361183167,\n",
              "  0.554295003414154,\n",
              "  0.5705024600028992,\n",
              "  0.5640194416046143,\n",
              "  0.5510534644126892,\n",
              "  0.5397082567214966,\n",
              "  0.5494327545166016,\n",
              "  0.5348460078239441,\n",
              "  0.5786061882972717,\n",
              "  0.5526742339134216,\n",
              "  0.5429497361183167,\n",
              "  0.5737439393997192,\n",
              "  0.5623987317085266,\n",
              "  0.5591571927070618,\n",
              "  0.5703125,\n",
              "  0.5380875468254089,\n",
              "  0.5429497361183167,\n",
              "  0.541329026222229,\n",
              "  0.5705024600028992,\n",
              "  0.5672609210014343,\n",
              "  0.5559157133102417,\n",
              "  0.5429497361183167,\n",
              "  0.5623987317085266,\n",
              "  0.5688816905021667,\n",
              "  0.5510534644126892,\n",
              "  0.554295003414154,\n",
              "  0.5818476676940918,\n",
              "  0.559374988079071,\n",
              "  0.5703125,\n",
              "  0.4984374940395355,\n",
              "  0.5591571927070618,\n",
              "  0.5591571927070618,\n",
              "  0.5607779622077942,\n",
              "  0.554295003414154,\n",
              "  0.5705024600028992,\n",
              "  0.5559157133102417,\n",
              "  0.5445705056190491,\n",
              "  0.5494327545166016,\n",
              "  0.591572105884552,\n",
              "  0.5510534644126892,\n",
              "  0.5526742339134216,\n",
              "  0.5591571927070618,\n",
              "  0.5688816905021667,\n",
              "  0.5559157133102417,\n",
              "  0.5478119850158691,\n",
              "  0.5703125,\n",
              "  0.5607779622077942,\n",
              "  0.5429497361183167,\n",
              "  0.5591571927070618,\n",
              "  0.5672609210014343,\n",
              "  0.5867098569869995,\n",
              "  0.5688816905021667,\n",
              "  0.5705024600028992,\n",
              "  0.5478119850158691,\n",
              "  0.5796874761581421,\n",
              "  0.578125,\n",
              "  0.5786061882972717,\n",
              "  0.5461912751197815,\n",
              "  0.554295003414154,\n",
              "  0.567187488079071,\n",
              "  0.578125,\n",
              "  0.5445705056190491,\n",
              "  0.5267422795295715,\n",
              "  0.541329026222229,\n",
              "  0.5559157133102417,\n",
              "  0.5672609210014343,\n",
              "  0.5737439393997192,\n",
              "  0.5867098569869995,\n",
              "  0.5380875468254089,\n",
              "  0.5575364828109741,\n",
              "  0.5397082567214966,\n",
              "  0.528363049030304,\n",
              "  0.5559157133102417,\n",
              "  0.5478119850158691,\n",
              "  0.5688816905021667,\n",
              "  0.5478119850158691,\n",
              "  0.528124988079071,\n",
              "  0.5769854187965393,\n",
              "  0.5559157133102417,\n",
              "  0.5786061882972717,\n",
              "  0.5769854187965393,\n",
              "  0.5429497361183167,\n",
              "  0.554295003414154,\n",
              "  0.5769854187965393,\n",
              "  0.5705024600028992,\n",
              "  0.554295003414154,\n",
              "  0.5705024600028992,\n",
              "  0.5769854187965393,\n",
              "  0.5380875468254089,\n",
              "  0.5478119850158691,\n",
              "  0.5591571927070618,\n",
              "  0.543749988079071,\n",
              "  0.5656402111053467,\n",
              "  0.5964343547821045,\n",
              "  0.5526742339134216,\n",
              "  0.5478119850158691,\n",
              "  0.5718749761581421,\n",
              "  0.5623987317085266,\n",
              "  0.554295003414154,\n",
              "  0.554295003414154,\n",
              "  0.5478119850158691,\n",
              "  0.5607779622077942,\n",
              "  0.5623987317085266,\n",
              "  0.5786061882972717,\n",
              "  0.5718749761581421,\n",
              "  0.5753646492958069,\n",
              "  0.554295003414154,\n",
              "  0.5737439393997192,\n",
              "  0.5656402111053467,\n",
              "  0.5948135852813721,\n",
              "  0.5980551242828369,\n",
              "  0.5640194416046143,\n",
              "  0.5170178413391113,\n",
              "  0.5526742339134216,\n",
              "  0.5721231698989868,\n",
              "  0.5510534644126892,\n",
              "  0.5703125,\n",
              "  0.5769854187965393,\n",
              "  0.5964343547821045,\n",
              "  0.5559157133102417,\n",
              "  0.6015625,\n",
              "  0.5859375,\n",
              "  0.5721231698989868,\n",
              "  0.5737439393997192,\n",
              "  0.5899513959884644,\n",
              "  0.5802268981933594,\n",
              "  0.5494327545166016,\n",
              "  0.5688816905021667,\n",
              "  0.5964343547821045,\n",
              "  0.5559157133102417,\n",
              "  0.601296603679657,\n",
              "  0.5332252979278564,\n",
              "  0.5769854187965393,\n",
              "  0.5705024600028992,\n",
              "  0.6304700374603271,\n",
              "  0.5867098569869995,\n",
              "  0.5834683775901794,\n",
              "  0.5688816905021667,\n",
              "  0.5559157133102417,\n",
              "  0.604538083076477,\n",
              "  0.5705024600028992,\n",
              "  0.5656402111053467,\n",
              "  0.5672609210014343,\n",
              "  0.5705024600028992,\n",
              "  0.5948135852813721,\n",
              "  0.5640194416046143,\n",
              "  0.5737439393997192,\n",
              "  0.5753646492958069,\n",
              "  0.5769854187965393,\n",
              "  0.5640194416046143,\n",
              "  0.5867098569869995,\n",
              "  0.5753646492958069,\n",
              "  0.5202593207359314,\n",
              "  0.5510534644126892,\n",
              "  0.581250011920929,\n",
              "  0.5721231698989868,\n",
              "  0.5591571927070618,\n",
              "  0.5867098569869995,\n",
              "  0.5546875,\n",
              "  0.559374988079071,\n",
              "  0.5964343547821045,\n",
              "  0.554295003414154,\n",
              "  0.5818476676940918,\n",
              "  0.5672609210014343,\n",
              "  0.5818476676940918,\n",
              "  0.5575364828109741,\n",
              "  0.5834683775901794,\n",
              "  0.604538083076477,\n",
              "  0.5850891470909119,\n",
              "  0.5867098569869995,\n",
              "  0.5769854187965393,\n",
              "  0.5843750238418579,\n",
              "  0.604538083076477,\n",
              "  0.5818476676940918,\n",
              "  0.5867098569869995,\n",
              "  0.5867098569869995,\n",
              "  0.5834683775901794,\n",
              "  0.5623987317085266,\n",
              "  0.6126418113708496,\n",
              "  0.5753646492958069,\n",
              "  0.5850891470909119,\n",
              "  0.5623987317085266,\n",
              "  0.5867098569869995,\n",
              "  0.5640194416046143,\n",
              "  0.5623987317085266,\n",
              "  0.5769854187965393,\n",
              "  0.6175040602684021,\n",
              "  0.574999988079071,\n",
              "  0.5802268981933594,\n",
              "  0.5786061882972717,\n",
              "  0.6029173135757446,\n",
              "  0.5899513959884644,\n",
              "  0.5818476676940918,\n",
              "  0.5656402111053467,\n",
              "  0.5623987317085266,\n",
              "  0.5996758341789246,\n",
              "  0.5899513959884644,\n",
              "  0.5753646492958069,\n",
              "  0.5796874761581421,\n",
              "  0.5867098569869995,\n",
              "  0.5996758341789246,\n",
              "  0.5899513959884644,\n",
              "  0.5964343547821045,\n",
              "  0.5980551242828369,\n",
              "  0.543749988079071,\n",
              "  0.5899513959884644,\n",
              "  0.5867098569869995,\n",
              "  0.5964343547821045,\n",
              "  0.554295003414154,\n",
              "  0.591572105884552,\n",
              "  0.6304700374603271,\n",
              "  0.5559157133102417,\n",
              "  0.5802268981933594,\n",
              "  0.5721231698989868,\n",
              "  0.5531250238418579,\n",
              "  0.5688816905021667,\n",
              "  0.554295003414154,\n",
              "  0.5883306264877319,\n",
              "  0.5623987317085266,\n",
              "  0.5364667773246765,\n",
              "  0.5883306264877319,\n",
              "  0.601296603679657,\n",
              "  0.6272284984588623,\n",
              "  0.614262580871582,\n",
              "  0.5883306264877319,\n",
              "  0.5883306264877319,\n",
              "  0.5883306264877319,\n",
              "  0.5843750238418579,\n",
              "  0.5996758341789246,\n",
              "  0.6110210418701172,\n",
              "  0.5786061882972717,\n",
              "  0.5931928753852844,\n",
              "  0.5656402111053467,\n",
              "  0.604538083076477,\n",
              "  0.5834683775901794,\n",
              "  0.5964343547821045,\n",
              "  0.5753646492958069,\n",
              "  0.5705024600028992,\n",
              "  0.5899513959884644,\n",
              "  0.5786061882972717,\n",
              "  0.5964343547821045,\n",
              "  0.5802268981933594,\n",
              "  0.5786061882972717,\n",
              "  0.5640194416046143,\n",
              "  0.5802268981933594,\n",
              "  0.5753646492958069,\n",
              "  0.5705024600028992,\n",
              "  0.5834683775901794,\n",
              "  0.601296603679657,\n",
              "  0.5656402111053467,\n",
              "  0.5623987317085266,\n",
              "  0.5850891470909119,\n",
              "  0.5575364828109741,\n",
              "  0.5883306264877319,\n",
              "  0.5867098569869995,\n",
              "  0.5705024600028992,\n",
              "  0.5883306264877319,\n",
              "  0.601296603679657,\n",
              "  0.604538083076477,\n",
              "  0.5931928753852844,\n",
              "  0.5705024600028992,\n",
              "  0.5850891470909119,\n",
              "  0.5575364828109741,\n",
              "  0.5737439393997192,\n",
              "  0.6207455396652222,\n",
              "  0.5996758341789246,\n",
              "  0.6110210418701172,\n",
              "  0.5786061882972717,\n",
              "  0.6061588525772095,\n",
              "  0.6061588525772095,\n",
              "  0.5526742339134216,\n",
              "  0.5753646492958069,\n",
              "  0.5834683775901794,\n",
              "  0.5931928753852844,\n",
              "  0.6239870190620422,\n",
              "  0.6029173135757446,\n",
              "  0.5818476676940918,\n",
              "  0.591572105884552,\n",
              "  0.6110210418701172,\n",
              "  0.5980551242828369,\n",
              "  0.6029173135757446,\n",
              "  0.5948135852813721,\n",
              "  0.5753646492958069,\n",
              "  0.6078125238418579,\n",
              "  0.6175040602684021,\n",
              "  0.6094003319740295,\n",
              "  0.601296603679657,\n",
              "  0.5964343547821045,\n",
              "  0.5964343547821045,\n",
              "  0.598437488079071,\n",
              "  0.5867098569869995,\n",
              "  0.59375,\n",
              "  0.6110210418701172,\n",
              "  0.6191247701644897,\n",
              "  0.5672609210014343,\n",
              "  0.6175040602684021,\n",
              "  0.5931928753852844,\n",
              "  0.5607779622077942,\n",
              "  0.5769854187965393,\n",
              "  0.5705024600028992,\n",
              "  0.5834683775901794,\n",
              "  0.6288492679595947,\n",
              "  0.5883306264877319,\n",
              "  0.5786061882972717,\n",
              "  0.628125011920929,\n",
              "  0.5834683775901794,\n",
              "  0.5964343547821045,\n",
              "  0.5859375,\n",
              "  0.5640194416046143,\n",
              "  0.5672609210014343,\n",
              "  0.6061588525772095,\n",
              "  0.5980551242828369,\n",
              "  0.5883306264877319,\n",
              "  0.5688816905021667,\n",
              "  0.601296603679657,\n",
              "  0.6337115168571472,\n",
              "  0.604538083076477,\n",
              "  0.5883306264877319,\n",
              "  0.614262580871582,\n",
              "  0.5769854187965393,\n",
              "  0.621874988079071,\n",
              "  0.5850891470909119,\n",
              "  0.6239870190620422,\n",
              "  0.5607779622077942,\n",
              "  0.604538083076477,\n",
              "  0.5834683775901794,\n",
              "  0.5786061882972717,\n",
              "  0.614262580871582,\n",
              "  0.5899513959884644,\n",
              "  0.604538083076477,\n",
              "  0.601296603679657,\n",
              "  0.625,\n",
              "  0.5786061882972717,\n",
              "  0.6029173135757446,\n",
              "  0.6029173135757446,\n",
              "  0.5703125,\n",
              "  0.5899513959884644,\n",
              "  0.6256077885627747,\n",
              "  0.601296603679657,\n",
              "  0.5883306264877319,\n",
              "  0.6256077885627747,\n",
              "  0.5883306264877319,\n",
              "  0.5948135852813721,\n",
              "  0.5818476676940918,\n",
              "  0.6126418113708496,\n",
              "  0.5980551242828369,\n",
              "  0.6223663091659546,\n",
              "  0.5753646492958069,\n",
              "  0.6110210418701172,\n",
              "  0.601296603679657,\n",
              "  0.5867098569869995,\n",
              "  0.6094003319740295,\n",
              "  0.5948135852813721,\n",
              "  0.6158832907676697,\n",
              "  0.5850891470909119,\n",
              "  0.6061588525772095,\n",
              "  0.6288492679595947,\n",
              "  0.6401944756507874,\n",
              "  0.6126418113708496,\n",
              "  0.614262580871582,\n",
              "  0.6234375238418579,\n",
              "  0.6272284984588623,\n",
              "  0.5769854187965393,\n",
              "  0.614262580871582,\n",
              "  0.5705024600028992,\n",
              "  0.6207455396652222,\n",
              "  0.5931928753852844,\n",
              "  0.601296603679657,\n",
              "  0.6223663091659546,\n",
              "  0.6126418113708496,\n",
              "  0.591572105884552,\n",
              "  0.6158832907676697,\n",
              "  0.614262580871582,\n",
              "  0.5964343547821045,\n",
              "  0.5883306264877319,\n",
              "  0.635937511920929,\n",
              "  0.6256077885627747,\n",
              "  0.6175040602684021,\n",
              "  0.6110210418701172,\n",
              "  0.5921875238418579,\n",
              "  0.6110210418701172,\n",
              "  0.5818476676940918,\n",
              "  0.606249988079071,\n",
              "  0.5867098569869995,\n",
              "  0.596875011920929,\n",
              "  0.5818476676940918,\n",
              "  0.6077795624732971,\n",
              "  0.6369529962539673,\n",
              "  0.6223663091659546,\n",
              "  0.6256077885627747,\n",
              "  0.6207455396652222,\n",
              "  0.6564019322395325,\n",
              "  0.6077795624732971,\n",
              "  0.5883306264877319,\n",
              "  0.6175040602684021,\n",
              "  0.601296603679657,\n",
              "  0.5818476676940918,\n",
              "  0.5948135852813721,\n",
              "  0.6191247701644897,\n",
              "  0.614262580871582,\n",
              "  0.6401944756507874,\n",
              "  0.601296603679657,\n",
              "  0.5964343547821045,\n",
              "  0.5818476676940918,\n",
              "  0.6304700374603271,\n",
              "  0.5850891470909119,\n",
              "  0.591572105884552,\n",
              "  0.6304700374603271,\n",
              "  0.6158832907676697,\n",
              "  0.614262580871582,\n",
              "  0.590624988079071,\n",
              "  0.6077795624732971,\n",
              "  0.5765625238418579,\n",
              "  0.6239870190620422,\n",
              "  0.5931928753852844,\n",
              "  0.6547812223434448,\n",
              "  0.5802268981933594,\n",
              "  0.6256077885627747,\n",
              "  0.5834683775901794,\n",
              "  0.601296603679657,\n",
              "  0.6239870190620422,\n",
              "  0.6207455396652222,\n",
              "  0.6061588525772095,\n",
              "  0.6466774940490723,\n",
              "  0.5753646492958069,\n",
              "  0.6207455396652222,\n",
              "  0.6158832907676697,\n",
              "  0.6061588525772095,\n",
              "  0.5769854187965393,\n",
              "  0.6239870190620422,\n",
              "  0.6000000238418579,\n",
              "  0.6207455396652222,\n",
              "  0.6077795624732971,\n",
              "  0.6175040602684021,\n",
              "  0.6158832907676697,\n",
              "  0.6126418113708496,\n",
              "  0.5899513959884644,\n",
              "  0.6077795624732971,\n",
              "  0.6288492679595947,\n",
              "  0.6078125238418579,\n",
              "  0.6126418113708496,\n",
              "  0.5834683775901794,\n",
              "  0.6304700374603271,\n",
              "  0.6547812223434448,\n",
              "  0.6175040602684021,\n",
              "  0.6094003319740295,\n",
              "  0.6126418113708496,\n",
              "  0.5883306264877319,\n",
              "  0.609375,\n",
              "  0.6223663091659546,\n",
              "  0.5883306264877319,\n",
              "  0.5883306264877319,\n",
              "  0.6109374761581421,\n",
              "  0.604538083076477,\n",
              "  0.591572105884552,\n",
              "  0.6256077885627747,\n",
              "  0.65153968334198,\n",
              "  0.6187499761581421,\n",
              "  0.5980551242828369,\n",
              "  0.604538083076477,\n",
              "  0.6337115168571472,\n",
              "  0.6191247701644897,\n",
              "  0.6158832907676697,\n",
              "  0.5948135852813721,\n",
              "  0.6272284984588623,\n",
              "  0.6304700374603271,\n",
              "  0.6320907473564148,\n",
              "  0.5867098569869995,\n",
              "  0.6094003319740295,\n",
              "  0.6234375238418579,\n",
              "  0.614262580871582,\n",
              "  0.6304700374603271,\n",
              "  0.601296603679657,\n",
              "  0.6256077885627747,\n",
              "  0.620312511920929,\n",
              "  0.6272284984588623,\n",
              "  0.6239870190620422,\n",
              "  0.6418152451515198,\n",
              "  0.6337115168571472,\n",
              "  0.6320907473564148,\n",
              "  0.6304700374603271,\n",
              "  0.6272284984588623,\n",
              "  0.614262580871582,\n",
              "  0.6434359550476074,\n",
              "  0.6466774940490723,\n",
              "  0.6126418113708496,\n",
              "  0.6337115168571472,\n",
              "  0.6029173135757446,\n",
              "  0.6272284984588623,\n",
              "  0.5964343547821045,\n",
              "  0.6061588525772095,\n",
              "  0.6061588525772095,\n",
              "  0.6482982039451599,\n",
              "  0.6434359550476074,\n",
              "  0.6094003319740295,\n",
              "  0.6578124761581421,\n",
              "  0.6434359550476074,\n",
              "  0.6175040602684021,\n",
              "  0.6320907473564148,\n",
              "  0.6158832907676697,\n",
              "  0.6304700374603271,\n",
              "  0.614262580871582,\n",
              "  0.6029173135757446,\n",
              "  0.6175040602684021,\n",
              "  0.6272284984588623,\n",
              "  0.6304700374603271,\n",
              "  0.6239870190620422,\n",
              "  0.6304700374603271,\n",
              "  0.6304700374603271,\n",
              "  0.6110210418701172,\n",
              "  0.6450567245483398,\n",
              "  0.6499189734458923,\n",
              "  0.6175040602684021,\n",
              "  0.598437488079071,\n",
              "  0.6158832907676697,\n",
              "  0.6158832907676697,\n",
              "  0.614262580871582,\n",
              "  0.601296603679657,\n",
              "  0.6239870190620422,\n",
              "  0.6385737657546997,\n",
              "  0.6466774940490723,\n",
              "  0.6061588525772095,\n",
              "  0.6450567245483398,\n",
              "  0.614262580871582,\n",
              "  0.6401944756507874,\n",
              "  0.6029173135757446,\n",
              "  0.6450567245483398,\n",
              "  0.6239870190620422,\n",
              "  0.6094003319740295,\n",
              "  0.6223663091659546,\n",
              "  0.6385737657546997,\n",
              "  0.6353322267532349,\n",
              "  0.6401944756507874,\n",
              "  0.601296603679657,\n",
              "  0.6207455396652222,\n",
              "  0.6482982039451599,\n",
              "  0.6256077885627747,\n",
              "  0.6175040602684021,\n",
              "  0.6256077885627747,\n",
              "  0.6434359550476074,\n",
              "  0.6434359550476074,\n",
              "  0.590624988079071,\n",
              "  0.6304700374603271],\n",
              " 'val_loss': [0.7033278942108154,\n",
              "  0.8686923980712891,\n",
              "  0.8427731990814209,\n",
              "  0.8621249198913574,\n",
              "  1.0940678119659424,\n",
              "  1.0294392108917236,\n",
              "  1.130836009979248,\n",
              "  1.0871847867965698,\n",
              "  1.2034022808074951,\n",
              "  1.2924686670303345,\n",
              "  1.3517757654190063,\n",
              "  1.2351844310760498,\n",
              "  1.3484773635864258,\n",
              "  1.3569766283035278,\n",
              "  1.201864242553711,\n",
              "  1.3045682907104492,\n",
              "  1.3731271028518677,\n",
              "  1.2455942630767822,\n",
              "  1.3671772480010986,\n",
              "  1.175402283668518,\n",
              "  1.4093823432922363,\n",
              "  1.2557533979415894,\n",
              "  1.362583041191101,\n",
              "  1.1615612506866455,\n",
              "  1.3086040019989014,\n",
              "  1.3667844533920288,\n",
              "  1.210761308670044,\n",
              "  1.3184771537780762,\n",
              "  1.296487808227539,\n",
              "  1.3832836151123047,\n",
              "  1.3585870265960693,\n",
              "  1.3120253086090088,\n",
              "  1.3366997241973877,\n",
              "  1.3144123554229736,\n",
              "  1.3312032222747803,\n",
              "  1.3121123313903809,\n",
              "  1.4039826393127441,\n",
              "  1.3632011413574219,\n",
              "  1.2164055109024048,\n",
              "  1.3528650999069214,\n",
              "  1.3453266620635986,\n",
              "  1.3061511516571045,\n",
              "  1.3501876592636108,\n",
              "  1.2898461818695068,\n",
              "  1.3124048709869385,\n",
              "  1.3484419584274292,\n",
              "  1.3291987180709839,\n",
              "  1.2159423828125,\n",
              "  1.1600234508514404,\n",
              "  1.4791839122772217,\n",
              "  1.2476452589035034,\n",
              "  1.3419842720031738,\n",
              "  1.327785849571228,\n",
              "  1.3795579671859741,\n",
              "  1.302227258682251,\n",
              "  1.2511849403381348,\n",
              "  1.3859412670135498,\n",
              "  1.380162000656128,\n",
              "  1.259603500366211,\n",
              "  1.3051278591156006,\n",
              "  1.4243009090423584,\n",
              "  1.3968110084533691,\n",
              "  1.4410055875778198,\n",
              "  1.3965972661972046,\n",
              "  1.4213383197784424,\n",
              "  1.2316043376922607,\n",
              "  1.3144724369049072,\n",
              "  1.4574389457702637,\n",
              "  1.3774746656417847,\n",
              "  1.5079712867736816,\n",
              "  1.4210193157196045,\n",
              "  1.347639560699463,\n",
              "  1.348081111907959,\n",
              "  1.2902617454528809,\n",
              "  1.29671049118042,\n",
              "  1.2171242237091064,\n",
              "  1.3558719158172607,\n",
              "  1.292470932006836,\n",
              "  1.3423988819122314,\n",
              "  1.355679988861084,\n",
              "  1.3160004615783691,\n",
              "  1.318434715270996,\n",
              "  1.4229977130889893,\n",
              "  1.4498393535614014,\n",
              "  1.4173557758331299,\n",
              "  1.373932123184204,\n",
              "  1.4302672147750854,\n",
              "  1.3774830102920532,\n",
              "  1.3466017246246338,\n",
              "  1.417083501815796,\n",
              "  1.336639165878296,\n",
              "  1.3450191020965576,\n",
              "  1.3135368824005127,\n",
              "  1.458925724029541,\n",
              "  1.400580644607544,\n",
              "  1.347917914390564,\n",
              "  1.4367356300354004,\n",
              "  1.3378843069076538,\n",
              "  1.454756498336792,\n",
              "  1.4258532524108887,\n",
              "  1.4214881658554077,\n",
              "  1.4986176490783691,\n",
              "  1.6235454082489014,\n",
              "  1.4660087823867798,\n",
              "  1.4562571048736572,\n",
              "  1.3598424196243286,\n",
              "  1.3777984380722046,\n",
              "  1.4515072107315063,\n",
              "  1.481414794921875,\n",
              "  1.5079991817474365,\n",
              "  1.450581431388855,\n",
              "  1.5263023376464844,\n",
              "  1.5812287330627441,\n",
              "  1.489248514175415,\n",
              "  1.348836898803711,\n",
              "  1.491712212562561,\n",
              "  1.4856061935424805,\n",
              "  1.353349208831787,\n",
              "  1.3523235321044922,\n",
              "  1.464423418045044,\n",
              "  1.3538531064987183,\n",
              "  1.4639965295791626,\n",
              "  1.3837354183197021,\n",
              "  1.4638113975524902,\n",
              "  1.4646209478378296,\n",
              "  1.3749250173568726,\n",
              "  1.4161195755004883,\n",
              "  1.4551796913146973,\n",
              "  1.433037281036377,\n",
              "  1.2836720943450928,\n",
              "  1.3710685968399048,\n",
              "  1.471346139907837,\n",
              "  1.4315274953842163,\n",
              "  1.4889435768127441,\n",
              "  1.3538545370101929,\n",
              "  1.510084629058838,\n",
              "  1.4920940399169922,\n",
              "  1.4152307510375977,\n",
              "  1.4871100187301636,\n",
              "  1.4357247352600098,\n",
              "  1.3819886445999146,\n",
              "  1.4022045135498047,\n",
              "  1.3933607339859009,\n",
              "  1.403943419456482,\n",
              "  1.3608698844909668,\n",
              "  1.4113733768463135,\n",
              "  1.3992352485656738,\n",
              "  1.3977372646331787,\n",
              "  1.5026307106018066,\n",
              "  1.4319825172424316,\n",
              "  1.3917425870895386,\n",
              "  1.4734899997711182,\n",
              "  1.408322811126709,\n",
              "  1.505806565284729,\n",
              "  1.4686849117279053,\n",
              "  1.4205950498580933,\n",
              "  1.3962832689285278,\n",
              "  1.4249192476272583,\n",
              "  1.5093320608139038,\n",
              "  1.4175437688827515,\n",
              "  1.3108737468719482,\n",
              "  1.4605216979980469,\n",
              "  1.4412617683410645,\n",
              "  1.3128244876861572,\n",
              "  1.4378492832183838,\n",
              "  1.41489839553833,\n",
              "  1.4428460597991943,\n",
              "  1.4686574935913086,\n",
              "  1.3186107873916626,\n",
              "  1.4302067756652832,\n",
              "  1.3795692920684814,\n",
              "  1.588354229927063,\n",
              "  1.5212140083312988,\n",
              "  1.5024334192276,\n",
              "  1.5049481391906738,\n",
              "  1.4535471200942993,\n",
              "  1.3831623792648315,\n",
              "  1.4764676094055176,\n",
              "  1.4167864322662354,\n",
              "  1.4657909870147705,\n",
              "  1.4192864894866943,\n",
              "  1.4680994749069214,\n",
              "  1.3263006210327148,\n",
              "  1.3573319911956787,\n",
              "  1.4298580884933472,\n",
              "  1.5396013259887695,\n",
              "  1.3948609828948975,\n",
              "  1.503833293914795,\n",
              "  1.27705717086792,\n",
              "  1.4506568908691406,\n",
              "  1.5060516595840454,\n",
              "  1.3216264247894287,\n",
              "  1.5007271766662598,\n",
              "  1.5070595741271973,\n",
              "  1.3716106414794922,\n",
              "  1.3868634700775146,\n",
              "  1.3025765419006348,\n",
              "  1.3690905570983887,\n",
              "  1.327881097793579,\n",
              "  1.4621036052703857,\n",
              "  1.4327970743179321,\n",
              "  1.4683032035827637,\n",
              "  1.5962014198303223,\n",
              "  1.3798272609710693,\n",
              "  1.4583185911178589,\n",
              "  1.4871623516082764,\n",
              "  1.4397878646850586,\n",
              "  1.5091274976730347,\n",
              "  1.4956471920013428,\n",
              "  1.349863886833191,\n",
              "  1.4537886381149292,\n",
              "  1.3840231895446777,\n",
              "  1.4660693407058716,\n",
              "  1.403374195098877,\n",
              "  1.4561907052993774,\n",
              "  1.4065881967544556,\n",
              "  1.5228766202926636,\n",
              "  1.3694802522659302,\n",
              "  1.2528977394104004,\n",
              "  1.3520662784576416,\n",
              "  1.4429703950881958,\n",
              "  1.524284839630127,\n",
              "  1.3730990886688232,\n",
              "  1.4435853958129883,\n",
              "  1.3557240962982178,\n",
              "  1.4145426750183105,\n",
              "  1.3981828689575195,\n",
              "  1.3724493980407715,\n",
              "  1.560046911239624,\n",
              "  1.4190517663955688,\n",
              "  1.3513160943984985,\n",
              "  1.4332040548324585,\n",
              "  1.43497633934021,\n",
              "  1.50974440574646,\n",
              "  1.4220895767211914,\n",
              "  1.2732049226760864,\n",
              "  1.490121603012085,\n",
              "  1.375093698501587,\n",
              "  1.3799216747283936,\n",
              "  1.4300460815429688,\n",
              "  1.3632843494415283,\n",
              "  1.413482427597046,\n",
              "  1.525468111038208,\n",
              "  1.3861806392669678,\n",
              "  1.414823055267334,\n",
              "  1.4369899034500122,\n",
              "  1.392162561416626,\n",
              "  1.382343053817749,\n",
              "  1.2932307720184326,\n",
              "  1.4471933841705322,\n",
              "  1.472597360610962,\n",
              "  1.537583351135254,\n",
              "  1.6298407316207886,\n",
              "  1.4157052040100098,\n",
              "  1.5278102159500122,\n",
              "  1.5197895765304565,\n",
              "  1.5592670440673828,\n",
              "  1.4643648862838745,\n",
              "  1.5698028802871704,\n",
              "  1.3393266201019287,\n",
              "  1.4179273843765259,\n",
              "  1.5045863389968872,\n",
              "  1.449958324432373,\n",
              "  1.3381024599075317,\n",
              "  1.440134882926941,\n",
              "  1.5513203144073486,\n",
              "  1.4180216789245605,\n",
              "  1.4550154209136963,\n",
              "  1.5004730224609375,\n",
              "  1.446916103363037,\n",
              "  1.4899972677230835,\n",
              "  1.3644529581069946,\n",
              "  1.5432014465332031,\n",
              "  1.5185240507125854,\n",
              "  1.4715690612792969,\n",
              "  1.4333124160766602,\n",
              "  1.474146842956543,\n",
              "  1.4876072406768799,\n",
              "  1.5372397899627686,\n",
              "  1.6305701732635498,\n",
              "  1.4298579692840576,\n",
              "  1.6435445547103882,\n",
              "  1.4405020475387573,\n",
              "  1.306254267692566,\n",
              "  1.505112886428833,\n",
              "  1.3753156661987305,\n",
              "  1.5515695810317993,\n",
              "  1.4103920459747314,\n",
              "  1.5171582698822021,\n",
              "  1.3602327108383179,\n",
              "  1.4227330684661865,\n",
              "  1.4488248825073242,\n",
              "  1.5333783626556396,\n",
              "  1.3761181831359863,\n",
              "  1.5171219110488892,\n",
              "  1.416603922843933,\n",
              "  1.5227253437042236,\n",
              "  1.4417569637298584,\n",
              "  1.5115933418273926,\n",
              "  1.5062097311019897,\n",
              "  1.5289891958236694,\n",
              "  1.4378070831298828,\n",
              "  1.558586835861206,\n",
              "  1.42490553855896,\n",
              "  1.3673077821731567,\n",
              "  1.4552431106567383,\n",
              "  1.4540576934814453,\n",
              "  1.475574254989624,\n",
              "  1.4030101299285889,\n",
              "  1.427386999130249,\n",
              "  1.4989399909973145,\n",
              "  1.407906413078308,\n",
              "  1.5651886463165283,\n",
              "  1.4150664806365967,\n",
              "  1.5203946828842163,\n",
              "  1.4850213527679443,\n",
              "  1.5057486295700073,\n",
              "  1.4717124700546265,\n",
              "  1.455730676651001,\n",
              "  1.333603858947754,\n",
              "  1.4371592998504639,\n",
              "  1.4814742803573608,\n",
              "  1.476154088973999,\n",
              "  1.5026718378067017,\n",
              "  1.5542685985565186,\n",
              "  1.4104522466659546,\n",
              "  1.5346555709838867,\n",
              "  1.5164554119110107,\n",
              "  1.4641941785812378,\n",
              "  1.5266964435577393,\n",
              "  1.4860535860061646,\n",
              "  1.5245256423950195,\n",
              "  1.5917723178863525,\n",
              "  1.4617602825164795,\n",
              "  1.4576804637908936,\n",
              "  1.6114031076431274,\n",
              "  1.5167872905731201,\n",
              "  1.5204739570617676,\n",
              "  1.4920696020126343,\n",
              "  1.5250544548034668,\n",
              "  1.4734870195388794,\n",
              "  1.511199712753296,\n",
              "  1.4816138744354248,\n",
              "  1.4091887474060059,\n",
              "  1.4606387615203857,\n",
              "  1.4440985918045044,\n",
              "  1.5101594924926758,\n",
              "  1.4004523754119873,\n",
              "  1.5416080951690674,\n",
              "  1.3663268089294434,\n",
              "  1.505251169204712,\n",
              "  1.3764426708221436,\n",
              "  1.3765783309936523,\n",
              "  1.4134910106658936,\n",
              "  1.4949910640716553,\n",
              "  1.5153895616531372,\n",
              "  1.6570030450820923,\n",
              "  1.4691016674041748,\n",
              "  1.4657334089279175,\n",
              "  1.4000654220581055,\n",
              "  1.4784724712371826,\n",
              "  1.5645864009857178,\n",
              "  1.5490537881851196,\n",
              "  1.380000352859497,\n",
              "  1.3884613513946533,\n",
              "  1.4218459129333496,\n",
              "  1.4442028999328613,\n",
              "  1.5888986587524414,\n",
              "  1.5604896545410156,\n",
              "  1.5743601322174072,\n",
              "  1.488304853439331,\n",
              "  1.4520909786224365,\n",
              "  1.457648515701294,\n",
              "  1.3926560878753662,\n",
              "  1.5144449472427368,\n",
              "  1.4941500425338745,\n",
              "  1.4106755256652832,\n",
              "  1.5051733255386353,\n",
              "  1.4772109985351562,\n",
              "  1.40946626663208,\n",
              "  1.442232370376587,\n",
              "  1.40252685546875,\n",
              "  1.2316821813583374,\n",
              "  1.385479211807251,\n",
              "  1.32615327835083,\n",
              "  1.464660406112671,\n",
              "  1.436206579208374,\n",
              "  1.4880948066711426,\n",
              "  1.462697982788086,\n",
              "  1.3922988176345825,\n",
              "  1.4259274005889893,\n",
              "  1.4545387029647827,\n",
              "  1.6224879026412964,\n",
              "  1.5118675231933594,\n",
              "  1.5055444240570068,\n",
              "  1.4952495098114014,\n",
              "  1.407444953918457,\n",
              "  1.353149652481079,\n",
              "  1.4999911785125732,\n",
              "  1.4944190979003906,\n",
              "  1.4436267614364624,\n",
              "  1.3567969799041748,\n",
              "  1.4255566596984863,\n",
              "  1.3655426502227783,\n",
              "  1.3666279315948486,\n",
              "  1.5192445516586304,\n",
              "  1.4430783987045288,\n",
              "  1.4929125308990479,\n",
              "  1.3522324562072754,\n",
              "  1.3856208324432373,\n",
              "  1.4803762435913086,\n",
              "  1.3918217420578003,\n",
              "  1.5081820487976074,\n",
              "  1.3613786697387695,\n",
              "  1.2829625606536865,\n",
              "  1.3836851119995117,\n",
              "  1.3889572620391846,\n",
              "  1.5209589004516602,\n",
              "  1.3147180080413818,\n",
              "  1.4197313785552979,\n",
              "  1.3489959239959717,\n",
              "  1.4442883729934692,\n",
              "  1.3559490442276,\n",
              "  1.462003231048584,\n",
              "  1.4150410890579224,\n",
              "  1.4816529750823975,\n",
              "  1.325366735458374,\n",
              "  1.4065954685211182,\n",
              "  1.3954253196716309,\n",
              "  1.5021038055419922,\n",
              "  1.3979164361953735,\n",
              "  1.4102846384048462,\n",
              "  1.5344481468200684,\n",
              "  1.457444190979004,\n",
              "  1.465301275253296,\n",
              "  1.5647177696228027,\n",
              "  1.4477686882019043,\n",
              "  1.500631332397461,\n",
              "  1.3983204364776611,\n",
              "  1.4706220626831055,\n",
              "  1.4809107780456543,\n",
              "  1.3972852230072021,\n",
              "  1.4126262664794922,\n",
              "  1.3671003580093384,\n",
              "  1.4856488704681396,\n",
              "  1.4271678924560547,\n",
              "  1.3973184823989868,\n",
              "  1.3395180702209473,\n",
              "  1.375234603881836,\n",
              "  1.3356475830078125,\n",
              "  1.3987584114074707,\n",
              "  1.5594840049743652,\n",
              "  1.3293955326080322,\n",
              "  1.2864255905151367,\n",
              "  1.5757256746292114,\n",
              "  1.4080499410629272,\n",
              "  1.3955614566802979,\n",
              "  1.4892971515655518,\n",
              "  1.3397313356399536,\n",
              "  1.317751169204712,\n",
              "  1.3417558670043945,\n",
              "  1.5245535373687744,\n",
              "  1.3642618656158447,\n",
              "  1.4173173904418945,\n",
              "  1.3901112079620361,\n",
              "  1.4104795455932617,\n",
              "  1.3606584072113037,\n",
              "  1.4728032350540161,\n",
              "  1.4182822704315186,\n",
              "  1.4234946966171265,\n",
              "  1.5136027336120605,\n",
              "  1.3478291034698486,\n",
              "  1.2912248373031616,\n",
              "  1.2168879508972168,\n",
              "  1.5969314575195312,\n",
              "  1.3881127834320068,\n",
              "  1.421919584274292,\n",
              "  1.350942850112915,\n",
              "  1.2658519744873047,\n",
              "  1.3269522190093994,\n",
              "  1.4076762199401855,\n",
              "  1.4345179796218872,\n",
              "  1.3190451860427856,\n",
              "  1.4503977298736572,\n",
              "  1.252335548400879,\n",
              "  1.4939565658569336,\n",
              "  1.5630168914794922,\n",
              "  1.4163070917129517,\n",
              "  1.4466509819030762,\n",
              "  1.4144930839538574,\n",
              "  1.5081106424331665,\n",
              "  1.4062234163284302,\n",
              "  1.5047539472579956,\n",
              "  1.3195502758026123,\n",
              "  1.3004381656646729,\n",
              "  1.430576205253601,\n",
              "  1.4143521785736084,\n",
              "  1.3253117799758911,\n",
              "  1.368051290512085,\n",
              "  1.4646954536437988,\n",
              "  1.4201960563659668,\n",
              "  1.3318710327148438,\n",
              "  1.361509084701538,\n",
              "  1.3383872509002686,\n",
              "  1.4219012260437012,\n",
              "  1.4216728210449219,\n",
              "  1.2765350341796875,\n",
              "  1.4225342273712158,\n",
              "  1.4996590614318848,\n",
              "  1.4598503112792969,\n",
              "  1.4317339658737183,\n",
              "  1.3586664199829102,\n",
              "  1.410244107246399,\n",
              "  1.4834929704666138,\n",
              "  1.4628171920776367,\n",
              "  1.445987582206726,\n",
              "  1.4305968284606934,\n",
              "  1.370833158493042,\n",
              "  1.3987082242965698,\n",
              "  1.3899811506271362,\n",
              "  1.527795433998108,\n",
              "  1.3305859565734863,\n",
              "  1.269885540008545,\n",
              "  1.5709460973739624,\n",
              "  1.3808577060699463,\n",
              "  1.401327133178711,\n",
              "  1.4381966590881348,\n",
              "  1.3231873512268066,\n",
              "  1.4339830875396729,\n",
              "  1.3761863708496094,\n",
              "  1.5246423482894897,\n",
              "  1.46533203125,\n",
              "  1.3978285789489746,\n",
              "  1.3090472221374512,\n",
              "  1.3536310195922852,\n",
              "  1.3778736591339111,\n",
              "  1.531003713607788,\n",
              "  1.4423797130584717,\n",
              "  1.4016048908233643,\n",
              "  1.374974012374878,\n",
              "  1.3332281112670898,\n",
              "  1.417555809020996,\n",
              "  1.4713778495788574,\n",
              "  1.2645373344421387,\n",
              "  1.3534940481185913,\n",
              "  1.3554768562316895,\n",
              "  1.3869792222976685,\n",
              "  1.337815523147583,\n",
              "  1.350942611694336,\n",
              "  1.4027371406555176,\n",
              "  1.3879942893981934,\n",
              "  1.370634913444519,\n",
              "  1.3075222969055176,\n",
              "  1.388061761856079,\n",
              "  1.266674280166626,\n",
              "  1.2928404808044434,\n",
              "  1.2674264907836914,\n",
              "  1.252261757850647,\n",
              "  1.358514666557312,\n",
              "  1.4302046298980713,\n",
              "  1.227220892906189,\n",
              "  1.3292133808135986,\n",
              "  1.2155661582946777,\n",
              "  1.337012529373169,\n",
              "  1.3302156925201416,\n",
              "  1.4782583713531494,\n",
              "  1.388162612915039,\n",
              "  1.3715929985046387,\n",
              "  1.3947317600250244,\n",
              "  1.3895756006240845,\n",
              "  1.3091883659362793,\n",
              "  1.3913862705230713,\n",
              "  1.2887407541275024,\n",
              "  1.4167439937591553,\n",
              "  1.4160397052764893,\n",
              "  1.4653522968292236,\n",
              "  1.3869576454162598,\n",
              "  1.28508722782135,\n",
              "  1.3368730545043945,\n",
              "  1.477531909942627,\n",
              "  1.4397813081741333,\n",
              "  1.3359100818634033,\n",
              "  1.4984798431396484,\n",
              "  1.3878145217895508,\n",
              "  1.3561042547225952,\n",
              "  1.3202168941497803,\n",
              "  1.520479440689087,\n",
              "  1.3871502876281738,\n",
              "  1.3762953281402588,\n",
              "  1.2590136528015137,\n",
              "  1.440576195716858,\n",
              "  1.4064371585845947,\n",
              "  1.3798930644989014,\n",
              "  1.4235048294067383,\n",
              "  1.3870303630828857,\n",
              "  1.4617908000946045,\n",
              "  1.323634147644043,\n",
              "  1.308252215385437,\n",
              "  1.496941328048706,\n",
              "  1.3704586029052734,\n",
              "  1.2619518041610718,\n",
              "  1.3577630519866943,\n",
              "  1.4382712841033936,\n",
              "  1.3325672149658203,\n",
              "  1.3571068048477173,\n",
              "  1.5056042671203613,\n",
              "  1.3027790784835815,\n",
              "  1.3157923221588135,\n",
              "  1.2952238321304321,\n",
              "  1.4494879245758057,\n",
              "  1.243749976158142,\n",
              "  1.2991336584091187,\n",
              "  1.4291936159133911,\n",
              "  1.4068228006362915,\n",
              "  1.2367889881134033,\n",
              "  1.3343262672424316,\n",
              "  1.3259108066558838,\n",
              "  1.4828940629959106,\n",
              "  1.379371166229248,\n",
              "  1.2685799598693848,\n",
              "  1.3881840705871582,\n",
              "  1.328050136566162,\n",
              "  1.4141592979431152,\n",
              "  1.3891828060150146,\n",
              "  1.462464451789856,\n",
              "  1.3983039855957031,\n",
              "  1.397343397140503,\n",
              "  1.3783056735992432,\n",
              "  1.3169598579406738,\n",
              "  1.4273000955581665,\n",
              "  1.4602184295654297,\n",
              "  1.4719146490097046,\n",
              "  1.3905493021011353,\n",
              "  1.403954267501831,\n",
              "  1.4755382537841797,\n",
              "  1.3869340419769287,\n",
              "  1.3970388174057007,\n",
              "  1.4615452289581299,\n",
              "  1.3266487121582031,\n",
              "  1.356276273727417,\n",
              "  1.499756097793579,\n",
              "  1.401900053024292,\n",
              "  1.4314639568328857,\n",
              "  1.4738638401031494,\n",
              "  1.325573444366455,\n",
              "  1.361337423324585,\n",
              "  1.402099847793579,\n",
              "  1.3205950260162354,\n",
              "  1.4608874320983887,\n",
              "  1.2727043628692627,\n",
              "  1.3680946826934814,\n",
              "  1.3859251737594604,\n",
              "  1.434455394744873,\n",
              "  1.4233222007751465,\n",
              "  1.1615468263626099,\n",
              "  1.3063241243362427,\n",
              "  1.358206033706665,\n",
              "  1.2893531322479248,\n",
              "  1.5370683670043945,\n",
              "  1.2830562591552734,\n",
              "  1.2479687929153442,\n",
              "  1.259962558746338,\n",
              "  1.3592138290405273,\n",
              "  1.3725416660308838,\n",
              "  1.302546501159668,\n",
              "  1.3221535682678223,\n",
              "  1.2768837213516235,\n",
              "  1.2800558805465698,\n",
              "  1.371666431427002,\n",
              "  1.3685286045074463,\n",
              "  1.3934533596038818,\n",
              "  1.2937078475952148,\n",
              "  1.32703697681427,\n",
              "  1.4207266569137573,\n",
              "  1.35226571559906,\n",
              "  1.4443610906600952,\n",
              "  1.5215580463409424,\n",
              "  1.271451711654663,\n",
              "  1.3166074752807617,\n",
              "  1.241739273071289,\n",
              "  1.4735620021820068,\n",
              "  1.292982578277588,\n",
              "  1.2756595611572266,\n",
              "  1.3563907146453857,\n",
              "  1.3554162979125977,\n",
              "  1.4787602424621582,\n",
              "  1.33445405960083,\n",
              "  1.306936502456665,\n",
              "  1.35873281955719,\n",
              "  1.398941993713379,\n",
              "  1.3521573543548584,\n",
              "  1.4648613929748535,\n",
              "  1.322501540184021,\n",
              "  1.419403076171875,\n",
              "  1.432091474533081,\n",
              "  1.3729031085968018,\n",
              "  1.4042046070098877,\n",
              "  1.3306183815002441,\n",
              "  1.3580822944641113,\n",
              "  1.3242738246917725,\n",
              "  1.390549898147583,\n",
              "  1.3001258373260498,\n",
              "  1.2914206981658936,\n",
              "  1.3343838453292847,\n",
              "  1.3368139266967773,\n",
              "  1.2738091945648193,\n",
              "  1.3560129404067993,\n",
              "  1.4240288734436035,\n",
              "  1.289623498916626,\n",
              "  1.434303879737854,\n",
              "  1.2585220336914062,\n",
              "  1.3866932392120361,\n",
              "  1.3085379600524902,\n",
              "  1.4308429956436157,\n",
              "  1.365349531173706,\n",
              "  1.2973337173461914,\n",
              "  1.366882562637329,\n",
              "  1.3657233715057373,\n",
              "  1.3766562938690186,\n",
              "  1.3256895542144775,\n",
              "  1.4087872505187988,\n",
              "  1.3576688766479492,\n",
              "  1.3110713958740234,\n",
              "  1.3344749212265015,\n",
              "  1.2904393672943115,\n",
              "  1.2776098251342773,\n",
              "  1.3395496606826782,\n",
              "  1.3903650045394897,\n",
              "  1.3238407373428345,\n",
              "  1.34473717212677,\n",
              "  1.3646172285079956,\n",
              "  1.264520525932312,\n",
              "  1.2591722011566162,\n",
              "  1.297494888305664,\n",
              "  1.2644245624542236,\n",
              "  1.3050005435943604,\n",
              "  1.3430224657058716,\n",
              "  1.370658278465271,\n",
              "  1.3686219453811646,\n",
              "  1.3625731468200684,\n",
              "  1.3286396265029907,\n",
              "  1.2773741483688354,\n",
              "  1.3408687114715576,\n",
              "  1.360175609588623,\n",
              "  1.295130729675293,\n",
              "  1.3773332834243774,\n",
              "  1.3843913078308105,\n",
              "  1.44781494140625,\n",
              "  1.400902509689331,\n",
              "  1.362127423286438,\n",
              "  1.3593780994415283,\n",
              "  1.2790541648864746,\n",
              "  1.354004979133606,\n",
              "  1.361616611480713,\n",
              "  1.3260159492492676,\n",
              "  1.305951476097107,\n",
              "  1.3944169282913208,\n",
              "  1.358298420906067,\n",
              "  1.3484461307525635,\n",
              "  1.37046217918396,\n",
              "  1.4341368675231934,\n",
              "  1.3309528827667236,\n",
              "  1.236135721206665,\n",
              "  1.3012117147445679,\n",
              "  1.283785104751587,\n",
              "  1.213150143623352,\n",
              "  1.2553153038024902,\n",
              "  1.428364872932434,\n",
              "  1.273796558380127,\n",
              "  1.2753338813781738,\n",
              "  1.4103479385375977,\n",
              "  1.3162477016448975,\n",
              "  1.2578791379928589,\n",
              "  1.4496173858642578,\n",
              "  1.3372262716293335,\n",
              "  1.2566046714782715,\n",
              "  1.3159997463226318,\n",
              "  1.2943098545074463,\n",
              "  1.2774326801300049,\n",
              "  1.326601505279541,\n",
              "  1.4056077003479004,\n",
              "  1.2681752443313599,\n",
              "  1.3592036962509155,\n",
              "  1.321606159210205,\n",
              "  1.2119081020355225,\n",
              "  1.2198646068572998,\n",
              "  1.314837098121643,\n",
              "  1.1909122467041016,\n",
              "  1.3104523420333862,\n",
              "  1.2908459901809692,\n",
              "  1.27303147315979,\n",
              "  1.3986554145812988,\n",
              "  1.1905518770217896,\n",
              "  1.4435791969299316,\n",
              "  1.3068318367004395,\n",
              "  1.3616902828216553,\n",
              "  1.3564091920852661,\n",
              "  1.2441977262496948,\n",
              "  1.2937650680541992,\n",
              "  1.3526420593261719,\n",
              "  1.2271170616149902,\n",
              "  1.3174959421157837,\n",
              "  1.419025182723999,\n",
              "  1.277878761291504,\n",
              "  1.3124759197235107,\n",
              "  1.253190279006958,\n",
              "  1.3327116966247559,\n",
              "  1.3071396350860596,\n",
              "  1.3093101978302002,\n",
              "  1.322331428527832,\n",
              "  1.3286747932434082,\n",
              "  1.3478178977966309,\n",
              "  1.4030466079711914,\n",
              "  1.1624910831451416,\n",
              "  1.2675528526306152,\n",
              "  1.297619104385376,\n",
              "  1.3017191886901855,\n",
              "  1.3576264381408691,\n",
              "  1.1305667161941528,\n",
              "  1.3277702331542969,\n",
              "  1.3954951763153076,\n",
              "  1.2741528749465942,\n",
              "  1.3975694179534912,\n",
              "  1.2676188945770264,\n",
              "  1.4029452800750732,\n",
              "  1.2538074254989624,\n",
              "  1.3485027551651,\n",
              "  1.21388840675354,\n",
              "  1.362052321434021,\n",
              "  1.2614842653274536,\n",
              "  1.318665862083435,\n",
              "  1.1870875358581543,\n",
              "  1.3310015201568604,\n",
              "  1.344419240951538,\n",
              "  1.3035366535186768,\n",
              "  1.2190651893615723,\n",
              "  1.272780418395996,\n",
              "  1.2808947563171387,\n",
              "  1.2735973596572876,\n",
              "  1.3575963973999023,\n",
              "  1.2413699626922607,\n",
              "  1.292375087738037,\n",
              "  1.3571709394454956,\n",
              "  1.2477238178253174,\n",
              "  1.2925883531570435,\n",
              "  1.3109179735183716,\n",
              "  1.2884060144424438,\n",
              "  1.3668830394744873,\n",
              "  1.284964680671692,\n",
              "  1.4275541305541992,\n",
              "  1.3365044593811035,\n",
              "  1.3492333889007568,\n",
              "  1.337024450302124,\n",
              "  1.3914780616760254,\n",
              "  1.4240304231643677,\n",
              "  1.2924902439117432,\n",
              "  1.34517502784729,\n",
              "  1.3602659702301025,\n",
              "  1.3413976430892944,\n",
              "  1.4107334613800049,\n",
              "  1.267812967300415,\n",
              "  1.4455604553222656,\n",
              "  1.3359674215316772,\n",
              "  1.3712025880813599,\n",
              "  1.3656680583953857,\n",
              "  1.2692196369171143,\n",
              "  1.3922282457351685,\n",
              "  1.328620195388794,\n",
              "  1.3544977903366089,\n",
              "  1.338174819946289,\n",
              "  1.3208274841308594,\n",
              "  1.2410482168197632,\n",
              "  1.4096094369888306,\n",
              "  1.2290258407592773,\n",
              "  1.3077373504638672,\n",
              "  1.4174038171768188,\n",
              "  1.4164485931396484,\n",
              "  1.2444978952407837,\n",
              "  1.2886167764663696,\n",
              "  1.2859063148498535,\n",
              "  1.2330753803253174,\n",
              "  1.3330473899841309,\n",
              "  1.3428857326507568,\n",
              "  1.3694709539413452,\n",
              "  1.2725085020065308,\n",
              "  1.2902981042861938,\n",
              "  1.466759443283081,\n",
              "  1.285691499710083,\n",
              "  1.3249284029006958,\n",
              "  1.3916535377502441,\n",
              "  1.395035982131958,\n",
              "  1.3283708095550537,\n",
              "  1.3332746028900146,\n",
              "  1.4130678176879883,\n",
              "  1.315100073814392,\n",
              "  1.3090407848358154,\n",
              "  1.2985522747039795,\n",
              "  1.2790312767028809,\n",
              "  1.323379635810852,\n",
              "  1.4077595472335815,\n",
              "  1.3428993225097656,\n",
              "  1.409295916557312,\n",
              "  1.3419352769851685,\n",
              "  1.294630527496338,\n",
              "  1.2917509078979492,\n",
              "  1.2846535444259644,\n",
              "  1.4525387287139893,\n",
              "  1.2480075359344482,\n",
              "  1.134261965751648,\n",
              "  1.365915060043335,\n",
              "  1.2678470611572266,\n",
              "  1.391118049621582,\n",
              "  1.2871354818344116,\n",
              "  1.3011035919189453,\n",
              "  1.3340892791748047,\n",
              "  1.2976605892181396,\n",
              "  1.2510701417922974,\n",
              "  1.2675137519836426,\n",
              "  1.3392246961593628,\n",
              "  1.3762538433074951,\n",
              "  1.2772026062011719,\n",
              "  1.346845030784607,\n",
              "  1.1358619928359985,\n",
              "  1.1925246715545654,\n",
              "  1.2902119159698486,\n",
              "  1.3393163681030273,\n",
              "  1.3720836639404297,\n",
              "  1.347578763961792,\n",
              "  1.362307071685791,\n",
              "  1.3451104164123535,\n",
              "  1.2937757968902588,\n",
              "  1.4028844833374023,\n",
              "  1.327772617340088,\n",
              "  1.2388006448745728,\n",
              "  1.334824562072754,\n",
              "  1.3042149543762207,\n",
              "  1.4543144702911377,\n",
              "  1.377636194229126,\n",
              "  1.2623708248138428,\n",
              "  1.399160623550415,\n",
              "  1.3348685503005981,\n",
              "  1.340620756149292,\n",
              "  1.391361951828003,\n",
              "  1.1740913391113281,\n",
              "  1.2153551578521729,\n",
              "  1.2765560150146484,\n",
              "  1.3191016912460327,\n",
              "  1.2234199047088623,\n",
              "  1.2920068502426147,\n",
              "  1.245327115058899,\n",
              "  1.2477927207946777,\n",
              "  1.2065534591674805,\n",
              "  1.4117343425750732,\n",
              "  1.3573970794677734,\n",
              "  1.2613306045532227,\n",
              "  1.2843449115753174,\n",
              "  1.3955899477005005,\n",
              "  1.277402400970459,\n",
              "  1.4030022621154785,\n",
              "  1.2919106483459473,\n",
              "  1.3128056526184082,\n",
              "  1.375929594039917,\n",
              "  1.3593937158584595,\n",
              "  1.347121000289917,\n",
              "  1.4739115238189697,\n",
              "  1.3388644456863403,\n",
              "  1.3698585033416748,\n",
              "  1.2165632247924805,\n",
              "  1.40231192111969,\n",
              "  1.2480201721191406,\n",
              "  1.3380088806152344,\n",
              "  1.4237558841705322,\n",
              "  1.448319435119629,\n",
              "  1.2547228336334229,\n",
              "  1.333680272102356,\n",
              "  1.2544829845428467,\n",
              "  1.2731618881225586,\n",
              "  1.3158705234527588,\n",
              "  1.1737570762634277,\n",
              "  1.3333840370178223,\n",
              "  1.3195021152496338,\n",
              "  1.2255983352661133,\n",
              "  1.2694679498672485,\n",
              "  1.372400164604187,\n",
              "  1.2998814582824707,\n",
              "  1.3053557872772217,\n",
              "  1.4399065971374512,\n",
              "  1.379791021347046,\n",
              "  1.4004364013671875,\n",
              "  1.3346037864685059,\n",
              "  1.2959697246551514,\n",
              "  1.2581944465637207,\n",
              "  1.3496551513671875,\n",
              "  1.234938621520996,\n",
              "  1.3190584182739258,\n",
              "  1.1534712314605713,\n",
              "  1.2242094278335571,\n",
              "  1.3335903882980347,\n",
              "  1.2489323616027832,\n",
              "  1.3231651782989502],\n",
              " 'val_acc': [0.734375,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.578125,\n",
              "  0.484375,\n",
              "  0.484375,\n",
              "  0.46875,\n",
              "  0.453125,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.21875,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.15625,\n",
              "  0.28125,\n",
              "  0.1875,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.171875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.328125,\n",
              "  0.1875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.1875,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.15625,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.15625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.15625,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.171875,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.15625,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.171875,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.203125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.328125,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "23ecee01-62ff-441a-eea3-e2310ec3535e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1_drop0.3.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1_drop0.3.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "1d8c6028-5bc5-49cd-883a-dffd3749826d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f2a97da-d457-49dd-acf4-934da16f9fa5\", \"newdata_SEM1_drop0.3.h5\", 16615536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}