{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOBWUN/BOXMR94NH6WRWrtt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/2Class_datanew_nsc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "124ce066-1448-4cbf-ecd2-4a5c1104ede5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data_nsc - ใช้อันนี้เทรน.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "efa36c2e-d523-4cdd-f700-4b507a8d1075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        No                                          Name_file  \\\n",
              "0        1  /content/drive/My Drive/modelnsc/acssuschemeng...   \n",
              "1        2  /content/drive/My Drive/modelnsc/acssuschemeng...   \n",
              "2        3  /content/drive/My Drive/modelnsc/acssuschemeng...   \n",
              "3        4  /content/drive/My Drive/modelnsc/acssuschemeng...   \n",
              "4        5  /content/drive/My Drive/modelnsc/acssuschemeng...   \n",
              "...    ...                                                ...   \n",
              "1321  1322                      1-s2.0-S0926669022000292-main   \n",
              "1322  1323                      1-s2.0-S0926669022000292-main   \n",
              "1323  1324                      1-s2.0-S0926669022000292-main   \n",
              "1324  1325                      1-s2.0-S0926669022000292-main   \n",
              "1325  1326                      1-s2.0-S0926669022000292-main   \n",
              "\n",
              "                                             Name_Paper  \\\n",
              "0     Sugarcane Biowaste-Derived Biochars as Capacit...   \n",
              "1     Sugarcane Biowaste-Derived Biochars as Capacit...   \n",
              "2     Sugarcane Biowaste-Derived Biochars as Capacit...   \n",
              "3     Sugarcane Biowaste-Derived Biochars as Capacit...   \n",
              "4     Sugarcane Biowaste-Derived Biochars as Capacit...   \n",
              "...                                                 ...   \n",
              "1321  Low-cost activated carbon preparation from Cor...   \n",
              "1322  Low-cost activated carbon preparation from Cor...   \n",
              "1323  Low-cost activated carbon preparation from Cor...   \n",
              "1324  Low-cost activated carbon preparation from Cor...   \n",
              "1325  Low-cost activated carbon preparation from Cor...   \n",
              "\n",
              "                                journal  \\\n",
              "0     Sustainable Chemistry&Engineering   \n",
              "1     Sustainable Chemistry&Engineering   \n",
              "2     Sustainable Chemistry&Engineering   \n",
              "3     Sustainable Chemistry&Engineering   \n",
              "4     Sustainable Chemistry&Engineering   \n",
              "...                                 ...   \n",
              "1321        Industrial Crops & Products   \n",
              "1322        Industrial Crops & Products   \n",
              "1323        Industrial Crops & Products   \n",
              "1324        Industrial Crops & Products   \n",
              "1325        Industrial Crops & Products   \n",
              "\n",
              "                                           path_Picture    detail     Class  \\\n",
              "0     /content/drive/My Drive/modelnsc/acssuschemeng...  original     0-400   \n",
              "1     /content/drive/My Drive/modelnsc/acssuschemeng...   resize1     0-400   \n",
              "2     /content/drive/My Drive/modelnsc/acssuschemeng...   resize2     0-400   \n",
              "3     /content/drive/My Drive/modelnsc/acssuschemeng...   resize3     0-400   \n",
              "4     /content/drive/My Drive/modelnsc/acssuschemeng...   resize4     0-400   \n",
              "...                                                 ...       ...       ...   \n",
              "1321  /content/drive/My Drive/modelnsc/1-s2.0-S09266...  original  401-3200   \n",
              "1322  /content/drive/My Drive/modelnsc/1-s2.0-S09266...  original  401-3200   \n",
              "1323  /content/drive/My Drive/modelnsc/1-s2.0-S09266...  original  401-3200   \n",
              "1324  /content/drive/My Drive/modelnsc/1-s2.0-S09266...  original     0-400   \n",
              "1325  /content/drive/My Drive/modelnsc/1-s2.0-S09266...  original     0-400   \n",
              "\n",
              "        BET  Size(mico)  \n",
              "0     240.0          10  \n",
              "1     240.0          20  \n",
              "2     240.0          20  \n",
              "3     240.0          20  \n",
              "4     240.0          20  \n",
              "...     ...         ...  \n",
              "1321  583.0          10  \n",
              "1322  820.0          10  \n",
              "1323  589.0          10  \n",
              "1324  389.0          10  \n",
              "1325   11.0          10  \n",
              "\n",
              "[1326 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64cd009b-b567-4c61-93bf-a913aee7ebf7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>Sugarcane Biowaste-Derived Biochars as Capacit...</td>\n",
              "      <td>Sustainable Chemistry&amp;Engineering</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-400</td>\n",
              "      <td>240.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>Sugarcane Biowaste-Derived Biochars as Capacit...</td>\n",
              "      <td>Sustainable Chemistry&amp;Engineering</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>resize1</td>\n",
              "      <td>0-400</td>\n",
              "      <td>240.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>Sugarcane Biowaste-Derived Biochars as Capacit...</td>\n",
              "      <td>Sustainable Chemistry&amp;Engineering</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>resize2</td>\n",
              "      <td>0-400</td>\n",
              "      <td>240.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>Sugarcane Biowaste-Derived Biochars as Capacit...</td>\n",
              "      <td>Sustainable Chemistry&amp;Engineering</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>resize3</td>\n",
              "      <td>0-400</td>\n",
              "      <td>240.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>Sugarcane Biowaste-Derived Biochars as Capacit...</td>\n",
              "      <td>Sustainable Chemistry&amp;Engineering</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/acssuschemeng...</td>\n",
              "      <td>resize4</td>\n",
              "      <td>0-400</td>\n",
              "      <td>240.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>1322</td>\n",
              "      <td>1-s2.0-S0926669022000292-main</td>\n",
              "      <td>Low-cost activated carbon preparation from Cor...</td>\n",
              "      <td>Industrial Crops &amp; Products</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/1-s2.0-S09266...</td>\n",
              "      <td>original</td>\n",
              "      <td>401-3200</td>\n",
              "      <td>583.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>1323</td>\n",
              "      <td>1-s2.0-S0926669022000292-main</td>\n",
              "      <td>Low-cost activated carbon preparation from Cor...</td>\n",
              "      <td>Industrial Crops &amp; Products</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/1-s2.0-S09266...</td>\n",
              "      <td>original</td>\n",
              "      <td>401-3200</td>\n",
              "      <td>820.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>1324</td>\n",
              "      <td>1-s2.0-S0926669022000292-main</td>\n",
              "      <td>Low-cost activated carbon preparation from Cor...</td>\n",
              "      <td>Industrial Crops &amp; Products</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/1-s2.0-S09266...</td>\n",
              "      <td>original</td>\n",
              "      <td>401-3200</td>\n",
              "      <td>589.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1324</th>\n",
              "      <td>1325</td>\n",
              "      <td>1-s2.0-S0926669022000292-main</td>\n",
              "      <td>Low-cost activated carbon preparation from Cor...</td>\n",
              "      <td>Industrial Crops &amp; Products</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/1-s2.0-S09266...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-400</td>\n",
              "      <td>389.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325</th>\n",
              "      <td>1326</td>\n",
              "      <td>1-s2.0-S0926669022000292-main</td>\n",
              "      <td>Low-cost activated carbon preparation from Cor...</td>\n",
              "      <td>Industrial Crops &amp; Products</td>\n",
              "      <td>/content/drive/My Drive/modelnsc/1-s2.0-S09266...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-400</td>\n",
              "      <td>11.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1326 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64cd009b-b567-4c61-93bf-a913aee7ebf7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64cd009b-b567-4c61-93bf-a913aee7ebf7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64cd009b-b567-4c61-93bf-a913aee7ebf7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "59896a3a-d04a-4543-87ff-c588620e4b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "a57150b7-5664-4fdf-df30-78903f77e26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-400','401-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "129e3222-a67c-40ff-b5e6-1c7cc2b6e1a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "3d7e36ab-617c-4c65-c45c-e312083fe90e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 1100, done.\u001b[K\n",
            "remote: Counting objects: 100% (263/263), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 1100 (delta 133), reused 241 (delta 121), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (1100/1100), 14.10 MiB | 22.17 MiB/s, done.\n",
            "Resolving deltas: 100% (630/630), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/modeldatansc'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-400')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '401-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-400')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '401-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-400')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '401-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(1281,1303)]\n",
        "train = df[df['No'].between(1,1280)]\n",
        "test = df[df['No'].between(1304,1326)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-400']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='401-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-400']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='401-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-400']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='401-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "d9505d6c-b5f5-4b21-cc28-17abeee7c214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 630\n",
            "total training 2 images: 642 \n",
            "\n",
            "total validation 1 images: 5\n",
            "total validation 2 images: 18 \n",
            "\n",
            "total test 1 images: 7\n",
            "total test 2 images: 16 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 1280  # จำนวนภาพ Train\n",
        "NUM_TEST = 22 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8613b984-1bdc-4682-ae13-e974eb093869"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "6f333191-246c-4e40-b5fe-8c8cb4cc80f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "17eba038-7663-4358-f037-80629e4646eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "e36d1863-7087-4d7f-def1-df98fbd71e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "64a28c9a-5e9e-4b01-a73f-1de8c5083be5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1272 images belonging to 2 classes.\n",
            "Found 23 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=8e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc27a86-9635-4728-d275-fff799e5756f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-5b290c04e531>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "80/80 [==============================] - 22s 94ms/step - loss: 1.3057 - acc: 0.4615 - val_loss: 0.6886 - val_acc: 0.6250\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 1.1558 - acc: 0.5024 - val_loss: 0.7833 - val_acc: 0.6250\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 1.1275 - acc: 0.5157 - val_loss: 0.6830 - val_acc: 0.5000\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 6s 65ms/step - loss: 1.0297 - acc: 0.5307 - val_loss: 0.7420 - val_acc: 0.5625\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 1.0492 - acc: 0.5322 - val_loss: 0.6822 - val_acc: 0.5000\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 1.0055 - acc: 0.5432 - val_loss: 0.7875 - val_acc: 0.4375\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.9894 - acc: 0.5550 - val_loss: 0.6843 - val_acc: 0.5000\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 6s 66ms/step - loss: 0.9401 - acc: 0.5755 - val_loss: 0.8759 - val_acc: 0.4375\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.9530 - acc: 0.5660 - val_loss: 0.6643 - val_acc: 0.6250\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.9329 - acc: 0.5841 - val_loss: 0.7644 - val_acc: 0.5625\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.8892 - acc: 0.5794 - val_loss: 0.5366 - val_acc: 0.6875\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.9282 - acc: 0.5653 - val_loss: 0.7491 - val_acc: 0.5625\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.8800 - acc: 0.5888 - val_loss: 0.5884 - val_acc: 0.6875\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.8795 - acc: 0.5849 - val_loss: 0.7454 - val_acc: 0.5000\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.8271 - acc: 0.5959 - val_loss: 0.8263 - val_acc: 0.4375\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.8484 - acc: 0.6030 - val_loss: 0.6610 - val_acc: 0.6250\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.7864 - acc: 0.6164 - val_loss: 0.6630 - val_acc: 0.5625\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.8052 - acc: 0.6140 - val_loss: 0.8357 - val_acc: 0.5000\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.7824 - acc: 0.6006 - val_loss: 0.7382 - val_acc: 0.6250\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.7921 - acc: 0.6108 - val_loss: 0.4147 - val_acc: 0.7500\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.8233 - acc: 0.5991 - val_loss: 0.6308 - val_acc: 0.6250\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.7839 - acc: 0.6014 - val_loss: 0.6794 - val_acc: 0.5625\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.7651 - acc: 0.6195 - val_loss: 0.7201 - val_acc: 0.6250\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.7724 - acc: 0.6038 - val_loss: 0.6170 - val_acc: 0.6250\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.7608 - acc: 0.6203 - val_loss: 0.6966 - val_acc: 0.5625\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.7663 - acc: 0.6274 - val_loss: 0.5377 - val_acc: 0.7500\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.7642 - acc: 0.6085 - val_loss: 0.7210 - val_acc: 0.5625\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.7199 - acc: 0.6431 - val_loss: 0.5698 - val_acc: 0.6875\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.7993 - acc: 0.6014 - val_loss: 0.4719 - val_acc: 0.7500\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.6955 - acc: 0.6454 - val_loss: 0.7081 - val_acc: 0.6250\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.7333 - acc: 0.6384 - val_loss: 0.6142 - val_acc: 0.6875\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.7203 - acc: 0.6470 - val_loss: 0.7965 - val_acc: 0.5625\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.7199 - acc: 0.6376 - val_loss: 0.6897 - val_acc: 0.5625\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.7222 - acc: 0.6360 - val_loss: 0.4851 - val_acc: 0.7500\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 6s 65ms/step - loss: 0.7088 - acc: 0.6329 - val_loss: 0.6152 - val_acc: 0.5625\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.7233 - acc: 0.6447 - val_loss: 0.6641 - val_acc: 0.5625\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 6s 66ms/step - loss: 0.6941 - acc: 0.6423 - val_loss: 0.6557 - val_acc: 0.6250\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.6770 - acc: 0.6541 - val_loss: 0.6075 - val_acc: 0.6250\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 6s 65ms/step - loss: 0.6959 - acc: 0.6486 - val_loss: 0.5501 - val_acc: 0.6875\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 6s 65ms/step - loss: 0.6729 - acc: 0.6439 - val_loss: 0.7033 - val_acc: 0.5000\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.7471 - acc: 0.6093 - val_loss: 0.6268 - val_acc: 0.6875\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 6s 65ms/step - loss: 0.6654 - acc: 0.6454 - val_loss: 0.4853 - val_acc: 0.8125\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.6884 - acc: 0.6399 - val_loss: 0.5705 - val_acc: 0.6250\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.6471 - acc: 0.6698 - val_loss: 0.5333 - val_acc: 0.7500\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.6864 - acc: 0.6321 - val_loss: 0.5900 - val_acc: 0.6875\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.6520 - acc: 0.6588 - val_loss: 0.6408 - val_acc: 0.6250\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.6537 - acc: 0.6494 - val_loss: 0.6009 - val_acc: 0.6250\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.6312 - acc: 0.6706 - val_loss: 0.5047 - val_acc: 0.6250\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.6328 - acc: 0.6761 - val_loss: 0.5605 - val_acc: 0.6875\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.6601 - acc: 0.6659 - val_loss: 0.6412 - val_acc: 0.5625\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.6608 - acc: 0.6360 - val_loss: 0.6247 - val_acc: 0.6250\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.6666 - acc: 0.6541 - val_loss: 0.5222 - val_acc: 0.6875\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.6344 - acc: 0.6950 - val_loss: 0.4843 - val_acc: 0.6875\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.6563 - acc: 0.6557 - val_loss: 0.6380 - val_acc: 0.6250\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.6314 - acc: 0.6737 - val_loss: 0.5430 - val_acc: 0.6875\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.6066 - acc: 0.6808 - val_loss: 0.4460 - val_acc: 0.6875\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.6136 - acc: 0.6847 - val_loss: 0.5777 - val_acc: 0.6250\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.6257 - acc: 0.6832 - val_loss: 0.4825 - val_acc: 0.6875\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.6351 - acc: 0.6745 - val_loss: 0.5835 - val_acc: 0.6250\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.6348 - acc: 0.6675 - val_loss: 0.6463 - val_acc: 0.5625\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.6400 - acc: 0.6651 - val_loss: 0.5911 - val_acc: 0.6250\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.6326 - acc: 0.6698 - val_loss: 0.5326 - val_acc: 0.6875\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.6305 - acc: 0.6714 - val_loss: 0.5754 - val_acc: 0.6875\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 6s 66ms/step - loss: 0.6220 - acc: 0.6745 - val_loss: 0.5052 - val_acc: 0.6875\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.6240 - acc: 0.6698 - val_loss: 0.4420 - val_acc: 0.6875\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5779 - acc: 0.6997 - val_loss: 0.5069 - val_acc: 0.6250\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5835 - acc: 0.7115 - val_loss: 0.5499 - val_acc: 0.6250\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 6s 66ms/step - loss: 0.6100 - acc: 0.6800 - val_loss: 0.4773 - val_acc: 0.6875\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5999 - acc: 0.6918 - val_loss: 0.5670 - val_acc: 0.6250\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5759 - acc: 0.7186 - val_loss: 0.5558 - val_acc: 0.6250\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5952 - acc: 0.6832 - val_loss: 0.5713 - val_acc: 0.6250\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.6035 - acc: 0.6800 - val_loss: 0.5962 - val_acc: 0.6250\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.6201 - acc: 0.6737 - val_loss: 0.5661 - val_acc: 0.6250\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5939 - acc: 0.6840 - val_loss: 0.5497 - val_acc: 0.6875\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5864 - acc: 0.6800 - val_loss: 0.4368 - val_acc: 0.7500\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5831 - acc: 0.6965 - val_loss: 0.6340 - val_acc: 0.6250\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5986 - acc: 0.6847 - val_loss: 0.4974 - val_acc: 0.6875\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5790 - acc: 0.6800 - val_loss: 0.5418 - val_acc: 0.7500\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 6s 66ms/step - loss: 0.5955 - acc: 0.6926 - val_loss: 0.4415 - val_acc: 0.7500\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5873 - acc: 0.6918 - val_loss: 0.5758 - val_acc: 0.5625\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5912 - acc: 0.6965 - val_loss: 0.5801 - val_acc: 0.6250\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.5903 - acc: 0.6863 - val_loss: 0.4958 - val_acc: 0.6250\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.6043 - acc: 0.6840 - val_loss: 0.4376 - val_acc: 0.6875\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.6064 - acc: 0.6918 - val_loss: 0.5689 - val_acc: 0.6875\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5986 - acc: 0.6840 - val_loss: 0.4607 - val_acc: 0.6875\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5847 - acc: 0.6958 - val_loss: 0.5258 - val_acc: 0.5625\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5848 - acc: 0.6887 - val_loss: 0.5231 - val_acc: 0.6250\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5814 - acc: 0.6926 - val_loss: 0.4229 - val_acc: 0.8125\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5835 - acc: 0.6958 - val_loss: 0.5777 - val_acc: 0.6250\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5810 - acc: 0.6950 - val_loss: 0.5371 - val_acc: 0.5625\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5607 - acc: 0.7201 - val_loss: 0.5721 - val_acc: 0.6875\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5837 - acc: 0.7005 - val_loss: 0.5199 - val_acc: 0.6250\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5801 - acc: 0.6997 - val_loss: 0.4119 - val_acc: 0.8750\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5672 - acc: 0.6989 - val_loss: 0.5994 - val_acc: 0.5625\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5836 - acc: 0.6958 - val_loss: 0.6057 - val_acc: 0.5625\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5841 - acc: 0.6989 - val_loss: 0.5929 - val_acc: 0.6250\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5832 - acc: 0.6887 - val_loss: 0.5069 - val_acc: 0.7500\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5651 - acc: 0.7162 - val_loss: 0.4600 - val_acc: 0.8125\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5629 - acc: 0.7138 - val_loss: 0.5581 - val_acc: 0.6250\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5725 - acc: 0.7044 - val_loss: 0.5232 - val_acc: 0.6250\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5637 - acc: 0.7036 - val_loss: 0.5698 - val_acc: 0.5625\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5646 - acc: 0.7201 - val_loss: 0.5784 - val_acc: 0.6875\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5449 - acc: 0.7209 - val_loss: 0.5680 - val_acc: 0.6875\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5688 - acc: 0.7123 - val_loss: 0.5996 - val_acc: 0.5625\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5765 - acc: 0.6863 - val_loss: 0.5266 - val_acc: 0.6250\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5586 - acc: 0.7115 - val_loss: 0.5587 - val_acc: 0.6875\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5496 - acc: 0.7193 - val_loss: 0.4648 - val_acc: 0.6875\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5956 - acc: 0.6895 - val_loss: 0.5386 - val_acc: 0.5625\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5539 - acc: 0.7123 - val_loss: 0.5396 - val_acc: 0.6875\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5828 - acc: 0.7005 - val_loss: 0.4738 - val_acc: 0.7500\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5643 - acc: 0.7075 - val_loss: 0.3866 - val_acc: 0.8750\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5706 - acc: 0.7146 - val_loss: 0.5275 - val_acc: 0.6875\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5805 - acc: 0.7013 - val_loss: 0.4336 - val_acc: 0.6875\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5507 - acc: 0.7170 - val_loss: 0.4656 - val_acc: 0.6875\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5546 - acc: 0.7162 - val_loss: 0.4543 - val_acc: 0.8750\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5674 - acc: 0.6989 - val_loss: 0.5438 - val_acc: 0.6250\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5455 - acc: 0.7264 - val_loss: 0.5215 - val_acc: 0.6875\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5566 - acc: 0.7186 - val_loss: 0.5066 - val_acc: 0.6875\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5583 - acc: 0.7154 - val_loss: 0.4555 - val_acc: 0.7500\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5591 - acc: 0.7123 - val_loss: 0.4395 - val_acc: 0.8125\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5612 - acc: 0.7178 - val_loss: 0.5463 - val_acc: 0.6875\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5710 - acc: 0.7107 - val_loss: 0.5368 - val_acc: 0.7500\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5359 - acc: 0.7296 - val_loss: 0.5272 - val_acc: 0.5625\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5517 - acc: 0.7186 - val_loss: 0.5164 - val_acc: 0.6875\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5615 - acc: 0.7099 - val_loss: 0.5057 - val_acc: 0.6875\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5551 - acc: 0.7138 - val_loss: 0.4656 - val_acc: 0.7500\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5779 - acc: 0.6958 - val_loss: 0.5239 - val_acc: 0.6875\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5561 - acc: 0.7146 - val_loss: 0.4589 - val_acc: 0.8125\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5549 - acc: 0.7193 - val_loss: 0.4190 - val_acc: 0.7500\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5553 - acc: 0.7193 - val_loss: 0.4371 - val_acc: 0.8125\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5495 - acc: 0.7178 - val_loss: 0.4653 - val_acc: 0.6875\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5393 - acc: 0.7248 - val_loss: 0.4895 - val_acc: 0.7500\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5409 - acc: 0.7217 - val_loss: 0.6299 - val_acc: 0.5000\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5513 - acc: 0.7123 - val_loss: 0.5187 - val_acc: 0.7500\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5526 - acc: 0.7201 - val_loss: 0.4602 - val_acc: 0.6250\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5553 - acc: 0.7131 - val_loss: 0.4362 - val_acc: 0.8125\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5814 - acc: 0.7060 - val_loss: 0.4932 - val_acc: 0.6875\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5441 - acc: 0.7288 - val_loss: 0.6290 - val_acc: 0.5625\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5741 - acc: 0.6879 - val_loss: 0.4600 - val_acc: 0.6875\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5697 - acc: 0.7005 - val_loss: 0.4483 - val_acc: 0.8125\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5457 - acc: 0.7178 - val_loss: 0.4388 - val_acc: 0.8125\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5466 - acc: 0.7162 - val_loss: 0.5713 - val_acc: 0.5625\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5549 - acc: 0.7178 - val_loss: 0.5008 - val_acc: 0.6875\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5277 - acc: 0.7280 - val_loss: 0.5496 - val_acc: 0.7500\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5585 - acc: 0.7115 - val_loss: 0.5015 - val_acc: 0.7500\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5625 - acc: 0.7131 - val_loss: 0.4699 - val_acc: 0.6250\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5324 - acc: 0.7272 - val_loss: 0.5847 - val_acc: 0.6875\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5344 - acc: 0.7303 - val_loss: 0.4291 - val_acc: 0.8125\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5571 - acc: 0.7162 - val_loss: 0.5397 - val_acc: 0.7500\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5432 - acc: 0.7272 - val_loss: 0.4748 - val_acc: 0.6875\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5431 - acc: 0.7343 - val_loss: 0.4233 - val_acc: 0.7500\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5374 - acc: 0.7256 - val_loss: 0.5753 - val_acc: 0.5625\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5359 - acc: 0.7256 - val_loss: 0.4815 - val_acc: 0.6875\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5565 - acc: 0.7005 - val_loss: 0.5680 - val_acc: 0.6250\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5376 - acc: 0.7311 - val_loss: 0.3818 - val_acc: 0.8125\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5454 - acc: 0.7131 - val_loss: 0.4891 - val_acc: 0.8125\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5462 - acc: 0.7241 - val_loss: 0.4998 - val_acc: 0.6250\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5458 - acc: 0.7154 - val_loss: 0.5191 - val_acc: 0.6875\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5375 - acc: 0.7256 - val_loss: 0.5574 - val_acc: 0.6875\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5346 - acc: 0.7382 - val_loss: 0.5685 - val_acc: 0.5625\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5565 - acc: 0.7091 - val_loss: 0.4524 - val_acc: 0.7500\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5594 - acc: 0.7115 - val_loss: 0.5047 - val_acc: 0.7500\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5483 - acc: 0.7091 - val_loss: 0.5963 - val_acc: 0.5000\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5595 - acc: 0.7115 - val_loss: 0.5158 - val_acc: 0.6875\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5422 - acc: 0.7186 - val_loss: 0.4823 - val_acc: 0.7500\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5523 - acc: 0.7209 - val_loss: 0.4413 - val_acc: 0.6875\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5460 - acc: 0.7209 - val_loss: 0.4645 - val_acc: 0.7500\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5423 - acc: 0.7296 - val_loss: 0.4789 - val_acc: 0.7500\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5502 - acc: 0.7225 - val_loss: 0.4098 - val_acc: 0.8750\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5559 - acc: 0.7075 - val_loss: 0.4716 - val_acc: 0.6875\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5306 - acc: 0.7398 - val_loss: 0.4534 - val_acc: 0.7500\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5513 - acc: 0.7146 - val_loss: 0.4574 - val_acc: 0.7500\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5468 - acc: 0.7154 - val_loss: 0.5997 - val_acc: 0.6250\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5517 - acc: 0.7052 - val_loss: 0.5744 - val_acc: 0.5625\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5267 - acc: 0.7311 - val_loss: 0.5752 - val_acc: 0.6875\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5314 - acc: 0.7233 - val_loss: 0.4521 - val_acc: 0.6875\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5308 - acc: 0.7264 - val_loss: 0.5675 - val_acc: 0.6875\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5338 - acc: 0.7500 - val_loss: 0.5611 - val_acc: 0.6875\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5171 - acc: 0.7445 - val_loss: 0.4900 - val_acc: 0.6875\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5363 - acc: 0.7366 - val_loss: 0.5175 - val_acc: 0.6875\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5402 - acc: 0.7154 - val_loss: 0.4032 - val_acc: 0.8125\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5369 - acc: 0.7382 - val_loss: 0.5051 - val_acc: 0.7500\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5274 - acc: 0.7248 - val_loss: 0.5414 - val_acc: 0.6875\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5418 - acc: 0.7241 - val_loss: 0.4288 - val_acc: 0.8125\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5395 - acc: 0.7390 - val_loss: 0.4659 - val_acc: 0.7500\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5492 - acc: 0.7178 - val_loss: 0.5095 - val_acc: 0.6875\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5265 - acc: 0.7248 - val_loss: 0.4284 - val_acc: 0.7500\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5434 - acc: 0.7131 - val_loss: 0.4345 - val_acc: 0.8125\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5554 - acc: 0.7178 - val_loss: 0.5254 - val_acc: 0.6250\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5302 - acc: 0.7233 - val_loss: 0.4649 - val_acc: 0.8125\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5509 - acc: 0.7248 - val_loss: 0.5001 - val_acc: 0.7500\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5345 - acc: 0.7335 - val_loss: 0.4958 - val_acc: 0.6250\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5532 - acc: 0.7303 - val_loss: 0.4692 - val_acc: 0.7500\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5253 - acc: 0.7461 - val_loss: 0.5047 - val_acc: 0.7500\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5413 - acc: 0.7264 - val_loss: 0.4325 - val_acc: 0.8125\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5575 - acc: 0.7146 - val_loss: 0.4915 - val_acc: 0.7500\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5339 - acc: 0.7248 - val_loss: 0.5551 - val_acc: 0.6250\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5354 - acc: 0.7178 - val_loss: 0.5300 - val_acc: 0.6875\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5466 - acc: 0.7288 - val_loss: 0.4269 - val_acc: 0.7500\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5365 - acc: 0.7248 - val_loss: 0.5396 - val_acc: 0.6875\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5443 - acc: 0.7311 - val_loss: 0.4600 - val_acc: 0.8125\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5245 - acc: 0.7296 - val_loss: 0.4297 - val_acc: 0.8125\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5315 - acc: 0.7343 - val_loss: 0.4581 - val_acc: 0.7500\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5332 - acc: 0.7445 - val_loss: 0.5125 - val_acc: 0.6250\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5453 - acc: 0.7335 - val_loss: 0.5050 - val_acc: 0.6875\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5277 - acc: 0.7193 - val_loss: 0.5189 - val_acc: 0.6250\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5462 - acc: 0.7241 - val_loss: 0.4965 - val_acc: 0.7500\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5489 - acc: 0.7146 - val_loss: 0.5956 - val_acc: 0.5625\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5094 - acc: 0.7351 - val_loss: 0.5546 - val_acc: 0.6250\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5276 - acc: 0.7358 - val_loss: 0.4706 - val_acc: 0.6875\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5156 - acc: 0.7626 - val_loss: 0.5485 - val_acc: 0.6250\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5264 - acc: 0.7343 - val_loss: 0.5181 - val_acc: 0.7500\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5535 - acc: 0.7201 - val_loss: 0.4435 - val_acc: 0.8125\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5378 - acc: 0.7390 - val_loss: 0.5480 - val_acc: 0.6250\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5436 - acc: 0.7256 - val_loss: 0.4999 - val_acc: 0.7500\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5394 - acc: 0.7217 - val_loss: 0.5293 - val_acc: 0.7500\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5318 - acc: 0.7319 - val_loss: 0.4830 - val_acc: 0.6875\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5437 - acc: 0.7217 - val_loss: 0.5993 - val_acc: 0.6250\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5270 - acc: 0.7539 - val_loss: 0.5858 - val_acc: 0.6875\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5413 - acc: 0.7241 - val_loss: 0.5042 - val_acc: 0.6875\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.5413 - acc: 0.7241Epoch 221/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5261 - acc: 0.7241 - val_loss: 0.4874 - val_acc: 0.8125\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5536 - acc: 0.7225 - val_loss: 0.6161 - val_acc: 0.5625\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5301 - acc: 0.7280 - val_loss: 0.5761 - val_acc: 0.6875\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5271 - acc: 0.7524 - val_loss: 0.3764 - val_acc: 0.6875\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5406 - acc: 0.7248 - val_loss: 0.4395 - val_acc: 0.6875\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5315 - acc: 0.7406 - val_loss: 0.4587 - val_acc: 0.6875\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5343 - acc: 0.7335 - val_loss: 0.3926 - val_acc: 0.8750\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5284 - acc: 0.7311 - val_loss: 0.5693 - val_acc: 0.6250\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5296 - acc: 0.7217 - val_loss: 0.4216 - val_acc: 0.8125\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 8s 82ms/step - loss: 0.5254 - acc: 0.7358 - val_loss: 0.4413 - val_acc: 0.6875\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5364 - acc: 0.7272 - val_loss: 0.4199 - val_acc: 0.6875\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5375 - acc: 0.7311 - val_loss: 0.4902 - val_acc: 0.6250\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5338 - acc: 0.7366 - val_loss: 0.5397 - val_acc: 0.6875\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 7s 70ms/step - loss: 0.5453 - acc: 0.7264 - val_loss: 0.4875 - val_acc: 0.6875\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5491 - acc: 0.7123 - val_loss: 0.3949 - val_acc: 0.7500\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5407 - acc: 0.7256 - val_loss: 0.5993 - val_acc: 0.6250\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5363 - acc: 0.7256 - val_loss: 0.5201 - val_acc: 0.6875\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5385 - acc: 0.7343 - val_loss: 0.4978 - val_acc: 0.6250\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5363 - acc: 0.7280 - val_loss: 0.6201 - val_acc: 0.5625\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5438 - acc: 0.7241 - val_loss: 0.5901 - val_acc: 0.6250\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5237 - acc: 0.7366 - val_loss: 0.4154 - val_acc: 0.7500\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5360 - acc: 0.7209 - val_loss: 0.3685 - val_acc: 0.8125\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5312 - acc: 0.7414 - val_loss: 0.5426 - val_acc: 0.7500\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5236 - acc: 0.7398 - val_loss: 0.4424 - val_acc: 0.6875\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5257 - acc: 0.7414 - val_loss: 0.5057 - val_acc: 0.6875\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5483 - acc: 0.7296 - val_loss: 0.4852 - val_acc: 0.6875\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5267 - acc: 0.7500 - val_loss: 0.3797 - val_acc: 0.8125\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5258 - acc: 0.7280 - val_loss: 0.5156 - val_acc: 0.7500\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5534 - acc: 0.7201 - val_loss: 0.4790 - val_acc: 0.7500\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5303 - acc: 0.7272 - val_loss: 0.5436 - val_acc: 0.6875\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5351 - acc: 0.7288 - val_loss: 0.5522 - val_acc: 0.6875\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5590 - acc: 0.7115 - val_loss: 0.5139 - val_acc: 0.7500\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5358 - acc: 0.7264 - val_loss: 0.4668 - val_acc: 0.7500\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5198 - acc: 0.7492 - val_loss: 0.5555 - val_acc: 0.7500\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5319 - acc: 0.7327 - val_loss: 0.5146 - val_acc: 0.7500\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5332 - acc: 0.7303 - val_loss: 0.5450 - val_acc: 0.6875\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5365 - acc: 0.7303 - val_loss: 0.5242 - val_acc: 0.7500\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5543 - acc: 0.7248 - val_loss: 0.4102 - val_acc: 0.7500\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5392 - acc: 0.7225 - val_loss: 0.4645 - val_acc: 0.8125\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5328 - acc: 0.7311 - val_loss: 0.5576 - val_acc: 0.7500\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5370 - acc: 0.7366 - val_loss: 0.4733 - val_acc: 0.6875\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5521 - acc: 0.7280 - val_loss: 0.4881 - val_acc: 0.7500\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5530 - acc: 0.7256 - val_loss: 0.5002 - val_acc: 0.7500\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5453 - acc: 0.7319 - val_loss: 0.4898 - val_acc: 0.7500\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5264 - acc: 0.7296 - val_loss: 0.4008 - val_acc: 0.8125\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5229 - acc: 0.7351 - val_loss: 0.4905 - val_acc: 0.7500\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5302 - acc: 0.7461 - val_loss: 0.4409 - val_acc: 0.7500\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5333 - acc: 0.7366 - val_loss: 0.5271 - val_acc: 0.6875\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5172 - acc: 0.7437 - val_loss: 0.4234 - val_acc: 0.8125\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5326 - acc: 0.7406 - val_loss: 0.4864 - val_acc: 0.8125\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5408 - acc: 0.7272 - val_loss: 0.5309 - val_acc: 0.6250\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5213 - acc: 0.7421 - val_loss: 0.6160 - val_acc: 0.6250\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5459 - acc: 0.7170 - val_loss: 0.6781 - val_acc: 0.6250\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5239 - acc: 0.7421 - val_loss: 0.5208 - val_acc: 0.6875\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5427 - acc: 0.7303 - val_loss: 0.5843 - val_acc: 0.6250\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5364 - acc: 0.7311 - val_loss: 0.4259 - val_acc: 0.7500\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5323 - acc: 0.7366 - val_loss: 0.4480 - val_acc: 0.8125\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5271 - acc: 0.7296 - val_loss: 0.4995 - val_acc: 0.7500\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5229 - acc: 0.7366 - val_loss: 0.5001 - val_acc: 0.7500\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5413 - acc: 0.7256 - val_loss: 0.5419 - val_acc: 0.6250\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5411 - acc: 0.7264 - val_loss: 0.5432 - val_acc: 0.6875\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5291 - acc: 0.7358 - val_loss: 0.5448 - val_acc: 0.6875\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5469 - acc: 0.7162 - val_loss: 0.4612 - val_acc: 0.7500\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5290 - acc: 0.7516 - val_loss: 0.4744 - val_acc: 0.8125\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5230 - acc: 0.7516 - val_loss: 0.5949 - val_acc: 0.6875\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 7s 71ms/step - loss: 0.5395 - acc: 0.7366 - val_loss: 0.5039 - val_acc: 0.7500\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5339 - acc: 0.7351 - val_loss: 0.5576 - val_acc: 0.6875\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5368 - acc: 0.7382 - val_loss: 0.4749 - val_acc: 0.7500\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5251 - acc: 0.7319 - val_loss: 0.5752 - val_acc: 0.5625\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5355 - acc: 0.7288 - val_loss: 0.5695 - val_acc: 0.6875\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5412 - acc: 0.7311 - val_loss: 0.5377 - val_acc: 0.6250\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5320 - acc: 0.7280 - val_loss: 0.3976 - val_acc: 0.8750\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5380 - acc: 0.7170 - val_loss: 0.5359 - val_acc: 0.6250\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5286 - acc: 0.7484 - val_loss: 0.4907 - val_acc: 0.6875\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5386 - acc: 0.7406 - val_loss: 0.4837 - val_acc: 0.8125\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5335 - acc: 0.7311 - val_loss: 0.4682 - val_acc: 0.7500\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5412 - acc: 0.7162 - val_loss: 0.5352 - val_acc: 0.6875\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5257 - acc: 0.7469 - val_loss: 0.4661 - val_acc: 0.8125\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5270 - acc: 0.7272 - val_loss: 0.3834 - val_acc: 0.7500\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5441 - acc: 0.7343 - val_loss: 0.3873 - val_acc: 0.8750\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5287 - acc: 0.7327 - val_loss: 0.5140 - val_acc: 0.8125\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5453 - acc: 0.7288 - val_loss: 0.4830 - val_acc: 0.6875\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5390 - acc: 0.7209 - val_loss: 0.5331 - val_acc: 0.6875\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5239 - acc: 0.7382 - val_loss: 0.5945 - val_acc: 0.6250\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5526 - acc: 0.7131 - val_loss: 0.5571 - val_acc: 0.6875\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5242 - acc: 0.7406 - val_loss: 0.6232 - val_acc: 0.6250\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5373 - acc: 0.7217 - val_loss: 0.5437 - val_acc: 0.6875\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5377 - acc: 0.7311 - val_loss: 0.5023 - val_acc: 0.7500\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5247 - acc: 0.7461 - val_loss: 0.5658 - val_acc: 0.6250\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5471 - acc: 0.7288 - val_loss: 0.4984 - val_acc: 0.8125\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5204 - acc: 0.7351 - val_loss: 0.5234 - val_acc: 0.6875\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5351 - acc: 0.7233 - val_loss: 0.5349 - val_acc: 0.6875\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 9s 103ms/step - loss: 0.5309 - acc: 0.7390 - val_loss: 0.4270 - val_acc: 0.7500\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5372 - acc: 0.7272 - val_loss: 0.5886 - val_acc: 0.6875\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 9s 104ms/step - loss: 0.5376 - acc: 0.7217 - val_loss: 0.4692 - val_acc: 0.7500\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5252 - acc: 0.7500 - val_loss: 0.5525 - val_acc: 0.7500\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5348 - acc: 0.7343 - val_loss: 0.3759 - val_acc: 0.8125\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5215 - acc: 0.7358 - val_loss: 0.5459 - val_acc: 0.7500\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5330 - acc: 0.7429 - val_loss: 0.6764 - val_acc: 0.5000\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5116 - acc: 0.7406 - val_loss: 0.4874 - val_acc: 0.7500\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5297 - acc: 0.7225 - val_loss: 0.5371 - val_acc: 0.7500\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5319 - acc: 0.7319 - val_loss: 0.6134 - val_acc: 0.6875\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5295 - acc: 0.7319 - val_loss: 0.2899 - val_acc: 0.9375\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5219 - acc: 0.7398 - val_loss: 0.5405 - val_acc: 0.6250\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5340 - acc: 0.7311 - val_loss: 0.3105 - val_acc: 0.8750\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5516 - acc: 0.7146 - val_loss: 0.5031 - val_acc: 0.7500\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5317 - acc: 0.7374 - val_loss: 0.4705 - val_acc: 0.8125\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5238 - acc: 0.7288 - val_loss: 0.6356 - val_acc: 0.5625\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5309 - acc: 0.7374 - val_loss: 0.3992 - val_acc: 0.8125\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5575 - acc: 0.7193 - val_loss: 0.5826 - val_acc: 0.6875\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5339 - acc: 0.7241 - val_loss: 0.5858 - val_acc: 0.6250\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5463 - acc: 0.7382 - val_loss: 0.5071 - val_acc: 0.7500\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5330 - acc: 0.7351 - val_loss: 0.5713 - val_acc: 0.7500\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5340 - acc: 0.7217 - val_loss: 0.4734 - val_acc: 0.8125\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5584 - acc: 0.7123 - val_loss: 0.4979 - val_acc: 0.6875\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5269 - acc: 0.7469 - val_loss: 0.5808 - val_acc: 0.6250\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5327 - acc: 0.7358 - val_loss: 0.5167 - val_acc: 0.6875\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 7s 68ms/step - loss: 0.5388 - acc: 0.7358 - val_loss: 0.4978 - val_acc: 0.6875\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5204 - acc: 0.7461 - val_loss: 0.4484 - val_acc: 0.7500\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5485 - acc: 0.7233 - val_loss: 0.3951 - val_acc: 0.7500\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5282 - acc: 0.7311 - val_loss: 0.5464 - val_acc: 0.6875\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5350 - acc: 0.7248 - val_loss: 0.4756 - val_acc: 0.8125\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5299 - acc: 0.7343 - val_loss: 0.3379 - val_acc: 0.8750\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5191 - acc: 0.7508 - val_loss: 0.5474 - val_acc: 0.7500\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5132 - acc: 0.7516 - val_loss: 0.5676 - val_acc: 0.6250\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5282 - acc: 0.7414 - val_loss: 0.5310 - val_acc: 0.6875\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5356 - acc: 0.7445 - val_loss: 0.5989 - val_acc: 0.6875\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5279 - acc: 0.7382 - val_loss: 0.4760 - val_acc: 0.6250\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5392 - acc: 0.7343 - val_loss: 0.5371 - val_acc: 0.7500\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 7s 71ms/step - loss: 0.5278 - acc: 0.7414 - val_loss: 0.4862 - val_acc: 0.7500\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5212 - acc: 0.7374 - val_loss: 0.5000 - val_acc: 0.6875\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5301 - acc: 0.7335 - val_loss: 0.5430 - val_acc: 0.6875\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5380 - acc: 0.7406 - val_loss: 0.5485 - val_acc: 0.6875\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5196 - acc: 0.7414 - val_loss: 0.4929 - val_acc: 0.8125\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5167 - acc: 0.7492 - val_loss: 0.6174 - val_acc: 0.6250\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5368 - acc: 0.7256 - val_loss: 0.5036 - val_acc: 0.8125\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5445 - acc: 0.7209 - val_loss: 0.6106 - val_acc: 0.6250\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5345 - acc: 0.7296 - val_loss: 0.5098 - val_acc: 0.6875\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5424 - acc: 0.7319 - val_loss: 0.5032 - val_acc: 0.6875\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5348 - acc: 0.7398 - val_loss: 0.5702 - val_acc: 0.6250\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5130 - acc: 0.7453 - val_loss: 0.4540 - val_acc: 0.8125\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5262 - acc: 0.7484 - val_loss: 0.5834 - val_acc: 0.6250\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5203 - acc: 0.7374 - val_loss: 0.4761 - val_acc: 0.7500\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5198 - acc: 0.7414 - val_loss: 0.4716 - val_acc: 0.6875\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5332 - acc: 0.7280 - val_loss: 0.5563 - val_acc: 0.6875\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5427 - acc: 0.7217 - val_loss: 0.4456 - val_acc: 0.7500\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5313 - acc: 0.7406 - val_loss: 0.3433 - val_acc: 0.8750\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 8s 83ms/step - loss: 0.5236 - acc: 0.7358 - val_loss: 0.5711 - val_acc: 0.6875\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5162 - acc: 0.7429 - val_loss: 0.4964 - val_acc: 0.7500\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5333 - acc: 0.7335 - val_loss: 0.5187 - val_acc: 0.6875\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5272 - acc: 0.7296 - val_loss: 0.4724 - val_acc: 0.6875\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5399 - acc: 0.7351 - val_loss: 0.5762 - val_acc: 0.6250\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5226 - acc: 0.7524 - val_loss: 0.4606 - val_acc: 0.7500\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5367 - acc: 0.7351 - val_loss: 0.4954 - val_acc: 0.6875\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5241 - acc: 0.7366 - val_loss: 0.5479 - val_acc: 0.6250\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5314 - acc: 0.7311 - val_loss: 0.4896 - val_acc: 0.7500\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5320 - acc: 0.7296 - val_loss: 0.5031 - val_acc: 0.7500\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5326 - acc: 0.7445 - val_loss: 0.4120 - val_acc: 0.6875\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5401 - acc: 0.7366 - val_loss: 0.5918 - val_acc: 0.6250\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5378 - acc: 0.7358 - val_loss: 0.6417 - val_acc: 0.6250\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5465 - acc: 0.7083 - val_loss: 0.5875 - val_acc: 0.6875\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5393 - acc: 0.7327 - val_loss: 0.3408 - val_acc: 0.8750\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5353 - acc: 0.7382 - val_loss: 0.4978 - val_acc: 0.7500\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5289 - acc: 0.7311 - val_loss: 0.5559 - val_acc: 0.6875\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5448 - acc: 0.7264 - val_loss: 0.5172 - val_acc: 0.7500\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5246 - acc: 0.7406 - val_loss: 0.6645 - val_acc: 0.6250\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5235 - acc: 0.7288 - val_loss: 0.6233 - val_acc: 0.6875\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 9s 98ms/step - loss: 0.5275 - acc: 0.7272 - val_loss: 0.5059 - val_acc: 0.7500\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5504 - acc: 0.7131 - val_loss: 0.4886 - val_acc: 0.7500\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5238 - acc: 0.7421 - val_loss: 0.5100 - val_acc: 0.7500\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5307 - acc: 0.7343 - val_loss: 0.5237 - val_acc: 0.6250\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5102 - acc: 0.7421 - val_loss: 0.4852 - val_acc: 0.8125\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5476 - acc: 0.7256 - val_loss: 0.4546 - val_acc: 0.8125\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5261 - acc: 0.7437 - val_loss: 0.4337 - val_acc: 0.7500\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5393 - acc: 0.7358 - val_loss: 0.5380 - val_acc: 0.7500\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5207 - acc: 0.7484 - val_loss: 0.4557 - val_acc: 0.7500\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5165 - acc: 0.7351 - val_loss: 0.4980 - val_acc: 0.8125\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5216 - acc: 0.7469 - val_loss: 0.4565 - val_acc: 0.8125\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5439 - acc: 0.7437 - val_loss: 0.4719 - val_acc: 0.7500\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5506 - acc: 0.7248 - val_loss: 0.4633 - val_acc: 0.8125\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5296 - acc: 0.7319 - val_loss: 0.5598 - val_acc: 0.7500\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5160 - acc: 0.7382 - val_loss: 0.4272 - val_acc: 0.8750\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5287 - acc: 0.7311 - val_loss: 0.5088 - val_acc: 0.8125\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5262 - acc: 0.7358 - val_loss: 0.4618 - val_acc: 0.7500\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5355 - acc: 0.7233 - val_loss: 0.4087 - val_acc: 0.8125\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5346 - acc: 0.7225 - val_loss: 0.5145 - val_acc: 0.6875\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5264 - acc: 0.7414 - val_loss: 0.5535 - val_acc: 0.6875\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5380 - acc: 0.7374 - val_loss: 0.5604 - val_acc: 0.6875\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5365 - acc: 0.7256 - val_loss: 0.4942 - val_acc: 0.6250\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5320 - acc: 0.7366 - val_loss: 0.4149 - val_acc: 0.8125\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5260 - acc: 0.7445 - val_loss: 0.4307 - val_acc: 0.7500\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5353 - acc: 0.7335 - val_loss: 0.4684 - val_acc: 0.6875\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5454 - acc: 0.7351 - val_loss: 0.6130 - val_acc: 0.6250\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5426 - acc: 0.7123 - val_loss: 0.4276 - val_acc: 0.8125\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 8s 98ms/step - loss: 0.5102 - acc: 0.7508 - val_loss: 0.5800 - val_acc: 0.6250\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5434 - acc: 0.7351 - val_loss: 0.6561 - val_acc: 0.6250\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5047 - acc: 0.7524 - val_loss: 0.5644 - val_acc: 0.7500\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 6s 68ms/step - loss: 0.5373 - acc: 0.7319 - val_loss: 0.4598 - val_acc: 0.8125\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5204 - acc: 0.7382 - val_loss: 0.3868 - val_acc: 0.8125\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5366 - acc: 0.7201 - val_loss: 0.5136 - val_acc: 0.6875\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5481 - acc: 0.7319 - val_loss: 0.4504 - val_acc: 0.7500\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5317 - acc: 0.7366 - val_loss: 0.4090 - val_acc: 0.8750\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5321 - acc: 0.7311 - val_loss: 0.5029 - val_acc: 0.7500\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5333 - acc: 0.7406 - val_loss: 0.5866 - val_acc: 0.6250\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 9s 99ms/step - loss: 0.5285 - acc: 0.7366 - val_loss: 0.5624 - val_acc: 0.6875\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5394 - acc: 0.7374 - val_loss: 0.4604 - val_acc: 0.7500\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5425 - acc: 0.7170 - val_loss: 0.5627 - val_acc: 0.6875\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5431 - acc: 0.7241 - val_loss: 0.4859 - val_acc: 0.7500\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5550 - acc: 0.7241 - val_loss: 0.4951 - val_acc: 0.7500\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5079 - acc: 0.7445 - val_loss: 0.5532 - val_acc: 0.6875\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5482 - acc: 0.7382 - val_loss: 0.5014 - val_acc: 0.7500\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5580 - acc: 0.7296 - val_loss: 0.3841 - val_acc: 0.8125\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 7s 71ms/step - loss: 0.5206 - acc: 0.7453 - val_loss: 0.5123 - val_acc: 0.7500\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5269 - acc: 0.7414 - val_loss: 0.5236 - val_acc: 0.7500\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5418 - acc: 0.7233 - val_loss: 0.4349 - val_acc: 0.7500\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5287 - acc: 0.7421 - val_loss: 0.4193 - val_acc: 0.7500\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5328 - acc: 0.7335 - val_loss: 0.4600 - val_acc: 0.6875\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5293 - acc: 0.7366 - val_loss: 0.5014 - val_acc: 0.6875\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5488 - acc: 0.7241 - val_loss: 0.5338 - val_acc: 0.6875\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5097 - acc: 0.7351 - val_loss: 0.5281 - val_acc: 0.7500\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5348 - acc: 0.7288 - val_loss: 0.5838 - val_acc: 0.6875\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5296 - acc: 0.7453 - val_loss: 0.5322 - val_acc: 0.6250\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5194 - acc: 0.7539 - val_loss: 0.4669 - val_acc: 0.7500\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5338 - acc: 0.7303 - val_loss: 0.5670 - val_acc: 0.6875\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5341 - acc: 0.7476 - val_loss: 0.6066 - val_acc: 0.6875\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5395 - acc: 0.7327 - val_loss: 0.5857 - val_acc: 0.6250\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5371 - acc: 0.7358 - val_loss: 0.5019 - val_acc: 0.6875\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5275 - acc: 0.7421 - val_loss: 0.4238 - val_acc: 0.6875\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5379 - acc: 0.7327 - val_loss: 0.5321 - val_acc: 0.6875\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5337 - acc: 0.7382 - val_loss: 0.5270 - val_acc: 0.6875\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5248 - acc: 0.7327 - val_loss: 0.3024 - val_acc: 0.8750\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5231 - acc: 0.7484 - val_loss: 0.4976 - val_acc: 0.7500\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5311 - acc: 0.7366 - val_loss: 0.4737 - val_acc: 0.8125\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5418 - acc: 0.7146 - val_loss: 0.5762 - val_acc: 0.6875\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5422 - acc: 0.7233 - val_loss: 0.4310 - val_acc: 0.8125\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5217 - acc: 0.7516 - val_loss: 0.5523 - val_acc: 0.6875\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5261 - acc: 0.7453 - val_loss: 0.5783 - val_acc: 0.6250\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5599 - acc: 0.7146 - val_loss: 0.4158 - val_acc: 0.7500\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5268 - acc: 0.7453 - val_loss: 0.5463 - val_acc: 0.6875\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5541 - acc: 0.7296 - val_loss: 0.4957 - val_acc: 0.6875\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5156 - acc: 0.7539 - val_loss: 0.5325 - val_acc: 0.6250\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5381 - acc: 0.7248 - val_loss: 0.5547 - val_acc: 0.6875\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5141 - acc: 0.7531 - val_loss: 0.5458 - val_acc: 0.6250\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5404 - acc: 0.7343 - val_loss: 0.4250 - val_acc: 0.8125\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 9s 106ms/step - loss: 0.5370 - acc: 0.7170 - val_loss: 0.5013 - val_acc: 0.6875\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5429 - acc: 0.7241 - val_loss: 0.4909 - val_acc: 0.7500\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5347 - acc: 0.7248 - val_loss: 0.4780 - val_acc: 0.7500\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5379 - acc: 0.7445 - val_loss: 0.5525 - val_acc: 0.7500\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5515 - acc: 0.7343 - val_loss: 0.4826 - val_acc: 0.7500\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 7s 71ms/step - loss: 0.5124 - acc: 0.7508 - val_loss: 0.5418 - val_acc: 0.7500\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5431 - acc: 0.7343 - val_loss: 0.4860 - val_acc: 0.8125\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5229 - acc: 0.7429 - val_loss: 0.4855 - val_acc: 0.6875\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5381 - acc: 0.7351 - val_loss: 0.4572 - val_acc: 0.8125\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5323 - acc: 0.7335 - val_loss: 0.5495 - val_acc: 0.6875\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5334 - acc: 0.7233 - val_loss: 0.5752 - val_acc: 0.6875\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5220 - acc: 0.7358 - val_loss: 0.4357 - val_acc: 0.8125\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5151 - acc: 0.7358 - val_loss: 0.4390 - val_acc: 0.7500\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5381 - acc: 0.7374 - val_loss: 0.4505 - val_acc: 0.8125\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5187 - acc: 0.7382 - val_loss: 0.4013 - val_acc: 0.7500\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5475 - acc: 0.7382 - val_loss: 0.5644 - val_acc: 0.6250\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5365 - acc: 0.7256 - val_loss: 0.5252 - val_acc: 0.7500\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5220 - acc: 0.7303 - val_loss: 0.4986 - val_acc: 0.6875\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5266 - acc: 0.7461 - val_loss: 0.3728 - val_acc: 0.8125\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5365 - acc: 0.7469 - val_loss: 0.4785 - val_acc: 0.8125\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5377 - acc: 0.7264 - val_loss: 0.3760 - val_acc: 0.8125\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5360 - acc: 0.7241 - val_loss: 0.5466 - val_acc: 0.6875\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5335 - acc: 0.7319 - val_loss: 0.4517 - val_acc: 0.8125\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5535 - acc: 0.7264 - val_loss: 0.3279 - val_acc: 0.8750\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 6s 67ms/step - loss: 0.5203 - acc: 0.7508 - val_loss: 0.4025 - val_acc: 0.8125\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5301 - acc: 0.7421 - val_loss: 0.5432 - val_acc: 0.8125\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5083 - acc: 0.7563 - val_loss: 0.5996 - val_acc: 0.6875\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5356 - acc: 0.7382 - val_loss: 0.6070 - val_acc: 0.6250\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5483 - acc: 0.7186 - val_loss: 0.6203 - val_acc: 0.6250\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5507 - acc: 0.7209 - val_loss: 0.5295 - val_acc: 0.7500\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5139 - acc: 0.7516 - val_loss: 0.5625 - val_acc: 0.7500\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5410 - acc: 0.7241 - val_loss: 0.4723 - val_acc: 0.6250\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 7s 70ms/step - loss: 0.5548 - acc: 0.7209 - val_loss: 0.5701 - val_acc: 0.6250\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.5524 - acc: 0.7343 - val_loss: 0.4880 - val_acc: 0.8125\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 8s 97ms/step - loss: 0.5309 - acc: 0.7327 - val_loss: 0.5071 - val_acc: 0.8125\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5267 - acc: 0.7248 - val_loss: 0.4350 - val_acc: 0.7500\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5204 - acc: 0.7374 - val_loss: 0.4780 - val_acc: 0.7500\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5247 - acc: 0.7453 - val_loss: 0.4057 - val_acc: 0.7500\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5365 - acc: 0.7233 - val_loss: 0.4462 - val_acc: 0.8125\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5257 - acc: 0.7421 - val_loss: 0.5279 - val_acc: 0.6250\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5269 - acc: 0.7382 - val_loss: 0.5608 - val_acc: 0.6250\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5515 - acc: 0.7327 - val_loss: 0.5303 - val_acc: 0.7500\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5414 - acc: 0.7280 - val_loss: 0.6068 - val_acc: 0.6875\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5584 - acc: 0.7288 - val_loss: 0.5525 - val_acc: 0.6875\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5411 - acc: 0.7358 - val_loss: 0.6105 - val_acc: 0.6875\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5372 - acc: 0.7272 - val_loss: 0.4164 - val_acc: 0.8125\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5134 - acc: 0.7335 - val_loss: 0.4466 - val_acc: 0.7500\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5384 - acc: 0.7366 - val_loss: 0.5157 - val_acc: 0.6875\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5032 - acc: 0.7563 - val_loss: 0.4588 - val_acc: 0.6875\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5319 - acc: 0.7319 - val_loss: 0.5387 - val_acc: 0.6875\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5254 - acc: 0.7303 - val_loss: 0.4533 - val_acc: 0.7500\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5521 - acc: 0.7178 - val_loss: 0.5259 - val_acc: 0.7500\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 8s 82ms/step - loss: 0.5398 - acc: 0.7311 - val_loss: 0.5073 - val_acc: 0.7500\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5457 - acc: 0.7296 - val_loss: 0.4439 - val_acc: 0.7500\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5279 - acc: 0.7429 - val_loss: 0.3407 - val_acc: 0.8750\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5315 - acc: 0.7319 - val_loss: 0.5384 - val_acc: 0.6875\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5373 - acc: 0.7288 - val_loss: 0.5368 - val_acc: 0.7500\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5294 - acc: 0.7382 - val_loss: 0.4472 - val_acc: 0.6875\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5540 - acc: 0.7162 - val_loss: 0.5366 - val_acc: 0.6875\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 8s 81ms/step - loss: 0.5233 - acc: 0.7453 - val_loss: 0.4845 - val_acc: 0.6875\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5267 - acc: 0.7366 - val_loss: 0.5216 - val_acc: 0.6875\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5231 - acc: 0.7382 - val_loss: 0.5003 - val_acc: 0.7500\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5284 - acc: 0.7382 - val_loss: 0.3721 - val_acc: 0.7500\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5216 - acc: 0.7358 - val_loss: 0.4808 - val_acc: 0.7500\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5359 - acc: 0.7374 - val_loss: 0.4677 - val_acc: 0.6875\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5202 - acc: 0.7421 - val_loss: 0.3802 - val_acc: 0.7500\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5398 - acc: 0.7398 - val_loss: 0.5622 - val_acc: 0.6250\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5188 - acc: 0.7484 - val_loss: 0.5948 - val_acc: 0.6875\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5292 - acc: 0.7241 - val_loss: 0.4772 - val_acc: 0.7500\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5404 - acc: 0.7272 - val_loss: 0.4765 - val_acc: 0.6875\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5391 - acc: 0.7225 - val_loss: 0.4081 - val_acc: 0.7500\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5354 - acc: 0.7280 - val_loss: 0.4944 - val_acc: 0.6250\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5373 - acc: 0.7296 - val_loss: 0.4684 - val_acc: 0.8125\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5286 - acc: 0.7327 - val_loss: 0.5725 - val_acc: 0.6875\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5098 - acc: 0.7500 - val_loss: 0.6239 - val_acc: 0.6875\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 9s 100ms/step - loss: 0.5296 - acc: 0.7390 - val_loss: 0.4523 - val_acc: 0.6875\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5425 - acc: 0.7327 - val_loss: 0.5067 - val_acc: 0.6875\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5390 - acc: 0.7264 - val_loss: 0.4368 - val_acc: 0.8125\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5578 - acc: 0.7319 - val_loss: 0.4141 - val_acc: 0.8125\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5231 - acc: 0.7288 - val_loss: 0.4718 - val_acc: 0.8125\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5434 - acc: 0.7335 - val_loss: 0.5506 - val_acc: 0.6250\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5408 - acc: 0.7138 - val_loss: 0.4714 - val_acc: 0.7500\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5258 - acc: 0.7539 - val_loss: 0.5646 - val_acc: 0.6875\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 9s 96ms/step - loss: 0.5381 - acc: 0.7272 - val_loss: 0.4234 - val_acc: 0.8125\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5456 - acc: 0.7075 - val_loss: 0.6222 - val_acc: 0.6250\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 8s 83ms/step - loss: 0.5219 - acc: 0.7382 - val_loss: 0.5590 - val_acc: 0.6875\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5411 - acc: 0.7280 - val_loss: 0.4964 - val_acc: 0.7500\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5098 - acc: 0.7484 - val_loss: 0.3708 - val_acc: 0.8750\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5258 - acc: 0.7406 - val_loss: 0.4688 - val_acc: 0.6875\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5442 - acc: 0.7437 - val_loss: 0.5424 - val_acc: 0.6875\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5280 - acc: 0.7469 - val_loss: 0.5313 - val_acc: 0.7500\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5332 - acc: 0.7343 - val_loss: 0.5428 - val_acc: 0.7500\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5444 - acc: 0.7343 - val_loss: 0.4040 - val_acc: 0.8125\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 10s 109ms/step - loss: 0.5237 - acc: 0.7429 - val_loss: 0.5200 - val_acc: 0.7500\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5216 - acc: 0.7500 - val_loss: 0.5867 - val_acc: 0.6875\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5404 - acc: 0.7225 - val_loss: 0.5376 - val_acc: 0.6875\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5535 - acc: 0.7343 - val_loss: 0.5529 - val_acc: 0.6250\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5437 - acc: 0.7303 - val_loss: 0.5088 - val_acc: 0.6875\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5464 - acc: 0.7335 - val_loss: 0.5951 - val_acc: 0.6875\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5297 - acc: 0.7343 - val_loss: 0.4606 - val_acc: 0.7500\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5180 - acc: 0.7476 - val_loss: 0.5231 - val_acc: 0.7500\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5308 - acc: 0.7303 - val_loss: 0.3565 - val_acc: 0.8125\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5205 - acc: 0.7429 - val_loss: 0.4857 - val_acc: 0.6875\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5422 - acc: 0.7327 - val_loss: 0.5107 - val_acc: 0.6875\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5427 - acc: 0.7288 - val_loss: 0.5641 - val_acc: 0.6875\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5268 - acc: 0.7508 - val_loss: 0.4812 - val_acc: 0.7500\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5191 - acc: 0.7469 - val_loss: 0.4292 - val_acc: 0.7500\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5248 - acc: 0.7461 - val_loss: 0.3589 - val_acc: 0.8750\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5220 - acc: 0.7406 - val_loss: 0.5714 - val_acc: 0.6875\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5180 - acc: 0.7311 - val_loss: 0.5383 - val_acc: 0.6875\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5183 - acc: 0.7492 - val_loss: 0.5163 - val_acc: 0.7500\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5397 - acc: 0.7288 - val_loss: 0.5071 - val_acc: 0.6875\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5356 - acc: 0.7272 - val_loss: 0.5182 - val_acc: 0.6875\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5265 - acc: 0.7398 - val_loss: 0.4336 - val_acc: 0.7500\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5267 - acc: 0.7319 - val_loss: 0.5788 - val_acc: 0.6875\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5478 - acc: 0.7280 - val_loss: 0.4650 - val_acc: 0.7500\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5273 - acc: 0.7476 - val_loss: 0.4312 - val_acc: 0.7500\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5422 - acc: 0.7264 - val_loss: 0.5038 - val_acc: 0.6875\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5239 - acc: 0.7500 - val_loss: 0.4659 - val_acc: 0.7500\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 9s 106ms/step - loss: 0.5373 - acc: 0.7311 - val_loss: 0.5864 - val_acc: 0.6875\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5362 - acc: 0.7358 - val_loss: 0.6126 - val_acc: 0.6875\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5345 - acc: 0.7437 - val_loss: 0.4544 - val_acc: 0.6875\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5269 - acc: 0.7264 - val_loss: 0.5559 - val_acc: 0.7500\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5327 - acc: 0.7398 - val_loss: 0.6115 - val_acc: 0.7500\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5280 - acc: 0.7390 - val_loss: 0.5789 - val_acc: 0.6875\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5362 - acc: 0.7319 - val_loss: 0.5630 - val_acc: 0.7500\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5344 - acc: 0.7555 - val_loss: 0.5526 - val_acc: 0.6875\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5335 - acc: 0.7398 - val_loss: 0.5588 - val_acc: 0.7500\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5334 - acc: 0.7272 - val_loss: 0.6215 - val_acc: 0.6250\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5267 - acc: 0.7303 - val_loss: 0.5759 - val_acc: 0.6250\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5417 - acc: 0.7288 - val_loss: 0.4873 - val_acc: 0.7500\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5362 - acc: 0.7421 - val_loss: 0.4795 - val_acc: 0.7500\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5409 - acc: 0.7248 - val_loss: 0.6116 - val_acc: 0.6250\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5431 - acc: 0.7311 - val_loss: 0.5353 - val_acc: 0.7500\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5480 - acc: 0.7288 - val_loss: 0.6119 - val_acc: 0.6250\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5318 - acc: 0.7374 - val_loss: 0.5041 - val_acc: 0.7500\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5225 - acc: 0.7311 - val_loss: 0.6056 - val_acc: 0.6875\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5449 - acc: 0.7319 - val_loss: 0.5758 - val_acc: 0.5625\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5462 - acc: 0.7154 - val_loss: 0.4884 - val_acc: 0.7500\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5276 - acc: 0.7358 - val_loss: 0.4991 - val_acc: 0.7500\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5364 - acc: 0.7335 - val_loss: 0.4320 - val_acc: 0.7500\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5444 - acc: 0.7288 - val_loss: 0.5177 - val_acc: 0.7500\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5452 - acc: 0.7248 - val_loss: 0.4394 - val_acc: 0.8125\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5271 - acc: 0.7374 - val_loss: 0.6359 - val_acc: 0.6250\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5228 - acc: 0.7209 - val_loss: 0.4849 - val_acc: 0.7500\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5337 - acc: 0.7421 - val_loss: 0.4276 - val_acc: 0.6875\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5166 - acc: 0.7453 - val_loss: 0.5042 - val_acc: 0.6875\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 9s 109ms/step - loss: 0.5164 - acc: 0.7421 - val_loss: 0.6390 - val_acc: 0.6250\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5286 - acc: 0.7484 - val_loss: 0.5063 - val_acc: 0.8125\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5543 - acc: 0.7335 - val_loss: 0.6355 - val_acc: 0.6875\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5283 - acc: 0.7382 - val_loss: 0.3947 - val_acc: 0.6875\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5408 - acc: 0.7280 - val_loss: 0.3645 - val_acc: 0.8125\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 8s 81ms/step - loss: 0.5503 - acc: 0.7201 - val_loss: 0.5311 - val_acc: 0.7500\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5139 - acc: 0.7594 - val_loss: 0.4692 - val_acc: 0.8125\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5430 - acc: 0.7406 - val_loss: 0.5076 - val_acc: 0.7500\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 8s 82ms/step - loss: 0.5149 - acc: 0.7531 - val_loss: 0.6161 - val_acc: 0.6875\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 8s 97ms/step - loss: 0.5431 - acc: 0.7248 - val_loss: 0.5251 - val_acc: 0.7500\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5302 - acc: 0.7327 - val_loss: 0.4990 - val_acc: 0.7500\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5141 - acc: 0.7626 - val_loss: 0.6034 - val_acc: 0.6250\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5519 - acc: 0.7233 - val_loss: 0.5894 - val_acc: 0.6875\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5429 - acc: 0.7209 - val_loss: 0.4740 - val_acc: 0.7500\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5184 - acc: 0.7508 - val_loss: 0.4885 - val_acc: 0.8125\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5179 - acc: 0.7358 - val_loss: 0.5245 - val_acc: 0.6875\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5303 - acc: 0.7366 - val_loss: 0.5452 - val_acc: 0.7500\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5173 - acc: 0.7602 - val_loss: 0.4040 - val_acc: 0.6875\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5128 - acc: 0.7311 - val_loss: 0.4070 - val_acc: 0.9375\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5407 - acc: 0.7193 - val_loss: 0.4646 - val_acc: 0.8125\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.5407 - acc: 0.7335 - val_loss: 0.6009 - val_acc: 0.6250\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5129 - acc: 0.7461 - val_loss: 0.5569 - val_acc: 0.7500\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5332 - acc: 0.7225 - val_loss: 0.4108 - val_acc: 0.7500\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5480 - acc: 0.7217 - val_loss: 0.5847 - val_acc: 0.6875\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5317 - acc: 0.7453 - val_loss: 0.3720 - val_acc: 0.8125\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5455 - acc: 0.7256 - val_loss: 0.4322 - val_acc: 0.7500\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5300 - acc: 0.7311 - val_loss: 0.5238 - val_acc: 0.7500\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5369 - acc: 0.7296 - val_loss: 0.5626 - val_acc: 0.6250\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5350 - acc: 0.7311 - val_loss: 0.6047 - val_acc: 0.6250\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5352 - acc: 0.7374 - val_loss: 0.5324 - val_acc: 0.6875\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5197 - acc: 0.7492 - val_loss: 0.4100 - val_acc: 0.7500\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5512 - acc: 0.7193 - val_loss: 0.5989 - val_acc: 0.6250\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5396 - acc: 0.7358 - val_loss: 0.5182 - val_acc: 0.6875\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5186 - acc: 0.7492 - val_loss: 0.4906 - val_acc: 0.7500\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5233 - acc: 0.7406 - val_loss: 0.5652 - val_acc: 0.6250\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5274 - acc: 0.7500 - val_loss: 0.4142 - val_acc: 0.7500\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5252 - acc: 0.7492 - val_loss: 0.5304 - val_acc: 0.6875\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5429 - acc: 0.7327 - val_loss: 0.6012 - val_acc: 0.6250\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 8s 80ms/step - loss: 0.5460 - acc: 0.7170 - val_loss: 0.5441 - val_acc: 0.7500\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5251 - acc: 0.7414 - val_loss: 0.6063 - val_acc: 0.6875\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5602 - acc: 0.7170 - val_loss: 0.5693 - val_acc: 0.6875\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 9s 97ms/step - loss: 0.5476 - acc: 0.7335 - val_loss: 0.5463 - val_acc: 0.6875\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5354 - acc: 0.7225 - val_loss: 0.5231 - val_acc: 0.7500\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5344 - acc: 0.7398 - val_loss: 0.5043 - val_acc: 0.7500\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5562 - acc: 0.7233 - val_loss: 0.6191 - val_acc: 0.6250\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5477 - acc: 0.7186 - val_loss: 0.3409 - val_acc: 0.8750\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5333 - acc: 0.7264 - val_loss: 0.3957 - val_acc: 0.8125\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5623 - acc: 0.7060 - val_loss: 0.4823 - val_acc: 0.6250\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5435 - acc: 0.7280 - val_loss: 0.5423 - val_acc: 0.6250\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.5382 - acc: 0.7421 - val_loss: 0.5492 - val_acc: 0.6875\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5479 - acc: 0.7256 - val_loss: 0.4076 - val_acc: 0.7500\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 9s 97ms/step - loss: 0.5754 - acc: 0.7005 - val_loss: 0.5773 - val_acc: 0.6250\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5390 - acc: 0.7311 - val_loss: 0.5198 - val_acc: 0.6875\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5124 - acc: 0.7476 - val_loss: 0.5590 - val_acc: 0.6875\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5535 - acc: 0.7193 - val_loss: 0.3823 - val_acc: 0.7500\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5474 - acc: 0.7280 - val_loss: 0.4360 - val_acc: 0.8125\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5224 - acc: 0.7390 - val_loss: 0.6319 - val_acc: 0.6250\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5451 - acc: 0.7107 - val_loss: 0.4526 - val_acc: 0.8125\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5244 - acc: 0.7421 - val_loss: 0.5677 - val_acc: 0.6875\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.5335 - acc: 0.7453 - val_loss: 0.5606 - val_acc: 0.6875\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5228 - acc: 0.7382 - val_loss: 0.6370 - val_acc: 0.6250\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5316 - acc: 0.7390 - val_loss: 0.6699 - val_acc: 0.5625\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5306 - acc: 0.7366 - val_loss: 0.4816 - val_acc: 0.7500\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5364 - acc: 0.7390 - val_loss: 0.4442 - val_acc: 0.8125\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5292 - acc: 0.7351 - val_loss: 0.5035 - val_acc: 0.8125\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5406 - acc: 0.7311 - val_loss: 0.5461 - val_acc: 0.7500\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5278 - acc: 0.7343 - val_loss: 0.5587 - val_acc: 0.6875\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5076 - acc: 0.7736 - val_loss: 0.5260 - val_acc: 0.7500\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5376 - acc: 0.7296 - val_loss: 0.5027 - val_acc: 0.6250\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5121 - acc: 0.7539 - val_loss: 0.4443 - val_acc: 0.7500\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5407 - acc: 0.7233 - val_loss: 0.5751 - val_acc: 0.6875\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5284 - acc: 0.7366 - val_loss: 0.5919 - val_acc: 0.6875\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5359 - acc: 0.7248 - val_loss: 0.5858 - val_acc: 0.7500\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5144 - acc: 0.7500 - val_loss: 0.4887 - val_acc: 0.7500\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5273 - acc: 0.7382 - val_loss: 0.4797 - val_acc: 0.6875\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5308 - acc: 0.7272 - val_loss: 0.5418 - val_acc: 0.7500\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5469 - acc: 0.7366 - val_loss: 0.4695 - val_acc: 0.8125\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5195 - acc: 0.7335 - val_loss: 0.5555 - val_acc: 0.7500\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5325 - acc: 0.7374 - val_loss: 0.5689 - val_acc: 0.6875\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5416 - acc: 0.7327 - val_loss: 0.4827 - val_acc: 0.7500\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5255 - acc: 0.7366 - val_loss: 0.4618 - val_acc: 0.8125\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5474 - acc: 0.7303 - val_loss: 0.5531 - val_acc: 0.6875\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5389 - acc: 0.7256 - val_loss: 0.3704 - val_acc: 0.8750\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5370 - acc: 0.7351 - val_loss: 0.4915 - val_acc: 0.8125\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5106 - acc: 0.7500 - val_loss: 0.4303 - val_acc: 0.8125\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5291 - acc: 0.7366 - val_loss: 0.4524 - val_acc: 0.8750\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5501 - acc: 0.7311 - val_loss: 0.5128 - val_acc: 0.6875\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5255 - acc: 0.7335 - val_loss: 0.5357 - val_acc: 0.8125\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 9s 105ms/step - loss: 0.5354 - acc: 0.7398 - val_loss: 0.4438 - val_acc: 0.8125\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5315 - acc: 0.7311 - val_loss: 0.4673 - val_acc: 0.7500\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5512 - acc: 0.7186 - val_loss: 0.5817 - val_acc: 0.6875\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5012 - acc: 0.7469 - val_loss: 0.5165 - val_acc: 0.7500\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5328 - acc: 0.7484 - val_loss: 0.4842 - val_acc: 0.7500\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5330 - acc: 0.7492 - val_loss: 0.5121 - val_acc: 0.7500\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5595 - acc: 0.7233 - val_loss: 0.4389 - val_acc: 0.7500\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5357 - acc: 0.7319 - val_loss: 0.4019 - val_acc: 0.8125\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 8s 82ms/step - loss: 0.5252 - acc: 0.7358 - val_loss: 0.4815 - val_acc: 0.8125\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5429 - acc: 0.7233 - val_loss: 0.4028 - val_acc: 0.8125\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5612 - acc: 0.7193 - val_loss: 0.5070 - val_acc: 0.6875\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5313 - acc: 0.7351 - val_loss: 0.3324 - val_acc: 0.8750\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5125 - acc: 0.7484 - val_loss: 0.4016 - val_acc: 0.7500\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5363 - acc: 0.7264 - val_loss: 0.4017 - val_acc: 0.8125\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5233 - acc: 0.7524 - val_loss: 0.5202 - val_acc: 0.6875\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 9s 104ms/step - loss: 0.5330 - acc: 0.7516 - val_loss: 0.5208 - val_acc: 0.6875\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5274 - acc: 0.7201 - val_loss: 0.4234 - val_acc: 0.8125\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5145 - acc: 0.7563 - val_loss: 0.3518 - val_acc: 0.7500\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 9s 98ms/step - loss: 0.5301 - acc: 0.7296 - val_loss: 0.5385 - val_acc: 0.6875\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5425 - acc: 0.7233 - val_loss: 0.3152 - val_acc: 0.8125\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5235 - acc: 0.7492 - val_loss: 0.5976 - val_acc: 0.6875\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5507 - acc: 0.7335 - val_loss: 0.5226 - val_acc: 0.6875\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5164 - acc: 0.7406 - val_loss: 0.4786 - val_acc: 0.7500\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5381 - acc: 0.7421 - val_loss: 0.4806 - val_acc: 0.7500\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5195 - acc: 0.7469 - val_loss: 0.4707 - val_acc: 0.8125\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5256 - acc: 0.7358 - val_loss: 0.4627 - val_acc: 0.7500\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5283 - acc: 0.7398 - val_loss: 0.5731 - val_acc: 0.6875\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5378 - acc: 0.7248 - val_loss: 0.5267 - val_acc: 0.7500\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5311 - acc: 0.7319 - val_loss: 0.5560 - val_acc: 0.6875\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5396 - acc: 0.7358 - val_loss: 0.4734 - val_acc: 0.7500\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5314 - acc: 0.7272 - val_loss: 0.4806 - val_acc: 0.7500\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5525 - acc: 0.7335 - val_loss: 0.3555 - val_acc: 0.8750\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5493 - acc: 0.7335 - val_loss: 0.5167 - val_acc: 0.6875\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5372 - acc: 0.7288 - val_loss: 0.4372 - val_acc: 0.8125\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5426 - acc: 0.7288 - val_loss: 0.5934 - val_acc: 0.6250\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5170 - acc: 0.7453 - val_loss: 0.3112 - val_acc: 0.8750\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5514 - acc: 0.7178 - val_loss: 0.4539 - val_acc: 0.8125\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5328 - acc: 0.7374 - val_loss: 0.5568 - val_acc: 0.6875\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5369 - acc: 0.7398 - val_loss: 0.4231 - val_acc: 0.8125\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5261 - acc: 0.7516 - val_loss: 0.3882 - val_acc: 0.8125\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 9s 98ms/step - loss: 0.5200 - acc: 0.7547 - val_loss: 0.3115 - val_acc: 0.8750\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5484 - acc: 0.7256 - val_loss: 0.4284 - val_acc: 0.7500\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5381 - acc: 0.7296 - val_loss: 0.5539 - val_acc: 0.6250\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5195 - acc: 0.7508 - val_loss: 0.5485 - val_acc: 0.6875\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5536 - acc: 0.7280 - val_loss: 0.4494 - val_acc: 0.7500\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5341 - acc: 0.7272 - val_loss: 0.5535 - val_acc: 0.6875\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5084 - acc: 0.7500 - val_loss: 0.4111 - val_acc: 0.8125\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5381 - acc: 0.7233 - val_loss: 0.4691 - val_acc: 0.7500\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5338 - acc: 0.7406 - val_loss: 0.5742 - val_acc: 0.6875\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5195 - acc: 0.7516 - val_loss: 0.4888 - val_acc: 0.8125\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 10s 112ms/step - loss: 0.5326 - acc: 0.7280 - val_loss: 0.5544 - val_acc: 0.6875\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5478 - acc: 0.7311 - val_loss: 0.4633 - val_acc: 0.6875\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5168 - acc: 0.7500 - val_loss: 0.4611 - val_acc: 0.7500\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5202 - acc: 0.7500 - val_loss: 0.5430 - val_acc: 0.8125\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5438 - acc: 0.7044 - val_loss: 0.5410 - val_acc: 0.7500\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5228 - acc: 0.7445 - val_loss: 0.4790 - val_acc: 0.7500\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5340 - acc: 0.7374 - val_loss: 0.5188 - val_acc: 0.6875\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 9s 96ms/step - loss: 0.5331 - acc: 0.7516 - val_loss: 0.4101 - val_acc: 0.8125\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5338 - acc: 0.7398 - val_loss: 0.5736 - val_acc: 0.6250\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5364 - acc: 0.7154 - val_loss: 0.4477 - val_acc: 0.7500\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5251 - acc: 0.7390 - val_loss: 0.5053 - val_acc: 0.6875\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5102 - acc: 0.7469 - val_loss: 0.5106 - val_acc: 0.8125\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5308 - acc: 0.7390 - val_loss: 0.6666 - val_acc: 0.5625\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5342 - acc: 0.7469 - val_loss: 0.6565 - val_acc: 0.5625\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5412 - acc: 0.7358 - val_loss: 0.4725 - val_acc: 0.7500\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5077 - acc: 0.7429 - val_loss: 0.5062 - val_acc: 0.6875\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5241 - acc: 0.7382 - val_loss: 0.5222 - val_acc: 0.6875\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5341 - acc: 0.7280 - val_loss: 0.5025 - val_acc: 0.7500\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5284 - acc: 0.7335 - val_loss: 0.4356 - val_acc: 0.8125\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5306 - acc: 0.7461 - val_loss: 0.5665 - val_acc: 0.6875\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5316 - acc: 0.7414 - val_loss: 0.4795 - val_acc: 0.6250\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.4958 - acc: 0.7571 - val_loss: 0.5710 - val_acc: 0.7500\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5317 - acc: 0.7382 - val_loss: 0.5614 - val_acc: 0.6875\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5258 - acc: 0.7406 - val_loss: 0.5275 - val_acc: 0.7500\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5350 - acc: 0.7429 - val_loss: 0.5983 - val_acc: 0.6250\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5349 - acc: 0.7374 - val_loss: 0.4296 - val_acc: 0.8750\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5406 - acc: 0.7327 - val_loss: 0.5632 - val_acc: 0.6875\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5398 - acc: 0.7288 - val_loss: 0.5177 - val_acc: 0.7500\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5229 - acc: 0.7374 - val_loss: 0.4972 - val_acc: 0.8125\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 8s 93ms/step - loss: 0.5311 - acc: 0.7461 - val_loss: 0.5340 - val_acc: 0.6250\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5206 - acc: 0.7414 - val_loss: 0.5996 - val_acc: 0.6250\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5508 - acc: 0.7170 - val_loss: 0.5715 - val_acc: 0.6875\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5367 - acc: 0.7406 - val_loss: 0.4637 - val_acc: 0.7500\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5203 - acc: 0.7469 - val_loss: 0.3707 - val_acc: 0.8750\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5309 - acc: 0.7366 - val_loss: 0.5641 - val_acc: 0.6875\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5406 - acc: 0.7256 - val_loss: 0.5410 - val_acc: 0.7500\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5234 - acc: 0.7398 - val_loss: 0.4890 - val_acc: 0.7500\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5496 - acc: 0.7091 - val_loss: 0.5150 - val_acc: 0.7500\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 10s 107ms/step - loss: 0.5027 - acc: 0.7492 - val_loss: 0.6182 - val_acc: 0.6250\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5308 - acc: 0.7414 - val_loss: 0.6206 - val_acc: 0.6875\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5454 - acc: 0.7217 - val_loss: 0.4276 - val_acc: 0.7500\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5262 - acc: 0.7382 - val_loss: 0.4517 - val_acc: 0.8125\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5239 - acc: 0.7374 - val_loss: 0.4781 - val_acc: 0.8750\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5281 - acc: 0.7327 - val_loss: 0.3837 - val_acc: 0.7500\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5344 - acc: 0.7366 - val_loss: 0.5639 - val_acc: 0.7500\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5331 - acc: 0.7484 - val_loss: 0.6481 - val_acc: 0.5625\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5293 - acc: 0.7288 - val_loss: 0.5184 - val_acc: 0.8125\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 9s 110ms/step - loss: 0.5309 - acc: 0.7288 - val_loss: 0.6648 - val_acc: 0.6250\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5177 - acc: 0.7437 - val_loss: 0.3571 - val_acc: 0.7500\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5331 - acc: 0.7461 - val_loss: 0.5859 - val_acc: 0.7500\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 7s 72ms/step - loss: 0.5424 - acc: 0.7437 - val_loss: 0.3890 - val_acc: 0.6875\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5345 - acc: 0.7319 - val_loss: 0.5211 - val_acc: 0.6875\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5311 - acc: 0.7382 - val_loss: 0.5776 - val_acc: 0.6250\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5455 - acc: 0.7382 - val_loss: 0.6364 - val_acc: 0.7500\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5335 - acc: 0.7272 - val_loss: 0.4882 - val_acc: 0.6250\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5052 - acc: 0.7610 - val_loss: 0.5707 - val_acc: 0.6875\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5070 - acc: 0.7689 - val_loss: 0.3760 - val_acc: 0.8125\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5033 - acc: 0.7492 - val_loss: 0.5644 - val_acc: 0.7500\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5420 - acc: 0.7256 - val_loss: 0.5305 - val_acc: 0.7500\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5421 - acc: 0.7429 - val_loss: 0.4972 - val_acc: 0.6875\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5242 - acc: 0.7358 - val_loss: 0.4685 - val_acc: 0.7500\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5349 - acc: 0.7358 - val_loss: 0.5296 - val_acc: 0.7500\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5256 - acc: 0.7366 - val_loss: 0.6236 - val_acc: 0.6250\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5287 - acc: 0.7390 - val_loss: 0.3763 - val_acc: 0.8125\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5267 - acc: 0.7335 - val_loss: 0.6165 - val_acc: 0.6875\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5124 - acc: 0.7476 - val_loss: 0.5561 - val_acc: 0.7500\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5257 - acc: 0.7421 - val_loss: 0.5206 - val_acc: 0.7500\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5142 - acc: 0.7547 - val_loss: 0.5330 - val_acc: 0.7500\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5323 - acc: 0.7280 - val_loss: 0.4100 - val_acc: 0.7500\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5457 - acc: 0.7170 - val_loss: 0.5513 - val_acc: 0.7500\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5406 - acc: 0.7288 - val_loss: 0.5531 - val_acc: 0.7500\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5402 - acc: 0.7170 - val_loss: 0.5668 - val_acc: 0.7500\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5251 - acc: 0.7398 - val_loss: 0.6607 - val_acc: 0.6875\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5346 - acc: 0.7327 - val_loss: 0.3751 - val_acc: 0.7500\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5317 - acc: 0.7374 - val_loss: 0.5715 - val_acc: 0.7500\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5228 - acc: 0.7406 - val_loss: 0.4582 - val_acc: 0.6875\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5349 - acc: 0.7335 - val_loss: 0.6804 - val_acc: 0.6250\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5257 - acc: 0.7437 - val_loss: 0.5532 - val_acc: 0.7500\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5433 - acc: 0.7264 - val_loss: 0.5965 - val_acc: 0.7500\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5659 - acc: 0.7036 - val_loss: 0.6178 - val_acc: 0.6875\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5537 - acc: 0.7107 - val_loss: 0.5799 - val_acc: 0.6875\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5400 - acc: 0.7445 - val_loss: 0.3283 - val_acc: 0.7500\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5295 - acc: 0.7414 - val_loss: 0.5060 - val_acc: 0.8125\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5292 - acc: 0.7406 - val_loss: 0.4742 - val_acc: 0.8750\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5399 - acc: 0.7248 - val_loss: 0.4820 - val_acc: 0.7500\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5259 - acc: 0.7241 - val_loss: 0.5456 - val_acc: 0.7500\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5326 - acc: 0.7461 - val_loss: 0.3528 - val_acc: 0.8750\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5371 - acc: 0.7335 - val_loss: 0.6121 - val_acc: 0.6875\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5145 - acc: 0.7476 - val_loss: 0.6748 - val_acc: 0.6250\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5362 - acc: 0.7366 - val_loss: 0.5554 - val_acc: 0.7500\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5053 - acc: 0.7406 - val_loss: 0.4522 - val_acc: 0.7500\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5341 - acc: 0.7311 - val_loss: 0.5176 - val_acc: 0.7500\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5529 - acc: 0.7225 - val_loss: 0.4013 - val_acc: 0.7500\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5365 - acc: 0.7225 - val_loss: 0.5762 - val_acc: 0.6875\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5310 - acc: 0.7335 - val_loss: 0.5645 - val_acc: 0.7500\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5370 - acc: 0.7264 - val_loss: 0.5905 - val_acc: 0.6875\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5501 - acc: 0.7366 - val_loss: 0.4614 - val_acc: 0.7500\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 9s 98ms/step - loss: 0.5245 - acc: 0.7437 - val_loss: 0.4637 - val_acc: 0.6875\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5282 - acc: 0.7437 - val_loss: 0.5906 - val_acc: 0.6875\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5530 - acc: 0.7288 - val_loss: 0.5179 - val_acc: 0.6875\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5237 - acc: 0.7351 - val_loss: 0.5656 - val_acc: 0.6875\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5136 - acc: 0.7398 - val_loss: 0.6282 - val_acc: 0.6875\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5262 - acc: 0.7398 - val_loss: 0.6641 - val_acc: 0.6250\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5384 - acc: 0.7225 - val_loss: 0.5973 - val_acc: 0.7500\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5165 - acc: 0.7476 - val_loss: 0.5059 - val_acc: 0.8125\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5412 - acc: 0.7390 - val_loss: 0.4107 - val_acc: 0.7500\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5176 - acc: 0.7406 - val_loss: 0.4546 - val_acc: 0.7500\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5267 - acc: 0.7327 - val_loss: 0.4476 - val_acc: 0.7500\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 8s 83ms/step - loss: 0.5393 - acc: 0.7178 - val_loss: 0.6193 - val_acc: 0.7500\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5370 - acc: 0.7382 - val_loss: 0.4371 - val_acc: 0.8750\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5030 - acc: 0.7649 - val_loss: 0.3840 - val_acc: 0.8125\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5517 - acc: 0.7343 - val_loss: 0.6038 - val_acc: 0.6875\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5198 - acc: 0.7563 - val_loss: 0.5089 - val_acc: 0.8125\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5418 - acc: 0.7272 - val_loss: 0.5779 - val_acc: 0.6875\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5464 - acc: 0.7209 - val_loss: 0.5121 - val_acc: 0.7500\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5517 - acc: 0.7193 - val_loss: 0.5057 - val_acc: 0.8125\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5419 - acc: 0.7319 - val_loss: 0.5145 - val_acc: 0.7500\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5347 - acc: 0.7358 - val_loss: 0.4183 - val_acc: 0.7500\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5532 - acc: 0.7068 - val_loss: 0.5540 - val_acc: 0.7500\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5039 - acc: 0.7586 - val_loss: 0.6714 - val_acc: 0.5625\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5562 - acc: 0.7138 - val_loss: 0.6705 - val_acc: 0.6875\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5555 - acc: 0.7225 - val_loss: 0.4969 - val_acc: 0.8125\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5416 - acc: 0.7343 - val_loss: 0.6154 - val_acc: 0.7500\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5169 - acc: 0.7547 - val_loss: 0.5297 - val_acc: 0.8125\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5326 - acc: 0.7272 - val_loss: 0.5656 - val_acc: 0.6875\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5317 - acc: 0.7358 - val_loss: 0.6361 - val_acc: 0.6250\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5314 - acc: 0.7327 - val_loss: 0.4218 - val_acc: 0.7500\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5296 - acc: 0.7343 - val_loss: 0.5260 - val_acc: 0.7500\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5080 - acc: 0.7421 - val_loss: 0.6048 - val_acc: 0.6875\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 9s 96ms/step - loss: 0.5430 - acc: 0.7390 - val_loss: 0.4256 - val_acc: 0.7500\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5512 - acc: 0.7201 - val_loss: 0.6048 - val_acc: 0.6250\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5045 - acc: 0.7531 - val_loss: 0.6325 - val_acc: 0.6875\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5419 - acc: 0.7358 - val_loss: 0.6523 - val_acc: 0.6875\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5247 - acc: 0.7539 - val_loss: 0.5981 - val_acc: 0.6875\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5317 - acc: 0.7272 - val_loss: 0.5731 - val_acc: 0.6875\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5208 - acc: 0.7469 - val_loss: 0.4941 - val_acc: 0.6875\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5310 - acc: 0.7358 - val_loss: 0.6597 - val_acc: 0.6875\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5342 - acc: 0.7193 - val_loss: 0.3854 - val_acc: 0.8125\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5238 - acc: 0.7390 - val_loss: 0.5797 - val_acc: 0.7500\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5365 - acc: 0.7280 - val_loss: 0.6317 - val_acc: 0.6875\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5387 - acc: 0.7414 - val_loss: 0.3166 - val_acc: 0.8125\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5466 - acc: 0.7241 - val_loss: 0.6296 - val_acc: 0.7500\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5438 - acc: 0.7256 - val_loss: 0.4914 - val_acc: 0.7500\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5277 - acc: 0.7382 - val_loss: 0.4206 - val_acc: 0.8125\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5244 - acc: 0.7539 - val_loss: 0.3832 - val_acc: 0.6875\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5412 - acc: 0.7280 - val_loss: 0.5642 - val_acc: 0.7500\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5356 - acc: 0.7256 - val_loss: 0.6703 - val_acc: 0.6875\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5318 - acc: 0.7296 - val_loss: 0.3036 - val_acc: 0.8750\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5234 - acc: 0.7335 - val_loss: 0.4752 - val_acc: 0.7500\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5217 - acc: 0.7398 - val_loss: 0.4166 - val_acc: 0.6875\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5146 - acc: 0.7461 - val_loss: 0.4746 - val_acc: 0.7500\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5569 - acc: 0.7186 - val_loss: 0.4832 - val_acc: 0.8125\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5254 - acc: 0.7476 - val_loss: 0.6661 - val_acc: 0.6250\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5402 - acc: 0.7335 - val_loss: 0.5425 - val_acc: 0.8125\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5419 - acc: 0.7201 - val_loss: 0.3460 - val_acc: 0.8125\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5166 - acc: 0.7453 - val_loss: 0.5335 - val_acc: 0.6875\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5129 - acc: 0.7484 - val_loss: 0.4035 - val_acc: 0.8125\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 8s 92ms/step - loss: 0.5461 - acc: 0.7186 - val_loss: 0.3914 - val_acc: 0.8125\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5520 - acc: 0.7296 - val_loss: 0.4252 - val_acc: 0.6875\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5409 - acc: 0.7358 - val_loss: 0.5563 - val_acc: 0.6875\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5358 - acc: 0.7437 - val_loss: 0.6211 - val_acc: 0.6250\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5348 - acc: 0.7264 - val_loss: 0.5420 - val_acc: 0.7500\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5203 - acc: 0.7374 - val_loss: 0.6248 - val_acc: 0.6875\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5387 - acc: 0.7311 - val_loss: 0.6947 - val_acc: 0.6250\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5425 - acc: 0.7170 - val_loss: 0.5433 - val_acc: 0.7500\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5550 - acc: 0.7154 - val_loss: 0.5430 - val_acc: 0.6875\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 8s 82ms/step - loss: 0.5586 - acc: 0.7201 - val_loss: 0.5220 - val_acc: 0.7500\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 8s 90ms/step - loss: 0.5363 - acc: 0.7390 - val_loss: 0.5728 - val_acc: 0.6875\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5544 - acc: 0.7217 - val_loss: 0.6292 - val_acc: 0.6250\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5437 - acc: 0.7319 - val_loss: 0.5905 - val_acc: 0.6250\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5505 - acc: 0.7146 - val_loss: 0.6391 - val_acc: 0.6875\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5415 - acc: 0.7398 - val_loss: 0.4903 - val_acc: 0.5625\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5299 - acc: 0.7335 - val_loss: 0.5637 - val_acc: 0.6875\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5306 - acc: 0.7445 - val_loss: 0.5781 - val_acc: 0.6250\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.5170 - acc: 0.7445 - val_loss: 0.5470 - val_acc: 0.7500\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 9s 100ms/step - loss: 0.5095 - acc: 0.7484 - val_loss: 0.5665 - val_acc: 0.6875\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5180 - acc: 0.7429 - val_loss: 0.6498 - val_acc: 0.6875\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5449 - acc: 0.7241 - val_loss: 0.6001 - val_acc: 0.6875\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 9s 100ms/step - loss: 0.5243 - acc: 0.7429 - val_loss: 0.5433 - val_acc: 0.7500\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5476 - acc: 0.7280 - val_loss: 0.5346 - val_acc: 0.6875\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 9s 107ms/step - loss: 0.5299 - acc: 0.7492 - val_loss: 0.4464 - val_acc: 0.8750\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5420 - acc: 0.7272 - val_loss: 0.5705 - val_acc: 0.6875\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 9s 100ms/step - loss: 0.5099 - acc: 0.7469 - val_loss: 0.5498 - val_acc: 0.7500\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 9s 99ms/step - loss: 0.5447 - acc: 0.7225 - val_loss: 0.6244 - val_acc: 0.6250\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5354 - acc: 0.7421 - val_loss: 0.5766 - val_acc: 0.6875\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5219 - acc: 0.7484 - val_loss: 0.5792 - val_acc: 0.6875\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5284 - acc: 0.7201 - val_loss: 0.5256 - val_acc: 0.8125\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5403 - acc: 0.7146 - val_loss: 0.4475 - val_acc: 0.6875\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5379 - acc: 0.7343 - val_loss: 0.3569 - val_acc: 0.7500\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5423 - acc: 0.7382 - val_loss: 0.5239 - val_acc: 0.6875\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5367 - acc: 0.7296 - val_loss: 0.5763 - val_acc: 0.6875\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5198 - acc: 0.7390 - val_loss: 0.3773 - val_acc: 0.6875\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5229 - acc: 0.7461 - val_loss: 0.5436 - val_acc: 0.6250\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 9s 103ms/step - loss: 0.5277 - acc: 0.7366 - val_loss: 0.5894 - val_acc: 0.6875\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5379 - acc: 0.7319 - val_loss: 0.6743 - val_acc: 0.6250\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.5439 - acc: 0.7319 - val_loss: 0.5371 - val_acc: 0.7500\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 9s 97ms/step - loss: 0.5392 - acc: 0.7406 - val_loss: 0.4883 - val_acc: 0.7500\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5242 - acc: 0.7366 - val_loss: 0.5711 - val_acc: 0.6875\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 8s 98ms/step - loss: 0.5154 - acc: 0.7437 - val_loss: 0.6276 - val_acc: 0.6250\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5238 - acc: 0.7492 - val_loss: 0.4467 - val_acc: 0.7500\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5325 - acc: 0.7358 - val_loss: 0.4104 - val_acc: 0.7500\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 8s 94ms/step - loss: 0.5161 - acc: 0.7311 - val_loss: 0.4335 - val_acc: 0.7500\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5343 - acc: 0.7358 - val_loss: 0.3542 - val_acc: 0.8125\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 8s 85ms/step - loss: 0.5297 - acc: 0.7398 - val_loss: 0.6395 - val_acc: 0.6250\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5461 - acc: 0.7327 - val_loss: 0.4985 - val_acc: 0.7500\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5402 - acc: 0.7414 - val_loss: 0.5369 - val_acc: 0.6875\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5334 - acc: 0.7476 - val_loss: 0.5998 - val_acc: 0.7500\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 7s 75ms/step - loss: 0.5547 - acc: 0.7398 - val_loss: 0.3795 - val_acc: 0.8125\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 8s 83ms/step - loss: 0.5188 - acc: 0.7469 - val_loss: 0.4533 - val_acc: 0.7500\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.5319 - acc: 0.7335 - val_loss: 0.4454 - val_acc: 0.7500\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.5384 - acc: 0.7264 - val_loss: 0.5495 - val_acc: 0.7500\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 10s 107ms/step - loss: 0.5180 - acc: 0.7524 - val_loss: 0.6017 - val_acc: 0.6875\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5486 - acc: 0.7264 - val_loss: 0.5849 - val_acc: 0.6875\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5303 - acc: 0.7414 - val_loss: 0.5203 - val_acc: 0.8125\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5241 - acc: 0.7484 - val_loss: 0.5035 - val_acc: 0.7500\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5378 - acc: 0.7256 - val_loss: 0.5514 - val_acc: 0.6250\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 8s 89ms/step - loss: 0.5481 - acc: 0.7162 - val_loss: 0.5746 - val_acc: 0.6875\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.5242 - acc: 0.7374 - val_loss: 0.3737 - val_acc: 0.7500\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5222 - acc: 0.7414 - val_loss: 0.4058 - val_acc: 0.8125\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5262 - acc: 0.7476 - val_loss: 0.5364 - val_acc: 0.6875\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 7s 77ms/step - loss: 0.5217 - acc: 0.7508 - val_loss: 0.3648 - val_acc: 0.8125\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 8s 84ms/step - loss: 0.5252 - acc: 0.7296 - val_loss: 0.5650 - val_acc: 0.6875\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 8s 95ms/step - loss: 0.5357 - acc: 0.7414 - val_loss: 0.5862 - val_acc: 0.6875\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5420 - acc: 0.7319 - val_loss: 0.5590 - val_acc: 0.6875\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 8s 91ms/step - loss: 0.5609 - acc: 0.7217 - val_loss: 0.4927 - val_acc: 0.7500\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5488 - acc: 0.7303 - val_loss: 0.5816 - val_acc: 0.6875\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 9s 110ms/step - loss: 0.4984 - acc: 0.7594 - val_loss: 0.5677 - val_acc: 0.7500\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5130 - acc: 0.7421 - val_loss: 0.5115 - val_acc: 0.7500\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 9s 93ms/step - loss: 0.5536 - acc: 0.7335 - val_loss: 0.5223 - val_acc: 0.7500\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5286 - acc: 0.7421 - val_loss: 0.5335 - val_acc: 0.7500\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 7s 80ms/step - loss: 0.5294 - acc: 0.7406 - val_loss: 0.5237 - val_acc: 0.8125\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 8s 97ms/step - loss: 0.5012 - acc: 0.7531 - val_loss: 0.6201 - val_acc: 0.6250\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 9s 101ms/step - loss: 0.5516 - acc: 0.7272 - val_loss: 0.6464 - val_acc: 0.5625\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5468 - acc: 0.7288 - val_loss: 0.5703 - val_acc: 0.6250\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5413 - acc: 0.7217 - val_loss: 0.6057 - val_acc: 0.5625\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 7s 74ms/step - loss: 0.5256 - acc: 0.7398 - val_loss: 0.3136 - val_acc: 0.8750\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.5225 - acc: 0.7414 - val_loss: 0.4243 - val_acc: 0.7500\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 8s 86ms/step - loss: 0.5198 - acc: 0.7586 - val_loss: 0.5676 - val_acc: 0.6875\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 8s 83ms/step - loss: 0.5468 - acc: 0.7233 - val_loss: 0.5320 - val_acc: 0.7500\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5466 - acc: 0.7414 - val_loss: 0.6302 - val_acc: 0.6250\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 8s 88ms/step - loss: 0.5162 - acc: 0.7547 - val_loss: 0.4363 - val_acc: 0.7500\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 7s 73ms/step - loss: 0.5212 - acc: 0.7406 - val_loss: 0.5662 - val_acc: 0.6875\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 8s 96ms/step - loss: 0.5204 - acc: 0.7531 - val_loss: 0.4351 - val_acc: 0.8125\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 9s 98ms/step - loss: 0.5184 - acc: 0.7398 - val_loss: 0.5229 - val_acc: 0.7500\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 7s 76ms/step - loss: 0.5352 - acc: 0.7248 - val_loss: 0.2978 - val_acc: 0.8125\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 7s 79ms/step - loss: 0.5257 - acc: 0.7508 - val_loss: 0.5988 - val_acc: 0.6875\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.5172 - acc: 0.7500 - val_loss: 0.6184 - val_acc: 0.6875\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.5436 - acc: 0.7335 - val_loss: 0.3973 - val_acc: 0.8125\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.5289 - acc: 0.7531 - val_loss: 0.5614 - val_acc: 0.7500\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 8s 87ms/step - loss: 0.5312 - acc: 0.7327 - val_loss: 0.5653 - val_acc: 0.8125\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 7s 78ms/step - loss: 0.5485 - acc: 0.7351 - val_loss: 0.6688 - val_acc: 0.5625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "25037b6e-2303-4a94-d5bd-78fb08b4f1a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.3057186603546143,\n",
              "  1.1558144092559814,\n",
              "  1.1274917125701904,\n",
              "  1.0297064781188965,\n",
              "  1.049226999282837,\n",
              "  1.0055174827575684,\n",
              "  0.9893553853034973,\n",
              "  0.940128743648529,\n",
              "  0.9529593586921692,\n",
              "  0.9329469203948975,\n",
              "  0.8891563415527344,\n",
              "  0.9282469153404236,\n",
              "  0.8800026774406433,\n",
              "  0.8795217871665955,\n",
              "  0.8270583152770996,\n",
              "  0.8483762741088867,\n",
              "  0.7863782644271851,\n",
              "  0.8052053451538086,\n",
              "  0.7823768258094788,\n",
              "  0.7920985817909241,\n",
              "  0.823295533657074,\n",
              "  0.7838538885116577,\n",
              "  0.765093982219696,\n",
              "  0.7724219560623169,\n",
              "  0.7607605457305908,\n",
              "  0.7663142085075378,\n",
              "  0.7642290592193604,\n",
              "  0.7199488878250122,\n",
              "  0.7993353009223938,\n",
              "  0.6954880356788635,\n",
              "  0.733309268951416,\n",
              "  0.7203372120857239,\n",
              "  0.7198500037193298,\n",
              "  0.7221956253051758,\n",
              "  0.7088181376457214,\n",
              "  0.7232759594917297,\n",
              "  0.6941075921058655,\n",
              "  0.6769745945930481,\n",
              "  0.6959375143051147,\n",
              "  0.6728742122650146,\n",
              "  0.7471405863761902,\n",
              "  0.6654260754585266,\n",
              "  0.6883667707443237,\n",
              "  0.6470946669578552,\n",
              "  0.6864162683486938,\n",
              "  0.6520177125930786,\n",
              "  0.653672456741333,\n",
              "  0.6311543583869934,\n",
              "  0.6328034400939941,\n",
              "  0.6601079702377319,\n",
              "  0.6607934236526489,\n",
              "  0.6665520071983337,\n",
              "  0.6343997716903687,\n",
              "  0.6563438773155212,\n",
              "  0.6314151883125305,\n",
              "  0.6066126823425293,\n",
              "  0.6136235594749451,\n",
              "  0.6256842613220215,\n",
              "  0.6350927948951721,\n",
              "  0.6348351836204529,\n",
              "  0.6399813890457153,\n",
              "  0.6325659155845642,\n",
              "  0.6304607391357422,\n",
              "  0.6219651699066162,\n",
              "  0.6240199208259583,\n",
              "  0.5779011845588684,\n",
              "  0.5834532380104065,\n",
              "  0.6099510192871094,\n",
              "  0.5999059677124023,\n",
              "  0.5758618712425232,\n",
              "  0.5952337384223938,\n",
              "  0.6034782528877258,\n",
              "  0.6201050877571106,\n",
              "  0.5938674211502075,\n",
              "  0.5864130854606628,\n",
              "  0.5831162333488464,\n",
              "  0.598627507686615,\n",
              "  0.57896488904953,\n",
              "  0.5955290794372559,\n",
              "  0.5872648358345032,\n",
              "  0.5912278890609741,\n",
              "  0.5903374552726746,\n",
              "  0.6042972803115845,\n",
              "  0.606404721736908,\n",
              "  0.5986147522926331,\n",
              "  0.5847263336181641,\n",
              "  0.5847641229629517,\n",
              "  0.5814033150672913,\n",
              "  0.583514392375946,\n",
              "  0.5810038447380066,\n",
              "  0.5607284903526306,\n",
              "  0.5836556553840637,\n",
              "  0.5801236033439636,\n",
              "  0.5672435760498047,\n",
              "  0.5835679173469543,\n",
              "  0.5841192007064819,\n",
              "  0.583175539970398,\n",
              "  0.5650702714920044,\n",
              "  0.5628660917282104,\n",
              "  0.5724875926971436,\n",
              "  0.5637277364730835,\n",
              "  0.5645889639854431,\n",
              "  0.5449172854423523,\n",
              "  0.5688387155532837,\n",
              "  0.5764997601509094,\n",
              "  0.558551013469696,\n",
              "  0.5495596528053284,\n",
              "  0.595569372177124,\n",
              "  0.5538836121559143,\n",
              "  0.5828033685684204,\n",
              "  0.5642582178115845,\n",
              "  0.570598840713501,\n",
              "  0.5805206298828125,\n",
              "  0.5507371425628662,\n",
              "  0.5546236634254456,\n",
              "  0.5674147605895996,\n",
              "  0.5455281734466553,\n",
              "  0.556589663028717,\n",
              "  0.558331310749054,\n",
              "  0.5590506196022034,\n",
              "  0.5611820816993713,\n",
              "  0.5709632039070129,\n",
              "  0.535933256149292,\n",
              "  0.551668107509613,\n",
              "  0.5614615678787231,\n",
              "  0.5551075339317322,\n",
              "  0.5778582692146301,\n",
              "  0.5560864210128784,\n",
              "  0.554941713809967,\n",
              "  0.5553467273712158,\n",
              "  0.5495117902755737,\n",
              "  0.5392565131187439,\n",
              "  0.5408512353897095,\n",
              "  0.5513414740562439,\n",
              "  0.5526217818260193,\n",
              "  0.555266797542572,\n",
              "  0.5814307928085327,\n",
              "  0.5441352725028992,\n",
              "  0.5740817785263062,\n",
              "  0.5696761012077332,\n",
              "  0.5457482933998108,\n",
              "  0.5466276407241821,\n",
              "  0.5549226403236389,\n",
              "  0.5277118682861328,\n",
              "  0.558476448059082,\n",
              "  0.5625255107879639,\n",
              "  0.5323502421379089,\n",
              "  0.534439742565155,\n",
              "  0.5571390986442566,\n",
              "  0.5431593656539917,\n",
              "  0.5430905222892761,\n",
              "  0.5373954772949219,\n",
              "  0.5359433889389038,\n",
              "  0.5565434098243713,\n",
              "  0.5375653505325317,\n",
              "  0.5453945398330688,\n",
              "  0.5462021827697754,\n",
              "  0.5458056330680847,\n",
              "  0.5374823808670044,\n",
              "  0.5345906615257263,\n",
              "  0.5564676523208618,\n",
              "  0.5593712329864502,\n",
              "  0.5482832193374634,\n",
              "  0.5595385432243347,\n",
              "  0.5422129034996033,\n",
              "  0.5522961020469666,\n",
              "  0.5459760427474976,\n",
              "  0.5423244833946228,\n",
              "  0.550220251083374,\n",
              "  0.5558788776397705,\n",
              "  0.5305514931678772,\n",
              "  0.551306426525116,\n",
              "  0.5467970967292786,\n",
              "  0.5517083406448364,\n",
              "  0.526685357093811,\n",
              "  0.5313971042633057,\n",
              "  0.5308062434196472,\n",
              "  0.5338122248649597,\n",
              "  0.51710444688797,\n",
              "  0.5362927317619324,\n",
              "  0.540195643901825,\n",
              "  0.5369012951850891,\n",
              "  0.5274263620376587,\n",
              "  0.5417695641517639,\n",
              "  0.5395265221595764,\n",
              "  0.5492350459098816,\n",
              "  0.5264748334884644,\n",
              "  0.5433905124664307,\n",
              "  0.5554023385047913,\n",
              "  0.5302363634109497,\n",
              "  0.5508569478988647,\n",
              "  0.5345443487167358,\n",
              "  0.5531662106513977,\n",
              "  0.5253177881240845,\n",
              "  0.5412957668304443,\n",
              "  0.5575137734413147,\n",
              "  0.5338505506515503,\n",
              "  0.5354122519493103,\n",
              "  0.5466292500495911,\n",
              "  0.5365265011787415,\n",
              "  0.5443176627159119,\n",
              "  0.5245365500450134,\n",
              "  0.5314531326293945,\n",
              "  0.5331939458847046,\n",
              "  0.5453294515609741,\n",
              "  0.5277134776115417,\n",
              "  0.5462496280670166,\n",
              "  0.5488831996917725,\n",
              "  0.5093995332717896,\n",
              "  0.5276247262954712,\n",
              "  0.5155870914459229,\n",
              "  0.5264204144477844,\n",
              "  0.5535087585449219,\n",
              "  0.5378344058990479,\n",
              "  0.5435783267021179,\n",
              "  0.5394008159637451,\n",
              "  0.5317747592926025,\n",
              "  0.543725311756134,\n",
              "  0.526979386806488,\n",
              "  0.5413291454315186,\n",
              "  0.5261475443840027,\n",
              "  0.5535669922828674,\n",
              "  0.5300561785697937,\n",
              "  0.5270550847053528,\n",
              "  0.5406480431556702,\n",
              "  0.531453013420105,\n",
              "  0.5342705845832825,\n",
              "  0.5284243822097778,\n",
              "  0.5295515656471252,\n",
              "  0.5254088640213013,\n",
              "  0.5363630652427673,\n",
              "  0.5375478863716125,\n",
              "  0.5338183045387268,\n",
              "  0.5453110337257385,\n",
              "  0.5491154789924622,\n",
              "  0.5407392382621765,\n",
              "  0.5362572073936462,\n",
              "  0.5385390520095825,\n",
              "  0.5363318920135498,\n",
              "  0.5437774658203125,\n",
              "  0.5236669778823853,\n",
              "  0.5360285043716431,\n",
              "  0.5311625003814697,\n",
              "  0.5235983729362488,\n",
              "  0.5256723165512085,\n",
              "  0.5482873320579529,\n",
              "  0.5266688466072083,\n",
              "  0.5257989764213562,\n",
              "  0.55336993932724,\n",
              "  0.5302792191505432,\n",
              "  0.5351205468177795,\n",
              "  0.5589718818664551,\n",
              "  0.5358246564865112,\n",
              "  0.5198383927345276,\n",
              "  0.53194659948349,\n",
              "  0.5331502556800842,\n",
              "  0.5364715456962585,\n",
              "  0.5543248653411865,\n",
              "  0.5392261147499084,\n",
              "  0.532760500907898,\n",
              "  0.5369506478309631,\n",
              "  0.5521278381347656,\n",
              "  0.5530191659927368,\n",
              "  0.545275866985321,\n",
              "  0.5264002680778503,\n",
              "  0.5228590965270996,\n",
              "  0.5301823616027832,\n",
              "  0.5332778096199036,\n",
              "  0.5172175168991089,\n",
              "  0.5326172113418579,\n",
              "  0.5407671332359314,\n",
              "  0.5212762355804443,\n",
              "  0.5458732843399048,\n",
              "  0.5238533020019531,\n",
              "  0.5426917672157288,\n",
              "  0.5363931059837341,\n",
              "  0.5322887301445007,\n",
              "  0.5271058678627014,\n",
              "  0.5228554606437683,\n",
              "  0.5413399934768677,\n",
              "  0.5410527586936951,\n",
              "  0.5290825366973877,\n",
              "  0.5468767285346985,\n",
              "  0.5289754271507263,\n",
              "  0.5230135321617126,\n",
              "  0.5395467877388,\n",
              "  0.5339071750640869,\n",
              "  0.5367987155914307,\n",
              "  0.5251159071922302,\n",
              "  0.5354523658752441,\n",
              "  0.5412084460258484,\n",
              "  0.531964898109436,\n",
              "  0.5379514098167419,\n",
              "  0.5285692811012268,\n",
              "  0.5385986566543579,\n",
              "  0.533522367477417,\n",
              "  0.5412154197692871,\n",
              "  0.5256941914558411,\n",
              "  0.5270176529884338,\n",
              "  0.5440536141395569,\n",
              "  0.5286955237388611,\n",
              "  0.5452646613121033,\n",
              "  0.5390110015869141,\n",
              "  0.5239027738571167,\n",
              "  0.5526396632194519,\n",
              "  0.5241948962211609,\n",
              "  0.5372945070266724,\n",
              "  0.5376711487770081,\n",
              "  0.5246548056602478,\n",
              "  0.5471047163009644,\n",
              "  0.5203637480735779,\n",
              "  0.5351492166519165,\n",
              "  0.5309387445449829,\n",
              "  0.5371604561805725,\n",
              "  0.5375770926475525,\n",
              "  0.5251739025115967,\n",
              "  0.5347588658332825,\n",
              "  0.5215051770210266,\n",
              "  0.5329717993736267,\n",
              "  0.5115769505500793,\n",
              "  0.5297387838363647,\n",
              "  0.5319133400917053,\n",
              "  0.5294862985610962,\n",
              "  0.5219271183013916,\n",
              "  0.5340442061424255,\n",
              "  0.5516246557235718,\n",
              "  0.5317118763923645,\n",
              "  0.5237943530082703,\n",
              "  0.5308535099029541,\n",
              "  0.5574616193771362,\n",
              "  0.5338945388793945,\n",
              "  0.5463029742240906,\n",
              "  0.5330256223678589,\n",
              "  0.53404700756073,\n",
              "  0.5583840608596802,\n",
              "  0.5268681645393372,\n",
              "  0.5327228307723999,\n",
              "  0.5388004779815674,\n",
              "  0.5204347372055054,\n",
              "  0.5484868288040161,\n",
              "  0.5282164812088013,\n",
              "  0.5350265502929688,\n",
              "  0.5299477577209473,\n",
              "  0.5191029906272888,\n",
              "  0.5132016539573669,\n",
              "  0.5282073020935059,\n",
              "  0.5356311202049255,\n",
              "  0.5278930068016052,\n",
              "  0.5392411351203918,\n",
              "  0.5277557969093323,\n",
              "  0.5212463140487671,\n",
              "  0.5301475524902344,\n",
              "  0.5380241274833679,\n",
              "  0.5196353197097778,\n",
              "  0.516654908657074,\n",
              "  0.5368172526359558,\n",
              "  0.5444868803024292,\n",
              "  0.5345001220703125,\n",
              "  0.5424380898475647,\n",
              "  0.5347663760185242,\n",
              "  0.5129836201667786,\n",
              "  0.5262095332145691,\n",
              "  0.5202977657318115,\n",
              "  0.5198225378990173,\n",
              "  0.5332217812538147,\n",
              "  0.5426592826843262,\n",
              "  0.5313065648078918,\n",
              "  0.5235515236854553,\n",
              "  0.5162261724472046,\n",
              "  0.5332528352737427,\n",
              "  0.5271501541137695,\n",
              "  0.5399209856987,\n",
              "  0.5226033926010132,\n",
              "  0.5366741418838501,\n",
              "  0.5241265296936035,\n",
              "  0.5314352512359619,\n",
              "  0.5320364236831665,\n",
              "  0.5326492786407471,\n",
              "  0.5401251912117004,\n",
              "  0.5377792716026306,\n",
              "  0.5464724898338318,\n",
              "  0.5393428206443787,\n",
              "  0.5353023409843445,\n",
              "  0.528925895690918,\n",
              "  0.5448014736175537,\n",
              "  0.5246154069900513,\n",
              "  0.5234510898590088,\n",
              "  0.5274854302406311,\n",
              "  0.5503870248794556,\n",
              "  0.5238223075866699,\n",
              "  0.53067547082901,\n",
              "  0.5101830363273621,\n",
              "  0.5475572347640991,\n",
              "  0.5261244177818298,\n",
              "  0.5392663478851318,\n",
              "  0.5206875801086426,\n",
              "  0.5165051221847534,\n",
              "  0.521574854850769,\n",
              "  0.5438809394836426,\n",
              "  0.5506492853164673,\n",
              "  0.5296291708946228,\n",
              "  0.5159632563591003,\n",
              "  0.5287201404571533,\n",
              "  0.5262176990509033,\n",
              "  0.5355018973350525,\n",
              "  0.5345834493637085,\n",
              "  0.5264002680778503,\n",
              "  0.5380117297172546,\n",
              "  0.5364605188369751,\n",
              "  0.5320329666137695,\n",
              "  0.5260399580001831,\n",
              "  0.5353147387504578,\n",
              "  0.5454278588294983,\n",
              "  0.5426337122917175,\n",
              "  0.510174572467804,\n",
              "  0.5434152483940125,\n",
              "  0.5046934485435486,\n",
              "  0.5372937321662903,\n",
              "  0.5204216837882996,\n",
              "  0.5365555286407471,\n",
              "  0.5481394529342651,\n",
              "  0.531675398349762,\n",
              "  0.5320611596107483,\n",
              "  0.533294677734375,\n",
              "  0.5284708142280579,\n",
              "  0.5394002199172974,\n",
              "  0.5425273776054382,\n",
              "  0.5431206822395325,\n",
              "  0.5549815893173218,\n",
              "  0.5079110860824585,\n",
              "  0.5482149124145508,\n",
              "  0.5580094456672668,\n",
              "  0.5205869078636169,\n",
              "  0.5268568992614746,\n",
              "  0.541753888130188,\n",
              "  0.5287410616874695,\n",
              "  0.5327521562576294,\n",
              "  0.5293129682540894,\n",
              "  0.5488218069076538,\n",
              "  0.5097017288208008,\n",
              "  0.5348303914070129,\n",
              "  0.5296095013618469,\n",
              "  0.5194198489189148,\n",
              "  0.5337883830070496,\n",
              "  0.5340923070907593,\n",
              "  0.5394967198371887,\n",
              "  0.5371365547180176,\n",
              "  0.527463972568512,\n",
              "  0.5378878116607666,\n",
              "  0.5337466597557068,\n",
              "  0.5248150825500488,\n",
              "  0.5230887532234192,\n",
              "  0.5310713648796082,\n",
              "  0.5417987704277039,\n",
              "  0.5421990752220154,\n",
              "  0.5216805934906006,\n",
              "  0.5260594487190247,\n",
              "  0.5598634481430054,\n",
              "  0.5267747640609741,\n",
              "  0.554146945476532,\n",
              "  0.5156059265136719,\n",
              "  0.5381138920783997,\n",
              "  0.5140900611877441,\n",
              "  0.5404040217399597,\n",
              "  0.5369539856910706,\n",
              "  0.5429137945175171,\n",
              "  0.5346561074256897,\n",
              "  0.537875235080719,\n",
              "  0.5514611005783081,\n",
              "  0.5123505592346191,\n",
              "  0.5431345105171204,\n",
              "  0.5229296684265137,\n",
              "  0.5380513668060303,\n",
              "  0.5323056578636169,\n",
              "  0.5333743095397949,\n",
              "  0.5219730734825134,\n",
              "  0.5150554180145264,\n",
              "  0.538094162940979,\n",
              "  0.5186618566513062,\n",
              "  0.5474751591682434,\n",
              "  0.5364639163017273,\n",
              "  0.5220112204551697,\n",
              "  0.5265510082244873,\n",
              "  0.5365053415298462,\n",
              "  0.5376645922660828,\n",
              "  0.5360129475593567,\n",
              "  0.5335432887077332,\n",
              "  0.553524374961853,\n",
              "  0.5203132629394531,\n",
              "  0.530132532119751,\n",
              "  0.5082787275314331,\n",
              "  0.5355827808380127,\n",
              "  0.5482521057128906,\n",
              "  0.5507452487945557,\n",
              "  0.5138979554176331,\n",
              "  0.5410017371177673,\n",
              "  0.5548189878463745,\n",
              "  0.5524258017539978,\n",
              "  0.5309116244316101,\n",
              "  0.5267391800880432,\n",
              "  0.5203685164451599,\n",
              "  0.5247181057929993,\n",
              "  0.5364844799041748,\n",
              "  0.5256872177124023,\n",
              "  0.5269012451171875,\n",
              "  0.5514513254165649,\n",
              "  0.541423499584198,\n",
              "  0.5583905577659607,\n",
              "  0.5410708785057068,\n",
              "  0.5371823310852051,\n",
              "  0.5134416818618774,\n",
              "  0.5384159684181213,\n",
              "  0.503197193145752,\n",
              "  0.5318806171417236,\n",
              "  0.5253627896308899,\n",
              "  0.552139163017273,\n",
              "  0.5398344993591309,\n",
              "  0.5457459688186646,\n",
              "  0.527906596660614,\n",
              "  0.5315138101577759,\n",
              "  0.5373294949531555,\n",
              "  0.5293721556663513,\n",
              "  0.5540252923965454,\n",
              "  0.5233350992202759,\n",
              "  0.5266628861427307,\n",
              "  0.5230545997619629,\n",
              "  0.528427243232727,\n",
              "  0.5215964913368225,\n",
              "  0.535902738571167,\n",
              "  0.5202460885047913,\n",
              "  0.5397584438323975,\n",
              "  0.5188261866569519,\n",
              "  0.5292359590530396,\n",
              "  0.5403643250465393,\n",
              "  0.5391383171081543,\n",
              "  0.5353759527206421,\n",
              "  0.5372925996780396,\n",
              "  0.5285714268684387,\n",
              "  0.5098434686660767,\n",
              "  0.5296302437782288,\n",
              "  0.5425058603286743,\n",
              "  0.5390192270278931,\n",
              "  0.5578316450119019,\n",
              "  0.5231297612190247,\n",
              "  0.5434300303459167,\n",
              "  0.5408010482788086,\n",
              "  0.5257877707481384,\n",
              "  0.5381276607513428,\n",
              "  0.5456061363220215,\n",
              "  0.5218546986579895,\n",
              "  0.5411299467086792,\n",
              "  0.5098486542701721,\n",
              "  0.5258104801177979,\n",
              "  0.5442264080047607,\n",
              "  0.5279554724693298,\n",
              "  0.5331839323043823,\n",
              "  0.544376015663147,\n",
              "  0.5236642956733704,\n",
              "  0.5215619802474976,\n",
              "  0.5404108762741089,\n",
              "  0.5534541606903076,\n",
              "  0.5436633825302124,\n",
              "  0.5464262962341309,\n",
              "  0.5296635031700134,\n",
              "  0.5179969072341919,\n",
              "  0.5308243036270142,\n",
              "  0.5205010175704956,\n",
              "  0.5421645045280457,\n",
              "  0.5426520109176636,\n",
              "  0.5267632603645325,\n",
              "  0.5191107988357544,\n",
              "  0.5247829556465149,\n",
              "  0.5220304131507874,\n",
              "  0.5180078744888306,\n",
              "  0.5183150172233582,\n",
              "  0.5397342443466187,\n",
              "  0.5356488227844238,\n",
              "  0.5265280604362488,\n",
              "  0.5267357230186462,\n",
              "  0.5477988123893738,\n",
              "  0.5273340940475464,\n",
              "  0.5422044396400452,\n",
              "  0.5238634943962097,\n",
              "  0.5372932553291321,\n",
              "  0.536154568195343,\n",
              "  0.5344995856285095,\n",
              "  0.5269410610198975,\n",
              "  0.5326517820358276,\n",
              "  0.5279616117477417,\n",
              "  0.5361548662185669,\n",
              "  0.5343534350395203,\n",
              "  0.5334869027137756,\n",
              "  0.5333760976791382,\n",
              "  0.5267047882080078,\n",
              "  0.5416847467422485,\n",
              "  0.5361729264259338,\n",
              "  0.5409014821052551,\n",
              "  0.5430727601051331,\n",
              "  0.547983705997467,\n",
              "  0.5318459272384644,\n",
              "  0.522549033164978,\n",
              "  0.544879138469696,\n",
              "  0.5462265610694885,\n",
              "  0.5275899767875671,\n",
              "  0.5364196300506592,\n",
              "  0.5443713068962097,\n",
              "  0.5452170968055725,\n",
              "  0.5271075367927551,\n",
              "  0.5228435397148132,\n",
              "  0.5337228178977966,\n",
              "  0.5165581703186035,\n",
              "  0.5164300203323364,\n",
              "  0.5285691022872925,\n",
              "  0.5542899966239929,\n",
              "  0.5283073782920837,\n",
              "  0.5408326387405396,\n",
              "  0.5502838492393494,\n",
              "  0.5138679146766663,\n",
              "  0.5429717898368835,\n",
              "  0.5149328708648682,\n",
              "  0.5431121587753296,\n",
              "  0.5301615595817566,\n",
              "  0.5140591859817505,\n",
              "  0.5518983006477356,\n",
              "  0.5428714752197266,\n",
              "  0.5183963775634766,\n",
              "  0.5178555846214294,\n",
              "  0.5302547812461853,\n",
              "  0.5173428654670715,\n",
              "  0.5128028988838196,\n",
              "  0.5406543612480164,\n",
              "  0.5406534671783447,\n",
              "  0.5129435062408447,\n",
              "  0.5331717133522034,\n",
              "  0.547964870929718,\n",
              "  0.5316622853279114,\n",
              "  0.5454951524734497,\n",
              "  0.5300161242485046,\n",
              "  0.536941409111023,\n",
              "  0.5349732637405396,\n",
              "  0.5352039337158203,\n",
              "  0.5196629166603088,\n",
              "  0.5512492060661316,\n",
              "  0.5396227836608887,\n",
              "  0.5185626149177551,\n",
              "  0.5232877135276794,\n",
              "  0.5274369716644287,\n",
              "  0.5252223014831543,\n",
              "  0.5429191589355469,\n",
              "  0.5459772348403931,\n",
              "  0.525097131729126,\n",
              "  0.560245931148529,\n",
              "  0.5476480722427368,\n",
              "  0.5353755354881287,\n",
              "  0.534412145614624,\n",
              "  0.5561561584472656,\n",
              "  0.5476514101028442,\n",
              "  0.5333435535430908,\n",
              "  0.5622851252555847,\n",
              "  0.5434927940368652,\n",
              "  0.5382360816001892,\n",
              "  0.5478683710098267,\n",
              "  0.5754251480102539,\n",
              "  0.5390074849128723,\n",
              "  0.5123726725578308,\n",
              "  0.5534965395927429,\n",
              "  0.5473983883857727,\n",
              "  0.522372841835022,\n",
              "  0.545106828212738,\n",
              "  0.5243818163871765,\n",
              "  0.5335119366645813,\n",
              "  0.522804319858551,\n",
              "  0.5316345691680908,\n",
              "  0.5305830836296082,\n",
              "  0.5364112257957458,\n",
              "  0.5292224287986755,\n",
              "  0.5405703186988831,\n",
              "  0.5277721285820007,\n",
              "  0.5076168775558472,\n",
              "  0.5376315116882324,\n",
              "  0.5120601058006287,\n",
              "  0.5406947731971741,\n",
              "  0.5284034013748169,\n",
              "  0.535921037197113,\n",
              "  0.514366090297699,\n",
              "  0.5273223519325256,\n",
              "  0.5307667255401611,\n",
              "  0.5468755960464478,\n",
              "  0.5194577574729919,\n",
              "  0.532518744468689,\n",
              "  0.5415644645690918,\n",
              "  0.5254544615745544,\n",
              "  0.5473875403404236,\n",
              "  0.5388659238815308,\n",
              "  0.5370308756828308,\n",
              "  0.5105887651443481,\n",
              "  0.5291329622268677,\n",
              "  0.5501488447189331,\n",
              "  0.5255336761474609,\n",
              "  0.5354046821594238,\n",
              "  0.5314958691596985,\n",
              "  0.5512402653694153,\n",
              "  0.5012260675430298,\n",
              "  0.5328468084335327,\n",
              "  0.5330130457878113,\n",
              "  0.5594608187675476,\n",
              "  0.5357486009597778,\n",
              "  0.5252383947372437,\n",
              "  0.5428739786148071,\n",
              "  0.561193585395813,\n",
              "  0.5312909483909607,\n",
              "  0.5124545693397522,\n",
              "  0.5363069772720337,\n",
              "  0.5232921838760376,\n",
              "  0.5330398678779602,\n",
              "  0.5274254679679871,\n",
              "  0.514467179775238,\n",
              "  0.5300621390342712,\n",
              "  0.5425206422805786,\n",
              "  0.5234764218330383,\n",
              "  0.550667941570282,\n",
              "  0.5163705945014954,\n",
              "  0.5380825996398926,\n",
              "  0.5194999575614929,\n",
              "  0.5255569815635681,\n",
              "  0.5283476114273071,\n",
              "  0.5377511978149414,\n",
              "  0.5311191082000732,\n",
              "  0.539603590965271,\n",
              "  0.5314331650733948,\n",
              "  0.5525364279747009,\n",
              "  0.5493334531784058,\n",
              "  0.5372334718704224,\n",
              "  0.5425562858581543,\n",
              "  0.5170481204986572,\n",
              "  0.5513710379600525,\n",
              "  0.5328454375267029,\n",
              "  0.536909282207489,\n",
              "  0.5260816216468811,\n",
              "  0.5199945569038391,\n",
              "  0.5484218597412109,\n",
              "  0.5381072163581848,\n",
              "  0.5195483565330505,\n",
              "  0.5535587072372437,\n",
              "  0.5341424345970154,\n",
              "  0.5084468126296997,\n",
              "  0.5381079316139221,\n",
              "  0.5338402390480042,\n",
              "  0.5195325613021851,\n",
              "  0.5326251983642578,\n",
              "  0.5478059649467468,\n",
              "  0.5167900323867798,\n",
              "  0.520207941532135,\n",
              "  0.5438405871391296,\n",
              "  0.5227775573730469,\n",
              "  0.5340189337730408,\n",
              "  0.5331397652626038,\n",
              "  0.533767580986023,\n",
              "  0.5363990664482117,\n",
              "  0.5250759720802307,\n",
              "  0.5102208256721497,\n",
              "  0.5308474898338318,\n",
              "  0.5341672301292419,\n",
              "  0.5412449836730957,\n",
              "  0.5076593160629272,\n",
              "  0.5240615010261536,\n",
              "  0.5341348648071289,\n",
              "  0.5283861756324768,\n",
              "  0.5305752754211426,\n",
              "  0.5316176414489746,\n",
              "  0.4958168864250183,\n",
              "  0.5317443013191223,\n",
              "  0.5258254408836365,\n",
              "  0.5350186228752136,\n",
              "  0.5349270701408386,\n",
              "  0.5406362414360046,\n",
              "  0.5397891402244568,\n",
              "  0.5229368209838867,\n",
              "  0.5311135053634644,\n",
              "  0.5205805897712708,\n",
              "  0.5507698059082031,\n",
              "  0.5367447137832642,\n",
              "  0.5203079581260681,\n",
              "  0.5309455990791321,\n",
              "  0.5405859351158142,\n",
              "  0.5234320163726807,\n",
              "  0.5496492385864258,\n",
              "  0.5026743412017822,\n",
              "  0.5307677984237671,\n",
              "  0.5454335808753967,\n",
              "  0.5261862277984619,\n",
              "  0.5238852500915527,\n",
              "  0.5280546545982361,\n",
              "  0.5344277620315552,\n",
              "  0.5331469178199768,\n",
              "  0.5292655825614929,\n",
              "  0.5308601260185242,\n",
              "  0.5177376866340637,\n",
              "  0.5330501198768616,\n",
              "  0.5423557758331299,\n",
              "  0.5345115661621094,\n",
              "  0.5311377644538879,\n",
              "  0.5455005764961243,\n",
              "  0.5334685444831848,\n",
              "  0.5051976442337036,\n",
              "  0.5069593191146851,\n",
              "  0.5033051371574402,\n",
              "  0.5420122146606445,\n",
              "  0.54212486743927,\n",
              "  0.5242480635643005,\n",
              "  0.5349417328834534,\n",
              "  0.5255839228630066,\n",
              "  0.528739333152771,\n",
              "  0.5266753435134888,\n",
              "  0.5123668313026428,\n",
              "  0.5256956815719604,\n",
              "  0.5141834020614624,\n",
              "  0.5322604179382324,\n",
              "  0.5456973314285278,\n",
              "  0.540560245513916,\n",
              "  0.5401648879051208,\n",
              "  0.5251396894454956,\n",
              "  0.5345723032951355,\n",
              "  0.5316717624664307,\n",
              "  0.5227695107460022,\n",
              "  0.534909188747406,\n",
              "  0.525672197341919,\n",
              "  0.5432572364807129,\n",
              "  0.5659425258636475,\n",
              "  0.5537219643592834,\n",
              "  0.5399996638298035,\n",
              "  0.529464602470398,\n",
              "  0.5291649103164673,\n",
              "  0.5399492979049683,\n",
              "  0.5259279012680054,\n",
              "  0.5325790643692017,\n",
              "  0.537124752998352,\n",
              "  0.5144593119621277,\n",
              "  0.5361877083778381,\n",
              "  0.5052891969680786,\n",
              "  0.5340936779975891,\n",
              "  0.5528691411018372,\n",
              "  0.536458432674408,\n",
              "  0.5309988856315613,\n",
              "  0.5369760990142822,\n",
              "  0.5500848293304443,\n",
              "  0.5245214700698853,\n",
              "  0.5281620025634766,\n",
              "  0.5530039668083191,\n",
              "  0.5236998200416565,\n",
              "  0.5135791897773743,\n",
              "  0.5261920690536499,\n",
              "  0.5384392142295837,\n",
              "  0.5164763927459717,\n",
              "  0.5411782264709473,\n",
              "  0.5175521373748779,\n",
              "  0.5267321467399597,\n",
              "  0.5392876267433167,\n",
              "  0.5369845032691956,\n",
              "  0.5029903650283813,\n",
              "  0.5516530275344849,\n",
              "  0.5197969079017639,\n",
              "  0.5418098568916321,\n",
              "  0.5464234948158264,\n",
              "  0.5516887903213501,\n",
              "  0.5418895483016968,\n",
              "  0.534678041934967,\n",
              "  0.5532169342041016,\n",
              "  0.5038571953773499,\n",
              "  0.5561808347702026,\n",
              "  0.5554642081260681,\n",
              "  0.5415537357330322,\n",
              "  0.5169330835342407,\n",
              "  0.5325992703437805,\n",
              "  0.5316886305809021,\n",
              "  0.5313819050788879,\n",
              "  0.5296334624290466,\n",
              "  0.5080212950706482,\n",
              "  0.5430219769477844,\n",
              "  0.5512480139732361,\n",
              "  0.5044960379600525,\n",
              "  0.541933000087738,\n",
              "  0.5246577858924866,\n",
              "  0.5317009687423706,\n",
              "  0.5207935571670532,\n",
              "  0.5309849381446838,\n",
              "  0.5342046618461609,\n",
              "  0.5238247513771057,\n",
              "  0.5364645719528198,\n",
              "  0.538671612739563,\n",
              "  0.5465995669364929,\n",
              "  0.5437795519828796,\n",
              "  0.5276831388473511,\n",
              "  0.5244107246398926,\n",
              "  0.5412343144416809,\n",
              "  0.5355552434921265,\n",
              "  0.53177410364151,\n",
              "  0.523367702960968,\n",
              "  0.5216692686080933,\n",
              "  0.5145695209503174,\n",
              "  0.5569082498550415,\n",
              "  0.5254326462745667,\n",
              "  0.5401614904403687,\n",
              "  0.5419085025787354,\n",
              "  0.5165975093841553,\n",
              "  0.5129134058952332,\n",
              "  0.5460886359214783,\n",
              "  0.5520404577255249,\n",
              "  0.5408729910850525,\n",
              "  0.5357516407966614,\n",
              "  0.5348033905029297,\n",
              "  0.5203069448471069,\n",
              "  0.538719117641449,\n",
              "  0.5425325036048889,\n",
              "  0.5549762845039368,\n",
              "  0.558603823184967,\n",
              "  0.5362758636474609,\n",
              "  0.5543966293334961,\n",
              "  0.5436833500862122,\n",
              "  0.5504513382911682,\n",
              "  0.5415384769439697,\n",
              "  0.5299210548400879,\n",
              "  0.5306280255317688,\n",
              "  0.5169961452484131,\n",
              "  0.5095242261886597,\n",
              "  0.5180394649505615,\n",
              "  0.5449241995811462,\n",
              "  0.5242739319801331,\n",
              "  0.5476245284080505,\n",
              "  0.5298808217048645,\n",
              "  0.5419908165931702,\n",
              "  0.5098711848258972,\n",
              "  0.5446897149085999,\n",
              "  0.5353995561599731,\n",
              "  0.5219022631645203,\n",
              "  0.5283724069595337,\n",
              "  0.540346622467041,\n",
              "  0.537903368473053,\n",
              "  0.542349636554718,\n",
              "  0.5367311835289001,\n",
              "  0.5198440551757812,\n",
              "  0.5229418277740479,\n",
              "  0.5277389287948608,\n",
              "  0.5379337072372437,\n",
              "  0.5439255237579346,\n",
              "  0.5392249226570129,\n",
              "  0.5242058038711548,\n",
              "  0.5153944492340088,\n",
              "  0.5237501859664917,\n",
              "  0.5325135588645935,\n",
              "  0.5161148905754089,\n",
              "  0.5342824459075928,\n",
              "  0.5297097563743591,\n",
              "  0.5460987687110901,\n",
              "  0.5402101278305054,\n",
              "  0.5334053635597229,\n",
              "  0.5547427535057068,\n",
              "  0.5187721848487854,\n",
              "  0.5319047570228577,\n",
              "  0.5384337306022644,\n",
              "  0.517959475517273,\n",
              "  0.5486392974853516,\n",
              "  0.5303145051002502,\n",
              "  0.5240700840950012,\n",
              "  0.537809431552887,\n",
              "  0.5480836629867554,\n",
              "  0.5242293477058411,\n",
              "  0.5221524238586426,\n",
              "  0.5261626243591309,\n",
              "  0.521716833114624,\n",
              "  0.5251744389533997,\n",
              "  0.5357176065444946,\n",
              "  0.5420107245445251,\n",
              "  0.5609064698219299,\n",
              "  0.5488159656524658,\n",
              "  0.49841150641441345,\n",
              "  0.512988805770874,\n",
              "  0.5535809993743896,\n",
              "  0.5286298990249634,\n",
              "  0.5293725728988647,\n",
              "  0.5012416243553162,\n",
              "  0.5516490340232849,\n",
              "  0.5468372106552124,\n",
              "  0.5413204431533813,\n",
              "  0.5256279706954956,\n",
              "  0.5225321650505066,\n",
              "  0.519820511341095,\n",
              "  0.5467855930328369,\n",
              "  0.5465514063835144,\n",
              "  0.5161739587783813,\n",
              "  0.5212103724479675,\n",
              "  0.5204471349716187,\n",
              "  0.5183500647544861,\n",
              "  0.5351812243461609,\n",
              "  0.5257377028465271,\n",
              "  0.5171981453895569,\n",
              "  0.5436169505119324,\n",
              "  0.5288532376289368,\n",
              "  0.5311937928199768,\n",
              "  0.5484544038772583],\n",
              " 'acc': [0.46147799491882324,\n",
              "  0.5023584961891174,\n",
              "  0.5157232880592346,\n",
              "  0.5306603908538818,\n",
              "  0.5322327017784119,\n",
              "  0.5432389974594116,\n",
              "  0.555031418800354,\n",
              "  0.5754716992378235,\n",
              "  0.5660377144813538,\n",
              "  0.5841194987297058,\n",
              "  0.579402506351471,\n",
              "  0.5652515888214111,\n",
              "  0.5888364911079407,\n",
              "  0.5849056839942932,\n",
              "  0.5959119200706482,\n",
              "  0.6029874086380005,\n",
              "  0.6163522005081177,\n",
              "  0.6139937043190002,\n",
              "  0.6006289124488831,\n",
              "  0.6108490824699402,\n",
              "  0.599056601524353,\n",
              "  0.6014150977134705,\n",
              "  0.6194968819618225,\n",
              "  0.6037735939025879,\n",
              "  0.6202830076217651,\n",
              "  0.6273584961891174,\n",
              "  0.6084905862808228,\n",
              "  0.643081784248352,\n",
              "  0.6014150977134705,\n",
              "  0.6454402804374695,\n",
              "  0.6383647918701172,\n",
              "  0.6470125913619995,\n",
              "  0.6375786066055298,\n",
              "  0.6360062956809998,\n",
              "  0.6328616142272949,\n",
              "  0.6446540951728821,\n",
              "  0.6422955989837646,\n",
              "  0.6540880799293518,\n",
              "  0.6485849022865295,\n",
              "  0.6438679099082947,\n",
              "  0.6092767119407654,\n",
              "  0.6454402804374695,\n",
              "  0.6399371027946472,\n",
              "  0.6698113083839417,\n",
              "  0.6320754885673523,\n",
              "  0.6588050127029419,\n",
              "  0.6493710875511169,\n",
              "  0.670597493648529,\n",
              "  0.6761006116867065,\n",
              "  0.6658805012702942,\n",
              "  0.6360062956809998,\n",
              "  0.6540880799293518,\n",
              "  0.694968581199646,\n",
              "  0.6556603908538818,\n",
              "  0.6737421154975891,\n",
              "  0.6808176040649414,\n",
              "  0.6847484111785889,\n",
              "  0.6831761002540588,\n",
              "  0.6745283007621765,\n",
              "  0.6674528121948242,\n",
              "  0.6650943160057068,\n",
              "  0.6698113083839417,\n",
              "  0.6713836193084717,\n",
              "  0.6745283007621765,\n",
              "  0.6698113083839417,\n",
              "  0.6996855139732361,\n",
              "  0.7114779949188232,\n",
              "  0.680031418800354,\n",
              "  0.6918238997459412,\n",
              "  0.7185534834861755,\n",
              "  0.6831761002540588,\n",
              "  0.680031418800354,\n",
              "  0.6737421154975891,\n",
              "  0.6839622855186462,\n",
              "  0.680031418800354,\n",
              "  0.696540892124176,\n",
              "  0.6847484111785889,\n",
              "  0.680031418800354,\n",
              "  0.6926100850105286,\n",
              "  0.6918238997459412,\n",
              "  0.696540892124176,\n",
              "  0.6863207817077637,\n",
              "  0.6839622855186462,\n",
              "  0.6918238997459412,\n",
              "  0.6839622855186462,\n",
              "  0.6957547068595886,\n",
              "  0.6886792182922363,\n",
              "  0.6926100850105286,\n",
              "  0.6957547068595886,\n",
              "  0.694968581199646,\n",
              "  0.7201257944107056,\n",
              "  0.7004716992378235,\n",
              "  0.6996855139732361,\n",
              "  0.6988993883132935,\n",
              "  0.6957547068595886,\n",
              "  0.6988993883132935,\n",
              "  0.6886792182922363,\n",
              "  0.7161949872970581,\n",
              "  0.7138364911079407,\n",
              "  0.704402506351471,\n",
              "  0.7036163806915283,\n",
              "  0.7201257944107056,\n",
              "  0.7209119200706482,\n",
              "  0.7122641801834106,\n",
              "  0.6863207817077637,\n",
              "  0.7114779949188232,\n",
              "  0.7193396091461182,\n",
              "  0.6894654035568237,\n",
              "  0.7122641801834106,\n",
              "  0.7004716992378235,\n",
              "  0.7075471878051758,\n",
              "  0.7146226167678833,\n",
              "  0.7012578845024109,\n",
              "  0.7169811129570007,\n",
              "  0.7161949872970581,\n",
              "  0.6988993883132935,\n",
              "  0.7264150977134705,\n",
              "  0.7185534834861755,\n",
              "  0.7154088020324707,\n",
              "  0.7122641801834106,\n",
              "  0.7177672982215881,\n",
              "  0.7106918096542358,\n",
              "  0.7295597195625305,\n",
              "  0.7185534834861755,\n",
              "  0.7099056839942932,\n",
              "  0.7138364911079407,\n",
              "  0.6957547068595886,\n",
              "  0.7146226167678833,\n",
              "  0.7193396091461182,\n",
              "  0.7193396091461182,\n",
              "  0.7177672982215881,\n",
              "  0.7248427867889404,\n",
              "  0.7216981053352356,\n",
              "  0.7122641801834106,\n",
              "  0.7201257944107056,\n",
              "  0.7130503058433533,\n",
              "  0.705974817276001,\n",
              "  0.7287735939025879,\n",
              "  0.6878930926322937,\n",
              "  0.7004716992378235,\n",
              "  0.7177672982215881,\n",
              "  0.7161949872970581,\n",
              "  0.7177672982215881,\n",
              "  0.7279874086380005,\n",
              "  0.7114779949188232,\n",
              "  0.7130503058433533,\n",
              "  0.7272012829780579,\n",
              "  0.7303459048271179,\n",
              "  0.7161949872970581,\n",
              "  0.7272012829780579,\n",
              "  0.7342767119407654,\n",
              "  0.7256289124488831,\n",
              "  0.7256289124488831,\n",
              "  0.7004716992378235,\n",
              "  0.7311320900917053,\n",
              "  0.7130503058433533,\n",
              "  0.724056601524353,\n",
              "  0.7154088020324707,\n",
              "  0.7256289124488831,\n",
              "  0.7382075190544128,\n",
              "  0.7091194987297058,\n",
              "  0.7114779949188232,\n",
              "  0.7091194987297058,\n",
              "  0.7114779949188232,\n",
              "  0.7185534834861755,\n",
              "  0.7209119200706482,\n",
              "  0.7209119200706482,\n",
              "  0.7295597195625305,\n",
              "  0.722484290599823,\n",
              "  0.7075471878051758,\n",
              "  0.7397798895835876,\n",
              "  0.7146226167678833,\n",
              "  0.7154088020324707,\n",
              "  0.7051886916160583,\n",
              "  0.7311320900917053,\n",
              "  0.7232704162597656,\n",
              "  0.7264150977134705,\n",
              "  0.75,\n",
              "  0.7444968819618225,\n",
              "  0.7366352081298828,\n",
              "  0.7154088020324707,\n",
              "  0.7382075190544128,\n",
              "  0.7248427867889404,\n",
              "  0.724056601524353,\n",
              "  0.7389937043190002,\n",
              "  0.7177672982215881,\n",
              "  0.7248427867889404,\n",
              "  0.7130503058433533,\n",
              "  0.7177672982215881,\n",
              "  0.7232704162597656,\n",
              "  0.7248427867889404,\n",
              "  0.7334905862808228,\n",
              "  0.7303459048271179,\n",
              "  0.7460691928863525,\n",
              "  0.7264150977134705,\n",
              "  0.7146226167678833,\n",
              "  0.7248427867889404,\n",
              "  0.7177672982215881,\n",
              "  0.7287735939025879,\n",
              "  0.7248427867889404,\n",
              "  0.7311320900917053,\n",
              "  0.7295597195625305,\n",
              "  0.7342767119407654,\n",
              "  0.7444968819618225,\n",
              "  0.7334905862808228,\n",
              "  0.7193396091461182,\n",
              "  0.724056601524353,\n",
              "  0.7146226167678833,\n",
              "  0.7350628972053528,\n",
              "  0.7358490824699402,\n",
              "  0.7625786066055298,\n",
              "  0.7342767119407654,\n",
              "  0.7201257944107056,\n",
              "  0.7389937043190002,\n",
              "  0.7256289124488831,\n",
              "  0.7216981053352356,\n",
              "  0.731918215751648,\n",
              "  0.7216981053352356,\n",
              "  0.7539308071136475,\n",
              "  0.724056601524353,\n",
              "  0.724056601524353,\n",
              "  0.722484290599823,\n",
              "  0.7279874086380005,\n",
              "  0.7523584961891174,\n",
              "  0.7248427867889404,\n",
              "  0.7405660152435303,\n",
              "  0.7334905862808228,\n",
              "  0.7311320900917053,\n",
              "  0.7216981053352356,\n",
              "  0.7358490824699402,\n",
              "  0.7272012829780579,\n",
              "  0.7311320900917053,\n",
              "  0.7366352081298828,\n",
              "  0.7264150977134705,\n",
              "  0.7122641801834106,\n",
              "  0.7256289124488831,\n",
              "  0.7256289124488831,\n",
              "  0.7342767119407654,\n",
              "  0.7279874086380005,\n",
              "  0.724056601524353,\n",
              "  0.7366352081298828,\n",
              "  0.7209119200706482,\n",
              "  0.7413522005081177,\n",
              "  0.7397798895835876,\n",
              "  0.7413522005081177,\n",
              "  0.7295597195625305,\n",
              "  0.75,\n",
              "  0.7279874086380005,\n",
              "  0.7201257944107056,\n",
              "  0.7272012829780579,\n",
              "  0.7287735939025879,\n",
              "  0.7114779949188232,\n",
              "  0.7264150977134705,\n",
              "  0.7492138147354126,\n",
              "  0.7327044010162354,\n",
              "  0.7303459048271179,\n",
              "  0.7303459048271179,\n",
              "  0.7248427867889404,\n",
              "  0.722484290599823,\n",
              "  0.7311320900917053,\n",
              "  0.7366352081298828,\n",
              "  0.7279874086380005,\n",
              "  0.7256289124488831,\n",
              "  0.731918215751648,\n",
              "  0.7295597195625305,\n",
              "  0.7350628972053528,\n",
              "  0.7460691928863525,\n",
              "  0.7366352081298828,\n",
              "  0.7437106966972351,\n",
              "  0.7405660152435303,\n",
              "  0.7272012829780579,\n",
              "  0.7421383857727051,\n",
              "  0.7169811129570007,\n",
              "  0.7421383857727051,\n",
              "  0.7303459048271179,\n",
              "  0.7311320900917053,\n",
              "  0.7366352081298828,\n",
              "  0.7295597195625305,\n",
              "  0.7366352081298828,\n",
              "  0.7256289124488831,\n",
              "  0.7264150977134705,\n",
              "  0.7358490824699402,\n",
              "  0.7161949872970581,\n",
              "  0.75157231092453,\n",
              "  0.75157231092453,\n",
              "  0.7366352081298828,\n",
              "  0.7350628972053528,\n",
              "  0.7382075190544128,\n",
              "  0.731918215751648,\n",
              "  0.7287735939025879,\n",
              "  0.7311320900917053,\n",
              "  0.7279874086380005,\n",
              "  0.7169811129570007,\n",
              "  0.74842768907547,\n",
              "  0.7405660152435303,\n",
              "  0.7311320900917053,\n",
              "  0.7161949872970581,\n",
              "  0.7468553185462952,\n",
              "  0.7272012829780579,\n",
              "  0.7342767119407654,\n",
              "  0.7327044010162354,\n",
              "  0.7287735939025879,\n",
              "  0.7209119200706482,\n",
              "  0.7382075190544128,\n",
              "  0.7130503058433533,\n",
              "  0.7405660152435303,\n",
              "  0.7216981053352356,\n",
              "  0.7311320900917053,\n",
              "  0.7460691928863525,\n",
              "  0.7287735939025879,\n",
              "  0.7350628972053528,\n",
              "  0.7232704162597656,\n",
              "  0.7389937043190002,\n",
              "  0.7272012829780579,\n",
              "  0.7216981053352356,\n",
              "  0.75,\n",
              "  0.7342767119407654,\n",
              "  0.7358490824699402,\n",
              "  0.7429245114326477,\n",
              "  0.7405660152435303,\n",
              "  0.722484290599823,\n",
              "  0.731918215751648,\n",
              "  0.731918215751648,\n",
              "  0.7397798895835876,\n",
              "  0.7311320900917053,\n",
              "  0.7146226167678833,\n",
              "  0.7374213933944702,\n",
              "  0.7287735939025879,\n",
              "  0.7374213933944702,\n",
              "  0.7193396091461182,\n",
              "  0.724056601524353,\n",
              "  0.7382075190544128,\n",
              "  0.7350628972053528,\n",
              "  0.7216981053352356,\n",
              "  0.7122641801834106,\n",
              "  0.7468553185462952,\n",
              "  0.7358490824699402,\n",
              "  0.7358490824699402,\n",
              "  0.7460691928863525,\n",
              "  0.7232704162597656,\n",
              "  0.7311320900917053,\n",
              "  0.7248427867889404,\n",
              "  0.7342767119407654,\n",
              "  0.7507861852645874,\n",
              "  0.75157231092453,\n",
              "  0.7413522005081177,\n",
              "  0.7444968819618225,\n",
              "  0.7382075190544128,\n",
              "  0.7342767119407654,\n",
              "  0.7413522005081177,\n",
              "  0.7374213933944702,\n",
              "  0.7334905862808228,\n",
              "  0.7405660152435303,\n",
              "  0.7413522005081177,\n",
              "  0.7492138147354126,\n",
              "  0.7256289124488831,\n",
              "  0.7209119200706482,\n",
              "  0.7295597195625305,\n",
              "  0.731918215751648,\n",
              "  0.7397798895835876,\n",
              "  0.7452830076217651,\n",
              "  0.74842768907547,\n",
              "  0.7374213933944702,\n",
              "  0.7413522005081177,\n",
              "  0.7279874086380005,\n",
              "  0.7216981053352356,\n",
              "  0.7405660152435303,\n",
              "  0.7358490824699402,\n",
              "  0.7429245114326477,\n",
              "  0.7334905862808228,\n",
              "  0.7295597195625305,\n",
              "  0.7350628972053528,\n",
              "  0.7523584961891174,\n",
              "  0.7350628972053528,\n",
              "  0.7366352081298828,\n",
              "  0.7311320900917053,\n",
              "  0.7295597195625305,\n",
              "  0.7444968819618225,\n",
              "  0.7366352081298828,\n",
              "  0.7358490824699402,\n",
              "  0.7083333134651184,\n",
              "  0.7327044010162354,\n",
              "  0.7382075190544128,\n",
              "  0.7311320900917053,\n",
              "  0.7264150977134705,\n",
              "  0.7405660152435303,\n",
              "  0.7287735939025879,\n",
              "  0.7272012829780579,\n",
              "  0.7130503058433533,\n",
              "  0.7421383857727051,\n",
              "  0.7342767119407654,\n",
              "  0.7421383857727051,\n",
              "  0.7256289124488831,\n",
              "  0.7437106966972351,\n",
              "  0.7358490824699402,\n",
              "  0.74842768907547,\n",
              "  0.7350628972053528,\n",
              "  0.7468553185462952,\n",
              "  0.7437106966972351,\n",
              "  0.7248427867889404,\n",
              "  0.731918215751648,\n",
              "  0.7382075190544128,\n",
              "  0.7311320900917053,\n",
              "  0.7358490824699402,\n",
              "  0.7232704162597656,\n",
              "  0.722484290599823,\n",
              "  0.7413522005081177,\n",
              "  0.7374213933944702,\n",
              "  0.7256289124488831,\n",
              "  0.7366352081298828,\n",
              "  0.7444968819618225,\n",
              "  0.7334905862808228,\n",
              "  0.7350628972053528,\n",
              "  0.7122641801834106,\n",
              "  0.7507861852645874,\n",
              "  0.7350628972053528,\n",
              "  0.7523584961891174,\n",
              "  0.731918215751648,\n",
              "  0.7382075190544128,\n",
              "  0.7201257944107056,\n",
              "  0.731918215751648,\n",
              "  0.7366352081298828,\n",
              "  0.7311320900917053,\n",
              "  0.7405660152435303,\n",
              "  0.7366352081298828,\n",
              "  0.7374213933944702,\n",
              "  0.7169811129570007,\n",
              "  0.724056601524353,\n",
              "  0.724056601524353,\n",
              "  0.7444968819618225,\n",
              "  0.7382075190544128,\n",
              "  0.7295597195625305,\n",
              "  0.7452830076217651,\n",
              "  0.7413522005081177,\n",
              "  0.7232704162597656,\n",
              "  0.7421383857727051,\n",
              "  0.7334905862808228,\n",
              "  0.7366352081298828,\n",
              "  0.724056601524353,\n",
              "  0.7350628972053528,\n",
              "  0.7287735939025879,\n",
              "  0.7452830076217651,\n",
              "  0.7539308071136475,\n",
              "  0.7303459048271179,\n",
              "  0.7476415038108826,\n",
              "  0.7327044010162354,\n",
              "  0.7358490824699402,\n",
              "  0.7421383857727051,\n",
              "  0.7327044010162354,\n",
              "  0.7382075190544128,\n",
              "  0.7327044010162354,\n",
              "  0.74842768907547,\n",
              "  0.7366352081298828,\n",
              "  0.7146226167678833,\n",
              "  0.7232704162597656,\n",
              "  0.75157231092453,\n",
              "  0.7452830076217651,\n",
              "  0.7146226167678833,\n",
              "  0.7452830076217651,\n",
              "  0.7295597195625305,\n",
              "  0.7539308071136475,\n",
              "  0.7248427867889404,\n",
              "  0.7531446814537048,\n",
              "  0.7342767119407654,\n",
              "  0.7169811129570007,\n",
              "  0.724056601524353,\n",
              "  0.7248427867889404,\n",
              "  0.7444968819618225,\n",
              "  0.7342767119407654,\n",
              "  0.7507861852645874,\n",
              "  0.7342767119407654,\n",
              "  0.7429245114326477,\n",
              "  0.7350628972053528,\n",
              "  0.7334905862808228,\n",
              "  0.7232704162597656,\n",
              "  0.7358490824699402,\n",
              "  0.7358490824699402,\n",
              "  0.7374213933944702,\n",
              "  0.7382075190544128,\n",
              "  0.7382075190544128,\n",
              "  0.7256289124488831,\n",
              "  0.7303459048271179,\n",
              "  0.7460691928863525,\n",
              "  0.7468553185462952,\n",
              "  0.7264150977134705,\n",
              "  0.724056601524353,\n",
              "  0.731918215751648,\n",
              "  0.7264150977134705,\n",
              "  0.7507861852645874,\n",
              "  0.7421383857727051,\n",
              "  0.7562893033027649,\n",
              "  0.7382075190544128,\n",
              "  0.7185534834861755,\n",
              "  0.7209119200706482,\n",
              "  0.75157231092453,\n",
              "  0.724056601524353,\n",
              "  0.7209119200706482,\n",
              "  0.7342767119407654,\n",
              "  0.7327044010162354,\n",
              "  0.7248427867889404,\n",
              "  0.7374213933944702,\n",
              "  0.7452830076217651,\n",
              "  0.7232704162597656,\n",
              "  0.7421383857727051,\n",
              "  0.7382075190544128,\n",
              "  0.7327044010162354,\n",
              "  0.7279874086380005,\n",
              "  0.7287735939025879,\n",
              "  0.7358490824699402,\n",
              "  0.7272012829780579,\n",
              "  0.7334905862808228,\n",
              "  0.7366352081298828,\n",
              "  0.7562893033027649,\n",
              "  0.731918215751648,\n",
              "  0.7303459048271179,\n",
              "  0.7177672982215881,\n",
              "  0.7311320900917053,\n",
              "  0.7295597195625305,\n",
              "  0.7429245114326477,\n",
              "  0.731918215751648,\n",
              "  0.7287735939025879,\n",
              "  0.7382075190544128,\n",
              "  0.7161949872970581,\n",
              "  0.7452830076217651,\n",
              "  0.7366352081298828,\n",
              "  0.7382075190544128,\n",
              "  0.7382075190544128,\n",
              "  0.7358490824699402,\n",
              "  0.7374213933944702,\n",
              "  0.7421383857727051,\n",
              "  0.7397798895835876,\n",
              "  0.74842768907547,\n",
              "  0.724056601524353,\n",
              "  0.7272012829780579,\n",
              "  0.722484290599823,\n",
              "  0.7279874086380005,\n",
              "  0.7295597195625305,\n",
              "  0.7327044010162354,\n",
              "  0.75,\n",
              "  0.7389937043190002,\n",
              "  0.7327044010162354,\n",
              "  0.7264150977134705,\n",
              "  0.731918215751648,\n",
              "  0.7287735939025879,\n",
              "  0.7334905862808228,\n",
              "  0.7138364911079407,\n",
              "  0.7539308071136475,\n",
              "  0.7272012829780579,\n",
              "  0.7075471878051758,\n",
              "  0.7382075190544128,\n",
              "  0.7279874086380005,\n",
              "  0.74842768907547,\n",
              "  0.7405660152435303,\n",
              "  0.7437106966972351,\n",
              "  0.7468553185462952,\n",
              "  0.7342767119407654,\n",
              "  0.7342767119407654,\n",
              "  0.7429245114326477,\n",
              "  0.75,\n",
              "  0.722484290599823,\n",
              "  0.7342767119407654,\n",
              "  0.7303459048271179,\n",
              "  0.7334905862808228,\n",
              "  0.7342767119407654,\n",
              "  0.7476415038108826,\n",
              "  0.7303459048271179,\n",
              "  0.7429245114326477,\n",
              "  0.7327044010162354,\n",
              "  0.7287735939025879,\n",
              "  0.7507861852645874,\n",
              "  0.7468553185462952,\n",
              "  0.7460691928863525,\n",
              "  0.7405660152435303,\n",
              "  0.7311320900917053,\n",
              "  0.7492138147354126,\n",
              "  0.7287735939025879,\n",
              "  0.7272012829780579,\n",
              "  0.7397798895835876,\n",
              "  0.731918215751648,\n",
              "  0.7279874086380005,\n",
              "  0.7476415038108826,\n",
              "  0.7264150977134705,\n",
              "  0.75,\n",
              "  0.7311320900917053,\n",
              "  0.7358490824699402,\n",
              "  0.7437106966972351,\n",
              "  0.7264150977134705,\n",
              "  0.7397798895835876,\n",
              "  0.7389937043190002,\n",
              "  0.731918215751648,\n",
              "  0.7555031180381775,\n",
              "  0.7397798895835876,\n",
              "  0.7272012829780579,\n",
              "  0.7303459048271179,\n",
              "  0.7287735939025879,\n",
              "  0.7421383857727051,\n",
              "  0.7248427867889404,\n",
              "  0.7311320900917053,\n",
              "  0.7287735939025879,\n",
              "  0.7374213933944702,\n",
              "  0.7311320900917053,\n",
              "  0.731918215751648,\n",
              "  0.7154088020324707,\n",
              "  0.7358490824699402,\n",
              "  0.7334905862808228,\n",
              "  0.7287735939025879,\n",
              "  0.7248427867889404,\n",
              "  0.7374213933944702,\n",
              "  0.7209119200706482,\n",
              "  0.7421383857727051,\n",
              "  0.7452830076217651,\n",
              "  0.7421383857727051,\n",
              "  0.74842768907547,\n",
              "  0.7334905862808228,\n",
              "  0.7382075190544128,\n",
              "  0.7279874086380005,\n",
              "  0.7201257944107056,\n",
              "  0.7594339847564697,\n",
              "  0.7405660152435303,\n",
              "  0.7531446814537048,\n",
              "  0.7248427867889404,\n",
              "  0.7327044010162354,\n",
              "  0.7625786066055298,\n",
              "  0.7232704162597656,\n",
              "  0.7209119200706482,\n",
              "  0.7507861852645874,\n",
              "  0.7358490824699402,\n",
              "  0.7366352081298828,\n",
              "  0.7602201104164124,\n",
              "  0.7311320900917053,\n",
              "  0.7193396091461182,\n",
              "  0.7334905862808228,\n",
              "  0.7460691928863525,\n",
              "  0.722484290599823,\n",
              "  0.7216981053352356,\n",
              "  0.7452830076217651,\n",
              "  0.7256289124488831,\n",
              "  0.7311320900917053,\n",
              "  0.7295597195625305,\n",
              "  0.7311320900917053,\n",
              "  0.7374213933944702,\n",
              "  0.7492138147354126,\n",
              "  0.7193396091461182,\n",
              "  0.7358490824699402,\n",
              "  0.7492138147354126,\n",
              "  0.7405660152435303,\n",
              "  0.75,\n",
              "  0.7492138147354126,\n",
              "  0.7327044010162354,\n",
              "  0.7169811129570007,\n",
              "  0.7413522005081177,\n",
              "  0.7169811129570007,\n",
              "  0.7334905862808228,\n",
              "  0.722484290599823,\n",
              "  0.7397798895835876,\n",
              "  0.7232704162597656,\n",
              "  0.7185534834861755,\n",
              "  0.7264150977134705,\n",
              "  0.705974817276001,\n",
              "  0.7279874086380005,\n",
              "  0.7421383857727051,\n",
              "  0.7256289124488831,\n",
              "  0.7004716992378235,\n",
              "  0.7311320900917053,\n",
              "  0.7476415038108826,\n",
              "  0.7193396091461182,\n",
              "  0.7279874086380005,\n",
              "  0.7389937043190002,\n",
              "  0.7106918096542358,\n",
              "  0.7421383857727051,\n",
              "  0.7452830076217651,\n",
              "  0.7382075190544128,\n",
              "  0.7389937043190002,\n",
              "  0.7366352081298828,\n",
              "  0.7389937043190002,\n",
              "  0.7350628972053528,\n",
              "  0.7311320900917053,\n",
              "  0.7342767119407654,\n",
              "  0.7735849022865295,\n",
              "  0.7295597195625305,\n",
              "  0.7539308071136475,\n",
              "  0.7232704162597656,\n",
              "  0.7366352081298828,\n",
              "  0.7248427867889404,\n",
              "  0.75,\n",
              "  0.7382075190544128,\n",
              "  0.7272012829780579,\n",
              "  0.7366352081298828,\n",
              "  0.7334905862808228,\n",
              "  0.7374213933944702,\n",
              "  0.7327044010162354,\n",
              "  0.7366352081298828,\n",
              "  0.7303459048271179,\n",
              "  0.7256289124488831,\n",
              "  0.7350628972053528,\n",
              "  0.75,\n",
              "  0.7366352081298828,\n",
              "  0.7311320900917053,\n",
              "  0.7334905862808228,\n",
              "  0.7397798895835876,\n",
              "  0.7311320900917053,\n",
              "  0.7185534834861755,\n",
              "  0.7468553185462952,\n",
              "  0.74842768907547,\n",
              "  0.7492138147354126,\n",
              "  0.7232704162597656,\n",
              "  0.731918215751648,\n",
              "  0.7358490824699402,\n",
              "  0.7232704162597656,\n",
              "  0.7193396091461182,\n",
              "  0.7350628972053528,\n",
              "  0.74842768907547,\n",
              "  0.7264150977134705,\n",
              "  0.7523584961891174,\n",
              "  0.75157231092453,\n",
              "  0.7201257944107056,\n",
              "  0.7562893033027649,\n",
              "  0.7295597195625305,\n",
              "  0.7232704162597656,\n",
              "  0.7492138147354126,\n",
              "  0.7334905862808228,\n",
              "  0.7405660152435303,\n",
              "  0.7421383857727051,\n",
              "  0.7468553185462952,\n",
              "  0.7358490824699402,\n",
              "  0.7397798895835876,\n",
              "  0.7248427867889404,\n",
              "  0.731918215751648,\n",
              "  0.7358490824699402,\n",
              "  0.7272012829780579,\n",
              "  0.7334905862808228,\n",
              "  0.7334905862808228,\n",
              "  0.7287735939025879,\n",
              "  0.7287735939025879,\n",
              "  0.7452830076217651,\n",
              "  0.7177672982215881,\n",
              "  0.7374213933944702,\n",
              "  0.7397798895835876,\n",
              "  0.75157231092453,\n",
              "  0.7547169923782349,\n",
              "  0.7256289124488831,\n",
              "  0.7295597195625305,\n",
              "  0.7507861852645874,\n",
              "  0.7279874086380005,\n",
              "  0.7272012829780579,\n",
              "  0.75,\n",
              "  0.7232704162597656,\n",
              "  0.7405660152435303,\n",
              "  0.75157231092453,\n",
              "  0.7279874086380005,\n",
              "  0.7311320900917053,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.704402506351471,\n",
              "  0.7444968819618225,\n",
              "  0.7374213933944702,\n",
              "  0.75157231092453,\n",
              "  0.7397798895835876,\n",
              "  0.7154088020324707,\n",
              "  0.7389937043190002,\n",
              "  0.7468553185462952,\n",
              "  0.7389937043190002,\n",
              "  0.7468553185462952,\n",
              "  0.7358490824699402,\n",
              "  0.7429245114326477,\n",
              "  0.7382075190544128,\n",
              "  0.7279874086380005,\n",
              "  0.7334905862808228,\n",
              "  0.7460691928863525,\n",
              "  0.7413522005081177,\n",
              "  0.7570754885673523,\n",
              "  0.7382075190544128,\n",
              "  0.7405660152435303,\n",
              "  0.7429245114326477,\n",
              "  0.7374213933944702,\n",
              "  0.7327044010162354,\n",
              "  0.7287735939025879,\n",
              "  0.7374213933944702,\n",
              "  0.7460691928863525,\n",
              "  0.7413522005081177,\n",
              "  0.7169811129570007,\n",
              "  0.7405660152435303,\n",
              "  0.7468553185462952,\n",
              "  0.7366352081298828,\n",
              "  0.7256289124488831,\n",
              "  0.7397798895835876,\n",
              "  0.7091194987297058,\n",
              "  0.7492138147354126,\n",
              "  0.7413522005081177,\n",
              "  0.7216981053352356,\n",
              "  0.7382075190544128,\n",
              "  0.7374213933944702,\n",
              "  0.7327044010162354,\n",
              "  0.7366352081298828,\n",
              "  0.74842768907547,\n",
              "  0.7287735939025879,\n",
              "  0.7287735939025879,\n",
              "  0.7437106966972351,\n",
              "  0.7460691928863525,\n",
              "  0.7437106966972351,\n",
              "  0.731918215751648,\n",
              "  0.7382075190544128,\n",
              "  0.7382075190544128,\n",
              "  0.7272012829780579,\n",
              "  0.7610062956809998,\n",
              "  0.7688679099082947,\n",
              "  0.7492138147354126,\n",
              "  0.7256289124488831,\n",
              "  0.7429245114326477,\n",
              "  0.7358490824699402,\n",
              "  0.7358490824699402,\n",
              "  0.7366352081298828,\n",
              "  0.7389937043190002,\n",
              "  0.7334905862808228,\n",
              "  0.7476415038108826,\n",
              "  0.7421383857727051,\n",
              "  0.7547169923782349,\n",
              "  0.7279874086380005,\n",
              "  0.7169811129570007,\n",
              "  0.7287735939025879,\n",
              "  0.7169811129570007,\n",
              "  0.7397798895835876,\n",
              "  0.7327044010162354,\n",
              "  0.7374213933944702,\n",
              "  0.7405660152435303,\n",
              "  0.7334905862808228,\n",
              "  0.7437106966972351,\n",
              "  0.7264150977134705,\n",
              "  0.7036163806915283,\n",
              "  0.7106918096542358,\n",
              "  0.7444968819618225,\n",
              "  0.7413522005081177,\n",
              "  0.7405660152435303,\n",
              "  0.7248427867889404,\n",
              "  0.724056601524353,\n",
              "  0.7460691928863525,\n",
              "  0.7334905862808228,\n",
              "  0.7476415038108826,\n",
              "  0.7366352081298828,\n",
              "  0.7405660152435303,\n",
              "  0.7311320900917053,\n",
              "  0.722484290599823,\n",
              "  0.722484290599823,\n",
              "  0.7334905862808228,\n",
              "  0.7264150977134705,\n",
              "  0.7366352081298828,\n",
              "  0.7437106966972351,\n",
              "  0.7437106966972351,\n",
              "  0.7287735939025879,\n",
              "  0.7350628972053528,\n",
              "  0.7397798895835876,\n",
              "  0.7397798895835876,\n",
              "  0.722484290599823,\n",
              "  0.7476415038108826,\n",
              "  0.7389937043190002,\n",
              "  0.7405660152435303,\n",
              "  0.7327044010162354,\n",
              "  0.7177672982215881,\n",
              "  0.7382075190544128,\n",
              "  0.7649371027946472,\n",
              "  0.7342767119407654,\n",
              "  0.7562893033027649,\n",
              "  0.7272012829780579,\n",
              "  0.7209119200706482,\n",
              "  0.7193396091461182,\n",
              "  0.731918215751648,\n",
              "  0.7358490824699402,\n",
              "  0.7067610025405884,\n",
              "  0.7586477994918823,\n",
              "  0.7138364911079407,\n",
              "  0.722484290599823,\n",
              "  0.7342767119407654,\n",
              "  0.7547169923782349,\n",
              "  0.7272012829780579,\n",
              "  0.7358490824699402,\n",
              "  0.7327044010162354,\n",
              "  0.7342767119407654,\n",
              "  0.7421383857727051,\n",
              "  0.7389937043190002,\n",
              "  0.7201257944107056,\n",
              "  0.7531446814537048,\n",
              "  0.7358490824699402,\n",
              "  0.7539308071136475,\n",
              "  0.7272012829780579,\n",
              "  0.7468553185462952,\n",
              "  0.7358490824699402,\n",
              "  0.7193396091461182,\n",
              "  0.7389937043190002,\n",
              "  0.7279874086380005,\n",
              "  0.7413522005081177,\n",
              "  0.724056601524353,\n",
              "  0.7256289124488831,\n",
              "  0.7382075190544128,\n",
              "  0.7539308071136475,\n",
              "  0.7279874086380005,\n",
              "  0.7256289124488831,\n",
              "  0.7295597195625305,\n",
              "  0.7334905862808228,\n",
              "  0.7397798895835876,\n",
              "  0.7460691928863525,\n",
              "  0.7185534834861755,\n",
              "  0.7476415038108826,\n",
              "  0.7334905862808228,\n",
              "  0.7201257944107056,\n",
              "  0.7452830076217651,\n",
              "  0.74842768907547,\n",
              "  0.7185534834861755,\n",
              "  0.7295597195625305,\n",
              "  0.7358490824699402,\n",
              "  0.7437106966972351,\n",
              "  0.7264150977134705,\n",
              "  0.7374213933944702,\n",
              "  0.7311320900917053,\n",
              "  0.7169811129570007,\n",
              "  0.7154088020324707,\n",
              "  0.7201257944107056,\n",
              "  0.7389937043190002,\n",
              "  0.7216981053352356,\n",
              "  0.731918215751648,\n",
              "  0.7146226167678833,\n",
              "  0.7397798895835876,\n",
              "  0.7334905862808228,\n",
              "  0.7444968819618225,\n",
              "  0.7444968819618225,\n",
              "  0.74842768907547,\n",
              "  0.7429245114326477,\n",
              "  0.724056601524353,\n",
              "  0.7429245114326477,\n",
              "  0.7279874086380005,\n",
              "  0.7492138147354126,\n",
              "  0.7272012829780579,\n",
              "  0.7468553185462952,\n",
              "  0.722484290599823,\n",
              "  0.7421383857727051,\n",
              "  0.74842768907547,\n",
              "  0.7201257944107056,\n",
              "  0.7146226167678833,\n",
              "  0.7342767119407654,\n",
              "  0.7382075190544128,\n",
              "  0.7295597195625305,\n",
              "  0.7389937043190002,\n",
              "  0.7460691928863525,\n",
              "  0.7366352081298828,\n",
              "  0.731918215751648,\n",
              "  0.731918215751648,\n",
              "  0.7405660152435303,\n",
              "  0.7366352081298828,\n",
              "  0.7437106966972351,\n",
              "  0.7492138147354126,\n",
              "  0.7358490824699402,\n",
              "  0.7311320900917053,\n",
              "  0.7358490824699402,\n",
              "  0.7397798895835876,\n",
              "  0.7327044010162354,\n",
              "  0.7413522005081177,\n",
              "  0.7476415038108826,\n",
              "  0.7397798895835876,\n",
              "  0.7468553185462952,\n",
              "  0.7334905862808228,\n",
              "  0.7264150977134705,\n",
              "  0.7523584961891174,\n",
              "  0.7264150977134705,\n",
              "  0.7413522005081177,\n",
              "  0.74842768907547,\n",
              "  0.7256289124488831,\n",
              "  0.7161949872970581,\n",
              "  0.7374213933944702,\n",
              "  0.7413522005081177,\n",
              "  0.7476415038108826,\n",
              "  0.7507861852645874,\n",
              "  0.7295597195625305,\n",
              "  0.7413522005081177,\n",
              "  0.731918215751648,\n",
              "  0.7216981053352356,\n",
              "  0.7303459048271179,\n",
              "  0.7594339847564697,\n",
              "  0.7421383857727051,\n",
              "  0.7334905862808228,\n",
              "  0.7421383857727051,\n",
              "  0.7405660152435303,\n",
              "  0.7531446814537048,\n",
              "  0.7272012829780579,\n",
              "  0.7287735939025879,\n",
              "  0.7216981053352356,\n",
              "  0.7397798895835876,\n",
              "  0.7413522005081177,\n",
              "  0.7586477994918823,\n",
              "  0.7232704162597656,\n",
              "  0.7413522005081177,\n",
              "  0.7547169923782349,\n",
              "  0.7405660152435303,\n",
              "  0.7531446814537048,\n",
              "  0.7397798895835876,\n",
              "  0.7248427867889404,\n",
              "  0.7507861852645874,\n",
              "  0.75,\n",
              "  0.7334905862808228,\n",
              "  0.7531446814537048,\n",
              "  0.7327044010162354,\n",
              "  0.7350628972053528],\n",
              " 'val_loss': [0.6885961294174194,\n",
              "  0.7832669615745544,\n",
              "  0.6830024719238281,\n",
              "  0.7419726848602295,\n",
              "  0.6821631193161011,\n",
              "  0.7875235080718994,\n",
              "  0.6843456029891968,\n",
              "  0.8759382963180542,\n",
              "  0.6643052697181702,\n",
              "  0.7643746137619019,\n",
              "  0.5365658402442932,\n",
              "  0.7490911483764648,\n",
              "  0.5884246230125427,\n",
              "  0.745432436466217,\n",
              "  0.8262691497802734,\n",
              "  0.6610302925109863,\n",
              "  0.6630235910415649,\n",
              "  0.8356748819351196,\n",
              "  0.7382491827011108,\n",
              "  0.4147456884384155,\n",
              "  0.630772590637207,\n",
              "  0.6794068813323975,\n",
              "  0.7201370596885681,\n",
              "  0.6170207262039185,\n",
              "  0.6965798139572144,\n",
              "  0.5377316474914551,\n",
              "  0.7209946513175964,\n",
              "  0.5698357820510864,\n",
              "  0.4718628525733948,\n",
              "  0.7081056237220764,\n",
              "  0.6142463684082031,\n",
              "  0.7964825630187988,\n",
              "  0.6897218227386475,\n",
              "  0.48509252071380615,\n",
              "  0.6152319312095642,\n",
              "  0.6641167402267456,\n",
              "  0.6557471752166748,\n",
              "  0.6075044870376587,\n",
              "  0.5500870943069458,\n",
              "  0.703295111656189,\n",
              "  0.6268297433853149,\n",
              "  0.48526689410209656,\n",
              "  0.5705432891845703,\n",
              "  0.5333430767059326,\n",
              "  0.589970588684082,\n",
              "  0.6408369541168213,\n",
              "  0.6008706092834473,\n",
              "  0.5047047734260559,\n",
              "  0.5605182647705078,\n",
              "  0.6412090063095093,\n",
              "  0.6247262954711914,\n",
              "  0.5221673250198364,\n",
              "  0.4843105673789978,\n",
              "  0.638039231300354,\n",
              "  0.5430139303207397,\n",
              "  0.445951372385025,\n",
              "  0.5777161121368408,\n",
              "  0.48247790336608887,\n",
              "  0.5834553241729736,\n",
              "  0.6463320255279541,\n",
              "  0.5911432504653931,\n",
              "  0.5325795412063599,\n",
              "  0.5753917694091797,\n",
              "  0.505165696144104,\n",
              "  0.4419884979724884,\n",
              "  0.5068582892417908,\n",
              "  0.5499176979064941,\n",
              "  0.47734248638153076,\n",
              "  0.5669736266136169,\n",
              "  0.5558251738548279,\n",
              "  0.5712990760803223,\n",
              "  0.5962003469467163,\n",
              "  0.5661355257034302,\n",
              "  0.5497481822967529,\n",
              "  0.4367770552635193,\n",
              "  0.6340049505233765,\n",
              "  0.49741631746292114,\n",
              "  0.5417936444282532,\n",
              "  0.4415282607078552,\n",
              "  0.5758435726165771,\n",
              "  0.5801073312759399,\n",
              "  0.49575114250183105,\n",
              "  0.43755319714546204,\n",
              "  0.5689029693603516,\n",
              "  0.4606628715991974,\n",
              "  0.5258264541625977,\n",
              "  0.5231215953826904,\n",
              "  0.42294925451278687,\n",
              "  0.5776633024215698,\n",
              "  0.5371279120445251,\n",
              "  0.5720676779747009,\n",
              "  0.5198754668235779,\n",
              "  0.4119373559951782,\n",
              "  0.5993647575378418,\n",
              "  0.6057008504867554,\n",
              "  0.5929328799247742,\n",
              "  0.5069056749343872,\n",
              "  0.4600369334220886,\n",
              "  0.5580633878707886,\n",
              "  0.5231583118438721,\n",
              "  0.5698238611221313,\n",
              "  0.5783778429031372,\n",
              "  0.5679722428321838,\n",
              "  0.599602460861206,\n",
              "  0.5266310572624207,\n",
              "  0.5586955547332764,\n",
              "  0.46480488777160645,\n",
              "  0.5386010408401489,\n",
              "  0.5395816564559937,\n",
              "  0.4738103449344635,\n",
              "  0.3865601420402527,\n",
              "  0.5275434255599976,\n",
              "  0.4335559904575348,\n",
              "  0.4656040072441101,\n",
              "  0.45429524779319763,\n",
              "  0.5437902808189392,\n",
              "  0.521510899066925,\n",
              "  0.5065844058990479,\n",
              "  0.45545223355293274,\n",
              "  0.43947935104370117,\n",
              "  0.5463197231292725,\n",
              "  0.5368010997772217,\n",
              "  0.5272358655929565,\n",
              "  0.5164144039154053,\n",
              "  0.5056586861610413,\n",
              "  0.4656497836112976,\n",
              "  0.523932158946991,\n",
              "  0.4589207172393799,\n",
              "  0.4190009832382202,\n",
              "  0.4370715022087097,\n",
              "  0.46530598402023315,\n",
              "  0.48953676223754883,\n",
              "  0.6298992037773132,\n",
              "  0.5187269449234009,\n",
              "  0.4602009356021881,\n",
              "  0.43620651960372925,\n",
              "  0.49315372109413147,\n",
              "  0.6290256977081299,\n",
              "  0.4599582552909851,\n",
              "  0.44832679629325867,\n",
              "  0.43877971172332764,\n",
              "  0.571297824382782,\n",
              "  0.5008029341697693,\n",
              "  0.5495792031288147,\n",
              "  0.5014934539794922,\n",
              "  0.4698903560638428,\n",
              "  0.5846849679946899,\n",
              "  0.4290603995323181,\n",
              "  0.5396574139595032,\n",
              "  0.4747629463672638,\n",
              "  0.4232989549636841,\n",
              "  0.5752608776092529,\n",
              "  0.4814535677433014,\n",
              "  0.5679686665534973,\n",
              "  0.3817700147628784,\n",
              "  0.4891263246536255,\n",
              "  0.4997889995574951,\n",
              "  0.5191415548324585,\n",
              "  0.5573886036872864,\n",
              "  0.5684529542922974,\n",
              "  0.452390193939209,\n",
              "  0.5046761631965637,\n",
              "  0.5962733626365662,\n",
              "  0.5158399343490601,\n",
              "  0.48226872086524963,\n",
              "  0.4413096308708191,\n",
              "  0.4645489454269409,\n",
              "  0.4789361357688904,\n",
              "  0.409765362739563,\n",
              "  0.4715607464313507,\n",
              "  0.4533693492412567,\n",
              "  0.4573501646518707,\n",
              "  0.5997390151023865,\n",
              "  0.5744285583496094,\n",
              "  0.575248122215271,\n",
              "  0.45214417576789856,\n",
              "  0.5675273537635803,\n",
              "  0.5610760450363159,\n",
              "  0.49003684520721436,\n",
              "  0.5175482630729675,\n",
              "  0.4032142758369446,\n",
              "  0.5050767064094543,\n",
              "  0.5413795709609985,\n",
              "  0.42879027128219604,\n",
              "  0.46591299772262573,\n",
              "  0.5095486640930176,\n",
              "  0.42839425802230835,\n",
              "  0.4345318078994751,\n",
              "  0.5254456400871277,\n",
              "  0.4648739993572235,\n",
              "  0.5001069903373718,\n",
              "  0.49583134055137634,\n",
              "  0.4691661596298218,\n",
              "  0.5046588778495789,\n",
              "  0.4325316250324249,\n",
              "  0.4915385842323303,\n",
              "  0.5551130175590515,\n",
              "  0.5299773812294006,\n",
              "  0.42693907022476196,\n",
              "  0.5395890474319458,\n",
              "  0.4600033760070801,\n",
              "  0.42969760298728943,\n",
              "  0.45814794301986694,\n",
              "  0.5124768614768982,\n",
              "  0.5050377249717712,\n",
              "  0.5188523530960083,\n",
              "  0.4964514374732971,\n",
              "  0.5955974459648132,\n",
              "  0.5545855760574341,\n",
              "  0.4706251919269562,\n",
              "  0.5485286116600037,\n",
              "  0.5181067585945129,\n",
              "  0.4435073733329773,\n",
              "  0.5480093955993652,\n",
              "  0.4999467134475708,\n",
              "  0.529290497303009,\n",
              "  0.48302584886550903,\n",
              "  0.5992771983146667,\n",
              "  0.5858005285263062,\n",
              "  0.5042190551757812,\n",
              "  0.4873676598072052,\n",
              "  0.6160866022109985,\n",
              "  0.5760689377784729,\n",
              "  0.37637078762054443,\n",
              "  0.4395042359828949,\n",
              "  0.4587293863296509,\n",
              "  0.3926277160644531,\n",
              "  0.5693307518959045,\n",
              "  0.42160966992378235,\n",
              "  0.44126272201538086,\n",
              "  0.41993632912635803,\n",
              "  0.49020400643348694,\n",
              "  0.5396934747695923,\n",
              "  0.48751431703567505,\n",
              "  0.3949282765388489,\n",
              "  0.5992770195007324,\n",
              "  0.5200944542884827,\n",
              "  0.49777740240097046,\n",
              "  0.6200627088546753,\n",
              "  0.5901476144790649,\n",
              "  0.415408730506897,\n",
              "  0.3684842586517334,\n",
              "  0.5426442623138428,\n",
              "  0.4423856735229492,\n",
              "  0.5056836605072021,\n",
              "  0.48517781496047974,\n",
              "  0.37971746921539307,\n",
              "  0.5155723094940186,\n",
              "  0.4790375232696533,\n",
              "  0.5435826778411865,\n",
              "  0.5522251129150391,\n",
              "  0.5139166116714478,\n",
              "  0.46684107184410095,\n",
              "  0.5554571747779846,\n",
              "  0.514579176902771,\n",
              "  0.5449773073196411,\n",
              "  0.524169921875,\n",
              "  0.4101839065551758,\n",
              "  0.46453338861465454,\n",
              "  0.5575586557388306,\n",
              "  0.47331246733665466,\n",
              "  0.4881420135498047,\n",
              "  0.500164806842804,\n",
              "  0.4898097515106201,\n",
              "  0.4008391499519348,\n",
              "  0.4904792904853821,\n",
              "  0.4409388303756714,\n",
              "  0.5271492004394531,\n",
              "  0.4234454929828644,\n",
              "  0.48641613125801086,\n",
              "  0.5308991074562073,\n",
              "  0.6160086393356323,\n",
              "  0.6781352162361145,\n",
              "  0.5207569599151611,\n",
              "  0.5842522382736206,\n",
              "  0.42592447996139526,\n",
              "  0.44798988103866577,\n",
              "  0.4995496869087219,\n",
              "  0.5000503659248352,\n",
              "  0.5419039130210876,\n",
              "  0.5432305932044983,\n",
              "  0.5447949767112732,\n",
              "  0.4611635208129883,\n",
              "  0.4744366407394409,\n",
              "  0.5948868989944458,\n",
              "  0.5038723349571228,\n",
              "  0.5575519800186157,\n",
              "  0.474856972694397,\n",
              "  0.5751782059669495,\n",
              "  0.5695434212684631,\n",
              "  0.5376605987548828,\n",
              "  0.3976438641548157,\n",
              "  0.5359197854995728,\n",
              "  0.49069637060165405,\n",
              "  0.48365411162376404,\n",
              "  0.4681907892227173,\n",
              "  0.5352352261543274,\n",
              "  0.4661167860031128,\n",
              "  0.38336992263793945,\n",
              "  0.38730576634407043,\n",
              "  0.513953447341919,\n",
              "  0.48295995593070984,\n",
              "  0.5330724716186523,\n",
              "  0.5944527387619019,\n",
              "  0.5571133494377136,\n",
              "  0.623193621635437,\n",
              "  0.5436897277832031,\n",
              "  0.5023355484008789,\n",
              "  0.565758466720581,\n",
              "  0.49836495518684387,\n",
              "  0.5233551263809204,\n",
              "  0.5348832607269287,\n",
              "  0.4270120859146118,\n",
              "  0.588630199432373,\n",
              "  0.469179630279541,\n",
              "  0.5525394678115845,\n",
              "  0.3758712708950043,\n",
              "  0.54590904712677,\n",
              "  0.676365077495575,\n",
              "  0.4874161183834076,\n",
              "  0.5370500087738037,\n",
              "  0.6134280562400818,\n",
              "  0.2899324297904968,\n",
              "  0.5405164361000061,\n",
              "  0.31048762798309326,\n",
              "  0.5030757784843445,\n",
              "  0.47049450874328613,\n",
              "  0.6356378793716431,\n",
              "  0.3991687297821045,\n",
              "  0.5826093554496765,\n",
              "  0.5857554078102112,\n",
              "  0.5071146488189697,\n",
              "  0.5712707042694092,\n",
              "  0.47338947653770447,\n",
              "  0.4979245662689209,\n",
              "  0.5807727575302124,\n",
              "  0.5166561007499695,\n",
              "  0.4977816939353943,\n",
              "  0.44835078716278076,\n",
              "  0.3951372802257538,\n",
              "  0.5464422702789307,\n",
              "  0.4756360650062561,\n",
              "  0.3378952443599701,\n",
              "  0.5473511219024658,\n",
              "  0.5675754547119141,\n",
              "  0.5310471653938293,\n",
              "  0.5989225506782532,\n",
              "  0.47598883509635925,\n",
              "  0.5371332764625549,\n",
              "  0.48620784282684326,\n",
              "  0.49999988079071045,\n",
              "  0.5430268049240112,\n",
              "  0.5485198497772217,\n",
              "  0.4929133653640747,\n",
              "  0.6173737645149231,\n",
              "  0.5035861730575562,\n",
              "  0.6106323599815369,\n",
              "  0.5097520351409912,\n",
              "  0.5032188892364502,\n",
              "  0.570245087146759,\n",
              "  0.45401260256767273,\n",
              "  0.5834262371063232,\n",
              "  0.4761151373386383,\n",
              "  0.4715675711631775,\n",
              "  0.5562883019447327,\n",
              "  0.44555824995040894,\n",
              "  0.343272864818573,\n",
              "  0.5710717439651489,\n",
              "  0.4963696599006653,\n",
              "  0.5186789035797119,\n",
              "  0.47236168384552,\n",
              "  0.5761808156967163,\n",
              "  0.4605708718299866,\n",
              "  0.4954238533973694,\n",
              "  0.547941267490387,\n",
              "  0.4895593225955963,\n",
              "  0.503057599067688,\n",
              "  0.41203850507736206,\n",
              "  0.5917524099349976,\n",
              "  0.641724169254303,\n",
              "  0.5875461101531982,\n",
              "  0.3407578468322754,\n",
              "  0.4978114366531372,\n",
              "  0.5558952689170837,\n",
              "  0.5172218084335327,\n",
              "  0.6644949316978455,\n",
              "  0.6232932209968567,\n",
              "  0.5058692693710327,\n",
              "  0.4885798394680023,\n",
              "  0.5100128054618835,\n",
              "  0.5236788988113403,\n",
              "  0.4851548671722412,\n",
              "  0.45459645986557007,\n",
              "  0.4337117373943329,\n",
              "  0.5379628539085388,\n",
              "  0.45573872327804565,\n",
              "  0.4980258047580719,\n",
              "  0.456454873085022,\n",
              "  0.47189393639564514,\n",
              "  0.4632945656776428,\n",
              "  0.5597821474075317,\n",
              "  0.4272076487541199,\n",
              "  0.5087640881538391,\n",
              "  0.46179139614105225,\n",
              "  0.4087120592594147,\n",
              "  0.5144688487052917,\n",
              "  0.553462028503418,\n",
              "  0.56043541431427,\n",
              "  0.49419093132019043,\n",
              "  0.41493526101112366,\n",
              "  0.43070220947265625,\n",
              "  0.4683779776096344,\n",
              "  0.6129673719406128,\n",
              "  0.4276072382926941,\n",
              "  0.580039918422699,\n",
              "  0.6561416387557983,\n",
              "  0.5643919110298157,\n",
              "  0.45979082584381104,\n",
              "  0.3867848217487335,\n",
              "  0.5136342644691467,\n",
              "  0.45036306977272034,\n",
              "  0.40902048349380493,\n",
              "  0.5028844475746155,\n",
              "  0.5866074562072754,\n",
              "  0.5624238848686218,\n",
              "  0.46044987440109253,\n",
              "  0.5626606345176697,\n",
              "  0.4859273135662079,\n",
              "  0.4950832426548004,\n",
              "  0.5532362461090088,\n",
              "  0.5013929605484009,\n",
              "  0.38407450914382935,\n",
              "  0.5123380422592163,\n",
              "  0.5236465334892273,\n",
              "  0.43494394421577454,\n",
              "  0.41931748390197754,\n",
              "  0.46001502871513367,\n",
              "  0.5014383792877197,\n",
              "  0.5337806344032288,\n",
              "  0.5281006097793579,\n",
              "  0.5838096141815186,\n",
              "  0.5322309136390686,\n",
              "  0.4669025242328644,\n",
              "  0.5670300722122192,\n",
              "  0.606592059135437,\n",
              "  0.5856680274009705,\n",
              "  0.5019118785858154,\n",
              "  0.4237942695617676,\n",
              "  0.5321429967880249,\n",
              "  0.5269727110862732,\n",
              "  0.30239540338516235,\n",
              "  0.4976288974285126,\n",
              "  0.47374752163887024,\n",
              "  0.5761911869049072,\n",
              "  0.431023508310318,\n",
              "  0.5523321032524109,\n",
              "  0.5783157348632812,\n",
              "  0.41575220227241516,\n",
              "  0.5463256239891052,\n",
              "  0.49572139978408813,\n",
              "  0.5324864983558655,\n",
              "  0.5547180771827698,\n",
              "  0.5457779169082642,\n",
              "  0.42499417066574097,\n",
              "  0.5013493299484253,\n",
              "  0.4909418225288391,\n",
              "  0.4780070185661316,\n",
              "  0.5524857640266418,\n",
              "  0.48264244198799133,\n",
              "  0.5417535901069641,\n",
              "  0.48601847887039185,\n",
              "  0.4854651689529419,\n",
              "  0.457222580909729,\n",
              "  0.5495408177375793,\n",
              "  0.5751848220825195,\n",
              "  0.4357266426086426,\n",
              "  0.43898242712020874,\n",
              "  0.4505320191383362,\n",
              "  0.40130624175071716,\n",
              "  0.5643936991691589,\n",
              "  0.5251930952072144,\n",
              "  0.4986311197280884,\n",
              "  0.3727772831916809,\n",
              "  0.4784888029098511,\n",
              "  0.3760104775428772,\n",
              "  0.5466057658195496,\n",
              "  0.45171743631362915,\n",
              "  0.32792288064956665,\n",
              "  0.40249398350715637,\n",
              "  0.5432114601135254,\n",
              "  0.5995513200759888,\n",
              "  0.6069763898849487,\n",
              "  0.6202876567840576,\n",
              "  0.5294908881187439,\n",
              "  0.5625125169754028,\n",
              "  0.472286194562912,\n",
              "  0.5700828433036804,\n",
              "  0.4880438446998596,\n",
              "  0.5071189403533936,\n",
              "  0.4349627196788788,\n",
              "  0.4779912829399109,\n",
              "  0.4057122468948364,\n",
              "  0.44621676206588745,\n",
              "  0.527907133102417,\n",
              "  0.5607602596282959,\n",
              "  0.5303347110748291,\n",
              "  0.6067730188369751,\n",
              "  0.5525346994400024,\n",
              "  0.6105039119720459,\n",
              "  0.41636356711387634,\n",
              "  0.44661808013916016,\n",
              "  0.5156651735305786,\n",
              "  0.4588017463684082,\n",
              "  0.5386911630630493,\n",
              "  0.45326414704322815,\n",
              "  0.5259321928024292,\n",
              "  0.5073332786560059,\n",
              "  0.4439419209957123,\n",
              "  0.3407461941242218,\n",
              "  0.5383959412574768,\n",
              "  0.5368062257766724,\n",
              "  0.44718024134635925,\n",
              "  0.536624014377594,\n",
              "  0.4844900071620941,\n",
              "  0.5215811133384705,\n",
              "  0.5003399848937988,\n",
              "  0.3721258342266083,\n",
              "  0.4808052182197571,\n",
              "  0.4677320718765259,\n",
              "  0.38017934560775757,\n",
              "  0.562151312828064,\n",
              "  0.5947889089584351,\n",
              "  0.4771755337715149,\n",
              "  0.47653597593307495,\n",
              "  0.4080960154533386,\n",
              "  0.49444517493247986,\n",
              "  0.4684479534626007,\n",
              "  0.5725241303443909,\n",
              "  0.6238958239555359,\n",
              "  0.45225727558135986,\n",
              "  0.5066670179367065,\n",
              "  0.43682652711868286,\n",
              "  0.41409409046173096,\n",
              "  0.47184497117996216,\n",
              "  0.5506196022033691,\n",
              "  0.4713831841945648,\n",
              "  0.5646472573280334,\n",
              "  0.42342910170555115,\n",
              "  0.6221747398376465,\n",
              "  0.5589619874954224,\n",
              "  0.4963987469673157,\n",
              "  0.37076079845428467,\n",
              "  0.46875518560409546,\n",
              "  0.5424019694328308,\n",
              "  0.5313056707382202,\n",
              "  0.542809247970581,\n",
              "  0.4039798378944397,\n",
              "  0.5199504494667053,\n",
              "  0.5866920948028564,\n",
              "  0.537638247013092,\n",
              "  0.5528901815414429,\n",
              "  0.5087863802909851,\n",
              "  0.5951090455055237,\n",
              "  0.460639625787735,\n",
              "  0.523135244846344,\n",
              "  0.3564593195915222,\n",
              "  0.4856533706188202,\n",
              "  0.5107195377349854,\n",
              "  0.5640509724617004,\n",
              "  0.48123636841773987,\n",
              "  0.42924392223358154,\n",
              "  0.35889720916748047,\n",
              "  0.5714178681373596,\n",
              "  0.5383074879646301,\n",
              "  0.5163325071334839,\n",
              "  0.5070972442626953,\n",
              "  0.5182439088821411,\n",
              "  0.433563768863678,\n",
              "  0.5788372755050659,\n",
              "  0.46503520011901855,\n",
              "  0.43116194009780884,\n",
              "  0.5038052201271057,\n",
              "  0.465855211019516,\n",
              "  0.5863771438598633,\n",
              "  0.6126362085342407,\n",
              "  0.45438048243522644,\n",
              "  0.555923581123352,\n",
              "  0.6114940643310547,\n",
              "  0.578851580619812,\n",
              "  0.5629928112030029,\n",
              "  0.5525675415992737,\n",
              "  0.5587867498397827,\n",
              "  0.6215131878852844,\n",
              "  0.5758614540100098,\n",
              "  0.48732051253318787,\n",
              "  0.47945597767829895,\n",
              "  0.6116346716880798,\n",
              "  0.535309910774231,\n",
              "  0.6118710041046143,\n",
              "  0.5040725469589233,\n",
              "  0.6056342124938965,\n",
              "  0.57578444480896,\n",
              "  0.48839670419692993,\n",
              "  0.4990694522857666,\n",
              "  0.4320284426212311,\n",
              "  0.5177000164985657,\n",
              "  0.4393683671951294,\n",
              "  0.6358554363250732,\n",
              "  0.48487940430641174,\n",
              "  0.4275726079940796,\n",
              "  0.5041717290878296,\n",
              "  0.6389675140380859,\n",
              "  0.5063386559486389,\n",
              "  0.6355195045471191,\n",
              "  0.3946872353553772,\n",
              "  0.3645254671573639,\n",
              "  0.5310862064361572,\n",
              "  0.4691578447818756,\n",
              "  0.5075811743736267,\n",
              "  0.6160997152328491,\n",
              "  0.5251234769821167,\n",
              "  0.4989871084690094,\n",
              "  0.6034015417098999,\n",
              "  0.5893768668174744,\n",
              "  0.4739544987678528,\n",
              "  0.48854586482048035,\n",
              "  0.5244927406311035,\n",
              "  0.5452038049697876,\n",
              "  0.404011070728302,\n",
              "  0.40702760219573975,\n",
              "  0.4645960330963135,\n",
              "  0.6009371876716614,\n",
              "  0.5568525195121765,\n",
              "  0.4108172655105591,\n",
              "  0.5847177505493164,\n",
              "  0.37202563881874084,\n",
              "  0.4322397708892822,\n",
              "  0.5237652063369751,\n",
              "  0.5625882148742676,\n",
              "  0.6047191023826599,\n",
              "  0.5324007272720337,\n",
              "  0.4100177586078644,\n",
              "  0.5989484190940857,\n",
              "  0.5181589126586914,\n",
              "  0.4905661344528198,\n",
              "  0.5652382373809814,\n",
              "  0.41417884826660156,\n",
              "  0.5304345488548279,\n",
              "  0.6012471914291382,\n",
              "  0.544131875038147,\n",
              "  0.6063399910926819,\n",
              "  0.5692614912986755,\n",
              "  0.5462989807128906,\n",
              "  0.5230710506439209,\n",
              "  0.5042514204978943,\n",
              "  0.6191053986549377,\n",
              "  0.34087899327278137,\n",
              "  0.3957154452800751,\n",
              "  0.48230981826782227,\n",
              "  0.5423426628112793,\n",
              "  0.549209475517273,\n",
              "  0.40755149722099304,\n",
              "  0.5773090124130249,\n",
              "  0.5197681784629822,\n",
              "  0.5590403079986572,\n",
              "  0.3822747468948364,\n",
              "  0.4359772503376007,\n",
              "  0.6319305896759033,\n",
              "  0.4526171088218689,\n",
              "  0.5676834583282471,\n",
              "  0.5606495141983032,\n",
              "  0.6370214223861694,\n",
              "  0.6699432134628296,\n",
              "  0.4815800189971924,\n",
              "  0.44420188665390015,\n",
              "  0.5034871101379395,\n",
              "  0.5460900068283081,\n",
              "  0.558682918548584,\n",
              "  0.5259518027305603,\n",
              "  0.5027368664741516,\n",
              "  0.4442739486694336,\n",
              "  0.5751343965530396,\n",
              "  0.5918697714805603,\n",
              "  0.5857770442962646,\n",
              "  0.48870524764060974,\n",
              "  0.47974783182144165,\n",
              "  0.5417969226837158,\n",
              "  0.4694969058036804,\n",
              "  0.55550217628479,\n",
              "  0.5689401030540466,\n",
              "  0.4827350080013275,\n",
              "  0.4617801904678345,\n",
              "  0.5530639290809631,\n",
              "  0.37039077281951904,\n",
              "  0.49151134490966797,\n",
              "  0.430255651473999,\n",
              "  0.4523927867412567,\n",
              "  0.5128390192985535,\n",
              "  0.5357229709625244,\n",
              "  0.4438208341598511,\n",
              "  0.46728917956352234,\n",
              "  0.5816922187805176,\n",
              "  0.5164752006530762,\n",
              "  0.48416945338249207,\n",
              "  0.5121074318885803,\n",
              "  0.43887609243392944,\n",
              "  0.4019356369972229,\n",
              "  0.48149025440216064,\n",
              "  0.40278637409210205,\n",
              "  0.5070226788520813,\n",
              "  0.3324144184589386,\n",
              "  0.4015963673591614,\n",
              "  0.4017196595668793,\n",
              "  0.5201540589332581,\n",
              "  0.5208395719528198,\n",
              "  0.42338019609451294,\n",
              "  0.3517872393131256,\n",
              "  0.5385240316390991,\n",
              "  0.31517791748046875,\n",
              "  0.5975741147994995,\n",
              "  0.5225539207458496,\n",
              "  0.4785660207271576,\n",
              "  0.48063015937805176,\n",
              "  0.4706527292728424,\n",
              "  0.4626876413822174,\n",
              "  0.5731414556503296,\n",
              "  0.526678204536438,\n",
              "  0.5559507608413696,\n",
              "  0.47343412041664124,\n",
              "  0.48061737418174744,\n",
              "  0.3555362820625305,\n",
              "  0.516740083694458,\n",
              "  0.437166303396225,\n",
              "  0.5933945178985596,\n",
              "  0.3111978769302368,\n",
              "  0.45387688279151917,\n",
              "  0.5568061470985413,\n",
              "  0.4230876564979553,\n",
              "  0.38817793130874634,\n",
              "  0.31145554780960083,\n",
              "  0.42843300104141235,\n",
              "  0.5538817644119263,\n",
              "  0.5484805107116699,\n",
              "  0.4493752717971802,\n",
              "  0.5535056591033936,\n",
              "  0.4111320972442627,\n",
              "  0.4691261053085327,\n",
              "  0.5742282867431641,\n",
              "  0.4888259768486023,\n",
              "  0.5543520450592041,\n",
              "  0.46326303482055664,\n",
              "  0.4610549509525299,\n",
              "  0.5430073738098145,\n",
              "  0.5409550666809082,\n",
              "  0.47901761531829834,\n",
              "  0.5187934637069702,\n",
              "  0.4101365804672241,\n",
              "  0.5736054182052612,\n",
              "  0.4476926326751709,\n",
              "  0.5052894949913025,\n",
              "  0.51063472032547,\n",
              "  0.6666489839553833,\n",
              "  0.6564505696296692,\n",
              "  0.47246357798576355,\n",
              "  0.5061614513397217,\n",
              "  0.5222464203834534,\n",
              "  0.5024583339691162,\n",
              "  0.43561190366744995,\n",
              "  0.5665314197540283,\n",
              "  0.4795457124710083,\n",
              "  0.5709571838378906,\n",
              "  0.5614362359046936,\n",
              "  0.5275020003318787,\n",
              "  0.5982811450958252,\n",
              "  0.4296228885650635,\n",
              "  0.5631530284881592,\n",
              "  0.5177202224731445,\n",
              "  0.4972497224807739,\n",
              "  0.5340073704719543,\n",
              "  0.5996080636978149,\n",
              "  0.5715335607528687,\n",
              "  0.4637047052383423,\n",
              "  0.37065523862838745,\n",
              "  0.5641461610794067,\n",
              "  0.5410051345825195,\n",
              "  0.4890030324459076,\n",
              "  0.5149902105331421,\n",
              "  0.6182447671890259,\n",
              "  0.6205803155899048,\n",
              "  0.42761480808258057,\n",
              "  0.4517025053501129,\n",
              "  0.47808346152305603,\n",
              "  0.3836717903614044,\n",
              "  0.5638558864593506,\n",
              "  0.6481324434280396,\n",
              "  0.518413782119751,\n",
              "  0.6648153066635132,\n",
              "  0.3571361303329468,\n",
              "  0.5858876705169678,\n",
              "  0.3890078663825989,\n",
              "  0.5211307406425476,\n",
              "  0.5776201486587524,\n",
              "  0.6364119052886963,\n",
              "  0.4881914258003235,\n",
              "  0.5707287788391113,\n",
              "  0.3760231137275696,\n",
              "  0.5643702745437622,\n",
              "  0.530541718006134,\n",
              "  0.49718594551086426,\n",
              "  0.4684613049030304,\n",
              "  0.52959144115448,\n",
              "  0.6236163377761841,\n",
              "  0.376258909702301,\n",
              "  0.6165115833282471,\n",
              "  0.5561491250991821,\n",
              "  0.5206480026245117,\n",
              "  0.5330449938774109,\n",
              "  0.41000598669052124,\n",
              "  0.5513418912887573,\n",
              "  0.5531424880027771,\n",
              "  0.5667681097984314,\n",
              "  0.6607036590576172,\n",
              "  0.37505385279655457,\n",
              "  0.5714502334594727,\n",
              "  0.4581755995750427,\n",
              "  0.6803790330886841,\n",
              "  0.5532318353652954,\n",
              "  0.5964725017547607,\n",
              "  0.6177608966827393,\n",
              "  0.5798736810684204,\n",
              "  0.3282564878463745,\n",
              "  0.506026029586792,\n",
              "  0.4742051959037781,\n",
              "  0.4819719195365906,\n",
              "  0.5455865859985352,\n",
              "  0.35275423526763916,\n",
              "  0.6120773553848267,\n",
              "  0.6748375296592712,\n",
              "  0.5553557872772217,\n",
              "  0.4521939158439636,\n",
              "  0.5175642371177673,\n",
              "  0.401313841342926,\n",
              "  0.5762064456939697,\n",
              "  0.5645012855529785,\n",
              "  0.5905334949493408,\n",
              "  0.46138542890548706,\n",
              "  0.46367955207824707,\n",
              "  0.5906094312667847,\n",
              "  0.517889678478241,\n",
              "  0.5656142234802246,\n",
              "  0.6281949281692505,\n",
              "  0.6641197204589844,\n",
              "  0.5972763299942017,\n",
              "  0.5058783888816833,\n",
              "  0.4107498228549957,\n",
              "  0.4545547664165497,\n",
              "  0.4475550651550293,\n",
              "  0.6193417310714722,\n",
              "  0.4371049404144287,\n",
              "  0.3839702308177948,\n",
              "  0.6037859916687012,\n",
              "  0.5088746547698975,\n",
              "  0.5779238343238831,\n",
              "  0.5120602250099182,\n",
              "  0.5057388544082642,\n",
              "  0.5144917964935303,\n",
              "  0.4183005094528198,\n",
              "  0.553974986076355,\n",
              "  0.6714104413986206,\n",
              "  0.6705294847488403,\n",
              "  0.49694082140922546,\n",
              "  0.6153894066810608,\n",
              "  0.5297071933746338,\n",
              "  0.5655927658081055,\n",
              "  0.6360548734664917,\n",
              "  0.4217689633369446,\n",
              "  0.5260308980941772,\n",
              "  0.6047933101654053,\n",
              "  0.4256071150302887,\n",
              "  0.6047943830490112,\n",
              "  0.632485032081604,\n",
              "  0.6522639989852905,\n",
              "  0.598087728023529,\n",
              "  0.5730507969856262,\n",
              "  0.49413183331489563,\n",
              "  0.6596535444259644,\n",
              "  0.3853992819786072,\n",
              "  0.5797171592712402,\n",
              "  0.6317411661148071,\n",
              "  0.3166208267211914,\n",
              "  0.6296063661575317,\n",
              "  0.4914356470108032,\n",
              "  0.4206421375274658,\n",
              "  0.38318756222724915,\n",
              "  0.5642412900924683,\n",
              "  0.6702687740325928,\n",
              "  0.30355051159858704,\n",
              "  0.47520911693573,\n",
              "  0.41660478711128235,\n",
              "  0.4745573401451111,\n",
              "  0.4832428991794586,\n",
              "  0.6660796999931335,\n",
              "  0.5425161123275757,\n",
              "  0.3459772765636444,\n",
              "  0.5335104465484619,\n",
              "  0.40351903438568115,\n",
              "  0.391371488571167,\n",
              "  0.4252309203147888,\n",
              "  0.5562840104103088,\n",
              "  0.6211277842521667,\n",
              "  0.5420190691947937,\n",
              "  0.6248207092285156,\n",
              "  0.6946913003921509,\n",
              "  0.5433006286621094,\n",
              "  0.5430015325546265,\n",
              "  0.5220205783843994,\n",
              "  0.5727882981300354,\n",
              "  0.6291747689247131,\n",
              "  0.590509295463562,\n",
              "  0.6390829086303711,\n",
              "  0.4902607798576355,\n",
              "  0.5636612176895142,\n",
              "  0.5781190395355225,\n",
              "  0.5469815731048584,\n",
              "  0.5665026903152466,\n",
              "  0.6498162746429443,\n",
              "  0.6001462340354919,\n",
              "  0.5432837009429932,\n",
              "  0.5346325635910034,\n",
              "  0.44644948840141296,\n",
              "  0.5705397129058838,\n",
              "  0.5497953295707703,\n",
              "  0.6243739724159241,\n",
              "  0.5765968561172485,\n",
              "  0.5791723728179932,\n",
              "  0.5256389379501343,\n",
              "  0.44750332832336426,\n",
              "  0.35692641139030457,\n",
              "  0.5239269137382507,\n",
              "  0.5763351917266846,\n",
              "  0.3773277997970581,\n",
              "  0.5435863137245178,\n",
              "  0.5893736481666565,\n",
              "  0.6743455529212952,\n",
              "  0.5371085405349731,\n",
              "  0.48829466104507446,\n",
              "  0.5710912942886353,\n",
              "  0.6276278495788574,\n",
              "  0.4466988444328308,\n",
              "  0.4104393422603607,\n",
              "  0.4335063099861145,\n",
              "  0.3541586399078369,\n",
              "  0.6394995450973511,\n",
              "  0.49848151206970215,\n",
              "  0.5369046330451965,\n",
              "  0.5998233556747437,\n",
              "  0.37951231002807617,\n",
              "  0.4532912075519562,\n",
              "  0.4453887939453125,\n",
              "  0.5495368242263794,\n",
              "  0.6016671061515808,\n",
              "  0.5849459171295166,\n",
              "  0.5202712416648865,\n",
              "  0.5034928917884827,\n",
              "  0.5513988733291626,\n",
              "  0.5745826959609985,\n",
              "  0.3736586570739746,\n",
              "  0.4057695269584656,\n",
              "  0.5363845229148865,\n",
              "  0.36484265327453613,\n",
              "  0.5650321245193481,\n",
              "  0.586216926574707,\n",
              "  0.5590301156044006,\n",
              "  0.4926929473876953,\n",
              "  0.5816498398780823,\n",
              "  0.5676665306091309,\n",
              "  0.511520266532898,\n",
              "  0.5223485231399536,\n",
              "  0.5334861278533936,\n",
              "  0.523694634437561,\n",
              "  0.6201493740081787,\n",
              "  0.6463748216629028,\n",
              "  0.5702533721923828,\n",
              "  0.6056530475616455,\n",
              "  0.3136462867259979,\n",
              "  0.4242880344390869,\n",
              "  0.5675950050354004,\n",
              "  0.5319867134094238,\n",
              "  0.6302154660224915,\n",
              "  0.4362850785255432,\n",
              "  0.5662033557891846,\n",
              "  0.4351275563240051,\n",
              "  0.5228581428527832,\n",
              "  0.2978115677833557,\n",
              "  0.5988339185714722,\n",
              "  0.6183609962463379,\n",
              "  0.39730319380760193,\n",
              "  0.5613943934440613,\n",
              "  0.5652626752853394,\n",
              "  0.6687887907028198],\n",
              " 'val_acc': [0.625,\n",
              "  0.625,\n",
              "  0.5,\n",
              "  0.5625,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.5,\n",
              "  0.4375,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.5,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.5,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.5,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.5,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.5,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.9375,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.5625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.9375,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.875,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.75,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.8125,\n",
              "  0.75,\n",
              "  0.8125,\n",
              "  0.5625]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "b0818fb3-0a11-438d-bc62-11d4b21d14af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/nsc.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/nsc.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "bbb60c3b-a0dd-4775-b1b1-255463721e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abdaee38-d8e9-47fd-bbed-59eb74b8db0e\", \"nsc.h5\", 16604176)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}