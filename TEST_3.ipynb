{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPLXeCjATvMfs5q1YKx6KUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/TEST_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeLTtSVtcC8l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QbdKKUPWcHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "ynE16LtRcIDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - All SEM Remake.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "2e1dBS9-cIBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/new/new_SEM2.h5')\n",
        "\n",
        "from efficientnet.layers import Swish, DropConnect\n",
        "from efficientnet.model import ConvKernalInitializer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'ConvKernalInitializer': ConvKernalInitializer,\n",
        "    'Swish': Swish,\n",
        "    'DropConnect':DropConnect\n",
        "})"
      ],
      "metadata": {
        "id": "xHc4x9_4cH_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model \n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/new/new_SEM2.h5')\n",
        "height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "MOBaYPEPcH9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/Project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "t40_cvUGcH69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "     \n",
        "\n",
        "#label\n",
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "sRyKR7apciUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = df[df['No'].between(288,320)] \n",
        "test"
      ],
      "metadata": {
        "id": "bWH_pHQfc_Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act = test['Class'].tolist()\n",
        "path = test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "6i-KZLLQeR0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def predict_image(img_path): #สร้างฟังก์ชัน \n",
        "    # Read the image and resize it\n",
        "    img = image.load_img(img_path, target_size=(height, width)) #รูปทุกรูปมีขนาดไม่เท่ากันจึงตั้งไว้ว่าเวลาจะให้ model ดึงรูปมาทำนายให้ดึงรูปเข้ามาตามขนาดที่ตั้งไว้ตาม พารามิตเตอร์\n",
        "    # Convert it to a Numpy array with target shape.\n",
        "    x = image.img_to_array(img) # model ไม่สามารถทำนายรูปภาพโดยตรงได้ จึงเเปลงรูปภาพให้เป็น array เเล้วให้โมเดลทำนาย\n",
        "    # Reshape\n",
        "    x = x.reshape((1,) + x.shape) # เพิ่ม ไดเมนชั่นของโมเดล ให้เป็น 4 ได้ เมนชั่น โดยที่ 1, คือบอกให้โมเดลนำเข้าทีละ 1 รูปเเล้วค่อยทำนาย เเละ + ไดเมนชั่น คือ 150 ,150 ,3(สีของ RGB)\n",
        "    x /= 255.\n",
        "    result = model.predict([x])\n",
        "    return result[0]"
      ],
      "metadata": {
        "id": "z5iqp4DRc_QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "h-EHdemCeHVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no = np.argmax(predict_image(path[0]))\n",
        "labels[no]"
      ],
      "metadata": {
        "id": "f9uMRIycd8-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_image(path[1]) "
      ],
      "metadata": {
        "id": "II-syTLoOlwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(predict_image(path[1]))"
      ],
      "metadata": {
        "id": "868vXuvwO68T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "pred_list = list()\n",
        "prob_list = list()\n",
        "img_path= path\n",
        "for i in range(0,len(img_path)):\n",
        "    predict = predict_image(img_path[i])\n",
        "    result = np.argmax(predict)\n",
        "    pred_list.append(labels[result])\n",
        "    prob_list.append(predict[result])"
      ],
      "metadata": {
        "id": "sgQx49sLeJry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "YGxk04fDUAHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_list[0]\n",
        " "
      ],
      "metadata": {
        "id": "etoqDSoJecau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list[0]"
      ],
      "metadata": {
        "id": "z_uBr0xBeeic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data_train = valframe\n",
        "# #เช็คคลาสใน Predicted\n",
        "# pred_class = set(data_train['category_ov'])\n",
        "# print('Predicted : ',len(pred_class))\n",
        "# print(pred_class)\n",
        "# print('-'*50)\n",
        "# #เช็คคลาสใน Actual\n",
        "# classe = set(data_train['class'])\n",
        "# print('Actual : ',len(classe))\n",
        "# print(classe)"
      ],
      "metadata": {
        "id": "Jzpqr7ntejB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array(pred_list)\n",
        "act = np.array(act)\n",
        "     "
      ],
      "metadata": {
        "id": "pD91tPR8ei_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act"
      ],
      "metadata": {
        "id": "AuhU1rO4N5yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "ctcOEKvlOAWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# act = data_train['class'].array\n",
        "# pred = data_train['category_ov'].array\n",
        "\n",
        "cmat = confusion_matrix(act, pred)\n",
        "print('classifier accuracy = {}%'.format((100.*np.trace(cmat))/(np.sum(cmat))))\n",
        "\n",
        "#Marking the Confusion Matrix\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(act, pred))#performance"
      ],
      "metadata": {
        "id": "5k4G5QTTei9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create CF \n",
        "data = {'Actual': act,'Predicted' : pred,}\n",
        "df = pd.DataFrame(data, columns=['Actual','Predicted'])\n",
        "conf_mat = pd.crosstab(df['Actual'],df['Predicted'],rownames=['Actual'],colnames=['Predicted'])\n",
        "\n",
        "#Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(act, pred)\n",
        "\n",
        "#plot Confusion matrix\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "ax = sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"YlGnBu\") #Blues,Oranges,Reds\n",
        "ax.set_title('Confusion matrix',fontsize=20)\n",
        "ax.set_ylabel('True label',fontsize=18)\n",
        "ax.set_xlabel('Predicted label',fontsize=18)"
      ],
      "metadata": {
        "id": "Lu6W3y4eenRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}