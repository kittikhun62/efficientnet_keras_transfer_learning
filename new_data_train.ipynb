{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+6chwZPfbMGf/bE0Xx4Gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "42938278-5771-4107-bd8f-a14d9b1cfdc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "c142a136-e03a-4821-cfc9-4779da42c28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "840  841  1-s2.0-S2095268622000210-main   \n",
              "841  842  1-s2.0-S2095268622000210-main   \n",
              "842  843  1-s2.0-S2095268622000210-main   \n",
              "843  844  1-s2.0-S2095268622000210-main   \n",
              "844  845  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "840  Integration of preparation of K, Na-embedded a...   \n",
              "841  Integration of preparation of K, Na-embedded a...   \n",
              "842  Integration of preparation of K, Na-embedded a...   \n",
              "843  Integration of preparation of K, Na-embedded a...   \n",
              "844  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "840  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "841  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "842  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "843  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "844  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture    detail  Class  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...  original  0-500   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom1  0-500   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom2  0-500   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom3  0-500   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom4  0-500   \n",
              "..                                                 ...       ...    ...   \n",
              "840  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom21  0-500   \n",
              "841  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom22  0-500   \n",
              "842  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom23  0-500   \n",
              "843  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom24  0-500   \n",
              "844  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom25  0-500   \n",
              "\n",
              "        BET  Size(mico)  \n",
              "0    135.06           5  \n",
              "1    135.06          10  \n",
              "2    135.06          10  \n",
              "3    135.06          10  \n",
              "4    135.06          10  \n",
              "..      ...         ...  \n",
              "840  301.70          10  \n",
              "841  301.70          10  \n",
              "842  301.70          10  \n",
              "843  301.70          10  \n",
              "844  301.70          10  \n",
              "\n",
              "[845 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a19039c-e409-413a-b358-82e8263b6265\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>841</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>842</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>843</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>844</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>845</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>845 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a19039c-e409-413a-b358-82e8263b6265')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a19039c-e409-413a-b358-82e8263b6265 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a19039c-e409-413a-b358-82e8263b6265');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "b4ad4ab4-4477-41d6-a8ef-d17f49725fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "db19a5b2-7526-4fb1-e6d3-adff1e5ef1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "e83483e8-3a3d-4f30-f990-ebe6c489d8c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "7485d51a-39a5-4e29-86c4-2ae96bdec67b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 653, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 653 (delta 143), reused 103 (delta 103), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (653/653), 12.00 MiB | 22.71 MiB/s, done.\n",
            "Resolving deltas: 100% (383/383), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(682,764)]\n",
        "train = df[df['No'].between(1,681)]\n",
        "test = df[df['No'].between(765,845)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "25c1016c-49e4-40bc-d9d4-ddbb5e6abcad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 59\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 62\n",
            "total test 2 images: 6\n",
            "total test 3 images: 15 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 681  # จำนวนภาพ Train\n",
        "NUM_TEST = 82 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "1e9f25bf-5ac1-4992-fc5b-eb722c82bb7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "441ae8d0-bfc8-4588-847a-52a2fb4d2515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "5883f8b2-07dd-47c4-89d5-a4a46414b490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "067b74fc-8f1c-4328-9056-c186bca6c8ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "431f33b4-50b8-4afb-befb-9231b5c5d8d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 81 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2844192c-1f28-4b09-ce55-a67f89fbb62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 35s 2s/step - loss: 2.2823 - acc: 0.4571 - val_loss: 2.7396 - val_acc: 0.1406\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 2.0491 - acc: 0.4538 - val_loss: 2.6373 - val_acc: 0.1562\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 2.1035 - acc: 0.4392 - val_loss: 2.1516 - val_acc: 0.1875\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.7835 - acc: 0.4425 - val_loss: 2.1842 - val_acc: 0.1562\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.7262 - acc: 0.4538 - val_loss: 1.9318 - val_acc: 0.1875\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.7001 - acc: 0.4360 - val_loss: 1.7350 - val_acc: 0.2344\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.6490 - acc: 0.4052 - val_loss: 1.6983 - val_acc: 0.2344\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.6319 - acc: 0.3906 - val_loss: 1.5700 - val_acc: 0.2969\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.6595 - acc: 0.4003 - val_loss: 1.5033 - val_acc: 0.3125\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5686 - acc: 0.4165 - val_loss: 1.4419 - val_acc: 0.3594\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.6126 - acc: 0.3906 - val_loss: 1.5756 - val_acc: 0.2969\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5789 - acc: 0.3841 - val_loss: 1.4646 - val_acc: 0.3594\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.5606 - acc: 0.4016 - val_loss: 1.4756 - val_acc: 0.3281\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4535 - acc: 0.4165 - val_loss: 1.4580 - val_acc: 0.3594\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5950 - acc: 0.4036 - val_loss: 1.4638 - val_acc: 0.3281\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5553 - acc: 0.3890 - val_loss: 1.4450 - val_acc: 0.3750\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5263 - acc: 0.4052 - val_loss: 1.4618 - val_acc: 0.3281\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5440 - acc: 0.3922 - val_loss: 1.4776 - val_acc: 0.3594\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5108 - acc: 0.4036 - val_loss: 1.4091 - val_acc: 0.4062\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5756 - acc: 0.3776 - val_loss: 1.3863 - val_acc: 0.3906\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4615 - acc: 0.3987 - val_loss: 1.3617 - val_acc: 0.3750\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5323 - acc: 0.3987 - val_loss: 1.4094 - val_acc: 0.3438\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5038 - acc: 0.3922 - val_loss: 1.4814 - val_acc: 0.3438\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4721 - acc: 0.3906 - val_loss: 1.4056 - val_acc: 0.3594\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5077 - acc: 0.3874 - val_loss: 1.3883 - val_acc: 0.3750\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.6380 - acc: 0.3647 - val_loss: 1.4273 - val_acc: 0.3438\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.4242 - acc: 0.4068 - val_loss: 1.3705 - val_acc: 0.3594\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4622 - acc: 0.3812 - val_loss: 1.3295 - val_acc: 0.4062\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5080 - acc: 0.3679 - val_loss: 1.3735 - val_acc: 0.3438\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4928 - acc: 0.3938 - val_loss: 1.3407 - val_acc: 0.3750\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4765 - acc: 0.3776 - val_loss: 1.4586 - val_acc: 0.3125\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4810 - acc: 0.3906 - val_loss: 1.4851 - val_acc: 0.3438\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5199 - acc: 0.3566 - val_loss: 1.4979 - val_acc: 0.2812\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4780 - acc: 0.4109 - val_loss: 1.3759 - val_acc: 0.3594\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4623 - acc: 0.3906 - val_loss: 1.4230 - val_acc: 0.3438\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3875 - acc: 0.4392 - val_loss: 1.4478 - val_acc: 0.3438\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4734 - acc: 0.4019 - val_loss: 1.4732 - val_acc: 0.3125\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4869 - acc: 0.4100 - val_loss: 1.3486 - val_acc: 0.3594\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4686 - acc: 0.4109 - val_loss: 1.4255 - val_acc: 0.3125\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.5285 - acc: 0.3760 - val_loss: 1.5100 - val_acc: 0.2656\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4068 - acc: 0.4182 - val_loss: 1.4733 - val_acc: 0.3125\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3604 - acc: 0.4376 - val_loss: 1.4518 - val_acc: 0.2969\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4461 - acc: 0.3857 - val_loss: 1.4975 - val_acc: 0.2656\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4283 - acc: 0.4036 - val_loss: 1.4900 - val_acc: 0.2344\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4727 - acc: 0.4198 - val_loss: 1.4706 - val_acc: 0.2969\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.3810 - acc: 0.4311 - val_loss: 1.4677 - val_acc: 0.2812\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4072 - acc: 0.4165 - val_loss: 1.4660 - val_acc: 0.2188\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4887 - acc: 0.3906 - val_loss: 1.4082 - val_acc: 0.2812\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4604 - acc: 0.4052 - val_loss: 1.4166 - val_acc: 0.2969\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3395 - acc: 0.4457 - val_loss: 1.5040 - val_acc: 0.2500\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4541 - acc: 0.3971 - val_loss: 1.5107 - val_acc: 0.2500\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4121 - acc: 0.4182 - val_loss: 1.4223 - val_acc: 0.3125\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4961 - acc: 0.4149 - val_loss: 1.5314 - val_acc: 0.2500\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4185 - acc: 0.4214 - val_loss: 1.4693 - val_acc: 0.2969\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3379 - acc: 0.4473 - val_loss: 1.4038 - val_acc: 0.2812\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4227 - acc: 0.3971 - val_loss: 1.4140 - val_acc: 0.2656\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.4033 - acc: 0.3825 - val_loss: 1.4977 - val_acc: 0.2344\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4013 - acc: 0.4052 - val_loss: 1.4802 - val_acc: 0.2656\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4595 - acc: 0.3971 - val_loss: 1.4301 - val_acc: 0.2969\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3976 - acc: 0.4266 - val_loss: 1.5415 - val_acc: 0.2344\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2811 - acc: 0.4538 - val_loss: 1.4551 - val_acc: 0.2812\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3674 - acc: 0.4214 - val_loss: 1.4571 - val_acc: 0.2656\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3246 - acc: 0.4554 - val_loss: 1.4877 - val_acc: 0.2656\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.4433 - acc: 0.4311 - val_loss: 1.4659 - val_acc: 0.2500\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3869 - acc: 0.4165 - val_loss: 1.5034 - val_acc: 0.2500\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3671 - acc: 0.4149 - val_loss: 1.5228 - val_acc: 0.2344\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4192 - acc: 0.4230 - val_loss: 1.4581 - val_acc: 0.2656\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3085 - acc: 0.4408 - val_loss: 1.5257 - val_acc: 0.2188\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3407 - acc: 0.4214 - val_loss: 1.5138 - val_acc: 0.2656\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4179 - acc: 0.3955 - val_loss: 1.4520 - val_acc: 0.2500\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.4136 - acc: 0.4344 - val_loss: 1.4100 - val_acc: 0.2969\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3624 - acc: 0.4133 - val_loss: 1.6273 - val_acc: 0.2031\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2755 - acc: 0.4344 - val_loss: 1.5127 - val_acc: 0.2500\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3642 - acc: 0.4230 - val_loss: 1.5289 - val_acc: 0.2188\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3452 - acc: 0.4327 - val_loss: 1.5790 - val_acc: 0.2188\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3699 - acc: 0.4214 - val_loss: 1.3981 - val_acc: 0.2812\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3160 - acc: 0.4230 - val_loss: 1.5024 - val_acc: 0.2344\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.4057 - acc: 0.3938 - val_loss: 1.5742 - val_acc: 0.2500\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2591 - acc: 0.4684 - val_loss: 1.5512 - val_acc: 0.2344\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3471 - acc: 0.3971 - val_loss: 1.5133 - val_acc: 0.2344\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3139 - acc: 0.4522 - val_loss: 1.5046 - val_acc: 0.2188\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3600 - acc: 0.4230 - val_loss: 1.5085 - val_acc: 0.2031\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 32s 2s/step - loss: 1.3483 - acc: 0.4182 - val_loss: 1.5730 - val_acc: 0.2188\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3498 - acc: 0.4360 - val_loss: 1.5533 - val_acc: 0.2031\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3997 - acc: 0.4141 - val_loss: 1.5156 - val_acc: 0.2031\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2603 - acc: 0.4392 - val_loss: 1.5285 - val_acc: 0.2344\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3927 - acc: 0.4295 - val_loss: 1.5424 - val_acc: 0.2656\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3713 - acc: 0.4279 - val_loss: 1.5794 - val_acc: 0.2344\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.3017 - acc: 0.4522 - val_loss: 1.5836 - val_acc: 0.2344\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3480 - acc: 0.3971 - val_loss: 1.5392 - val_acc: 0.2188\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3739 - acc: 0.4198 - val_loss: 1.6113 - val_acc: 0.2188\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2845 - acc: 0.4376 - val_loss: 1.5720 - val_acc: 0.1719\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2984 - acc: 0.4376 - val_loss: 1.5334 - val_acc: 0.2500\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3086 - acc: 0.4376 - val_loss: 1.5372 - val_acc: 0.2188\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3286 - acc: 0.4198 - val_loss: 1.5515 - val_acc: 0.1875\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3782 - acc: 0.3906 - val_loss: 1.6279 - val_acc: 0.1562\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3620 - acc: 0.4246 - val_loss: 1.5563 - val_acc: 0.2344\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3474 - acc: 0.4263 - val_loss: 1.4828 - val_acc: 0.2344\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3705 - acc: 0.4327 - val_loss: 1.5594 - val_acc: 0.2188\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3130 - acc: 0.4425 - val_loss: 1.5016 - val_acc: 0.2344\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3288 - acc: 0.4133 - val_loss: 1.4863 - val_acc: 0.2500\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3041 - acc: 0.4506 - val_loss: 1.5608 - val_acc: 0.2344\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3378 - acc: 0.4489 - val_loss: 1.6044 - val_acc: 0.2031\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3109 - acc: 0.4376 - val_loss: 1.5880 - val_acc: 0.2344\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3022 - acc: 0.4408 - val_loss: 1.5778 - val_acc: 0.2188\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2660 - acc: 0.4603 - val_loss: 1.5066 - val_acc: 0.2188\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.2712 - acc: 0.4571 - val_loss: 1.5419 - val_acc: 0.2344\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1929 - acc: 0.4652 - val_loss: 1.6041 - val_acc: 0.1875\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3071 - acc: 0.4538 - val_loss: 1.4992 - val_acc: 0.2344\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2520 - acc: 0.4716 - val_loss: 1.5693 - val_acc: 0.2188\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1992 - acc: 0.4959 - val_loss: 1.5609 - val_acc: 0.2188\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3094 - acc: 0.4230 - val_loss: 1.4605 - val_acc: 0.2500\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3337 - acc: 0.4149 - val_loss: 1.5525 - val_acc: 0.2344\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3085 - acc: 0.4328 - val_loss: 1.5510 - val_acc: 0.2500\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.3007 - acc: 0.4344 - val_loss: 1.5772 - val_acc: 0.2031\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2527 - acc: 0.4554 - val_loss: 1.5285 - val_acc: 0.2500\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2802 - acc: 0.4441 - val_loss: 1.5826 - val_acc: 0.2500\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2095 - acc: 0.4635 - val_loss: 1.5881 - val_acc: 0.2344\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2563 - acc: 0.4522 - val_loss: 1.5203 - val_acc: 0.2500\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2117 - acc: 0.4635 - val_loss: 1.5808 - val_acc: 0.2500\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2243 - acc: 0.4700 - val_loss: 1.5696 - val_acc: 0.2031\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1986 - acc: 0.4641 - val_loss: 1.4888 - val_acc: 0.2344\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1789 - acc: 0.4862 - val_loss: 1.5827 - val_acc: 0.1875\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2878 - acc: 0.4652 - val_loss: 1.6114 - val_acc: 0.2344\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2616 - acc: 0.4149 - val_loss: 1.6136 - val_acc: 0.2188\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2717 - acc: 0.4311 - val_loss: 1.5101 - val_acc: 0.2500\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 27s 2s/step - loss: 1.3054 - acc: 0.4391 - val_loss: 1.6064 - val_acc: 0.2344\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2069 - acc: 0.4862 - val_loss: 1.5022 - val_acc: 0.2500\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2375 - acc: 0.4619 - val_loss: 1.6475 - val_acc: 0.1875\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2186 - acc: 0.4749 - val_loss: 1.5964 - val_acc: 0.2031\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.2656 - acc: 0.4473 - val_loss: 1.6469 - val_acc: 0.2188\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2262 - acc: 0.4635 - val_loss: 1.4885 - val_acc: 0.2500\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2052 - acc: 0.4684 - val_loss: 1.5659 - val_acc: 0.2031\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2465 - acc: 0.4489 - val_loss: 1.5778 - val_acc: 0.2656\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1932 - acc: 0.4927 - val_loss: 1.5633 - val_acc: 0.2031\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2375 - acc: 0.4716 - val_loss: 1.5341 - val_acc: 0.2188\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2219 - acc: 0.4846 - val_loss: 1.6357 - val_acc: 0.2344\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2419 - acc: 0.4391 - val_loss: 1.5987 - val_acc: 0.2344\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2094 - acc: 0.5057 - val_loss: 1.5745 - val_acc: 0.2188\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1824 - acc: 0.4895 - val_loss: 1.6418 - val_acc: 0.2188\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2831 - acc: 0.4554 - val_loss: 1.6244 - val_acc: 0.2344\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1886 - acc: 0.4652 - val_loss: 1.6672 - val_acc: 0.2031\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2209 - acc: 0.4781 - val_loss: 1.6396 - val_acc: 0.2188\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2298 - acc: 0.4473 - val_loss: 1.6769 - val_acc: 0.1875\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2140 - acc: 0.4441 - val_loss: 1.5838 - val_acc: 0.2188\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1672 - acc: 0.4587 - val_loss: 1.6331 - val_acc: 0.2188\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2175 - acc: 0.4911 - val_loss: 1.6472 - val_acc: 0.1875\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1789 - acc: 0.4781 - val_loss: 1.6607 - val_acc: 0.2344\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1541 - acc: 0.4927 - val_loss: 1.6368 - val_acc: 0.2500\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2535 - acc: 0.4571 - val_loss: 1.5763 - val_acc: 0.2656\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2431 - acc: 0.4652 - val_loss: 1.7252 - val_acc: 0.1875\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2182 - acc: 0.4506 - val_loss: 1.7123 - val_acc: 0.1875\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1873 - acc: 0.4959 - val_loss: 1.5912 - val_acc: 0.2344\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2171 - acc: 0.4668 - val_loss: 1.5953 - val_acc: 0.2500\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2489 - acc: 0.4716 - val_loss: 1.5676 - val_acc: 0.2344\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1526 - acc: 0.4895 - val_loss: 1.6514 - val_acc: 0.2344\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2055 - acc: 0.4781 - val_loss: 1.5874 - val_acc: 0.2344\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2113 - acc: 0.4668 - val_loss: 1.5926 - val_acc: 0.2344\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1893 - acc: 0.4781 - val_loss: 1.5477 - val_acc: 0.2344\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1665 - acc: 0.4652 - val_loss: 1.5331 - val_acc: 0.2500\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1739 - acc: 0.4797 - val_loss: 1.6087 - val_acc: 0.2188\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1615 - acc: 0.4943 - val_loss: 1.6088 - val_acc: 0.2500\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1554 - acc: 0.5041 - val_loss: 1.5987 - val_acc: 0.2188\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2321 - acc: 0.4733 - val_loss: 1.5975 - val_acc: 0.2031\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1850 - acc: 0.4895 - val_loss: 1.5649 - val_acc: 0.2188\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1848 - acc: 0.4846 - val_loss: 1.6527 - val_acc: 0.1875\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1873 - acc: 0.4700 - val_loss: 1.5908 - val_acc: 0.2500\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1787 - acc: 0.4814 - val_loss: 1.7400 - val_acc: 0.2031\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1702 - acc: 0.4895 - val_loss: 1.7206 - val_acc: 0.1875\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2328 - acc: 0.4668 - val_loss: 1.5741 - val_acc: 0.2344\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1603 - acc: 0.4927 - val_loss: 1.6981 - val_acc: 0.1875\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1499 - acc: 0.4927 - val_loss: 1.6811 - val_acc: 0.2031\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2002 - acc: 0.4765 - val_loss: 1.6649 - val_acc: 0.2188\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1691 - acc: 0.4959 - val_loss: 1.6013 - val_acc: 0.2188\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1868 - acc: 0.4878 - val_loss: 1.6055 - val_acc: 0.2188\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1752 - acc: 0.4943 - val_loss: 1.6310 - val_acc: 0.2188\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1613 - acc: 0.4830 - val_loss: 1.6563 - val_acc: 0.2031\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1943 - acc: 0.4733 - val_loss: 1.6827 - val_acc: 0.2031\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1474 - acc: 0.4878 - val_loss: 1.6408 - val_acc: 0.2031\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1521 - acc: 0.5089 - val_loss: 1.6537 - val_acc: 0.1719\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1920 - acc: 0.4765 - val_loss: 1.5867 - val_acc: 0.2188\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1736 - acc: 0.4878 - val_loss: 1.6551 - val_acc: 0.1875\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2446 - acc: 0.4619 - val_loss: 1.6507 - val_acc: 0.1875\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1564 - acc: 0.4814 - val_loss: 1.5909 - val_acc: 0.2500\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2010 - acc: 0.4814 - val_loss: 1.6031 - val_acc: 0.2188\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1419 - acc: 0.5057 - val_loss: 1.6499 - val_acc: 0.1875\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1560 - acc: 0.4878 - val_loss: 1.6036 - val_acc: 0.2500\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1757 - acc: 0.4862 - val_loss: 1.6770 - val_acc: 0.2031\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2092 - acc: 0.4797 - val_loss: 1.5968 - val_acc: 0.2344\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.2234 - acc: 0.4734 - val_loss: 1.6413 - val_acc: 0.2344\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1821 - acc: 0.4765 - val_loss: 1.6412 - val_acc: 0.2344\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1731 - acc: 0.4814 - val_loss: 1.5899 - val_acc: 0.2188\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1443 - acc: 0.5138 - val_loss: 1.5392 - val_acc: 0.2656\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2312 - acc: 0.4473 - val_loss: 1.6401 - val_acc: 0.2344\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1160 - acc: 0.4943 - val_loss: 1.6101 - val_acc: 0.2188\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1368 - acc: 0.5122 - val_loss: 1.6536 - val_acc: 0.2188\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1214 - acc: 0.5154 - val_loss: 1.7321 - val_acc: 0.1719\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1328 - acc: 0.4830 - val_loss: 1.7077 - val_acc: 0.2031\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1890 - acc: 0.4862 - val_loss: 1.5184 - val_acc: 0.2344\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0869 - acc: 0.5235 - val_loss: 1.6467 - val_acc: 0.2656\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1157 - acc: 0.4911 - val_loss: 1.5728 - val_acc: 0.2344\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1508 - acc: 0.4814 - val_loss: 1.7087 - val_acc: 0.1875\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1454 - acc: 0.5105 - val_loss: 1.7185 - val_acc: 0.2031\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0729 - acc: 0.5138 - val_loss: 1.5531 - val_acc: 0.2344\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1628 - acc: 0.4862 - val_loss: 1.7312 - val_acc: 0.1719\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1561 - acc: 0.4911 - val_loss: 1.6806 - val_acc: 0.2031\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1361 - acc: 0.5024 - val_loss: 1.5620 - val_acc: 0.2344\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0882 - acc: 0.5105 - val_loss: 1.6115 - val_acc: 0.2344\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1174 - acc: 0.5089 - val_loss: 1.6637 - val_acc: 0.2344\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0977 - acc: 0.5219 - val_loss: 1.7061 - val_acc: 0.1875\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1186 - acc: 0.4959 - val_loss: 1.6053 - val_acc: 0.2188\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1147 - acc: 0.5219 - val_loss: 1.6942 - val_acc: 0.2188\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1551 - acc: 0.4846 - val_loss: 1.5974 - val_acc: 0.2031\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1373 - acc: 0.4943 - val_loss: 1.5746 - val_acc: 0.2344\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1190 - acc: 0.5170 - val_loss: 1.6334 - val_acc: 0.2344\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0522 - acc: 0.5057 - val_loss: 1.6781 - val_acc: 0.2188\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1264 - acc: 0.4927 - val_loss: 1.6107 - val_acc: 0.2344\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1876 - acc: 0.4992 - val_loss: 1.6200 - val_acc: 0.2188\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1368 - acc: 0.5089 - val_loss: 1.7339 - val_acc: 0.2188\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1415 - acc: 0.5057 - val_loss: 1.6064 - val_acc: 0.2344\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0786 - acc: 0.5156 - val_loss: 1.6626 - val_acc: 0.1875\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1460 - acc: 0.5141 - val_loss: 1.6528 - val_acc: 0.1875\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1064 - acc: 0.5041 - val_loss: 1.6766 - val_acc: 0.2188\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1619 - acc: 0.4959 - val_loss: 1.6744 - val_acc: 0.1719\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1626 - acc: 0.5154 - val_loss: 1.7009 - val_acc: 0.2031\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1399 - acc: 0.5235 - val_loss: 1.7155 - val_acc: 0.2031\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1374 - acc: 0.5057 - val_loss: 1.6179 - val_acc: 0.2031\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0947 - acc: 0.5073 - val_loss: 1.6705 - val_acc: 0.1875\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1153 - acc: 0.5024 - val_loss: 1.6482 - val_acc: 0.2188\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1583 - acc: 0.4700 - val_loss: 1.5638 - val_acc: 0.2656\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0985 - acc: 0.4992 - val_loss: 1.7351 - val_acc: 0.1719\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1301 - acc: 0.5188 - val_loss: 1.6990 - val_acc: 0.1562\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0909 - acc: 0.5235 - val_loss: 1.6108 - val_acc: 0.2188\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2317 - acc: 0.4862 - val_loss: 1.7226 - val_acc: 0.1875\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0569 - acc: 0.5446 - val_loss: 1.6985 - val_acc: 0.2031\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1390 - acc: 0.5122 - val_loss: 1.5962 - val_acc: 0.2500\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1097 - acc: 0.5186 - val_loss: 1.5899 - val_acc: 0.2188\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1484 - acc: 0.4927 - val_loss: 1.5824 - val_acc: 0.2500\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1547 - acc: 0.4992 - val_loss: 1.5530 - val_acc: 0.2500\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1003 - acc: 0.5138 - val_loss: 1.6865 - val_acc: 0.1875\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0718 - acc: 0.5089 - val_loss: 1.5865 - val_acc: 0.2031\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1098 - acc: 0.4927 - val_loss: 1.6668 - val_acc: 0.2031\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0927 - acc: 0.5235 - val_loss: 1.6135 - val_acc: 0.2500\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1024 - acc: 0.5429 - val_loss: 1.7126 - val_acc: 0.1875\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 28s 2s/step - loss: 1.0640 - acc: 0.5219 - val_loss: 1.6970 - val_acc: 0.2188\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0746 - acc: 0.5154 - val_loss: 1.6974 - val_acc: 0.1875\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0545 - acc: 0.5122 - val_loss: 1.6394 - val_acc: 0.2188\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1024 - acc: 0.5138 - val_loss: 1.6381 - val_acc: 0.2344\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0958 - acc: 0.5235 - val_loss: 1.5564 - val_acc: 0.2344\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0509 - acc: 0.5219 - val_loss: 1.7109 - val_acc: 0.2188\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0910 - acc: 0.5105 - val_loss: 1.6936 - val_acc: 0.1719\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0404 - acc: 0.5429 - val_loss: 1.5855 - val_acc: 0.2344\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.2040 - acc: 0.4976 - val_loss: 1.6408 - val_acc: 0.2188\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0537 - acc: 0.5073 - val_loss: 1.7153 - val_acc: 0.1875\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0436 - acc: 0.5219 - val_loss: 1.5928 - val_acc: 0.2500\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1327 - acc: 0.4846 - val_loss: 1.6508 - val_acc: 0.1875\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1246 - acc: 0.5024 - val_loss: 1.6159 - val_acc: 0.2500\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1299 - acc: 0.5057 - val_loss: 1.7057 - val_acc: 0.1719\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0621 - acc: 0.5154 - val_loss: 1.6419 - val_acc: 0.2344\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0862 - acc: 0.5057 - val_loss: 1.7142 - val_acc: 0.1719\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0332 - acc: 0.5332 - val_loss: 1.7248 - val_acc: 0.1875\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0835 - acc: 0.5219 - val_loss: 1.6181 - val_acc: 0.2344\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 30s 2s/step - loss: 1.1034 - acc: 0.5397 - val_loss: 1.5160 - val_acc: 0.2500\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0902 - acc: 0.5138 - val_loss: 1.6462 - val_acc: 0.1875\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0940 - acc: 0.4911 - val_loss: 1.6222 - val_acc: 0.2031\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0811 - acc: 0.5300 - val_loss: 1.5830 - val_acc: 0.2031\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0207 - acc: 0.5673 - val_loss: 1.5940 - val_acc: 0.2344\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0974 - acc: 0.5312 - val_loss: 1.6946 - val_acc: 0.1719\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1041 - acc: 0.4943 - val_loss: 1.7185 - val_acc: 0.1875\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0921 - acc: 0.5105 - val_loss: 1.7135 - val_acc: 0.2031\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0552 - acc: 0.5203 - val_loss: 1.8009 - val_acc: 0.1719\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0719 - acc: 0.5511 - val_loss: 1.6077 - val_acc: 0.2031\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0862 - acc: 0.5186 - val_loss: 1.6119 - val_acc: 0.2344\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0506 - acc: 0.5624 - val_loss: 1.6759 - val_acc: 0.2031\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0835 - acc: 0.5397 - val_loss: 1.6654 - val_acc: 0.2500\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0588 - acc: 0.5365 - val_loss: 1.5979 - val_acc: 0.2500\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1210 - acc: 0.5008 - val_loss: 1.6655 - val_acc: 0.2188\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0135 - acc: 0.5478 - val_loss: 1.7113 - val_acc: 0.1875\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0600 - acc: 0.5172 - val_loss: 1.5347 - val_acc: 0.2500\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 33s 3s/step - loss: 1.1159 - acc: 0.5219 - val_loss: 1.6059 - val_acc: 0.2500\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.1323 - acc: 0.5266 - val_loss: 1.6560 - val_acc: 0.2031\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0744 - acc: 0.5122 - val_loss: 1.6408 - val_acc: 0.2031\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0919 - acc: 0.5235 - val_loss: 1.5927 - val_acc: 0.2031\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0365 - acc: 0.5397 - val_loss: 1.6133 - val_acc: 0.2188\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0302 - acc: 0.5332 - val_loss: 1.7143 - val_acc: 0.1719\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0980 - acc: 0.5348 - val_loss: 1.6412 - val_acc: 0.2031\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1145 - acc: 0.5024 - val_loss: 1.7076 - val_acc: 0.2188\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0293 - acc: 0.5494 - val_loss: 1.6353 - val_acc: 0.1875\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0322 - acc: 0.5527 - val_loss: 1.6298 - val_acc: 0.2188\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9991 - acc: 0.5624 - val_loss: 1.6870 - val_acc: 0.1875\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1150 - acc: 0.5284 - val_loss: 1.6417 - val_acc: 0.2031\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0367 - acc: 0.5267 - val_loss: 1.6348 - val_acc: 0.2188\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1293 - acc: 0.5105 - val_loss: 1.6870 - val_acc: 0.1719\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0988 - acc: 0.5251 - val_loss: 1.6811 - val_acc: 0.1875\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0592 - acc: 0.5186 - val_loss: 1.5643 - val_acc: 0.2031\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.0476 - acc: 0.5284 - val_loss: 1.6273 - val_acc: 0.2188\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0569 - acc: 0.5575 - val_loss: 1.6460 - val_acc: 0.2031\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0521 - acc: 0.5219 - val_loss: 1.6882 - val_acc: 0.1719\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.0125 - acc: 0.5413 - val_loss: 1.6419 - val_acc: 0.2031\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0239 - acc: 0.5138 - val_loss: 1.6701 - val_acc: 0.2188\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0969 - acc: 0.5413 - val_loss: 1.5509 - val_acc: 0.2344\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0484 - acc: 0.5391 - val_loss: 1.6828 - val_acc: 0.2031\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0346 - acc: 0.5365 - val_loss: 1.6101 - val_acc: 0.2344\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0265 - acc: 0.5527 - val_loss: 1.6313 - val_acc: 0.2031\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0307 - acc: 0.5266 - val_loss: 1.5961 - val_acc: 0.2344\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0665 - acc: 0.5251 - val_loss: 1.6694 - val_acc: 0.2031\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0250 - acc: 0.5511 - val_loss: 1.7196 - val_acc: 0.2031\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0581 - acc: 0.5624 - val_loss: 1.6094 - val_acc: 0.2031\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0549 - acc: 0.5267 - val_loss: 1.7399 - val_acc: 0.1875\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0561 - acc: 0.5300 - val_loss: 1.5990 - val_acc: 0.2500\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0299 - acc: 0.5284 - val_loss: 1.6648 - val_acc: 0.2031\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9970 - acc: 0.5609 - val_loss: 1.6254 - val_acc: 0.2500\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0233 - acc: 0.5462 - val_loss: 1.6618 - val_acc: 0.2031\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.1300 - acc: 0.4927 - val_loss: 1.6776 - val_acc: 0.2031\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.1087 - acc: 0.5016 - val_loss: 1.6460 - val_acc: 0.2031\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0762 - acc: 0.5413 - val_loss: 1.5506 - val_acc: 0.2344\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0698 - acc: 0.5267 - val_loss: 1.7036 - val_acc: 0.1719\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0674 - acc: 0.5413 - val_loss: 1.7143 - val_acc: 0.2188\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0199 - acc: 0.5316 - val_loss: 1.7150 - val_acc: 0.2031\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0312 - acc: 0.5462 - val_loss: 1.6645 - val_acc: 0.2344\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0829 - acc: 0.5057 - val_loss: 1.6659 - val_acc: 0.2031\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0222 - acc: 0.5562 - val_loss: 1.6833 - val_acc: 0.2031\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0081 - acc: 0.5594 - val_loss: 1.6367 - val_acc: 0.2031\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0311 - acc: 0.5689 - val_loss: 1.6634 - val_acc: 0.2344\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0083 - acc: 0.5429 - val_loss: 1.6938 - val_acc: 0.2188\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0409 - acc: 0.5235 - val_loss: 1.6301 - val_acc: 0.1875\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0373 - acc: 0.5348 - val_loss: 1.6265 - val_acc: 0.2188\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0172 - acc: 0.5478 - val_loss: 1.6823 - val_acc: 0.1875\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9917 - acc: 0.5575 - val_loss: 1.6201 - val_acc: 0.2031\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0238 - acc: 0.5397 - val_loss: 1.7261 - val_acc: 0.1719\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9750 - acc: 0.5689 - val_loss: 1.6366 - val_acc: 0.2031\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0411 - acc: 0.5397 - val_loss: 1.5647 - val_acc: 0.2188\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0159 - acc: 0.5478 - val_loss: 1.6201 - val_acc: 0.2031\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0695 - acc: 0.5527 - val_loss: 1.6488 - val_acc: 0.2188\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0173 - acc: 0.5640 - val_loss: 1.7047 - val_acc: 0.1875\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0455 - acc: 0.5381 - val_loss: 1.6944 - val_acc: 0.1875\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0262 - acc: 0.5511 - val_loss: 1.5961 - val_acc: 0.2656\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0306 - acc: 0.5348 - val_loss: 1.6397 - val_acc: 0.2188\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0094 - acc: 0.5478 - val_loss: 1.6653 - val_acc: 0.2188\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 29s 2s/step - loss: 1.0451 - acc: 0.5453 - val_loss: 1.6862 - val_acc: 0.2031\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0504 - acc: 0.5381 - val_loss: 1.6002 - val_acc: 0.2188\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0198 - acc: 0.5413 - val_loss: 1.6673 - val_acc: 0.2500\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0269 - acc: 0.5284 - val_loss: 1.6855 - val_acc: 0.2188\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0051 - acc: 0.5219 - val_loss: 1.6917 - val_acc: 0.2344\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9963 - acc: 0.5754 - val_loss: 1.7366 - val_acc: 0.1875\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9961 - acc: 0.5656 - val_loss: 1.7062 - val_acc: 0.2188\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0166 - acc: 0.5332 - val_loss: 1.6848 - val_acc: 0.2188\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9903 - acc: 0.5511 - val_loss: 1.7511 - val_acc: 0.1719\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0589 - acc: 0.5154 - val_loss: 1.6383 - val_acc: 0.2344\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9781 - acc: 0.5673 - val_loss: 1.7197 - val_acc: 0.1719\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9945 - acc: 0.5511 - val_loss: 1.6905 - val_acc: 0.1719\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0199 - acc: 0.5494 - val_loss: 1.6702 - val_acc: 0.1875\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9892 - acc: 0.5300 - val_loss: 1.6349 - val_acc: 0.2188\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0576 - acc: 0.5429 - val_loss: 1.7197 - val_acc: 0.1719\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9924 - acc: 0.5462 - val_loss: 1.6738 - val_acc: 0.1875\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0371 - acc: 0.5170 - val_loss: 1.6798 - val_acc: 0.2031\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9546 - acc: 0.5494 - val_loss: 1.7352 - val_acc: 0.1719\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9939 - acc: 0.5575 - val_loss: 1.6984 - val_acc: 0.2188\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9797 - acc: 0.5656 - val_loss: 1.7014 - val_acc: 0.1875\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0116 - acc: 0.5478 - val_loss: 1.7126 - val_acc: 0.1875\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0286 - acc: 0.5516 - val_loss: 1.7012 - val_acc: 0.1719\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0204 - acc: 0.5592 - val_loss: 1.6104 - val_acc: 0.1875\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9987 - acc: 0.5770 - val_loss: 1.6424 - val_acc: 0.1875\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0143 - acc: 0.5624 - val_loss: 1.6296 - val_acc: 0.1562\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9972 - acc: 0.5559 - val_loss: 1.5890 - val_acc: 0.2031\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0277 - acc: 0.5575 - val_loss: 1.7238 - val_acc: 0.2188\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0692 - acc: 0.5381 - val_loss: 1.7403 - val_acc: 0.1406\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9859 - acc: 0.5413 - val_loss: 1.7319 - val_acc: 0.1719\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0067 - acc: 0.5559 - val_loss: 1.6558 - val_acc: 0.2188\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0376 - acc: 0.5575 - val_loss: 1.7071 - val_acc: 0.1719\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0266 - acc: 0.5737 - val_loss: 1.6693 - val_acc: 0.1875\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9808 - acc: 0.5705 - val_loss: 1.6288 - val_acc: 0.2031\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0332 - acc: 0.5547 - val_loss: 1.6322 - val_acc: 0.2031\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0293 - acc: 0.5543 - val_loss: 1.5669 - val_acc: 0.2344\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9991 - acc: 0.5578 - val_loss: 1.6143 - val_acc: 0.2031\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0142 - acc: 0.5594 - val_loss: 1.6141 - val_acc: 0.1875\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0080 - acc: 0.5562 - val_loss: 1.7593 - val_acc: 0.1406\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0537 - acc: 0.5381 - val_loss: 1.6084 - val_acc: 0.2188\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9996 - acc: 0.5705 - val_loss: 1.7073 - val_acc: 0.2031\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0569 - acc: 0.5186 - val_loss: 1.8014 - val_acc: 0.1406\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0539 - acc: 0.5446 - val_loss: 1.6600 - val_acc: 0.1875\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9770 - acc: 0.5656 - val_loss: 1.6962 - val_acc: 0.2031\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9868 - acc: 0.5721 - val_loss: 1.6673 - val_acc: 0.1875\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0194 - acc: 0.5624 - val_loss: 1.5863 - val_acc: 0.2031\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9997 - acc: 0.5673 - val_loss: 1.6940 - val_acc: 0.1719\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9125 - acc: 0.5867 - val_loss: 1.7499 - val_acc: 0.1875\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0365 - acc: 0.5689 - val_loss: 1.6720 - val_acc: 0.1719\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0341 - acc: 0.5381 - val_loss: 1.6149 - val_acc: 0.1875\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0196 - acc: 0.5608 - val_loss: 1.6913 - val_acc: 0.1875\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9960 - acc: 0.5413 - val_loss: 1.6961 - val_acc: 0.1719\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0005 - acc: 0.5608 - val_loss: 1.6891 - val_acc: 0.2031\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0009 - acc: 0.5494 - val_loss: 1.6888 - val_acc: 0.1719\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9383 - acc: 0.5851 - val_loss: 1.6893 - val_acc: 0.1719\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9629 - acc: 0.5750 - val_loss: 1.6427 - val_acc: 0.2031\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0100 - acc: 0.5300 - val_loss: 1.5992 - val_acc: 0.2031\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0142 - acc: 0.5462 - val_loss: 1.6119 - val_acc: 0.1875\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9590 - acc: 0.5997 - val_loss: 1.6265 - val_acc: 0.2031\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9794 - acc: 0.5656 - val_loss: 1.6836 - val_acc: 0.1875\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0119 - acc: 0.5640 - val_loss: 1.5233 - val_acc: 0.2188\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9716 - acc: 0.5835 - val_loss: 1.6838 - val_acc: 0.1875\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9670 - acc: 0.5608 - val_loss: 1.6115 - val_acc: 0.1875\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9726 - acc: 0.5750 - val_loss: 1.6065 - val_acc: 0.2031\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9933 - acc: 0.5721 - val_loss: 1.6137 - val_acc: 0.1875\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9428 - acc: 0.5867 - val_loss: 1.7047 - val_acc: 0.1875\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9795 - acc: 0.5446 - val_loss: 1.6197 - val_acc: 0.1875\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9744 - acc: 0.5381 - val_loss: 1.6086 - val_acc: 0.1875\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9500 - acc: 0.5640 - val_loss: 1.7217 - val_acc: 0.1406\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9780 - acc: 0.5656 - val_loss: 1.6500 - val_acc: 0.1719\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9747 - acc: 0.5705 - val_loss: 1.5930 - val_acc: 0.2344\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9596 - acc: 0.5900 - val_loss: 1.6217 - val_acc: 0.2188\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9500 - acc: 0.5750 - val_loss: 1.6427 - val_acc: 0.1875\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9370 - acc: 0.5770 - val_loss: 1.6149 - val_acc: 0.2188\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9852 - acc: 0.5559 - val_loss: 1.6734 - val_acc: 0.1562\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9224 - acc: 0.5818 - val_loss: 1.6942 - val_acc: 0.1875\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9737 - acc: 0.5953 - val_loss: 1.6707 - val_acc: 0.1562\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0311 - acc: 0.5446 - val_loss: 1.7185 - val_acc: 0.1719\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9908 - acc: 0.5592 - val_loss: 1.6819 - val_acc: 0.2188\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9486 - acc: 0.5818 - val_loss: 1.6965 - val_acc: 0.1875\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9753 - acc: 0.5624 - val_loss: 1.6963 - val_acc: 0.1562\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0457 - acc: 0.5203 - val_loss: 1.7479 - val_acc: 0.1719\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9442 - acc: 0.5770 - val_loss: 1.6116 - val_acc: 0.2188\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0358 - acc: 0.5559 - val_loss: 1.6466 - val_acc: 0.1875\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0323 - acc: 0.5494 - val_loss: 1.6677 - val_acc: 0.1719\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9431 - acc: 0.5673 - val_loss: 1.6925 - val_acc: 0.1562\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9446 - acc: 0.5835 - val_loss: 1.5885 - val_acc: 0.2188\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9696 - acc: 0.5543 - val_loss: 1.6351 - val_acc: 0.1875\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9818 - acc: 0.5802 - val_loss: 1.6648 - val_acc: 0.2031\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9862 - acc: 0.5734 - val_loss: 1.5881 - val_acc: 0.2031\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9796 - acc: 0.5754 - val_loss: 1.6172 - val_acc: 0.1875\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9250 - acc: 0.5721 - val_loss: 1.6921 - val_acc: 0.1719\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9542 - acc: 0.5802 - val_loss: 1.6201 - val_acc: 0.2031\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9459 - acc: 0.5851 - val_loss: 1.6798 - val_acc: 0.1406\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0218 - acc: 0.5437 - val_loss: 1.6473 - val_acc: 0.2344\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9795 - acc: 0.5818 - val_loss: 1.6860 - val_acc: 0.1719\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9331 - acc: 0.5689 - val_loss: 1.6495 - val_acc: 0.2031\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9588 - acc: 0.5867 - val_loss: 1.7314 - val_acc: 0.1406\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9780 - acc: 0.5381 - val_loss: 1.6548 - val_acc: 0.1875\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9692 - acc: 0.5527 - val_loss: 1.7151 - val_acc: 0.1719\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9345 - acc: 0.5770 - val_loss: 1.6184 - val_acc: 0.1719\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9901 - acc: 0.5624 - val_loss: 1.7761 - val_acc: 0.1406\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9535 - acc: 0.5737 - val_loss: 1.7245 - val_acc: 0.2188\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9838 - acc: 0.5656 - val_loss: 1.6307 - val_acc: 0.1719\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9244 - acc: 0.6159 - val_loss: 1.5822 - val_acc: 0.2188\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9991 - acc: 0.5624 - val_loss: 1.6687 - val_acc: 0.2188\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0246 - acc: 0.5737 - val_loss: 1.7206 - val_acc: 0.1562\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9510 - acc: 0.5851 - val_loss: 1.6773 - val_acc: 0.1875\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9831 - acc: 0.5721 - val_loss: 1.6407 - val_acc: 0.1875\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9381 - acc: 0.5705 - val_loss: 1.6473 - val_acc: 0.1875\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9430 - acc: 0.5770 - val_loss: 1.6582 - val_acc: 0.1875\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9694 - acc: 0.5624 - val_loss: 1.6679 - val_acc: 0.2031\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9486 - acc: 0.5802 - val_loss: 1.6775 - val_acc: 0.2031\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9481 - acc: 0.5981 - val_loss: 1.6616 - val_acc: 0.2188\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9672 - acc: 0.5818 - val_loss: 1.6963 - val_acc: 0.1719\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9305 - acc: 0.5818 - val_loss: 1.6465 - val_acc: 0.1875\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9739 - acc: 0.5494 - val_loss: 1.7624 - val_acc: 0.1562\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9734 - acc: 0.5559 - val_loss: 1.6928 - val_acc: 0.1875\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9073 - acc: 0.6110 - val_loss: 1.7313 - val_acc: 0.1875\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9466 - acc: 0.5673 - val_loss: 1.6138 - val_acc: 0.2031\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9425 - acc: 0.5818 - val_loss: 1.6321 - val_acc: 0.2031\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9525 - acc: 0.5844 - val_loss: 1.6178 - val_acc: 0.2344\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9188 - acc: 0.6029 - val_loss: 1.7241 - val_acc: 0.1562\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9837 - acc: 0.5705 - val_loss: 1.7748 - val_acc: 0.1719\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9789 - acc: 0.5786 - val_loss: 1.7212 - val_acc: 0.1719\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9451 - acc: 0.5891 - val_loss: 1.6869 - val_acc: 0.2031\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0084 - acc: 0.5527 - val_loss: 1.6867 - val_acc: 0.2031\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 1.0078 - acc: 0.5562 - val_loss: 1.7169 - val_acc: 0.1875\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9212 - acc: 0.5754 - val_loss: 1.7471 - val_acc: 0.1719\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9578 - acc: 0.5835 - val_loss: 1.5981 - val_acc: 0.2031\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9625 - acc: 0.5705 - val_loss: 1.6270 - val_acc: 0.1719\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0098 - acc: 0.5656 - val_loss: 1.6647 - val_acc: 0.1719\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9446 - acc: 0.5835 - val_loss: 1.6640 - val_acc: 0.2031\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9880 - acc: 0.5770 - val_loss: 1.7421 - val_acc: 0.1719\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9544 - acc: 0.6029 - val_loss: 1.7183 - val_acc: 0.1875\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9585 - acc: 0.5916 - val_loss: 1.6925 - val_acc: 0.1406\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9439 - acc: 0.5802 - val_loss: 1.6611 - val_acc: 0.1719\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9921 - acc: 0.5786 - val_loss: 1.5740 - val_acc: 0.2031\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8905 - acc: 0.6045 - val_loss: 1.6332 - val_acc: 0.2031\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9753 - acc: 0.5770 - val_loss: 1.5725 - val_acc: 0.2031\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9534 - acc: 0.5592 - val_loss: 1.6209 - val_acc: 0.1719\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9123 - acc: 0.5948 - val_loss: 1.6144 - val_acc: 0.1875\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9513 - acc: 0.5640 - val_loss: 1.7236 - val_acc: 0.1719\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9371 - acc: 0.5851 - val_loss: 1.6006 - val_acc: 0.2031\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9189 - acc: 0.5916 - val_loss: 1.6462 - val_acc: 0.1875\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9470 - acc: 0.5948 - val_loss: 1.7036 - val_acc: 0.1719\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9567 - acc: 0.5770 - val_loss: 1.5596 - val_acc: 0.2188\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9147 - acc: 0.5770 - val_loss: 1.6572 - val_acc: 0.1562\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9779 - acc: 0.5381 - val_loss: 1.7757 - val_acc: 0.1719\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9505 - acc: 0.5673 - val_loss: 1.7020 - val_acc: 0.2031\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9981 - acc: 0.5689 - val_loss: 1.6921 - val_acc: 0.1562\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9099 - acc: 0.5964 - val_loss: 1.6616 - val_acc: 0.1719\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8841 - acc: 0.6078 - val_loss: 1.6282 - val_acc: 0.2031\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9068 - acc: 0.5851 - val_loss: 1.7661 - val_acc: 0.1719\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9321 - acc: 0.5851 - val_loss: 1.7183 - val_acc: 0.1875\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9496 - acc: 0.5754 - val_loss: 1.7191 - val_acc: 0.1562\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9517 - acc: 0.5997 - val_loss: 1.6615 - val_acc: 0.1875\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9725 - acc: 0.5592 - val_loss: 1.7076 - val_acc: 0.1875\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9158 - acc: 0.5705 - val_loss: 1.6274 - val_acc: 0.1875\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0116 - acc: 0.5689 - val_loss: 1.6836 - val_acc: 0.1719\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9650 - acc: 0.5737 - val_loss: 1.6775 - val_acc: 0.1562\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9463 - acc: 0.5891 - val_loss: 1.6595 - val_acc: 0.1719\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9414 - acc: 0.5851 - val_loss: 1.6125 - val_acc: 0.2031\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9611 - acc: 0.5786 - val_loss: 1.6599 - val_acc: 0.1719\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9118 - acc: 0.5932 - val_loss: 1.6528 - val_acc: 0.2031\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9386 - acc: 0.5932 - val_loss: 1.6576 - val_acc: 0.1719\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9581 - acc: 0.5948 - val_loss: 1.6722 - val_acc: 0.2031\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8978 - acc: 0.5802 - val_loss: 1.6749 - val_acc: 0.1875\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9358 - acc: 0.5964 - val_loss: 1.7220 - val_acc: 0.1562\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9727 - acc: 0.5705 - val_loss: 1.6249 - val_acc: 0.1875\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9282 - acc: 0.6078 - val_loss: 1.6927 - val_acc: 0.1719\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9473 - acc: 0.5673 - val_loss: 1.6730 - val_acc: 0.1719\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9520 - acc: 0.5656 - val_loss: 1.6005 - val_acc: 0.2188\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9587 - acc: 0.5737 - val_loss: 1.5707 - val_acc: 0.2031\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9353 - acc: 0.5766 - val_loss: 1.6855 - val_acc: 0.1875\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9482 - acc: 0.5770 - val_loss: 1.6900 - val_acc: 0.2031\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9108 - acc: 0.6175 - val_loss: 1.7427 - val_acc: 0.1406\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9069 - acc: 0.6191 - val_loss: 1.6936 - val_acc: 0.2031\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9366 - acc: 0.5948 - val_loss: 1.6100 - val_acc: 0.1875\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9350 - acc: 0.6013 - val_loss: 1.6758 - val_acc: 0.2031\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9686 - acc: 0.5851 - val_loss: 1.7455 - val_acc: 0.1719\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9033 - acc: 0.6203 - val_loss: 1.7192 - val_acc: 0.1562\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8702 - acc: 0.5948 - val_loss: 1.5703 - val_acc: 0.2031\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9611 - acc: 0.5835 - val_loss: 1.7318 - val_acc: 0.1250\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9280 - acc: 0.5656 - val_loss: 1.6250 - val_acc: 0.1875\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9069 - acc: 0.5932 - val_loss: 1.6855 - val_acc: 0.1875\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9258 - acc: 0.5922 - val_loss: 1.6325 - val_acc: 0.1875\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9149 - acc: 0.5964 - val_loss: 1.6585 - val_acc: 0.1875\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9889 - acc: 0.5543 - val_loss: 1.6142 - val_acc: 0.2031\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9048 - acc: 0.6029 - val_loss: 1.5627 - val_acc: 0.2188\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9474 - acc: 0.5900 - val_loss: 1.5575 - val_acc: 0.1875\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9025 - acc: 0.6126 - val_loss: 1.6038 - val_acc: 0.2031\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9183 - acc: 0.6047 - val_loss: 1.6666 - val_acc: 0.1875\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9010 - acc: 0.6013 - val_loss: 1.6867 - val_acc: 0.1875\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9636 - acc: 0.5953 - val_loss: 1.7113 - val_acc: 0.1562\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9349 - acc: 0.5932 - val_loss: 1.5704 - val_acc: 0.1719\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9580 - acc: 0.5851 - val_loss: 1.6245 - val_acc: 0.2031\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9282 - acc: 0.5948 - val_loss: 1.6607 - val_acc: 0.2188\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9053 - acc: 0.5997 - val_loss: 1.6451 - val_acc: 0.2031\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9064 - acc: 0.5802 - val_loss: 1.5787 - val_acc: 0.2188\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9343 - acc: 0.5770 - val_loss: 1.5785 - val_acc: 0.2031\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9307 - acc: 0.5916 - val_loss: 1.7120 - val_acc: 0.1719\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9050 - acc: 0.6191 - val_loss: 1.6856 - val_acc: 0.2031\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.0016 - acc: 0.5640 - val_loss: 1.6825 - val_acc: 0.2031\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9813 - acc: 0.5750 - val_loss: 1.6956 - val_acc: 0.1719\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9336 - acc: 0.6029 - val_loss: 1.6912 - val_acc: 0.1875\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9807 - acc: 0.5705 - val_loss: 1.6506 - val_acc: 0.1875\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9089 - acc: 0.5916 - val_loss: 1.6778 - val_acc: 0.1719\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9163 - acc: 0.6029 - val_loss: 1.7718 - val_acc: 0.1406\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9230 - acc: 0.5883 - val_loss: 1.7209 - val_acc: 0.1719\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9515 - acc: 0.5835 - val_loss: 1.6670 - val_acc: 0.1875\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9149 - acc: 0.5900 - val_loss: 1.5569 - val_acc: 0.2344\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9260 - acc: 0.5916 - val_loss: 1.6645 - val_acc: 0.2031\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9108 - acc: 0.6207 - val_loss: 1.5887 - val_acc: 0.2031\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8818 - acc: 0.6094 - val_loss: 1.6424 - val_acc: 0.2344\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9051 - acc: 0.6029 - val_loss: 1.6422 - val_acc: 0.2031\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9333 - acc: 0.5835 - val_loss: 1.7367 - val_acc: 0.1562\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9056 - acc: 0.6045 - val_loss: 1.6646 - val_acc: 0.1719\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9155 - acc: 0.5964 - val_loss: 1.7293 - val_acc: 0.1875\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9007 - acc: 0.5964 - val_loss: 1.6469 - val_acc: 0.1719\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8904 - acc: 0.5900 - val_loss: 1.6594 - val_acc: 0.2031\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8712 - acc: 0.6272 - val_loss: 1.5955 - val_acc: 0.2188\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9911 - acc: 0.5818 - val_loss: 1.6392 - val_acc: 0.1719\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9726 - acc: 0.5754 - val_loss: 1.6714 - val_acc: 0.1875\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9430 - acc: 0.5640 - val_loss: 1.6753 - val_acc: 0.1875\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9266 - acc: 0.6159 - val_loss: 1.6480 - val_acc: 0.2031\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8806 - acc: 0.6029 - val_loss: 1.6009 - val_acc: 0.2344\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9203 - acc: 0.6045 - val_loss: 1.6069 - val_acc: 0.2031\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9053 - acc: 0.6234 - val_loss: 1.7632 - val_acc: 0.1562\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8646 - acc: 0.6207 - val_loss: 1.7182 - val_acc: 0.1562\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9425 - acc: 0.5592 - val_loss: 1.5812 - val_acc: 0.2188\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9110 - acc: 0.6078 - val_loss: 1.7038 - val_acc: 0.1719\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8874 - acc: 0.5964 - val_loss: 1.6454 - val_acc: 0.2031\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8800 - acc: 0.6013 - val_loss: 1.6232 - val_acc: 0.2031\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9014 - acc: 0.6240 - val_loss: 1.7536 - val_acc: 0.1562\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9309 - acc: 0.5948 - val_loss: 1.6000 - val_acc: 0.2188\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9660 - acc: 0.5737 - val_loss: 1.6420 - val_acc: 0.2031\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8990 - acc: 0.5916 - val_loss: 1.6004 - val_acc: 0.2344\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8623 - acc: 0.6143 - val_loss: 1.6721 - val_acc: 0.1875\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9175 - acc: 0.6029 - val_loss: 1.6892 - val_acc: 0.1562\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9112 - acc: 0.6045 - val_loss: 1.6348 - val_acc: 0.2031\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9071 - acc: 0.5867 - val_loss: 1.6345 - val_acc: 0.2344\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8990 - acc: 0.6045 - val_loss: 1.5999 - val_acc: 0.2344\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9333 - acc: 0.5844 - val_loss: 1.7357 - val_acc: 0.1406\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9255 - acc: 0.5689 - val_loss: 1.6964 - val_acc: 0.1562\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9020 - acc: 0.6109 - val_loss: 1.6378 - val_acc: 0.2188\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9383 - acc: 0.5883 - val_loss: 1.5810 - val_acc: 0.2031\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8811 - acc: 0.6110 - val_loss: 1.6001 - val_acc: 0.2188\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8845 - acc: 0.6159 - val_loss: 1.7114 - val_acc: 0.1719\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9068 - acc: 0.5981 - val_loss: 1.6523 - val_acc: 0.2188\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8859 - acc: 0.5802 - val_loss: 1.5425 - val_acc: 0.2188\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9093 - acc: 0.5964 - val_loss: 1.6496 - val_acc: 0.2031\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9374 - acc: 0.5737 - val_loss: 1.5714 - val_acc: 0.2031\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8805 - acc: 0.5964 - val_loss: 1.7168 - val_acc: 0.1719\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8729 - acc: 0.5964 - val_loss: 1.5400 - val_acc: 0.2500\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8814 - acc: 0.5818 - val_loss: 1.5631 - val_acc: 0.2500\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9315 - acc: 0.5948 - val_loss: 1.5921 - val_acc: 0.2188\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9191 - acc: 0.5867 - val_loss: 1.6188 - val_acc: 0.2031\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9006 - acc: 0.5883 - val_loss: 1.5218 - val_acc: 0.2344\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.9068 - acc: 0.6031 - val_loss: 1.5896 - val_acc: 0.2188\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9515 - acc: 0.5689 - val_loss: 1.6092 - val_acc: 0.2344\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9029 - acc: 0.5916 - val_loss: 1.6854 - val_acc: 0.1875\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8882 - acc: 0.5981 - val_loss: 1.6203 - val_acc: 0.1875\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8762 - acc: 0.6207 - val_loss: 1.5899 - val_acc: 0.2188\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8933 - acc: 0.6062 - val_loss: 1.6702 - val_acc: 0.2188\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8559 - acc: 0.6029 - val_loss: 1.7017 - val_acc: 0.2031\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8831 - acc: 0.5900 - val_loss: 1.7547 - val_acc: 0.1406\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8691 - acc: 0.6078 - val_loss: 1.5739 - val_acc: 0.2344\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9373 - acc: 0.5656 - val_loss: 1.6406 - val_acc: 0.2188\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8771 - acc: 0.5981 - val_loss: 1.5619 - val_acc: 0.2344\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8833 - acc: 0.5916 - val_loss: 1.7095 - val_acc: 0.1562\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9162 - acc: 0.5883 - val_loss: 1.5602 - val_acc: 0.2344\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8690 - acc: 0.6110 - val_loss: 1.6550 - val_acc: 0.1719\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9322 - acc: 0.5835 - val_loss: 1.6493 - val_acc: 0.1875\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8875 - acc: 0.6029 - val_loss: 1.6071 - val_acc: 0.1875\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9311 - acc: 0.5818 - val_loss: 1.6897 - val_acc: 0.2031\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8983 - acc: 0.6062 - val_loss: 1.6361 - val_acc: 0.2031\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8837 - acc: 0.6240 - val_loss: 1.6742 - val_acc: 0.2031\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8731 - acc: 0.5916 - val_loss: 1.6222 - val_acc: 0.2031\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8668 - acc: 0.6272 - val_loss: 1.6568 - val_acc: 0.2031\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8476 - acc: 0.6288 - val_loss: 1.6454 - val_acc: 0.1875\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9130 - acc: 0.5997 - val_loss: 1.5796 - val_acc: 0.2188\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8890 - acc: 0.6126 - val_loss: 1.6360 - val_acc: 0.1875\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8937 - acc: 0.6013 - val_loss: 1.7520 - val_acc: 0.1562\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.8913 - acc: 0.6207 - val_loss: 1.5845 - val_acc: 0.2344\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9181 - acc: 0.5818 - val_loss: 1.6524 - val_acc: 0.2188\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.8538 - acc: 0.6386 - val_loss: 1.4835 - val_acc: 0.2500\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.9176 - acc: 0.5981 - val_loss: 1.5759 - val_acc: 0.2344\n",
            "Epoch 627/1000\n",
            " 5/10 [==============>...............] - ETA: 8s - loss: 0.8997 - acc: 0.5844"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}