{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+/dWjwXh6rlTFmrITjUWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "eee15545-6959-473c-b690-7e60a7cfb76d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data Remake.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "ab7d7bbc-fe47-45c1-9cf7-9dd78911f71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-500  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-500  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-500  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-500  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-500  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-500  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-500  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-500  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-500  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-500  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fce34bb-4037-431f-b465-c577ba4020ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fce34bb-4037-431f-b465-c577ba4020ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fce34bb-4037-431f-b465-c577ba4020ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fce34bb-4037-431f-b465-c577ba4020ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "740fd150-27f6-4c29-b3a6-26585ad8eee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "eaeef999-5c02-4dfe-8868-fd6a79c890ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "c499ecac-f873-4711-e53a-1bd0189bf307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "28f2510b-38fb-46b7-fa50-232dd0eecfb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 692, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 692 (delta 167), reused 161 (delta 136), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (692/692), 12.32 MiB | 12.53 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "34bc4861-a755-4d08-e16d-36577dfd0575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 197\n",
            "total training 2 images: 138\n",
            "total training 3 images: 315 \n",
            "\n",
            "total validation 1 images: 59\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 6 \n",
            "\n",
            "total test 1 images: 59\n",
            "total test 2 images: 3\n",
            "total test 3 images: 13 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 650  # จำนวนภาพ Train\n",
        "NUM_TEST = 75 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "57a0f14f-1692-49b2-c19f-d0815ca976c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "d807827d-ae02-4f9b-920f-720940ee4b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "fbdef86f-f83a-4ad5-af2d-5d90a1580de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "02ccef97-2102-4f46-d7b7-24c185942ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "50e2c970-f559-4a67-edd4-5f468ed7263f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 650 images belonging to 3 classes.\n",
            "Found 75 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cdcc03-58ea-4c54-967f-6a94b29a897e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 17s 313ms/step - loss: 2.3608 - acc: 0.2253 - val_loss: 1.9418 - val_acc: 0.1719\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 4s 297ms/step - loss: 2.0282 - acc: 0.2747 - val_loss: 1.9535 - val_acc: 0.1562\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 4s 316ms/step - loss: 2.0668 - acc: 0.2662 - val_loss: 1.7740 - val_acc: 0.1719\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 4s 296ms/step - loss: 1.9471 - acc: 0.2696 - val_loss: 1.6301 - val_acc: 0.2188\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 1.7645 - acc: 0.3055 - val_loss: 1.5995 - val_acc: 0.2344\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 4s 290ms/step - loss: 1.7217 - acc: 0.3191 - val_loss: 1.5923 - val_acc: 0.2188\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 6s 635ms/step - loss: 1.6143 - acc: 0.3174 - val_loss: 1.5722 - val_acc: 0.2344\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 4s 311ms/step - loss: 1.6478 - acc: 0.3311 - val_loss: 1.5318 - val_acc: 0.2344\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 4s 310ms/step - loss: 1.5890 - acc: 0.3584 - val_loss: 1.4951 - val_acc: 0.2812\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 4s 297ms/step - loss: 1.4452 - acc: 0.3891 - val_loss: 1.5133 - val_acc: 0.2812\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 4s 315ms/step - loss: 1.5726 - acc: 0.3498 - val_loss: 1.5145 - val_acc: 0.2969\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.4249 - acc: 0.3925 - val_loss: 1.4666 - val_acc: 0.2969\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 4s 315ms/step - loss: 1.4376 - acc: 0.3857 - val_loss: 1.4736 - val_acc: 0.3125\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.4958 - acc: 0.3635 - val_loss: 1.3919 - val_acc: 0.2812\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 5s 292ms/step - loss: 1.5549 - acc: 0.3669 - val_loss: 1.5427 - val_acc: 0.2969\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.5263 - acc: 0.3857 - val_loss: 1.5357 - val_acc: 0.2656\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 4s 304ms/step - loss: 1.4066 - acc: 0.4391 - val_loss: 1.5334 - val_acc: 0.3281\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.4713 - acc: 0.4113 - val_loss: 1.5264 - val_acc: 0.2812\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.4689 - acc: 0.3969 - val_loss: 1.5286 - val_acc: 0.2969\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 4s 450ms/step - loss: 1.5404 - acc: 0.3874 - val_loss: 1.5203 - val_acc: 0.2656\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 5s 463ms/step - loss: 1.3936 - acc: 0.4505 - val_loss: 1.5334 - val_acc: 0.2969\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 1.4790 - acc: 0.4113 - val_loss: 1.5440 - val_acc: 0.2812\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 4s 307ms/step - loss: 1.3820 - acc: 0.4215 - val_loss: 1.4536 - val_acc: 0.2812\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 4s 451ms/step - loss: 1.4579 - acc: 0.4027 - val_loss: 1.4626 - val_acc: 0.3281\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.4685 - acc: 0.3993 - val_loss: 1.4374 - val_acc: 0.2812\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.3639 - acc: 0.4215 - val_loss: 1.4991 - val_acc: 0.2812\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.4076 - acc: 0.4164 - val_loss: 1.4872 - val_acc: 0.2969\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 5s 301ms/step - loss: 1.4331 - acc: 0.4352 - val_loss: 1.5066 - val_acc: 0.2656\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.4415 - acc: 0.4010 - val_loss: 1.5267 - val_acc: 0.2969\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.4832 - acc: 0.3805 - val_loss: 1.5480 - val_acc: 0.2500\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.3719 - acc: 0.4198 - val_loss: 1.5290 - val_acc: 0.2969\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.4748 - acc: 0.3857 - val_loss: 1.4753 - val_acc: 0.2656\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.4028 - acc: 0.4352 - val_loss: 1.5436 - val_acc: 0.2656\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.3621 - acc: 0.4516 - val_loss: 1.4402 - val_acc: 0.2969\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 4s 321ms/step - loss: 1.3596 - acc: 0.4266 - val_loss: 1.4613 - val_acc: 0.3281\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.3517 - acc: 0.4659 - val_loss: 1.5044 - val_acc: 0.2812\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 4s 449ms/step - loss: 1.3720 - acc: 0.4078 - val_loss: 1.4421 - val_acc: 0.2969\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.4501 - acc: 0.4164 - val_loss: 1.4549 - val_acc: 0.2812\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.4301 - acc: 0.4113 - val_loss: 1.5114 - val_acc: 0.2969\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 4s 313ms/step - loss: 1.3147 - acc: 0.4386 - val_loss: 1.4457 - val_acc: 0.3125\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.3701 - acc: 0.4547 - val_loss: 1.5329 - val_acc: 0.2812\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 4s 307ms/step - loss: 1.3745 - acc: 0.4266 - val_loss: 1.5333 - val_acc: 0.2656\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.3761 - acc: 0.4573 - val_loss: 1.3830 - val_acc: 0.3281\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.3788 - acc: 0.4181 - val_loss: 1.3983 - val_acc: 0.3281\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.4079 - acc: 0.4369 - val_loss: 1.4501 - val_acc: 0.3281\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.3424 - acc: 0.4096 - val_loss: 1.4414 - val_acc: 0.3438\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 1.3563 - acc: 0.4203 - val_loss: 1.5421 - val_acc: 0.2500\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3318 - acc: 0.4164 - val_loss: 1.5078 - val_acc: 0.3125\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.3412 - acc: 0.4181 - val_loss: 1.5442 - val_acc: 0.2344\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 5s 459ms/step - loss: 1.3165 - acc: 0.4693 - val_loss: 1.4542 - val_acc: 0.2969\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 1.2941 - acc: 0.4539 - val_loss: 1.5041 - val_acc: 0.2500\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.3866 - acc: 0.4420 - val_loss: 1.4953 - val_acc: 0.3125\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.3054 - acc: 0.4469 - val_loss: 1.5047 - val_acc: 0.2969\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.3224 - acc: 0.4556 - val_loss: 1.5489 - val_acc: 0.2812\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 5s 304ms/step - loss: 1.3436 - acc: 0.4406 - val_loss: 1.5284 - val_acc: 0.2656\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 6s 439ms/step - loss: 1.3735 - acc: 0.4266 - val_loss: 1.5024 - val_acc: 0.2500\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3022 - acc: 0.4539 - val_loss: 1.5774 - val_acc: 0.2344\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 5s 462ms/step - loss: 1.3551 - acc: 0.4317 - val_loss: 1.5801 - val_acc: 0.2500\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 5s 301ms/step - loss: 1.3711 - acc: 0.4198 - val_loss: 1.5416 - val_acc: 0.2656\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 4s 318ms/step - loss: 1.3277 - acc: 0.4386 - val_loss: 1.4872 - val_acc: 0.2656\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.3941 - acc: 0.4471 - val_loss: 1.4735 - val_acc: 0.2656\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.4194 - acc: 0.4113 - val_loss: 1.4332 - val_acc: 0.2500\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 1.2794 - acc: 0.4471 - val_loss: 1.5751 - val_acc: 0.2500\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.3322 - acc: 0.4403 - val_loss: 1.5258 - val_acc: 0.2500\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 5s 304ms/step - loss: 1.2836 - acc: 0.4471 - val_loss: 1.5665 - val_acc: 0.2969\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 5s 304ms/step - loss: 1.3573 - acc: 0.4386 - val_loss: 1.5806 - val_acc: 0.2500\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.3408 - acc: 0.4437 - val_loss: 1.5734 - val_acc: 0.2188\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.2953 - acc: 0.4573 - val_loss: 1.4659 - val_acc: 0.2500\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.3330 - acc: 0.4676 - val_loss: 1.4347 - val_acc: 0.2812\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.2115 - acc: 0.4915 - val_loss: 1.5240 - val_acc: 0.2500\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 5s 465ms/step - loss: 1.2544 - acc: 0.4590 - val_loss: 1.5340 - val_acc: 0.2188\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.2946 - acc: 0.4608 - val_loss: 1.5751 - val_acc: 0.2344\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 1.3451 - acc: 0.4539 - val_loss: 1.5231 - val_acc: 0.2344\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.2584 - acc: 0.4744 - val_loss: 1.5851 - val_acc: 0.1719\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.2636 - acc: 0.4625 - val_loss: 1.5018 - val_acc: 0.2656\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.2192 - acc: 0.4727 - val_loss: 1.4620 - val_acc: 0.2500\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.2878 - acc: 0.4625 - val_loss: 1.5276 - val_acc: 0.2344\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 5s 301ms/step - loss: 1.2082 - acc: 0.4846 - val_loss: 1.4765 - val_acc: 0.2500\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.3177 - acc: 0.4625 - val_loss: 1.6135 - val_acc: 0.2188\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 1.2432 - acc: 0.4727 - val_loss: 1.5151 - val_acc: 0.2500\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.2619 - acc: 0.4693 - val_loss: 1.4924 - val_acc: 0.2656\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 4s 315ms/step - loss: 1.2627 - acc: 0.4608 - val_loss: 1.5801 - val_acc: 0.1875\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.3837 - acc: 0.4147 - val_loss: 1.5173 - val_acc: 0.2188\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.2969 - acc: 0.4403 - val_loss: 1.5667 - val_acc: 0.2344\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.2805 - acc: 0.4829 - val_loss: 1.5511 - val_acc: 0.2188\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3083 - acc: 0.4590 - val_loss: 1.5638 - val_acc: 0.2188\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3555 - acc: 0.4334 - val_loss: 1.6115 - val_acc: 0.2344\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 4s 314ms/step - loss: 1.2783 - acc: 0.4642 - val_loss: 1.4577 - val_acc: 0.2500\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.3105 - acc: 0.4573 - val_loss: 1.5341 - val_acc: 0.2188\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 1.2655 - acc: 0.4437 - val_loss: 1.5056 - val_acc: 0.2344\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 5s 466ms/step - loss: 1.2599 - acc: 0.4727 - val_loss: 1.5388 - val_acc: 0.1875\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 1.3091 - acc: 0.4590 - val_loss: 1.5204 - val_acc: 0.2344\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 4s 298ms/step - loss: 1.2996 - acc: 0.4573 - val_loss: 1.5643 - val_acc: 0.2188\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3388 - acc: 0.4344 - val_loss: 1.4249 - val_acc: 0.2500\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.2542 - acc: 0.4420 - val_loss: 1.4978 - val_acc: 0.2344\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.2747 - acc: 0.4881 - val_loss: 1.5583 - val_acc: 0.2188\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.2971 - acc: 0.4608 - val_loss: 1.5565 - val_acc: 0.2344\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.3450 - acc: 0.4505 - val_loss: 1.6352 - val_acc: 0.2031\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.2786 - acc: 0.4710 - val_loss: 1.6277 - val_acc: 0.2031\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.2066 - acc: 0.4625 - val_loss: 1.5928 - val_acc: 0.2188\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.2158 - acc: 0.4881 - val_loss: 1.6045 - val_acc: 0.1875\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.3288 - acc: 0.4710 - val_loss: 1.5015 - val_acc: 0.2188\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.2793 - acc: 0.4744 - val_loss: 1.5595 - val_acc: 0.2344\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 6s 492ms/step - loss: 1.2686 - acc: 0.4573 - val_loss: 1.5680 - val_acc: 0.2344\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.2199 - acc: 0.4727 - val_loss: 1.5325 - val_acc: 0.2344\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.2510 - acc: 0.4693 - val_loss: 1.5059 - val_acc: 0.2500\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.2980 - acc: 0.4556 - val_loss: 1.6187 - val_acc: 0.1875\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 5s 295ms/step - loss: 1.2480 - acc: 0.4693 - val_loss: 1.5147 - val_acc: 0.2344\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 5s 289ms/step - loss: 1.2214 - acc: 0.4812 - val_loss: 1.4346 - val_acc: 0.2188\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.2941 - acc: 0.4300 - val_loss: 1.4787 - val_acc: 0.2500\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.2240 - acc: 0.4983 - val_loss: 1.5262 - val_acc: 0.2031\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 4s 312ms/step - loss: 1.2698 - acc: 0.4778 - val_loss: 1.5754 - val_acc: 0.2188\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 4s 296ms/step - loss: 1.2890 - acc: 0.4710 - val_loss: 1.4747 - val_acc: 0.2188\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 5s 457ms/step - loss: 1.2828 - acc: 0.4863 - val_loss: 1.5186 - val_acc: 0.2500\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.2273 - acc: 0.4846 - val_loss: 1.5766 - val_acc: 0.2188\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.2348 - acc: 0.4710 - val_loss: 1.4980 - val_acc: 0.2500\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 4s 300ms/step - loss: 1.2638 - acc: 0.4932 - val_loss: 1.5319 - val_acc: 0.2031\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 5s 451ms/step - loss: 1.2489 - acc: 0.4915 - val_loss: 1.5351 - val_acc: 0.2031\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 4s 289ms/step - loss: 1.2266 - acc: 0.4915 - val_loss: 1.5933 - val_acc: 0.1875\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.2142 - acc: 0.4795 - val_loss: 1.5785 - val_acc: 0.2188\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 5s 293ms/step - loss: 1.3044 - acc: 0.4659 - val_loss: 1.5114 - val_acc: 0.2500\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.2521 - acc: 0.4778 - val_loss: 1.5441 - val_acc: 0.2031\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 4s 298ms/step - loss: 1.2751 - acc: 0.4522 - val_loss: 1.5474 - val_acc: 0.2344\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.2340 - acc: 0.4608 - val_loss: 1.6079 - val_acc: 0.2031\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.1976 - acc: 0.4693 - val_loss: 1.4707 - val_acc: 0.2344\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.2335 - acc: 0.4781 - val_loss: 1.5546 - val_acc: 0.2500\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 1.2634 - acc: 0.4744 - val_loss: 1.5360 - val_acc: 0.2188\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.2446 - acc: 0.4608 - val_loss: 1.6107 - val_acc: 0.1875\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 5s 296ms/step - loss: 1.2730 - acc: 0.4727 - val_loss: 1.5260 - val_acc: 0.2188\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.2376 - acc: 0.4795 - val_loss: 1.5070 - val_acc: 0.2188\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.2049 - acc: 0.4846 - val_loss: 1.5132 - val_acc: 0.2031\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.2698 - acc: 0.4761 - val_loss: 1.6118 - val_acc: 0.2188\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.2398 - acc: 0.4744 - val_loss: 1.5499 - val_acc: 0.2344\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.2150 - acc: 0.5017 - val_loss: 1.5508 - val_acc: 0.2031\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.3022 - acc: 0.4608 - val_loss: 1.6315 - val_acc: 0.2031\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 4s 309ms/step - loss: 1.1940 - acc: 0.4949 - val_loss: 1.5411 - val_acc: 0.2031\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 4s 314ms/step - loss: 1.2111 - acc: 0.4983 - val_loss: 1.5632 - val_acc: 0.2344\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.2470 - acc: 0.4859 - val_loss: 1.5134 - val_acc: 0.2188\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.2150 - acc: 0.4829 - val_loss: 1.6067 - val_acc: 0.2188\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.2288 - acc: 0.4642 - val_loss: 1.5696 - val_acc: 0.1875\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.2148 - acc: 0.4949 - val_loss: 1.6020 - val_acc: 0.2188\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.1796 - acc: 0.5034 - val_loss: 1.5186 - val_acc: 0.2344\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.1454 - acc: 0.4983 - val_loss: 1.5731 - val_acc: 0.2188\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 5s 464ms/step - loss: 1.1459 - acc: 0.5239 - val_loss: 1.5511 - val_acc: 0.2031\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 4s 301ms/step - loss: 1.1815 - acc: 0.4710 - val_loss: 1.5054 - val_acc: 0.2344\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.2362 - acc: 0.4812 - val_loss: 1.5497 - val_acc: 0.2188\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.2134 - acc: 0.4863 - val_loss: 1.5608 - val_acc: 0.2031\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 1.1959 - acc: 0.4949 - val_loss: 1.5556 - val_acc: 0.2344\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.2498 - acc: 0.4710 - val_loss: 1.5237 - val_acc: 0.2188\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.1883 - acc: 0.4761 - val_loss: 1.5655 - val_acc: 0.2344\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.1973 - acc: 0.4829 - val_loss: 1.5519 - val_acc: 0.2344\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 6s 479ms/step - loss: 1.2244 - acc: 0.4812 - val_loss: 1.4677 - val_acc: 0.2344\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.2056 - acc: 0.4966 - val_loss: 1.5314 - val_acc: 0.2188\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.2126 - acc: 0.4812 - val_loss: 1.5019 - val_acc: 0.2031\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.1929 - acc: 0.4812 - val_loss: 1.4350 - val_acc: 0.2500\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 5s 457ms/step - loss: 1.2331 - acc: 0.4863 - val_loss: 1.5110 - val_acc: 0.2188\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.2210 - acc: 0.4922 - val_loss: 1.5243 - val_acc: 0.2344\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.2090 - acc: 0.4829 - val_loss: 1.5691 - val_acc: 0.2031\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.1657 - acc: 0.4966 - val_loss: 1.5245 - val_acc: 0.2031\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.1759 - acc: 0.5094 - val_loss: 1.4814 - val_acc: 0.2500\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 5s 463ms/step - loss: 1.2039 - acc: 0.5017 - val_loss: 1.5824 - val_acc: 0.2188\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.2203 - acc: 0.4891 - val_loss: 1.5453 - val_acc: 0.2344\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.1623 - acc: 0.5154 - val_loss: 1.5460 - val_acc: 0.2500\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 1.2406 - acc: 0.5000 - val_loss: 1.5908 - val_acc: 0.2344\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 1.1906 - acc: 0.5154 - val_loss: 1.6167 - val_acc: 0.2188\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.1749 - acc: 0.5119 - val_loss: 1.5751 - val_acc: 0.2188\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.2031 - acc: 0.5000 - val_loss: 1.5676 - val_acc: 0.1875\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.1512 - acc: 0.5154 - val_loss: 1.5929 - val_acc: 0.1875\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.1951 - acc: 0.4881 - val_loss: 1.6072 - val_acc: 0.1875\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 5s 305ms/step - loss: 1.1316 - acc: 0.5222 - val_loss: 1.6583 - val_acc: 0.1875\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.1242 - acc: 0.5239 - val_loss: 1.4937 - val_acc: 0.2656\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.2373 - acc: 0.4693 - val_loss: 1.5082 - val_acc: 0.2344\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 5s 457ms/step - loss: 1.1784 - acc: 0.5051 - val_loss: 1.5804 - val_acc: 0.2500\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.2554 - acc: 0.4505 - val_loss: 1.4232 - val_acc: 0.2656\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 4s 297ms/step - loss: 1.0908 - acc: 0.5239 - val_loss: 1.5489 - val_acc: 0.2188\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.2250 - acc: 0.4710 - val_loss: 1.5154 - val_acc: 0.2344\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 5s 298ms/step - loss: 1.1536 - acc: 0.4983 - val_loss: 1.5780 - val_acc: 0.2344\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 5s 292ms/step - loss: 1.1202 - acc: 0.5137 - val_loss: 1.5409 - val_acc: 0.2344\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 5s 470ms/step - loss: 1.2395 - acc: 0.4881 - val_loss: 1.5798 - val_acc: 0.2344\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 4s 291ms/step - loss: 1.1539 - acc: 0.5102 - val_loss: 1.5619 - val_acc: 0.2188\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 1.2020 - acc: 0.4859 - val_loss: 1.6105 - val_acc: 0.2031\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1502 - acc: 0.5000 - val_loss: 1.5036 - val_acc: 0.2188\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.1505 - acc: 0.5188 - val_loss: 1.6477 - val_acc: 0.1875\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.1955 - acc: 0.5094 - val_loss: 1.6476 - val_acc: 0.2031\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 5s 458ms/step - loss: 1.1381 - acc: 0.4829 - val_loss: 1.6287 - val_acc: 0.1875\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.1681 - acc: 0.5256 - val_loss: 1.5703 - val_acc: 0.2188\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.2055 - acc: 0.4693 - val_loss: 1.6419 - val_acc: 0.2031\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 5s 377ms/step - loss: 1.2132 - acc: 0.5051 - val_loss: 1.5331 - val_acc: 0.2031\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.1680 - acc: 0.5171 - val_loss: 1.6519 - val_acc: 0.1875\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 1.2040 - acc: 0.4898 - val_loss: 1.5513 - val_acc: 0.2500\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 1.2247 - acc: 0.4656 - val_loss: 1.5704 - val_acc: 0.2344\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.2217 - acc: 0.4949 - val_loss: 1.5786 - val_acc: 0.2188\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 5s 302ms/step - loss: 1.1579 - acc: 0.5102 - val_loss: 1.5694 - val_acc: 0.2031\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.1482 - acc: 0.4984 - val_loss: 1.6093 - val_acc: 0.2031\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 5s 467ms/step - loss: 1.1302 - acc: 0.5068 - val_loss: 1.5946 - val_acc: 0.2031\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0497 - acc: 0.5324 - val_loss: 1.5586 - val_acc: 0.2344\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.1572 - acc: 0.5512 - val_loss: 1.5872 - val_acc: 0.2188\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.2045 - acc: 0.4881 - val_loss: 1.5702 - val_acc: 0.2188\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 6s 424ms/step - loss: 1.1293 - acc: 0.5171 - val_loss: 1.4719 - val_acc: 0.2500\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.1371 - acc: 0.5051 - val_loss: 1.6045 - val_acc: 0.2344\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0910 - acc: 0.5154 - val_loss: 1.6091 - val_acc: 0.2031\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 5s 293ms/step - loss: 1.1139 - acc: 0.5273 - val_loss: 1.5963 - val_acc: 0.2188\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 5s 366ms/step - loss: 1.1784 - acc: 0.5256 - val_loss: 1.5682 - val_acc: 0.2500\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.1395 - acc: 0.5273 - val_loss: 1.5217 - val_acc: 0.2500\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.1466 - acc: 0.5109 - val_loss: 1.6076 - val_acc: 0.2031\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.1434 - acc: 0.5154 - val_loss: 1.5836 - val_acc: 0.2344\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.2538 - acc: 0.4915 - val_loss: 1.5787 - val_acc: 0.2031\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.1034 - acc: 0.5205 - val_loss: 1.5742 - val_acc: 0.2188\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.1473 - acc: 0.5444 - val_loss: 1.5305 - val_acc: 0.2344\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.1509 - acc: 0.4915 - val_loss: 1.5173 - val_acc: 0.2500\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.1567 - acc: 0.5078 - val_loss: 1.5723 - val_acc: 0.2344\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.1855 - acc: 0.4734 - val_loss: 1.5672 - val_acc: 0.2188\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.1656 - acc: 0.4863 - val_loss: 1.6143 - val_acc: 0.1562\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.1582 - acc: 0.5154 - val_loss: 1.6620 - val_acc: 0.1719\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 1.1868 - acc: 0.5137 - val_loss: 1.5490 - val_acc: 0.2344\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 5s 458ms/step - loss: 1.1778 - acc: 0.5017 - val_loss: 1.6332 - val_acc: 0.1719\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.1113 - acc: 0.4983 - val_loss: 1.5612 - val_acc: 0.2031\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.1501 - acc: 0.5068 - val_loss: 1.5646 - val_acc: 0.2188\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 4s 450ms/step - loss: 1.1327 - acc: 0.5017 - val_loss: 1.5436 - val_acc: 0.2188\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.1765 - acc: 0.4898 - val_loss: 1.6146 - val_acc: 0.1719\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1450 - acc: 0.5085 - val_loss: 1.6382 - val_acc: 0.1406\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1496 - acc: 0.5034 - val_loss: 1.6053 - val_acc: 0.1562\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1205 - acc: 0.5444 - val_loss: 1.6236 - val_acc: 0.1875\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.2259 - acc: 0.4932 - val_loss: 1.6011 - val_acc: 0.2188\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.1363 - acc: 0.5156 - val_loss: 1.5281 - val_acc: 0.2344\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.1294 - acc: 0.5085 - val_loss: 1.5983 - val_acc: 0.1875\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.1010 - acc: 0.5273 - val_loss: 1.6261 - val_acc: 0.1875\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0775 - acc: 0.5256 - val_loss: 1.6291 - val_acc: 0.1875\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.1543 - acc: 0.5102 - val_loss: 1.6142 - val_acc: 0.1875\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 5s 295ms/step - loss: 1.1444 - acc: 0.5307 - val_loss: 1.6103 - val_acc: 0.1719\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.1473 - acc: 0.5273 - val_loss: 1.6380 - val_acc: 0.1875\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.1890 - acc: 0.4932 - val_loss: 1.5648 - val_acc: 0.2031\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0703 - acc: 0.5307 - val_loss: 1.5731 - val_acc: 0.1875\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 1.1305 - acc: 0.5234 - val_loss: 1.5971 - val_acc: 0.2031\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 5s 470ms/step - loss: 1.0509 - acc: 0.5785 - val_loss: 1.5900 - val_acc: 0.2188\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.1651 - acc: 0.5102 - val_loss: 1.5452 - val_acc: 0.1875\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 1.2065 - acc: 0.4829 - val_loss: 1.5446 - val_acc: 0.1875\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.1141 - acc: 0.5239 - val_loss: 1.5817 - val_acc: 0.2031\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 5s 305ms/step - loss: 1.1514 - acc: 0.4642 - val_loss: 1.4942 - val_acc: 0.2031\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 5s 468ms/step - loss: 1.0946 - acc: 0.5495 - val_loss: 1.4835 - val_acc: 0.2031\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 5s 297ms/step - loss: 1.1020 - acc: 0.5222 - val_loss: 1.6255 - val_acc: 0.1719\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 5s 462ms/step - loss: 1.1142 - acc: 0.5171 - val_loss: 1.6118 - val_acc: 0.2031\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.1539 - acc: 0.5341 - val_loss: 1.6219 - val_acc: 0.1875\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 5s 305ms/step - loss: 1.1791 - acc: 0.5222 - val_loss: 1.6284 - val_acc: 0.1406\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 6s 446ms/step - loss: 1.1358 - acc: 0.5344 - val_loss: 1.6227 - val_acc: 0.1562\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.1050 - acc: 0.5085 - val_loss: 1.6073 - val_acc: 0.1875\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0881 - acc: 0.5614 - val_loss: 1.6196 - val_acc: 0.1719\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.1403 - acc: 0.5034 - val_loss: 1.5008 - val_acc: 0.2188\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.1197 - acc: 0.5068 - val_loss: 1.4991 - val_acc: 0.2188\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 1.1092 - acc: 0.5273 - val_loss: 1.5490 - val_acc: 0.2188\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.1176 - acc: 0.5281 - val_loss: 1.5104 - val_acc: 0.2344\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0801 - acc: 0.5529 - val_loss: 1.5892 - val_acc: 0.2188\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 5s 465ms/step - loss: 1.1495 - acc: 0.5137 - val_loss: 1.6183 - val_acc: 0.1875\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1119 - acc: 0.5358 - val_loss: 1.5909 - val_acc: 0.2031\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0277 - acc: 0.5341 - val_loss: 1.5957 - val_acc: 0.1875\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.1259 - acc: 0.5341 - val_loss: 1.6053 - val_acc: 0.1719\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.0675 - acc: 0.5358 - val_loss: 1.5477 - val_acc: 0.2188\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 5s 298ms/step - loss: 1.0612 - acc: 0.5427 - val_loss: 1.5863 - val_acc: 0.1875\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.1385 - acc: 0.5341 - val_loss: 1.6398 - val_acc: 0.2188\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0654 - acc: 0.5358 - val_loss: 1.6246 - val_acc: 0.1875\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 4s 320ms/step - loss: 1.0803 - acc: 0.5563 - val_loss: 1.5952 - val_acc: 0.1719\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.1348 - acc: 0.5273 - val_loss: 1.6063 - val_acc: 0.1719\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.1163 - acc: 0.5017 - val_loss: 1.5755 - val_acc: 0.2031\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 1.1151 - acc: 0.5297 - val_loss: 1.5249 - val_acc: 0.1875\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.0541 - acc: 0.5648 - val_loss: 1.5668 - val_acc: 0.2031\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 1.0756 - acc: 0.5266 - val_loss: 1.6218 - val_acc: 0.2031\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.1008 - acc: 0.5307 - val_loss: 1.5607 - val_acc: 0.2188\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 5s 284ms/step - loss: 1.1788 - acc: 0.5137 - val_loss: 1.5422 - val_acc: 0.1875\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.0844 - acc: 0.5478 - val_loss: 1.6267 - val_acc: 0.1875\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.1143 - acc: 0.5068 - val_loss: 1.5038 - val_acc: 0.2344\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.0727 - acc: 0.5341 - val_loss: 1.6695 - val_acc: 0.1562\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.0474 - acc: 0.5495 - val_loss: 1.5770 - val_acc: 0.1875\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0964 - acc: 0.5171 - val_loss: 1.6082 - val_acc: 0.2031\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 1.0663 - acc: 0.5375 - val_loss: 1.6088 - val_acc: 0.1875\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 1.1790 - acc: 0.4915 - val_loss: 1.5620 - val_acc: 0.1875\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.0729 - acc: 0.5444 - val_loss: 1.6017 - val_acc: 0.1875\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.1190 - acc: 0.5068 - val_loss: 1.5948 - val_acc: 0.1875\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0987 - acc: 0.5341 - val_loss: 1.5859 - val_acc: 0.1875\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0546 - acc: 0.5375 - val_loss: 1.5137 - val_acc: 0.2031\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 1.0933 - acc: 0.5205 - val_loss: 1.5695 - val_acc: 0.2188\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.1353 - acc: 0.5222 - val_loss: 1.6766 - val_acc: 0.1562\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.0515 - acc: 0.5563 - val_loss: 1.6554 - val_acc: 0.1875\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.1280 - acc: 0.4983 - val_loss: 1.6119 - val_acc: 0.2188\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.1065 - acc: 0.5102 - val_loss: 1.5680 - val_acc: 0.2188\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.1815 - acc: 0.4966 - val_loss: 1.6154 - val_acc: 0.1875\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 1.0674 - acc: 0.5205 - val_loss: 1.5582 - val_acc: 0.2188\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 1.1341 - acc: 0.5017 - val_loss: 1.5234 - val_acc: 0.2188\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0548 - acc: 0.5222 - val_loss: 1.5551 - val_acc: 0.2188\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 5s 458ms/step - loss: 1.0797 - acc: 0.5307 - val_loss: 1.6563 - val_acc: 0.1719\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.1835 - acc: 0.4590 - val_loss: 1.5820 - val_acc: 0.1875\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0121 - acc: 0.5614 - val_loss: 1.5688 - val_acc: 0.2031\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 1.1087 - acc: 0.5256 - val_loss: 1.5849 - val_acc: 0.1875\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 6s 315ms/step - loss: 1.0830 - acc: 0.5205 - val_loss: 1.6115 - val_acc: 0.2188\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0694 - acc: 0.5427 - val_loss: 1.6033 - val_acc: 0.2031\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 1.0907 - acc: 0.5324 - val_loss: 1.5712 - val_acc: 0.1875\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0941 - acc: 0.5266 - val_loss: 1.6207 - val_acc: 0.2031\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 1.0901 - acc: 0.5358 - val_loss: 1.6864 - val_acc: 0.1719\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.0645 - acc: 0.5666 - val_loss: 1.5674 - val_acc: 0.1875\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.0999 - acc: 0.5614 - val_loss: 1.5224 - val_acc: 0.2188\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 1.1103 - acc: 0.5392 - val_loss: 1.5705 - val_acc: 0.2188\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.0815 - acc: 0.5188 - val_loss: 1.6056 - val_acc: 0.1719\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.0279 - acc: 0.5392 - val_loss: 1.5621 - val_acc: 0.1875\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.0472 - acc: 0.5171 - val_loss: 1.5957 - val_acc: 0.1719\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.0812 - acc: 0.5495 - val_loss: 1.5880 - val_acc: 0.1875\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0606 - acc: 0.5580 - val_loss: 1.5318 - val_acc: 0.1875\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0680 - acc: 0.5392 - val_loss: 1.5700 - val_acc: 0.2031\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 1.1038 - acc: 0.5341 - val_loss: 1.5580 - val_acc: 0.1562\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 1.1416 - acc: 0.4881 - val_loss: 1.6243 - val_acc: 0.2031\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.0892 - acc: 0.5256 - val_loss: 1.6751 - val_acc: 0.1562\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 1.0038 - acc: 0.5478 - val_loss: 1.5438 - val_acc: 0.1719\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.0955 - acc: 0.5461 - val_loss: 1.5336 - val_acc: 0.2188\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.1421 - acc: 0.4906 - val_loss: 1.5531 - val_acc: 0.1719\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.1036 - acc: 0.5392 - val_loss: 1.5913 - val_acc: 0.1562\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 5s 466ms/step - loss: 1.1014 - acc: 0.5478 - val_loss: 1.5924 - val_acc: 0.2031\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.0719 - acc: 0.5222 - val_loss: 1.5770 - val_acc: 0.1562\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 5s 472ms/step - loss: 1.0750 - acc: 0.5222 - val_loss: 1.5509 - val_acc: 0.2188\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 1.0978 - acc: 0.5444 - val_loss: 1.6198 - val_acc: 0.2031\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 5s 467ms/step - loss: 1.0911 - acc: 0.5444 - val_loss: 1.6011 - val_acc: 0.1562\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 5s 468ms/step - loss: 1.0643 - acc: 0.5324 - val_loss: 1.6110 - val_acc: 0.1719\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0315 - acc: 0.5512 - val_loss: 1.5818 - val_acc: 0.1719\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.0871 - acc: 0.5444 - val_loss: 1.5710 - val_acc: 0.2031\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.0610 - acc: 0.5597 - val_loss: 1.4470 - val_acc: 0.2344\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 1.0865 - acc: 0.5222 - val_loss: 1.5102 - val_acc: 0.2188\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 1.0251 - acc: 0.5563 - val_loss: 1.5679 - val_acc: 0.1875\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.0554 - acc: 0.5580 - val_loss: 1.6258 - val_acc: 0.2031\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.1432 - acc: 0.4983 - val_loss: 1.5503 - val_acc: 0.1875\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.0692 - acc: 0.5375 - val_loss: 1.6158 - val_acc: 0.1719\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.0821 - acc: 0.5410 - val_loss: 1.6738 - val_acc: 0.1562\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 1.0835 - acc: 0.5392 - val_loss: 1.6537 - val_acc: 0.1562\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0485 - acc: 0.5410 - val_loss: 1.5602 - val_acc: 0.2188\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0638 - acc: 0.5444 - val_loss: 1.5741 - val_acc: 0.2031\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0546 - acc: 0.5341 - val_loss: 1.5382 - val_acc: 0.2188\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 0.9655 - acc: 0.5768 - val_loss: 1.5031 - val_acc: 0.2188\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 1.0311 - acc: 0.5887 - val_loss: 1.5552 - val_acc: 0.2344\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.1070 - acc: 0.5358 - val_loss: 1.5192 - val_acc: 0.2344\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0793 - acc: 0.5324 - val_loss: 1.5938 - val_acc: 0.1719\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0972 - acc: 0.5239 - val_loss: 1.6498 - val_acc: 0.1719\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.0184 - acc: 0.5597 - val_loss: 1.5215 - val_acc: 0.2031\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 1.1213 - acc: 0.5427 - val_loss: 1.5355 - val_acc: 0.1875\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 6s 536ms/step - loss: 1.1096 - acc: 0.5205 - val_loss: 1.6105 - val_acc: 0.1562\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 1.1694 - acc: 0.5016 - val_loss: 1.6541 - val_acc: 0.1406\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0462 - acc: 0.5410 - val_loss: 1.5859 - val_acc: 0.1875\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0331 - acc: 0.5785 - val_loss: 1.5119 - val_acc: 0.2344\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.0420 - acc: 0.5631 - val_loss: 1.5986 - val_acc: 0.1562\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.0357 - acc: 0.5594 - val_loss: 1.5717 - val_acc: 0.2031\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0574 - acc: 0.5580 - val_loss: 1.5162 - val_acc: 0.2031\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0291 - acc: 0.5273 - val_loss: 1.6507 - val_acc: 0.1719\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 1.0061 - acc: 0.5648 - val_loss: 1.5592 - val_acc: 0.2031\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0274 - acc: 0.5631 - val_loss: 1.6114 - val_acc: 0.1719\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.0726 - acc: 0.5512 - val_loss: 1.5966 - val_acc: 0.1719\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 1.0218 - acc: 0.5768 - val_loss: 1.5669 - val_acc: 0.1719\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 1.0926 - acc: 0.5358 - val_loss: 1.6266 - val_acc: 0.1875\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.0189 - acc: 0.5529 - val_loss: 1.6246 - val_acc: 0.1875\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.0335 - acc: 0.5785 - val_loss: 1.6787 - val_acc: 0.1719\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0262 - acc: 0.5406 - val_loss: 1.6476 - val_acc: 0.1406\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.0199 - acc: 0.5358 - val_loss: 1.6034 - val_acc: 0.1875\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 1.0329 - acc: 0.5683 - val_loss: 1.5630 - val_acc: 0.1562\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 0.9974 - acc: 0.5717 - val_loss: 1.6173 - val_acc: 0.1719\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.0309 - acc: 0.5358 - val_loss: 1.6859 - val_acc: 0.1406\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0258 - acc: 0.5580 - val_loss: 1.5986 - val_acc: 0.1719\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 1.0580 - acc: 0.5580 - val_loss: 1.6538 - val_acc: 0.1406\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0600 - acc: 0.5444 - val_loss: 1.6191 - val_acc: 0.1875\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 5s 456ms/step - loss: 1.0308 - acc: 0.5666 - val_loss: 1.6484 - val_acc: 0.1406\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0814 - acc: 0.5392 - val_loss: 1.6772 - val_acc: 0.1719\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0433 - acc: 0.5341 - val_loss: 1.6372 - val_acc: 0.1406\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 1.0254 - acc: 0.5683 - val_loss: 1.5546 - val_acc: 0.2188\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 1.0732 - acc: 0.5328 - val_loss: 1.5755 - val_acc: 0.2031\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0526 - acc: 0.5341 - val_loss: 1.6349 - val_acc: 0.1719\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.0593 - acc: 0.5392 - val_loss: 1.6017 - val_acc: 0.2031\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.0268 - acc: 0.5469 - val_loss: 1.5416 - val_acc: 0.1875\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0500 - acc: 0.5461 - val_loss: 1.4690 - val_acc: 0.2188\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0218 - acc: 0.5700 - val_loss: 1.5942 - val_acc: 0.1719\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 1.0126 - acc: 0.5734 - val_loss: 1.5285 - val_acc: 0.2344\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 1.0959 - acc: 0.5461 - val_loss: 1.5593 - val_acc: 0.2031\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0107 - acc: 0.5819 - val_loss: 1.6511 - val_acc: 0.1719\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.0606 - acc: 0.5427 - val_loss: 1.5384 - val_acc: 0.2031\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 5s 464ms/step - loss: 1.0188 - acc: 0.5392 - val_loss: 1.5196 - val_acc: 0.1875\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0758 - acc: 0.5546 - val_loss: 1.6053 - val_acc: 0.1875\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.0877 - acc: 0.5188 - val_loss: 1.5715 - val_acc: 0.1875\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.0404 - acc: 0.5546 - val_loss: 1.6359 - val_acc: 0.1719\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 1.0376 - acc: 0.5751 - val_loss: 1.5339 - val_acc: 0.1562\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0515 - acc: 0.5391 - val_loss: 1.5353 - val_acc: 0.1875\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0310 - acc: 0.5256 - val_loss: 1.5278 - val_acc: 0.1875\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9828 - acc: 0.5836 - val_loss: 1.7101 - val_acc: 0.1250\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 1.0488 - acc: 0.5444 - val_loss: 1.4998 - val_acc: 0.2031\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 6s 500ms/step - loss: 1.0250 - acc: 0.5529 - val_loss: 1.6013 - val_acc: 0.1875\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.0527 - acc: 0.5410 - val_loss: 1.6364 - val_acc: 0.1719\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0436 - acc: 0.5563 - val_loss: 1.5874 - val_acc: 0.1875\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9704 - acc: 0.5956 - val_loss: 1.5592 - val_acc: 0.1562\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.0481 - acc: 0.5375 - val_loss: 1.6078 - val_acc: 0.1562\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9728 - acc: 0.5785 - val_loss: 1.5193 - val_acc: 0.1719\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 1.0665 - acc: 0.5410 - val_loss: 1.6589 - val_acc: 0.1719\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 5s 297ms/step - loss: 0.9892 - acc: 0.5648 - val_loss: 1.5777 - val_acc: 0.1719\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.0393 - acc: 0.5461 - val_loss: 1.5727 - val_acc: 0.1875\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 1.0368 - acc: 0.5563 - val_loss: 1.5431 - val_acc: 0.1875\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0204 - acc: 0.5461 - val_loss: 1.6334 - val_acc: 0.1719\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 1.0100 - acc: 0.5422 - val_loss: 1.5323 - val_acc: 0.2031\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 1.0720 - acc: 0.5444 - val_loss: 1.5283 - val_acc: 0.2031\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9790 - acc: 0.5529 - val_loss: 1.6525 - val_acc: 0.1562\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 1.0309 - acc: 0.5648 - val_loss: 1.6236 - val_acc: 0.2031\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9430 - acc: 0.6177 - val_loss: 1.6123 - val_acc: 0.1875\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 1.0039 - acc: 0.5478 - val_loss: 1.5597 - val_acc: 0.1875\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 5s 296ms/step - loss: 1.0342 - acc: 0.5717 - val_loss: 1.5442 - val_acc: 0.1875\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 1.0648 - acc: 0.5239 - val_loss: 1.6490 - val_acc: 0.1719\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 1.0121 - acc: 0.5683 - val_loss: 1.5989 - val_acc: 0.1875\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.0084 - acc: 0.5802 - val_loss: 1.6091 - val_acc: 0.1719\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0065 - acc: 0.5734 - val_loss: 1.5510 - val_acc: 0.2188\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 1.0521 - acc: 0.5495 - val_loss: 1.6975 - val_acc: 0.1406\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 5s 477ms/step - loss: 1.0547 - acc: 0.5495 - val_loss: 1.6109 - val_acc: 0.1875\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0272 - acc: 0.5666 - val_loss: 1.5927 - val_acc: 0.1562\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 5s 477ms/step - loss: 1.0611 - acc: 0.5410 - val_loss: 1.6055 - val_acc: 0.1562\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0440 - acc: 0.5444 - val_loss: 1.6014 - val_acc: 0.1562\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9906 - acc: 0.5734 - val_loss: 1.5639 - val_acc: 0.1875\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9459 - acc: 0.6007 - val_loss: 1.5298 - val_acc: 0.1719\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9776 - acc: 0.5870 - val_loss: 1.5781 - val_acc: 0.2031\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0418 - acc: 0.5410 - val_loss: 1.5610 - val_acc: 0.1875\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 1.0596 - acc: 0.5375 - val_loss: 1.5740 - val_acc: 0.2031\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 5s 294ms/step - loss: 0.9703 - acc: 0.5819 - val_loss: 1.5862 - val_acc: 0.1875\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 5s 463ms/step - loss: 1.0154 - acc: 0.5631 - val_loss: 1.6279 - val_acc: 0.1406\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 1.0375 - acc: 0.5751 - val_loss: 1.6415 - val_acc: 0.1719\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 5s 308ms/step - loss: 1.0081 - acc: 0.5580 - val_loss: 1.6331 - val_acc: 0.1562\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 5s 467ms/step - loss: 1.0349 - acc: 0.5427 - val_loss: 1.6301 - val_acc: 0.2031\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.9522 - acc: 0.5870 - val_loss: 1.5856 - val_acc: 0.1875\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0023 - acc: 0.5529 - val_loss: 1.5887 - val_acc: 0.1875\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 0.9952 - acc: 0.5734 - val_loss: 1.5555 - val_acc: 0.1719\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0862 - acc: 0.5222 - val_loss: 1.6140 - val_acc: 0.1719\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0164 - acc: 0.5700 - val_loss: 1.6078 - val_acc: 0.1719\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 0.9922 - acc: 0.5666 - val_loss: 1.6628 - val_acc: 0.1250\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9832 - acc: 0.5734 - val_loss: 1.6205 - val_acc: 0.1719\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 1.0150 - acc: 0.5666 - val_loss: 1.6113 - val_acc: 0.1719\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0135 - acc: 0.5666 - val_loss: 1.5897 - val_acc: 0.1875\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 5s 295ms/step - loss: 0.9803 - acc: 0.5836 - val_loss: 1.6706 - val_acc: 0.1719\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 6s 412ms/step - loss: 0.9882 - acc: 0.5648 - val_loss: 1.5706 - val_acc: 0.2031\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9757 - acc: 0.5875 - val_loss: 1.5280 - val_acc: 0.1875\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9519 - acc: 0.5922 - val_loss: 1.5589 - val_acc: 0.1719\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0701 - acc: 0.5222 - val_loss: 1.5729 - val_acc: 0.1875\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0069 - acc: 0.5597 - val_loss: 1.5229 - val_acc: 0.1875\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9960 - acc: 0.5614 - val_loss: 1.5549 - val_acc: 0.1875\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 1.0377 - acc: 0.5495 - val_loss: 1.5863 - val_acc: 0.1719\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 1.0102 - acc: 0.5546 - val_loss: 1.5050 - val_acc: 0.2031\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 1.0222 - acc: 0.5597 - val_loss: 1.5407 - val_acc: 0.1719\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 1.0154 - acc: 0.5495 - val_loss: 1.5147 - val_acc: 0.1875\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9787 - acc: 0.5802 - val_loss: 1.5551 - val_acc: 0.2031\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.0118 - acc: 0.5666 - val_loss: 1.5442 - val_acc: 0.1875\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 5s 303ms/step - loss: 0.9810 - acc: 0.5751 - val_loss: 1.5626 - val_acc: 0.1719\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.0292 - acc: 0.5500 - val_loss: 1.6076 - val_acc: 0.1875\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0867 - acc: 0.5312 - val_loss: 1.5589 - val_acc: 0.2188\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.0267 - acc: 0.5802 - val_loss: 1.5381 - val_acc: 0.2031\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 0.9701 - acc: 0.5922 - val_loss: 1.5021 - val_acc: 0.2188\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0493 - acc: 0.5648 - val_loss: 1.6241 - val_acc: 0.1875\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 5s 479ms/step - loss: 1.0756 - acc: 0.5563 - val_loss: 1.5910 - val_acc: 0.1875\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9990 - acc: 0.5836 - val_loss: 1.6327 - val_acc: 0.1719\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 1.0653 - acc: 0.5444 - val_loss: 1.5422 - val_acc: 0.1875\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9987 - acc: 0.5700 - val_loss: 1.5716 - val_acc: 0.2031\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 1.0434 - acc: 0.5341 - val_loss: 1.5828 - val_acc: 0.1406\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 1.0525 - acc: 0.5648 - val_loss: 1.5236 - val_acc: 0.2031\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9335 - acc: 0.5922 - val_loss: 1.5351 - val_acc: 0.1875\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 1.0584 - acc: 0.5529 - val_loss: 1.5907 - val_acc: 0.1719\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9657 - acc: 0.5819 - val_loss: 1.5708 - val_acc: 0.1875\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 1.0416 - acc: 0.5614 - val_loss: 1.5318 - val_acc: 0.1875\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.0103 - acc: 0.5648 - val_loss: 1.4797 - val_acc: 0.2188\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 0.9869 - acc: 0.5797 - val_loss: 1.6270 - val_acc: 0.1562\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.9452 - acc: 0.5734 - val_loss: 1.6498 - val_acc: 0.1719\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9350 - acc: 0.5819 - val_loss: 1.5493 - val_acc: 0.2031\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0219 - acc: 0.5614 - val_loss: 1.5252 - val_acc: 0.1719\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9870 - acc: 0.5717 - val_loss: 1.6360 - val_acc: 0.1875\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 1.0015 - acc: 0.5939 - val_loss: 1.6086 - val_acc: 0.1562\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 1.0201 - acc: 0.5580 - val_loss: 1.5695 - val_acc: 0.1719\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 5s 299ms/step - loss: 1.0164 - acc: 0.5802 - val_loss: 1.5861 - val_acc: 0.1562\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 5s 456ms/step - loss: 1.0391 - acc: 0.5700 - val_loss: 1.6698 - val_acc: 0.1562\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 0.9797 - acc: 0.5768 - val_loss: 1.5920 - val_acc: 0.1719\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9638 - acc: 0.5819 - val_loss: 1.5509 - val_acc: 0.1562\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 0.9695 - acc: 0.5768 - val_loss: 1.5539 - val_acc: 0.1875\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9504 - acc: 0.5819 - val_loss: 1.4941 - val_acc: 0.2188\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9850 - acc: 0.5666 - val_loss: 1.5106 - val_acc: 0.1719\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.9317 - acc: 0.6016 - val_loss: 1.4967 - val_acc: 0.2031\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9188 - acc: 0.5904 - val_loss: 1.6488 - val_acc: 0.1562\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 1.0113 - acc: 0.5597 - val_loss: 1.5835 - val_acc: 0.1875\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 5s 364ms/step - loss: 0.9451 - acc: 0.5797 - val_loss: 1.5179 - val_acc: 0.2031\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 6s 443ms/step - loss: 1.0197 - acc: 0.5563 - val_loss: 1.6275 - val_acc: 0.1719\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 1.0079 - acc: 0.5580 - val_loss: 1.6362 - val_acc: 0.1719\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9523 - acc: 0.5768 - val_loss: 1.7037 - val_acc: 0.1562\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.9549 - acc: 0.5819 - val_loss: 1.6642 - val_acc: 0.1719\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9747 - acc: 0.5580 - val_loss: 1.5106 - val_acc: 0.1875\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 5s 310ms/step - loss: 0.9620 - acc: 0.5836 - val_loss: 1.5413 - val_acc: 0.1875\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.9913 - acc: 0.5875 - val_loss: 1.5987 - val_acc: 0.2031\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0107 - acc: 0.5717 - val_loss: 1.5362 - val_acc: 0.1562\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9337 - acc: 0.5828 - val_loss: 1.6217 - val_acc: 0.1875\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 5s 297ms/step - loss: 0.9795 - acc: 0.5802 - val_loss: 1.5608 - val_acc: 0.1875\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.9848 - acc: 0.5700 - val_loss: 1.6262 - val_acc: 0.1875\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0100 - acc: 0.5683 - val_loss: 1.6115 - val_acc: 0.1562\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 1.0184 - acc: 0.5819 - val_loss: 1.6545 - val_acc: 0.1719\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 0.9575 - acc: 0.5375 - val_loss: 1.6059 - val_acc: 0.1719\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9843 - acc: 0.5870 - val_loss: 1.5830 - val_acc: 0.1562\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 1.0656 - acc: 0.5341 - val_loss: 1.5158 - val_acc: 0.1719\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 1.0320 - acc: 0.5631 - val_loss: 1.5851 - val_acc: 0.1719\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.9310 - acc: 0.5904 - val_loss: 1.5634 - val_acc: 0.1875\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 1.0011 - acc: 0.5614 - val_loss: 1.6064 - val_acc: 0.1562\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 1.0114 - acc: 0.5614 - val_loss: 1.6682 - val_acc: 0.1406\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 1.0102 - acc: 0.5531 - val_loss: 1.5055 - val_acc: 0.1875\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 5s 478ms/step - loss: 0.9191 - acc: 0.5887 - val_loss: 1.6299 - val_acc: 0.1562\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 0.9590 - acc: 0.5904 - val_loss: 1.5021 - val_acc: 0.2188\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.9869 - acc: 0.5683 - val_loss: 1.4813 - val_acc: 0.2188\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 5s 478ms/step - loss: 1.0624 - acc: 0.5222 - val_loss: 1.5411 - val_acc: 0.1875\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 0.9743 - acc: 0.5751 - val_loss: 1.5097 - val_acc: 0.2031\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 0.9557 - acc: 0.5717 - val_loss: 1.5154 - val_acc: 0.2031\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 5s 466ms/step - loss: 0.9451 - acc: 0.5836 - val_loss: 1.5369 - val_acc: 0.2031\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 0.9902 - acc: 0.5358 - val_loss: 1.5454 - val_acc: 0.2031\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.9866 - acc: 0.5870 - val_loss: 1.6285 - val_acc: 0.1562\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 5s 306ms/step - loss: 0.9919 - acc: 0.5512 - val_loss: 1.5726 - val_acc: 0.1719\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.9673 - acc: 0.5614 - val_loss: 1.6396 - val_acc: 0.1406\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0385 - acc: 0.5700 - val_loss: 1.5462 - val_acc: 0.1406\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 1.0134 - acc: 0.5666 - val_loss: 1.5492 - val_acc: 0.1719\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.9534 - acc: 0.5939 - val_loss: 1.6386 - val_acc: 0.1406\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 0.8939 - acc: 0.6092 - val_loss: 1.6188 - val_acc: 0.1719\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9859 - acc: 0.5734 - val_loss: 1.5997 - val_acc: 0.1562\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9067 - acc: 0.5904 - val_loss: 1.6560 - val_acc: 0.1406\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9789 - acc: 0.5870 - val_loss: 1.5937 - val_acc: 0.1875\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 5s 301ms/step - loss: 0.9920 - acc: 0.5717 - val_loss: 1.5669 - val_acc: 0.1719\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 1.0039 - acc: 0.5717 - val_loss: 1.5637 - val_acc: 0.1562\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.9880 - acc: 0.5700 - val_loss: 1.5533 - val_acc: 0.1875\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.9740 - acc: 0.5939 - val_loss: 1.5894 - val_acc: 0.1719\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 1.0335 - acc: 0.5625 - val_loss: 1.6047 - val_acc: 0.1719\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 0.9948 - acc: 0.5597 - val_loss: 1.5679 - val_acc: 0.1875\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.9272 - acc: 0.5922 - val_loss: 1.5790 - val_acc: 0.1875\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 1.0048 - acc: 0.5631 - val_loss: 1.5484 - val_acc: 0.1719\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 6s 348ms/step - loss: 0.9666 - acc: 0.5768 - val_loss: 1.6874 - val_acc: 0.1406\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 5s 358ms/step - loss: 0.9751 - acc: 0.5836 - val_loss: 1.5496 - val_acc: 0.1875\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.9935 - acc: 0.5546 - val_loss: 1.6486 - val_acc: 0.1719\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 0.9646 - acc: 0.5785 - val_loss: 1.6221 - val_acc: 0.1875\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 0.9014 - acc: 0.6280 - val_loss: 1.6595 - val_acc: 0.1250\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 1.0095 - acc: 0.5444 - val_loss: 1.6074 - val_acc: 0.1719\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9142 - acc: 0.5973 - val_loss: 1.5587 - val_acc: 0.2188\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9991 - acc: 0.5700 - val_loss: 1.6329 - val_acc: 0.1406\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9863 - acc: 0.5717 - val_loss: 1.5719 - val_acc: 0.1719\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 5s 454ms/step - loss: 0.9801 - acc: 0.5666 - val_loss: 1.6062 - val_acc: 0.1875\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9631 - acc: 0.5700 - val_loss: 1.4962 - val_acc: 0.1875\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 5s 470ms/step - loss: 0.9844 - acc: 0.5631 - val_loss: 1.5296 - val_acc: 0.2031\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 5s 478ms/step - loss: 0.9225 - acc: 0.5802 - val_loss: 1.6000 - val_acc: 0.1875\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9910 - acc: 0.5700 - val_loss: 1.5656 - val_acc: 0.1875\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.9630 - acc: 0.5717 - val_loss: 1.6003 - val_acc: 0.1562\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9996 - acc: 0.5700 - val_loss: 1.5966 - val_acc: 0.1406\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.9513 - acc: 0.5853 - val_loss: 1.6157 - val_acc: 0.1719\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9662 - acc: 0.5666 - val_loss: 1.5680 - val_acc: 0.2031\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9605 - acc: 0.5751 - val_loss: 1.5715 - val_acc: 0.2031\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 0.9164 - acc: 0.5700 - val_loss: 1.6003 - val_acc: 0.1250\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9687 - acc: 0.5717 - val_loss: 1.6269 - val_acc: 0.1562\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 1.0116 - acc: 0.5922 - val_loss: 1.5989 - val_acc: 0.1719\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 0.9353 - acc: 0.5870 - val_loss: 1.6997 - val_acc: 0.1562\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9376 - acc: 0.5922 - val_loss: 1.5682 - val_acc: 0.1875\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9152 - acc: 0.6092 - val_loss: 1.6113 - val_acc: 0.1562\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.9231 - acc: 0.5859 - val_loss: 1.6805 - val_acc: 0.1250\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9894 - acc: 0.5631 - val_loss: 1.6206 - val_acc: 0.1719\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9858 - acc: 0.5768 - val_loss: 1.5241 - val_acc: 0.2031\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.9984 - acc: 0.5444 - val_loss: 1.5217 - val_acc: 0.1875\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.9993 - acc: 0.5597 - val_loss: 1.5756 - val_acc: 0.1875\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.9702 - acc: 0.5802 - val_loss: 1.5083 - val_acc: 0.1719\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9727 - acc: 0.5819 - val_loss: 1.5593 - val_acc: 0.1875\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.9441 - acc: 0.5870 - val_loss: 1.5862 - val_acc: 0.2031\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9844 - acc: 0.5719 - val_loss: 1.6387 - val_acc: 0.1719\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9736 - acc: 0.5785 - val_loss: 1.5774 - val_acc: 0.1719\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 5s 468ms/step - loss: 0.9581 - acc: 0.5785 - val_loss: 1.6543 - val_acc: 0.1562\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8620 - acc: 0.6451 - val_loss: 1.5761 - val_acc: 0.1875\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9859 - acc: 0.5853 - val_loss: 1.5729 - val_acc: 0.1875\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9550 - acc: 0.5751 - val_loss: 1.6233 - val_acc: 0.1562\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9890 - acc: 0.5990 - val_loss: 1.5908 - val_acc: 0.1562\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9446 - acc: 0.6007 - val_loss: 1.6128 - val_acc: 0.1875\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9819 - acc: 0.5768 - val_loss: 1.5791 - val_acc: 0.2031\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 5s 479ms/step - loss: 0.9682 - acc: 0.5751 - val_loss: 1.5398 - val_acc: 0.1875\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.9332 - acc: 0.5875 - val_loss: 1.5758 - val_acc: 0.2031\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.9424 - acc: 0.5836 - val_loss: 1.6110 - val_acc: 0.1719\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9389 - acc: 0.5802 - val_loss: 1.6053 - val_acc: 0.1719\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 6s 474ms/step - loss: 0.9710 - acc: 0.5734 - val_loss: 1.6174 - val_acc: 0.1562\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 5s 297ms/step - loss: 0.9603 - acc: 0.5922 - val_loss: 1.6026 - val_acc: 0.2031\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 0.9902 - acc: 0.5870 - val_loss: 1.5300 - val_acc: 0.1875\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.9197 - acc: 0.5939 - val_loss: 1.5957 - val_acc: 0.1875\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9029 - acc: 0.6007 - val_loss: 1.5956 - val_acc: 0.1719\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9713 - acc: 0.5700 - val_loss: 1.5949 - val_acc: 0.1406\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.9811 - acc: 0.5875 - val_loss: 1.6151 - val_acc: 0.1719\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9407 - acc: 0.6024 - val_loss: 1.5277 - val_acc: 0.1875\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 1.0288 - acc: 0.5750 - val_loss: 1.6593 - val_acc: 0.1406\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 5s 316ms/step - loss: 0.9569 - acc: 0.5973 - val_loss: 1.6407 - val_acc: 0.1719\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 5s 315ms/step - loss: 0.9448 - acc: 0.5802 - val_loss: 1.5694 - val_acc: 0.1719\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 0.9951 - acc: 0.5870 - val_loss: 1.5961 - val_acc: 0.1719\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9567 - acc: 0.5700 - val_loss: 1.5796 - val_acc: 0.1562\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.9710 - acc: 0.5939 - val_loss: 1.7263 - val_acc: 0.1250\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.9645 - acc: 0.6031 - val_loss: 1.6333 - val_acc: 0.1562\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 5s 307ms/step - loss: 0.9345 - acc: 0.5870 - val_loss: 1.6293 - val_acc: 0.1562\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9584 - acc: 0.5828 - val_loss: 1.6387 - val_acc: 0.1719\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 0.9673 - acc: 0.5939 - val_loss: 1.5680 - val_acc: 0.1875\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9447 - acc: 0.5853 - val_loss: 1.5955 - val_acc: 0.1875\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.9325 - acc: 0.5836 - val_loss: 1.5848 - val_acc: 0.1719\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.9590 - acc: 0.5836 - val_loss: 1.5846 - val_acc: 0.1719\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9531 - acc: 0.5700 - val_loss: 1.5899 - val_acc: 0.1875\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9195 - acc: 0.5887 - val_loss: 1.5848 - val_acc: 0.1875\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9415 - acc: 0.5631 - val_loss: 1.6634 - val_acc: 0.1562\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9796 - acc: 0.5751 - val_loss: 1.6179 - val_acc: 0.1406\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.9934 - acc: 0.5853 - val_loss: 1.5179 - val_acc: 0.1875\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 5s 471ms/step - loss: 0.9399 - acc: 0.6041 - val_loss: 1.5657 - val_acc: 0.1875\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.8985 - acc: 0.6212 - val_loss: 1.5473 - val_acc: 0.2188\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.9374 - acc: 0.6126 - val_loss: 1.6254 - val_acc: 0.1875\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9208 - acc: 0.6143 - val_loss: 1.6799 - val_acc: 0.1250\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.9376 - acc: 0.5751 - val_loss: 1.6228 - val_acc: 0.1875\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.9821 - acc: 0.5922 - val_loss: 1.5770 - val_acc: 0.1875\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.9299 - acc: 0.5802 - val_loss: 1.6797 - val_acc: 0.1719\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9766 - acc: 0.5853 - val_loss: 1.5650 - val_acc: 0.1719\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 5s 359ms/step - loss: 0.8931 - acc: 0.6024 - val_loss: 1.5085 - val_acc: 0.1875\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.9805 - acc: 0.5734 - val_loss: 1.6306 - val_acc: 0.1406\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9058 - acc: 0.6007 - val_loss: 1.5349 - val_acc: 0.2031\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 5s 300ms/step - loss: 0.9464 - acc: 0.5836 - val_loss: 1.5119 - val_acc: 0.1719\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9786 - acc: 0.5836 - val_loss: 1.5874 - val_acc: 0.1875\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9372 - acc: 0.5819 - val_loss: 1.5865 - val_acc: 0.1719\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.9332 - acc: 0.5870 - val_loss: 1.5472 - val_acc: 0.1875\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 0.8846 - acc: 0.6229 - val_loss: 1.5810 - val_acc: 0.1875\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.9269 - acc: 0.5768 - val_loss: 1.5508 - val_acc: 0.2031\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.9327 - acc: 0.6092 - val_loss: 1.5748 - val_acc: 0.1562\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.9529 - acc: 0.5887 - val_loss: 1.5338 - val_acc: 0.1875\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9523 - acc: 0.5853 - val_loss: 1.5971 - val_acc: 0.2031\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9000 - acc: 0.5768 - val_loss: 1.5557 - val_acc: 0.1719\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 6s 324ms/step - loss: 0.9278 - acc: 0.5990 - val_loss: 1.6158 - val_acc: 0.1875\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 5s 471ms/step - loss: 0.9195 - acc: 0.5853 - val_loss: 1.5737 - val_acc: 0.1719\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.8960 - acc: 0.6092 - val_loss: 1.6254 - val_acc: 0.1562\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 0.9448 - acc: 0.5802 - val_loss: 1.5314 - val_acc: 0.1875\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 5s 476ms/step - loss: 0.9227 - acc: 0.5717 - val_loss: 1.6020 - val_acc: 0.1719\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8927 - acc: 0.5956 - val_loss: 1.6134 - val_acc: 0.1719\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9067 - acc: 0.6177 - val_loss: 1.5960 - val_acc: 0.1562\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9893 - acc: 0.5751 - val_loss: 1.5787 - val_acc: 0.1875\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9338 - acc: 0.6024 - val_loss: 1.6547 - val_acc: 0.1562\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.9826 - acc: 0.5802 - val_loss: 1.6105 - val_acc: 0.1562\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.9337 - acc: 0.5904 - val_loss: 1.5586 - val_acc: 0.2031\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.9441 - acc: 0.6058 - val_loss: 1.5921 - val_acc: 0.2188\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9521 - acc: 0.5939 - val_loss: 1.6399 - val_acc: 0.2031\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 0.9052 - acc: 0.5844 - val_loss: 1.6435 - val_acc: 0.1875\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9238 - acc: 0.5922 - val_loss: 1.6344 - val_acc: 0.1719\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.9429 - acc: 0.5785 - val_loss: 1.5790 - val_acc: 0.1875\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.9821 - acc: 0.5751 - val_loss: 1.6191 - val_acc: 0.1562\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 0.9890 - acc: 0.5785 - val_loss: 1.5811 - val_acc: 0.1719\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9414 - acc: 0.5785 - val_loss: 1.5664 - val_acc: 0.1875\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.8956 - acc: 0.6143 - val_loss: 1.5453 - val_acc: 0.1719\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.9295 - acc: 0.5870 - val_loss: 1.5312 - val_acc: 0.2031\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 5s 477ms/step - loss: 0.8714 - acc: 0.6331 - val_loss: 1.6495 - val_acc: 0.1562\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9127 - acc: 0.6075 - val_loss: 1.5937 - val_acc: 0.1875\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.9334 - acc: 0.5887 - val_loss: 1.5445 - val_acc: 0.1562\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9163 - acc: 0.5836 - val_loss: 1.5627 - val_acc: 0.1875\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 0.8564 - acc: 0.6092 - val_loss: 1.5351 - val_acc: 0.1875\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.9846 - acc: 0.5785 - val_loss: 1.4791 - val_acc: 0.1875\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9229 - acc: 0.6024 - val_loss: 1.5432 - val_acc: 0.1875\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9622 - acc: 0.6007 - val_loss: 1.6237 - val_acc: 0.1406\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9164 - acc: 0.6075 - val_loss: 1.6226 - val_acc: 0.1562\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9160 - acc: 0.6229 - val_loss: 1.5534 - val_acc: 0.2031\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9621 - acc: 0.5751 - val_loss: 1.5555 - val_acc: 0.1719\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.9281 - acc: 0.6024 - val_loss: 1.5671 - val_acc: 0.1719\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9111 - acc: 0.6126 - val_loss: 1.5892 - val_acc: 0.1562\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.9216 - acc: 0.5938 - val_loss: 1.5244 - val_acc: 0.1875\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8689 - acc: 0.6280 - val_loss: 1.5954 - val_acc: 0.1719\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8779 - acc: 0.6075 - val_loss: 1.5375 - val_acc: 0.1719\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9465 - acc: 0.5984 - val_loss: 1.5223 - val_acc: 0.1875\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 0.9573 - acc: 0.5802 - val_loss: 1.5011 - val_acc: 0.1875\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9626 - acc: 0.5751 - val_loss: 1.5291 - val_acc: 0.2031\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9500 - acc: 0.5990 - val_loss: 1.5901 - val_acc: 0.1719\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8842 - acc: 0.6314 - val_loss: 1.6070 - val_acc: 0.1719\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9369 - acc: 0.6058 - val_loss: 1.5211 - val_acc: 0.2031\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.9404 - acc: 0.5990 - val_loss: 1.5578 - val_acc: 0.1719\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8903 - acc: 0.6075 - val_loss: 1.5897 - val_acc: 0.2031\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8767 - acc: 0.6399 - val_loss: 1.6236 - val_acc: 0.1562\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 6s 350ms/step - loss: 0.8590 - acc: 0.6160 - val_loss: 1.5350 - val_acc: 0.2031\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8577 - acc: 0.6212 - val_loss: 1.5770 - val_acc: 0.1875\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.9427 - acc: 0.5990 - val_loss: 1.5801 - val_acc: 0.2031\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9194 - acc: 0.6109 - val_loss: 1.5679 - val_acc: 0.2031\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.9015 - acc: 0.5904 - val_loss: 1.5915 - val_acc: 0.1875\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.8925 - acc: 0.5922 - val_loss: 1.6554 - val_acc: 0.1875\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9422 - acc: 0.5870 - val_loss: 1.5705 - val_acc: 0.1719\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8502 - acc: 0.6433 - val_loss: 1.6308 - val_acc: 0.1562\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.8983 - acc: 0.5904 - val_loss: 1.5958 - val_acc: 0.1719\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.9175 - acc: 0.6125 - val_loss: 1.6246 - val_acc: 0.1562\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 5s 365ms/step - loss: 0.9496 - acc: 0.5973 - val_loss: 1.6227 - val_acc: 0.1719\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.9354 - acc: 0.5768 - val_loss: 1.6061 - val_acc: 0.1719\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.8634 - acc: 0.6433 - val_loss: 1.6212 - val_acc: 0.1719\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 0.8782 - acc: 0.6143 - val_loss: 1.5256 - val_acc: 0.1875\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.8893 - acc: 0.5973 - val_loss: 1.5614 - val_acc: 0.1719\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.9228 - acc: 0.5990 - val_loss: 1.5465 - val_acc: 0.2031\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9469 - acc: 0.5990 - val_loss: 1.6666 - val_acc: 0.1719\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.8940 - acc: 0.6007 - val_loss: 1.6698 - val_acc: 0.1875\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 0.9020 - acc: 0.6177 - val_loss: 1.6742 - val_acc: 0.1562\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8913 - acc: 0.6212 - val_loss: 1.6426 - val_acc: 0.1719\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 0.8939 - acc: 0.6280 - val_loss: 1.6106 - val_acc: 0.1875\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8555 - acc: 0.6177 - val_loss: 1.6347 - val_acc: 0.1562\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9103 - acc: 0.6058 - val_loss: 1.5285 - val_acc: 0.2031\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 0.9411 - acc: 0.6195 - val_loss: 1.5750 - val_acc: 0.1875\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9240 - acc: 0.6109 - val_loss: 1.5145 - val_acc: 0.1875\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 5s 314ms/step - loss: 0.9567 - acc: 0.6126 - val_loss: 1.5334 - val_acc: 0.1875\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9395 - acc: 0.5819 - val_loss: 1.5438 - val_acc: 0.1875\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.9461 - acc: 0.5887 - val_loss: 1.5853 - val_acc: 0.1875\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 5s 477ms/step - loss: 0.8432 - acc: 0.6229 - val_loss: 1.5800 - val_acc: 0.2031\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.9236 - acc: 0.5990 - val_loss: 1.6353 - val_acc: 0.1562\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8697 - acc: 0.6041 - val_loss: 1.5158 - val_acc: 0.2031\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 5s 311ms/step - loss: 0.8955 - acc: 0.6177 - val_loss: 1.5191 - val_acc: 0.1875\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.9138 - acc: 0.6092 - val_loss: 1.6301 - val_acc: 0.1562\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.9557 - acc: 0.5939 - val_loss: 1.6586 - val_acc: 0.1250\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.8795 - acc: 0.6399 - val_loss: 1.6252 - val_acc: 0.1562\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 5s 312ms/step - loss: 0.9149 - acc: 0.5870 - val_loss: 1.5702 - val_acc: 0.2031\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.9107 - acc: 0.6058 - val_loss: 1.5639 - val_acc: 0.2031\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9222 - acc: 0.6031 - val_loss: 1.5369 - val_acc: 0.1875\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.9788 - acc: 0.5734 - val_loss: 1.5297 - val_acc: 0.2031\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 5s 376ms/step - loss: 0.8944 - acc: 0.6263 - val_loss: 1.5809 - val_acc: 0.1719\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9057 - acc: 0.6007 - val_loss: 1.5824 - val_acc: 0.1875\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9040 - acc: 0.5973 - val_loss: 1.5125 - val_acc: 0.2188\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9100 - acc: 0.6234 - val_loss: 1.5511 - val_acc: 0.1719\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.9106 - acc: 0.5853 - val_loss: 1.5711 - val_acc: 0.1562\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.9088 - acc: 0.6041 - val_loss: 1.6092 - val_acc: 0.1719\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 0.9152 - acc: 0.6109 - val_loss: 1.5836 - val_acc: 0.1719\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 7s 469ms/step - loss: 0.8964 - acc: 0.6143 - val_loss: 1.6842 - val_acc: 0.1875\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8803 - acc: 0.6195 - val_loss: 1.6211 - val_acc: 0.1562\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.8743 - acc: 0.6195 - val_loss: 1.6776 - val_acc: 0.1406\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 5s 309ms/step - loss: 0.9260 - acc: 0.6075 - val_loss: 1.7191 - val_acc: 0.1562\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.8874 - acc: 0.6177 - val_loss: 1.6615 - val_acc: 0.1875\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.9114 - acc: 0.6160 - val_loss: 1.6029 - val_acc: 0.2031\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 0.9488 - acc: 0.5802 - val_loss: 1.6276 - val_acc: 0.1719\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.9092 - acc: 0.6143 - val_loss: 1.6189 - val_acc: 0.1719\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.9107 - acc: 0.5870 - val_loss: 1.6126 - val_acc: 0.1719\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.8539 - acc: 0.6058 - val_loss: 1.6734 - val_acc: 0.1562\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.8614 - acc: 0.6143 - val_loss: 1.6576 - val_acc: 0.1719\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8934 - acc: 0.6141 - val_loss: 1.5646 - val_acc: 0.1719\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 5s 378ms/step - loss: 0.9291 - acc: 0.6143 - val_loss: 1.6277 - val_acc: 0.2188\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.8455 - acc: 0.6331 - val_loss: 1.5653 - val_acc: 0.1875\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.9587 - acc: 0.5813 - val_loss: 1.5126 - val_acc: 0.1875\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.8835 - acc: 0.6109 - val_loss: 1.5795 - val_acc: 0.1562\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8786 - acc: 0.6075 - val_loss: 1.5495 - val_acc: 0.2031\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.9342 - acc: 0.5819 - val_loss: 1.5847 - val_acc: 0.1719\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8954 - acc: 0.6024 - val_loss: 1.6619 - val_acc: 0.1562\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8822 - acc: 0.5990 - val_loss: 1.5286 - val_acc: 0.2031\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.9008 - acc: 0.6263 - val_loss: 1.5782 - val_acc: 0.1719\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8912 - acc: 0.6229 - val_loss: 1.5243 - val_acc: 0.1562\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.9129 - acc: 0.5904 - val_loss: 1.6397 - val_acc: 0.1562\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9137 - acc: 0.6007 - val_loss: 1.5715 - val_acc: 0.1875\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.9220 - acc: 0.6058 - val_loss: 1.6011 - val_acc: 0.1719\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 0.8599 - acc: 0.6263 - val_loss: 1.5184 - val_acc: 0.2031\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.9054 - acc: 0.6058 - val_loss: 1.5436 - val_acc: 0.2031\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.9230 - acc: 0.6126 - val_loss: 1.5640 - val_acc: 0.1875\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 5s 365ms/step - loss: 0.8836 - acc: 0.5973 - val_loss: 1.6159 - val_acc: 0.1719\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9025 - acc: 0.6280 - val_loss: 1.5505 - val_acc: 0.1719\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.8831 - acc: 0.6143 - val_loss: 1.6325 - val_acc: 0.1719\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.8873 - acc: 0.5973 - val_loss: 1.6179 - val_acc: 0.1719\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8598 - acc: 0.6092 - val_loss: 1.6099 - val_acc: 0.1562\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.9050 - acc: 0.6024 - val_loss: 1.5926 - val_acc: 0.1875\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.9134 - acc: 0.6024 - val_loss: 1.5982 - val_acc: 0.1875\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 5s 468ms/step - loss: 0.9184 - acc: 0.6007 - val_loss: 1.5650 - val_acc: 0.1875\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.8671 - acc: 0.6195 - val_loss: 1.5989 - val_acc: 0.1562\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 0.9283 - acc: 0.5990 - val_loss: 1.5444 - val_acc: 0.1875\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8671 - acc: 0.6263 - val_loss: 1.6222 - val_acc: 0.1719\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.8858 - acc: 0.6092 - val_loss: 1.5525 - val_acc: 0.1719\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9185 - acc: 0.5870 - val_loss: 1.5711 - val_acc: 0.2031\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.9100 - acc: 0.5870 - val_loss: 1.5648 - val_acc: 0.2031\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.8973 - acc: 0.6314 - val_loss: 1.6371 - val_acc: 0.1719\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.8833 - acc: 0.6348 - val_loss: 1.5083 - val_acc: 0.1875\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.9284 - acc: 0.6263 - val_loss: 1.4210 - val_acc: 0.2188\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9017 - acc: 0.6075 - val_loss: 1.6376 - val_acc: 0.1562\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 7s 675ms/step - loss: 0.8438 - acc: 0.6536 - val_loss: 1.6054 - val_acc: 0.1719\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9368 - acc: 0.5904 - val_loss: 1.5992 - val_acc: 0.1719\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.8927 - acc: 0.6007 - val_loss: 1.6071 - val_acc: 0.1562\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8550 - acc: 0.6263 - val_loss: 1.6391 - val_acc: 0.1562\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.9203 - acc: 0.5973 - val_loss: 1.5913 - val_acc: 0.1719\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8716 - acc: 0.6062 - val_loss: 1.6840 - val_acc: 0.1719\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8761 - acc: 0.6143 - val_loss: 1.6703 - val_acc: 0.1875\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8889 - acc: 0.6126 - val_loss: 1.6052 - val_acc: 0.1562\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 1.0000 - acc: 0.5785 - val_loss: 1.6088 - val_acc: 0.1562\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.9039 - acc: 0.6143 - val_loss: 1.4779 - val_acc: 0.2031\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8749 - acc: 0.5785 - val_loss: 1.6154 - val_acc: 0.1719\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8921 - acc: 0.6195 - val_loss: 1.6095 - val_acc: 0.1719\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8840 - acc: 0.6109 - val_loss: 1.6107 - val_acc: 0.1875\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8773 - acc: 0.6058 - val_loss: 1.5838 - val_acc: 0.2031\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.9166 - acc: 0.6058 - val_loss: 1.6106 - val_acc: 0.1719\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8992 - acc: 0.6058 - val_loss: 1.5574 - val_acc: 0.1719\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.9359 - acc: 0.5887 - val_loss: 1.6263 - val_acc: 0.1562\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 0.9174 - acc: 0.6075 - val_loss: 1.5795 - val_acc: 0.1719\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9072 - acc: 0.5956 - val_loss: 1.6265 - val_acc: 0.2188\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.8529 - acc: 0.6348 - val_loss: 1.6444 - val_acc: 0.1875\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.8405 - acc: 0.6109 - val_loss: 1.5942 - val_acc: 0.1406\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 5s 329ms/step - loss: 0.9007 - acc: 0.5939 - val_loss: 1.5742 - val_acc: 0.1562\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8904 - acc: 0.6024 - val_loss: 1.5002 - val_acc: 0.1875\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9511 - acc: 0.5683 - val_loss: 1.6378 - val_acc: 0.1719\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9437 - acc: 0.5990 - val_loss: 1.5547 - val_acc: 0.1875\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.8868 - acc: 0.6280 - val_loss: 1.6320 - val_acc: 0.1719\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.9080 - acc: 0.5969 - val_loss: 1.6195 - val_acc: 0.1719\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.8706 - acc: 0.6177 - val_loss: 1.6379 - val_acc: 0.1719\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 5s 327ms/step - loss: 0.8879 - acc: 0.6263 - val_loss: 1.6428 - val_acc: 0.1719\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 0.9505 - acc: 0.6041 - val_loss: 1.6372 - val_acc: 0.1875\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 5s 325ms/step - loss: 0.8831 - acc: 0.6263 - val_loss: 1.6707 - val_acc: 0.1719\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.8731 - acc: 0.6229 - val_loss: 1.6017 - val_acc: 0.1875\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8519 - acc: 0.6195 - val_loss: 1.5680 - val_acc: 0.1562\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.8611 - acc: 0.6212 - val_loss: 1.4851 - val_acc: 0.1875\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.8824 - acc: 0.5802 - val_loss: 1.5270 - val_acc: 0.2031\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.9523 - acc: 0.5887 - val_loss: 1.5517 - val_acc: 0.2031\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8722 - acc: 0.6195 - val_loss: 1.5209 - val_acc: 0.1875\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8767 - acc: 0.6314 - val_loss: 1.5214 - val_acc: 0.2031\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.8433 - acc: 0.6365 - val_loss: 1.5282 - val_acc: 0.2031\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 0.8904 - acc: 0.5956 - val_loss: 1.5572 - val_acc: 0.2031\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8888 - acc: 0.6344 - val_loss: 1.5859 - val_acc: 0.1719\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8659 - acc: 0.6041 - val_loss: 1.5909 - val_acc: 0.1875\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 5s 313ms/step - loss: 0.9442 - acc: 0.5785 - val_loss: 1.6288 - val_acc: 0.1562\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8776 - acc: 0.6024 - val_loss: 1.6118 - val_acc: 0.2031\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.8954 - acc: 0.6007 - val_loss: 1.5887 - val_acc: 0.1875\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 6s 596ms/step - loss: 0.8907 - acc: 0.5751 - val_loss: 1.5352 - val_acc: 0.1875\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8064 - acc: 0.6433 - val_loss: 1.6112 - val_acc: 0.1406\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9055 - acc: 0.6024 - val_loss: 1.5974 - val_acc: 0.1875\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8548 - acc: 0.6382 - val_loss: 1.5089 - val_acc: 0.2188\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9588 - acc: 0.5939 - val_loss: 1.5072 - val_acc: 0.1875\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.8839 - acc: 0.6075 - val_loss: 1.6182 - val_acc: 0.1719\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8648 - acc: 0.6416 - val_loss: 1.5776 - val_acc: 0.2031\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8659 - acc: 0.6314 - val_loss: 1.5551 - val_acc: 0.2031\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 5s 320ms/step - loss: 0.8938 - acc: 0.6280 - val_loss: 1.5088 - val_acc: 0.2188\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.8945 - acc: 0.5802 - val_loss: 1.5412 - val_acc: 0.1719\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8922 - acc: 0.6109 - val_loss: 1.5537 - val_acc: 0.2031\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.8730 - acc: 0.6331 - val_loss: 1.5381 - val_acc: 0.2031\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.8200 - acc: 0.6399 - val_loss: 1.5492 - val_acc: 0.1875\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8278 - acc: 0.6297 - val_loss: 1.5918 - val_acc: 0.1719\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.9284 - acc: 0.6229 - val_loss: 1.5944 - val_acc: 0.2031\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.8291 - acc: 0.6485 - val_loss: 1.6350 - val_acc: 0.1719\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9171 - acc: 0.5683 - val_loss: 1.6093 - val_acc: 0.2031\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9219 - acc: 0.6016 - val_loss: 1.5812 - val_acc: 0.1719\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 5s 501ms/step - loss: 0.8902 - acc: 0.6041 - val_loss: 1.5704 - val_acc: 0.1875\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8548 - acc: 0.6229 - val_loss: 1.5353 - val_acc: 0.2031\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8859 - acc: 0.6195 - val_loss: 1.5955 - val_acc: 0.1875\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8555 - acc: 0.6229 - val_loss: 1.6077 - val_acc: 0.1875\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8895 - acc: 0.5904 - val_loss: 1.5810 - val_acc: 0.1719\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 0.9337 - acc: 0.6007 - val_loss: 1.5254 - val_acc: 0.2031\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 5s 328ms/step - loss: 0.8443 - acc: 0.6263 - val_loss: 1.5312 - val_acc: 0.1875\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.9220 - acc: 0.6024 - val_loss: 1.5127 - val_acc: 0.1875\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8461 - acc: 0.6266 - val_loss: 1.5418 - val_acc: 0.1719\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8828 - acc: 0.5939 - val_loss: 1.5787 - val_acc: 0.1875\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.8633 - acc: 0.6280 - val_loss: 1.5434 - val_acc: 0.2031\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8468 - acc: 0.6331 - val_loss: 1.5681 - val_acc: 0.2031\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.8709 - acc: 0.6058 - val_loss: 1.5262 - val_acc: 0.2188\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 5s 317ms/step - loss: 0.8573 - acc: 0.6382 - val_loss: 1.5076 - val_acc: 0.1875\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8421 - acc: 0.5939 - val_loss: 1.5813 - val_acc: 0.1719\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.8674 - acc: 0.6024 - val_loss: 1.5937 - val_acc: 0.1719\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.8714 - acc: 0.5939 - val_loss: 1.6099 - val_acc: 0.1875\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 5s 361ms/step - loss: 0.9284 - acc: 0.5922 - val_loss: 1.6275 - val_acc: 0.1719\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8751 - acc: 0.6331 - val_loss: 1.5159 - val_acc: 0.2188\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.9150 - acc: 0.5939 - val_loss: 1.5619 - val_acc: 0.2031\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.8903 - acc: 0.6195 - val_loss: 1.5394 - val_acc: 0.2031\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.8359 - acc: 0.6422 - val_loss: 1.5427 - val_acc: 0.2188\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8560 - acc: 0.6092 - val_loss: 1.5260 - val_acc: 0.1875\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.8640 - acc: 0.6160 - val_loss: 1.4861 - val_acc: 0.2344\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.9003 - acc: 0.5939 - val_loss: 1.5637 - val_acc: 0.2031\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 5s 323ms/step - loss: 0.9258 - acc: 0.5990 - val_loss: 1.4970 - val_acc: 0.2500\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8524 - acc: 0.6195 - val_loss: 1.5067 - val_acc: 0.2344\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 7s 455ms/step - loss: 0.8396 - acc: 0.6314 - val_loss: 1.5532 - val_acc: 0.2188\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 0.8852 - acc: 0.6126 - val_loss: 1.6111 - val_acc: 0.1875\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 5s 339ms/step - loss: 0.9142 - acc: 0.5922 - val_loss: 1.5651 - val_acc: 0.2031\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8765 - acc: 0.5990 - val_loss: 1.4954 - val_acc: 0.1875\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8569 - acc: 0.6195 - val_loss: 1.5569 - val_acc: 0.1875\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8708 - acc: 0.6007 - val_loss: 1.5697 - val_acc: 0.2188\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.8295 - acc: 0.6314 - val_loss: 1.5513 - val_acc: 0.1719\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.8365 - acc: 0.6092 - val_loss: 1.5773 - val_acc: 0.1875\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 5s 358ms/step - loss: 0.9010 - acc: 0.6041 - val_loss: 1.5232 - val_acc: 0.2188\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 5s 359ms/step - loss: 0.8576 - acc: 0.6297 - val_loss: 1.5906 - val_acc: 0.2031\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8638 - acc: 0.6263 - val_loss: 1.5433 - val_acc: 0.2031\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 5s 319ms/step - loss: 0.8651 - acc: 0.6092 - val_loss: 1.5150 - val_acc: 0.2188\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 5s 333ms/step - loss: 0.8904 - acc: 0.6075 - val_loss: 1.4958 - val_acc: 0.2031\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 5s 372ms/step - loss: 0.8624 - acc: 0.6062 - val_loss: 1.5868 - val_acc: 0.1875\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 5s 374ms/step - loss: 0.8457 - acc: 0.6092 - val_loss: 1.5173 - val_acc: 0.2188\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 5s 359ms/step - loss: 0.8457 - acc: 0.6348 - val_loss: 1.5059 - val_acc: 0.2188\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9025 - acc: 0.6058 - val_loss: 1.5201 - val_acc: 0.1875\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 5s 368ms/step - loss: 0.8560 - acc: 0.6328 - val_loss: 1.5049 - val_acc: 0.2188\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.7992 - acc: 0.6433 - val_loss: 1.4711 - val_acc: 0.2031\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8063 - acc: 0.6280 - val_loss: 1.5632 - val_acc: 0.2188\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8551 - acc: 0.6160 - val_loss: 1.5031 - val_acc: 0.2031\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.8649 - acc: 0.6126 - val_loss: 1.5378 - val_acc: 0.2188\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8751 - acc: 0.6229 - val_loss: 1.5703 - val_acc: 0.1719\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8121 - acc: 0.6399 - val_loss: 1.5182 - val_acc: 0.2031\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.8928 - acc: 0.6041 - val_loss: 1.4779 - val_acc: 0.2031\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8657 - acc: 0.6177 - val_loss: 1.4628 - val_acc: 0.2031\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8345 - acc: 0.6365 - val_loss: 1.5580 - val_acc: 0.2188\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.8969 - acc: 0.6246 - val_loss: 1.5576 - val_acc: 0.1875\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8625 - acc: 0.6143 - val_loss: 1.5146 - val_acc: 0.1875\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.8188 - acc: 0.6365 - val_loss: 1.5198 - val_acc: 0.2188\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.8733 - acc: 0.6092 - val_loss: 1.5583 - val_acc: 0.2031\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8965 - acc: 0.6058 - val_loss: 1.5912 - val_acc: 0.1875\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8662 - acc: 0.6109 - val_loss: 1.4477 - val_acc: 0.2344\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.8554 - acc: 0.6587 - val_loss: 1.5331 - val_acc: 0.1875\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.8518 - acc: 0.6313 - val_loss: 1.5486 - val_acc: 0.2031\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.8885 - acc: 0.6348 - val_loss: 1.5849 - val_acc: 0.1875\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 0.8181 - acc: 0.6280 - val_loss: 1.5757 - val_acc: 0.1719\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8510 - acc: 0.6177 - val_loss: 1.5150 - val_acc: 0.2188\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 0.8337 - acc: 0.6246 - val_loss: 1.5980 - val_acc: 0.1719\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.8598 - acc: 0.6041 - val_loss: 1.5530 - val_acc: 0.2344\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 5s 330ms/step - loss: 0.9019 - acc: 0.6058 - val_loss: 1.4736 - val_acc: 0.2344\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.9226 - acc: 0.5956 - val_loss: 1.4694 - val_acc: 0.2188\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.8919 - acc: 0.6263 - val_loss: 1.5459 - val_acc: 0.1875\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.9129 - acc: 0.5870 - val_loss: 1.5997 - val_acc: 0.1719\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 7s 531ms/step - loss: 0.8350 - acc: 0.6416 - val_loss: 1.5301 - val_acc: 0.2188\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 0.8777 - acc: 0.6177 - val_loss: 1.5526 - val_acc: 0.1875\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8639 - acc: 0.6229 - val_loss: 1.5515 - val_acc: 0.1875\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 5s 365ms/step - loss: 0.8811 - acc: 0.6007 - val_loss: 1.5454 - val_acc: 0.2031\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.9072 - acc: 0.5956 - val_loss: 1.5800 - val_acc: 0.1719\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 5s 358ms/step - loss: 0.9162 - acc: 0.6092 - val_loss: 1.6072 - val_acc: 0.1875\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 5s 362ms/step - loss: 0.8502 - acc: 0.6177 - val_loss: 1.6469 - val_acc: 0.1562\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.8542 - acc: 0.6314 - val_loss: 1.5502 - val_acc: 0.2031\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.7882 - acc: 0.6536 - val_loss: 1.6024 - val_acc: 0.1875\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8524 - acc: 0.6160 - val_loss: 1.6207 - val_acc: 0.1719\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.8276 - acc: 0.6348 - val_loss: 1.6461 - val_acc: 0.1719\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.8607 - acc: 0.6058 - val_loss: 1.5951 - val_acc: 0.1875\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.8613 - acc: 0.6126 - val_loss: 1.5213 - val_acc: 0.2188\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 5s 343ms/step - loss: 0.8541 - acc: 0.6280 - val_loss: 1.5513 - val_acc: 0.2031\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 5s 366ms/step - loss: 0.8225 - acc: 0.6570 - val_loss: 1.4895 - val_acc: 0.2031\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8570 - acc: 0.6229 - val_loss: 1.5227 - val_acc: 0.1719\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8798 - acc: 0.6092 - val_loss: 1.5884 - val_acc: 0.1562\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8148 - acc: 0.6485 - val_loss: 1.5100 - val_acc: 0.2188\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 5s 318ms/step - loss: 0.8561 - acc: 0.6246 - val_loss: 1.5826 - val_acc: 0.2031\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 5s 321ms/step - loss: 0.8805 - acc: 0.6263 - val_loss: 1.5015 - val_acc: 0.2031\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8710 - acc: 0.6109 - val_loss: 1.5632 - val_acc: 0.1719\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8959 - acc: 0.6007 - val_loss: 1.5183 - val_acc: 0.2031\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8238 - acc: 0.6485 - val_loss: 1.5886 - val_acc: 0.2031\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.8629 - acc: 0.6382 - val_loss: 1.5939 - val_acc: 0.1719\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8451 - acc: 0.6229 - val_loss: 1.5797 - val_acc: 0.1875\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8513 - acc: 0.6365 - val_loss: 1.6173 - val_acc: 0.1719\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.8723 - acc: 0.6280 - val_loss: 1.5647 - val_acc: 0.2188\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8672 - acc: 0.6331 - val_loss: 1.5245 - val_acc: 0.2188\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 5s 361ms/step - loss: 0.8187 - acc: 0.6570 - val_loss: 1.5832 - val_acc: 0.1875\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 5s 360ms/step - loss: 0.8874 - acc: 0.6314 - val_loss: 1.6298 - val_acc: 0.1719\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.8034 - acc: 0.6348 - val_loss: 1.5730 - val_acc: 0.2031\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.8047 - acc: 0.6502 - val_loss: 1.5043 - val_acc: 0.2188\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 5s 366ms/step - loss: 0.8163 - acc: 0.6578 - val_loss: 1.5805 - val_acc: 0.1719\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 5s 360ms/step - loss: 0.8713 - acc: 0.6143 - val_loss: 1.5292 - val_acc: 0.2344\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 0.8320 - acc: 0.6314 - val_loss: 1.5602 - val_acc: 0.2031\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.8673 - acc: 0.6297 - val_loss: 1.5399 - val_acc: 0.1875\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 5s 363ms/step - loss: 0.8448 - acc: 0.6314 - val_loss: 1.5573 - val_acc: 0.1875\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 5s 326ms/step - loss: 0.8804 - acc: 0.6263 - val_loss: 1.5720 - val_acc: 0.1719\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.9100 - acc: 0.6007 - val_loss: 1.5455 - val_acc: 0.2188\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 5s 360ms/step - loss: 0.8083 - acc: 0.6348 - val_loss: 1.4946 - val_acc: 0.2188\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 5s 364ms/step - loss: 0.8315 - acc: 0.6234 - val_loss: 1.6092 - val_acc: 0.1719\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 6s 526ms/step - loss: 0.8079 - acc: 0.6348 - val_loss: 1.4827 - val_acc: 0.1875\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8376 - acc: 0.6502 - val_loss: 1.5006 - val_acc: 0.2188\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8151 - acc: 0.6348 - val_loss: 1.5113 - val_acc: 0.2188\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 5s 324ms/step - loss: 0.8271 - acc: 0.6109 - val_loss: 1.4545 - val_acc: 0.2344\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.8548 - acc: 0.6024 - val_loss: 1.4575 - val_acc: 0.1875\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.8494 - acc: 0.6416 - val_loss: 1.5819 - val_acc: 0.2031\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.9246 - acc: 0.5785 - val_loss: 1.5308 - val_acc: 0.2031\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 5s 355ms/step - loss: 0.8392 - acc: 0.6468 - val_loss: 1.5038 - val_acc: 0.2500\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8882 - acc: 0.6075 - val_loss: 1.4867 - val_acc: 0.2031\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8621 - acc: 0.6212 - val_loss: 1.5467 - val_acc: 0.1719\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.8828 - acc: 0.5973 - val_loss: 1.4996 - val_acc: 0.1875\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8322 - acc: 0.6328 - val_loss: 1.5338 - val_acc: 0.2031\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 5s 332ms/step - loss: 0.8077 - acc: 0.6263 - val_loss: 1.5691 - val_acc: 0.1719\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 5s 336ms/step - loss: 0.8514 - acc: 0.6229 - val_loss: 1.5243 - val_acc: 0.2031\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8248 - acc: 0.6416 - val_loss: 1.5607 - val_acc: 0.1875\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8512 - acc: 0.6195 - val_loss: 1.5366 - val_acc: 0.2031\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.8611 - acc: 0.6177 - val_loss: 1.5916 - val_acc: 0.2031\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 5s 334ms/step - loss: 0.8104 - acc: 0.6655 - val_loss: 1.5470 - val_acc: 0.1875\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8750 - acc: 0.5939 - val_loss: 1.5654 - val_acc: 0.1719\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8851 - acc: 0.6263 - val_loss: 1.5424 - val_acc: 0.1875\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8626 - acc: 0.6485 - val_loss: 1.5332 - val_acc: 0.1875\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.7852 - acc: 0.6655 - val_loss: 1.5342 - val_acc: 0.1875\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.8104 - acc: 0.6570 - val_loss: 1.5665 - val_acc: 0.2031\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8716 - acc: 0.6246 - val_loss: 1.6370 - val_acc: 0.1562\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8229 - acc: 0.6365 - val_loss: 1.5734 - val_acc: 0.1875\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 0.8275 - acc: 0.6519 - val_loss: 1.5800 - val_acc: 0.1719\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8717 - acc: 0.6331 - val_loss: 1.5287 - val_acc: 0.1719\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 5s 353ms/step - loss: 0.8231 - acc: 0.6485 - val_loss: 1.5190 - val_acc: 0.1875\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8615 - acc: 0.6195 - val_loss: 1.4504 - val_acc: 0.2188\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8452 - acc: 0.6382 - val_loss: 1.5798 - val_acc: 0.1562\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 0.8433 - acc: 0.6177 - val_loss: 1.5364 - val_acc: 0.2188\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8741 - acc: 0.6126 - val_loss: 1.4874 - val_acc: 0.2500\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 5s 356ms/step - loss: 0.8576 - acc: 0.6195 - val_loss: 1.4973 - val_acc: 0.2188\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 5s 322ms/step - loss: 0.8752 - acc: 0.6024 - val_loss: 1.5491 - val_acc: 0.2188\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 5s 348ms/step - loss: 0.8613 - acc: 0.6092 - val_loss: 1.5309 - val_acc: 0.1875\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8036 - acc: 0.6263 - val_loss: 1.5335 - val_acc: 0.2031\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8578 - acc: 0.6007 - val_loss: 1.6548 - val_acc: 0.1562\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 5s 399ms/step - loss: 0.8388 - acc: 0.6314 - val_loss: 1.6423 - val_acc: 0.2188\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8054 - acc: 0.6406 - val_loss: 1.6251 - val_acc: 0.1719\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 5s 357ms/step - loss: 0.8535 - acc: 0.6375 - val_loss: 1.5294 - val_acc: 0.1719\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.7758 - acc: 0.6519 - val_loss: 1.5834 - val_acc: 0.2031\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 5s 352ms/step - loss: 0.8377 - acc: 0.6485 - val_loss: 1.5606 - val_acc: 0.1719\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8204 - acc: 0.6451 - val_loss: 1.5883 - val_acc: 0.1562\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 6s 368ms/step - loss: 0.8133 - acc: 0.6195 - val_loss: 1.6458 - val_acc: 0.1719\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 5s 342ms/step - loss: 0.7953 - acc: 0.6638 - val_loss: 1.6474 - val_acc: 0.1875\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8782 - acc: 0.6280 - val_loss: 1.6591 - val_acc: 0.1719\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8448 - acc: 0.6331 - val_loss: 1.4998 - val_acc: 0.2188\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.8332 - acc: 0.6365 - val_loss: 1.5024 - val_acc: 0.2344\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 5s 345ms/step - loss: 0.7944 - acc: 0.6328 - val_loss: 1.6157 - val_acc: 0.1562\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 5s 346ms/step - loss: 0.8152 - acc: 0.6502 - val_loss: 1.6060 - val_acc: 0.1875\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 5s 351ms/step - loss: 0.7951 - acc: 0.6468 - val_loss: 1.6510 - val_acc: 0.1719\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.7924 - acc: 0.6570 - val_loss: 1.5778 - val_acc: 0.1719\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 5s 366ms/step - loss: 0.8707 - acc: 0.6212 - val_loss: 1.4748 - val_acc: 0.2344\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8485 - acc: 0.6160 - val_loss: 1.5500 - val_acc: 0.1875\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 5s 347ms/step - loss: 0.8280 - acc: 0.6365 - val_loss: 1.5288 - val_acc: 0.2188\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8888 - acc: 0.6195 - val_loss: 1.5036 - val_acc: 0.2031\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.8211 - acc: 0.6553 - val_loss: 1.5967 - val_acc: 0.1719\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 5s 359ms/step - loss: 0.8118 - acc: 0.6570 - val_loss: 1.5994 - val_acc: 0.1719\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 5s 341ms/step - loss: 0.8541 - acc: 0.6212 - val_loss: 1.5609 - val_acc: 0.1875\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 5s 349ms/step - loss: 0.8534 - acc: 0.6229 - val_loss: 1.5170 - val_acc: 0.2188\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 5s 350ms/step - loss: 0.8444 - acc: 0.6177 - val_loss: 1.5166 - val_acc: 0.2031\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 5s 331ms/step - loss: 0.8311 - acc: 0.6438 - val_loss: 1.5433 - val_acc: 0.2031\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 5s 335ms/step - loss: 0.8086 - acc: 0.6758 - val_loss: 1.5074 - val_acc: 0.1719\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 5s 354ms/step - loss: 0.8385 - acc: 0.6348 - val_loss: 1.6202 - val_acc: 0.1875\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 5s 340ms/step - loss: 0.8498 - acc: 0.6399 - val_loss: 1.5227 - val_acc: 0.2031\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 0.8069 - acc: 0.6468 - val_loss: 1.5443 - val_acc: 0.2188\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 5s 344ms/step - loss: 0.8372 - acc: 0.6365 - val_loss: 1.5831 - val_acc: 0.1719\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 5s 337ms/step - loss: 0.8408 - acc: 0.6212 - val_loss: 1.5776 - val_acc: 0.1875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "dcc2f1ec-880f-47e1-e164-c10c2bf72a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [2.3607523441314697,\n",
              "  2.0282063484191895,\n",
              "  2.066767454147339,\n",
              "  1.9470527172088623,\n",
              "  1.7645068168640137,\n",
              "  1.7216697931289673,\n",
              "  1.6143064498901367,\n",
              "  1.6477558612823486,\n",
              "  1.588979721069336,\n",
              "  1.445236086845398,\n",
              "  1.5726279020309448,\n",
              "  1.42490816116333,\n",
              "  1.4376403093338013,\n",
              "  1.4957516193389893,\n",
              "  1.5548577308654785,\n",
              "  1.5262601375579834,\n",
              "  1.4066110849380493,\n",
              "  1.4713075160980225,\n",
              "  1.4689223766326904,\n",
              "  1.540380597114563,\n",
              "  1.3935837745666504,\n",
              "  1.4789903163909912,\n",
              "  1.3819670677185059,\n",
              "  1.457876205444336,\n",
              "  1.4685475826263428,\n",
              "  1.3638560771942139,\n",
              "  1.407580018043518,\n",
              "  1.433118224143982,\n",
              "  1.4414912462234497,\n",
              "  1.4831750392913818,\n",
              "  1.3719251155853271,\n",
              "  1.4748166799545288,\n",
              "  1.4027808904647827,\n",
              "  1.3620707988739014,\n",
              "  1.3595552444458008,\n",
              "  1.351668119430542,\n",
              "  1.3720470666885376,\n",
              "  1.4501148462295532,\n",
              "  1.4300522804260254,\n",
              "  1.314663052558899,\n",
              "  1.3701132535934448,\n",
              "  1.3744750022888184,\n",
              "  1.376094102859497,\n",
              "  1.3787660598754883,\n",
              "  1.4079352617263794,\n",
              "  1.3424439430236816,\n",
              "  1.356261968612671,\n",
              "  1.3317540884017944,\n",
              "  1.3412485122680664,\n",
              "  1.3164591789245605,\n",
              "  1.2941309213638306,\n",
              "  1.3865571022033691,\n",
              "  1.3054112195968628,\n",
              "  1.3223854303359985,\n",
              "  1.3436301946640015,\n",
              "  1.3734850883483887,\n",
              "  1.3021886348724365,\n",
              "  1.3551487922668457,\n",
              "  1.3711355924606323,\n",
              "  1.3276805877685547,\n",
              "  1.394074559211731,\n",
              "  1.4194282293319702,\n",
              "  1.279448390007019,\n",
              "  1.332232117652893,\n",
              "  1.2836103439331055,\n",
              "  1.357348918914795,\n",
              "  1.3408172130584717,\n",
              "  1.295302152633667,\n",
              "  1.3329918384552002,\n",
              "  1.2114698886871338,\n",
              "  1.2543965578079224,\n",
              "  1.294589877128601,\n",
              "  1.3450907468795776,\n",
              "  1.258368730545044,\n",
              "  1.263645052909851,\n",
              "  1.2192150354385376,\n",
              "  1.2878412008285522,\n",
              "  1.2081584930419922,\n",
              "  1.317657709121704,\n",
              "  1.2431666851043701,\n",
              "  1.2618727684020996,\n",
              "  1.2627261877059937,\n",
              "  1.383670687675476,\n",
              "  1.2969423532485962,\n",
              "  1.2805265188217163,\n",
              "  1.3083194494247437,\n",
              "  1.355541706085205,\n",
              "  1.278280258178711,\n",
              "  1.310476303100586,\n",
              "  1.2655162811279297,\n",
              "  1.2599180936813354,\n",
              "  1.3090832233428955,\n",
              "  1.2995938062667847,\n",
              "  1.3387601375579834,\n",
              "  1.2541565895080566,\n",
              "  1.2746617794036865,\n",
              "  1.2970819473266602,\n",
              "  1.3449797630310059,\n",
              "  1.2786107063293457,\n",
              "  1.2065918445587158,\n",
              "  1.215806007385254,\n",
              "  1.3287655115127563,\n",
              "  1.279327392578125,\n",
              "  1.268578052520752,\n",
              "  1.219863772392273,\n",
              "  1.2509918212890625,\n",
              "  1.2979843616485596,\n",
              "  1.2479937076568604,\n",
              "  1.2214040756225586,\n",
              "  1.294144868850708,\n",
              "  1.2239683866500854,\n",
              "  1.2697745561599731,\n",
              "  1.2890362739562988,\n",
              "  1.2827948331832886,\n",
              "  1.22730553150177,\n",
              "  1.2347919940948486,\n",
              "  1.2638441324234009,\n",
              "  1.248942255973816,\n",
              "  1.2266266345977783,\n",
              "  1.214194893836975,\n",
              "  1.3044406175613403,\n",
              "  1.2521148920059204,\n",
              "  1.27511465549469,\n",
              "  1.2339812517166138,\n",
              "  1.1976076364517212,\n",
              "  1.2334531545639038,\n",
              "  1.2633569240570068,\n",
              "  1.24457585811615,\n",
              "  1.2729774713516235,\n",
              "  1.2375861406326294,\n",
              "  1.2049094438552856,\n",
              "  1.2697694301605225,\n",
              "  1.2398145198822021,\n",
              "  1.2149674892425537,\n",
              "  1.3022223711013794,\n",
              "  1.1939688920974731,\n",
              "  1.211076021194458,\n",
              "  1.2469875812530518,\n",
              "  1.215009331703186,\n",
              "  1.2288159132003784,\n",
              "  1.2148401737213135,\n",
              "  1.179629921913147,\n",
              "  1.1453711986541748,\n",
              "  1.145935297012329,\n",
              "  1.1814913749694824,\n",
              "  1.2362337112426758,\n",
              "  1.2133504152297974,\n",
              "  1.195886492729187,\n",
              "  1.249847173690796,\n",
              "  1.1882755756378174,\n",
              "  1.1972734928131104,\n",
              "  1.2244322299957275,\n",
              "  1.2056357860565186,\n",
              "  1.2125697135925293,\n",
              "  1.1929423809051514,\n",
              "  1.2331234216690063,\n",
              "  1.2210103273391724,\n",
              "  1.2090383768081665,\n",
              "  1.1657438278198242,\n",
              "  1.1759001016616821,\n",
              "  1.203897476196289,\n",
              "  1.2203376293182373,\n",
              "  1.162346363067627,\n",
              "  1.2406184673309326,\n",
              "  1.1905642747879028,\n",
              "  1.1749199628829956,\n",
              "  1.2030714750289917,\n",
              "  1.1512056589126587,\n",
              "  1.1950812339782715,\n",
              "  1.1316120624542236,\n",
              "  1.1242252588272095,\n",
              "  1.2372633218765259,\n",
              "  1.1784323453903198,\n",
              "  1.2553869485855103,\n",
              "  1.0907832384109497,\n",
              "  1.2249819040298462,\n",
              "  1.1535828113555908,\n",
              "  1.1201977729797363,\n",
              "  1.2394838333129883,\n",
              "  1.153886318206787,\n",
              "  1.201963186264038,\n",
              "  1.1502244472503662,\n",
              "  1.1505413055419922,\n",
              "  1.1955333948135376,\n",
              "  1.1381422281265259,\n",
              "  1.1681078672409058,\n",
              "  1.2054520845413208,\n",
              "  1.2131916284561157,\n",
              "  1.168026328086853,\n",
              "  1.2040141820907593,\n",
              "  1.2246863842010498,\n",
              "  1.2217403650283813,\n",
              "  1.1578668355941772,\n",
              "  1.148208737373352,\n",
              "  1.1302331686019897,\n",
              "  1.049715518951416,\n",
              "  1.1572372913360596,\n",
              "  1.204507827758789,\n",
              "  1.1293041706085205,\n",
              "  1.1370574235916138,\n",
              "  1.0909792184829712,\n",
              "  1.113921046257019,\n",
              "  1.1784484386444092,\n",
              "  1.1395363807678223,\n",
              "  1.1465661525726318,\n",
              "  1.143414855003357,\n",
              "  1.2537556886672974,\n",
              "  1.103399395942688,\n",
              "  1.147279977798462,\n",
              "  1.1509027481079102,\n",
              "  1.156741738319397,\n",
              "  1.1854746341705322,\n",
              "  1.1655656099319458,\n",
              "  1.1581686735153198,\n",
              "  1.1867927312850952,\n",
              "  1.1777666807174683,\n",
              "  1.1112730503082275,\n",
              "  1.1500821113586426,\n",
              "  1.132725477218628,\n",
              "  1.1764838695526123,\n",
              "  1.1450207233428955,\n",
              "  1.1496378183364868,\n",
              "  1.1204533576965332,\n",
              "  1.2259409427642822,\n",
              "  1.1362649202346802,\n",
              "  1.129365086555481,\n",
              "  1.1009995937347412,\n",
              "  1.0774587392807007,\n",
              "  1.154325246810913,\n",
              "  1.1444096565246582,\n",
              "  1.1472628116607666,\n",
              "  1.1890015602111816,\n",
              "  1.0702720880508423,\n",
              "  1.1304566860198975,\n",
              "  1.050887942314148,\n",
              "  1.1650968790054321,\n",
              "  1.2065236568450928,\n",
              "  1.1141270399093628,\n",
              "  1.151393175125122,\n",
              "  1.0945606231689453,\n",
              "  1.101961374282837,\n",
              "  1.114190936088562,\n",
              "  1.1539417505264282,\n",
              "  1.1790974140167236,\n",
              "  1.1357529163360596,\n",
              "  1.10504150390625,\n",
              "  1.0881201028823853,\n",
              "  1.140268087387085,\n",
              "  1.1197116374969482,\n",
              "  1.1092090606689453,\n",
              "  1.1175724267959595,\n",
              "  1.080129861831665,\n",
              "  1.1495026350021362,\n",
              "  1.1119210720062256,\n",
              "  1.0276637077331543,\n",
              "  1.1258615255355835,\n",
              "  1.0674681663513184,\n",
              "  1.061225175857544,\n",
              "  1.1385341882705688,\n",
              "  1.0653924942016602,\n",
              "  1.0802764892578125,\n",
              "  1.1348246335983276,\n",
              "  1.116255760192871,\n",
              "  1.1150996685028076,\n",
              "  1.05409836769104,\n",
              "  1.0756101608276367,\n",
              "  1.1007565259933472,\n",
              "  1.1787943840026855,\n",
              "  1.084420084953308,\n",
              "  1.114274024963379,\n",
              "  1.0726544857025146,\n",
              "  1.047441005706787,\n",
              "  1.096436619758606,\n",
              "  1.0663435459136963,\n",
              "  1.1790368556976318,\n",
              "  1.0729236602783203,\n",
              "  1.119036316871643,\n",
              "  1.0987017154693604,\n",
              "  1.054573655128479,\n",
              "  1.0933029651641846,\n",
              "  1.135293960571289,\n",
              "  1.0515375137329102,\n",
              "  1.1280375719070435,\n",
              "  1.1064716577529907,\n",
              "  1.1814714670181274,\n",
              "  1.0674259662628174,\n",
              "  1.1340895891189575,\n",
              "  1.0547624826431274,\n",
              "  1.0796617269515991,\n",
              "  1.1834533214569092,\n",
              "  1.0121490955352783,\n",
              "  1.1086748838424683,\n",
              "  1.0830316543579102,\n",
              "  1.0693669319152832,\n",
              "  1.0906648635864258,\n",
              "  1.0941044092178345,\n",
              "  1.0900864601135254,\n",
              "  1.0645475387573242,\n",
              "  1.0999135971069336,\n",
              "  1.1103041172027588,\n",
              "  1.0814707279205322,\n",
              "  1.027930498123169,\n",
              "  1.0471981763839722,\n",
              "  1.0811798572540283,\n",
              "  1.0605992078781128,\n",
              "  1.0679820775985718,\n",
              "  1.1037836074829102,\n",
              "  1.1416053771972656,\n",
              "  1.0892432928085327,\n",
              "  1.0037808418273926,\n",
              "  1.0954924821853638,\n",
              "  1.1420522928237915,\n",
              "  1.1035993099212646,\n",
              "  1.101374864578247,\n",
              "  1.071925401687622,\n",
              "  1.0749738216400146,\n",
              "  1.0978343486785889,\n",
              "  1.0911130905151367,\n",
              "  1.0642671585083008,\n",
              "  1.031510591506958,\n",
              "  1.0871368646621704,\n",
              "  1.06101393699646,\n",
              "  1.0865424871444702,\n",
              "  1.0251485109329224,\n",
              "  1.0553901195526123,\n",
              "  1.1432424783706665,\n",
              "  1.069169282913208,\n",
              "  1.0821001529693604,\n",
              "  1.0834988355636597,\n",
              "  1.0485180616378784,\n",
              "  1.0638365745544434,\n",
              "  1.0545716285705566,\n",
              "  0.9654723405838013,\n",
              "  1.0311247110366821,\n",
              "  1.1070243120193481,\n",
              "  1.0792646408081055,\n",
              "  1.09719717502594,\n",
              "  1.0184040069580078,\n",
              "  1.1212787628173828,\n",
              "  1.1095879077911377,\n",
              "  1.1693943738937378,\n",
              "  1.0462197065353394,\n",
              "  1.0331058502197266,\n",
              "  1.0420154333114624,\n",
              "  1.0356647968292236,\n",
              "  1.0573917627334595,\n",
              "  1.0291166305541992,\n",
              "  1.006110668182373,\n",
              "  1.0273778438568115,\n",
              "  1.072614073753357,\n",
              "  1.0217775106430054,\n",
              "  1.09256911277771,\n",
              "  1.018912672996521,\n",
              "  1.0334513187408447,\n",
              "  1.0262315273284912,\n",
              "  1.0199148654937744,\n",
              "  1.0329492092132568,\n",
              "  0.997403085231781,\n",
              "  1.0309009552001953,\n",
              "  1.0257714986801147,\n",
              "  1.0579791069030762,\n",
              "  1.0599952936172485,\n",
              "  1.0308113098144531,\n",
              "  1.0813747644424438,\n",
              "  1.0433080196380615,\n",
              "  1.0254273414611816,\n",
              "  1.0732057094573975,\n",
              "  1.0525871515274048,\n",
              "  1.0592936277389526,\n",
              "  1.0268118381500244,\n",
              "  1.050004482269287,\n",
              "  1.0218397378921509,\n",
              "  1.0125621557235718,\n",
              "  1.0959488153457642,\n",
              "  1.0107038021087646,\n",
              "  1.0606117248535156,\n",
              "  1.0188090801239014,\n",
              "  1.0758225917816162,\n",
              "  1.0876532793045044,\n",
              "  1.0403997898101807,\n",
              "  1.0375595092773438,\n",
              "  1.05146324634552,\n",
              "  1.0309710502624512,\n",
              "  0.9828298687934875,\n",
              "  1.048822045326233,\n",
              "  1.0249927043914795,\n",
              "  1.0526931285858154,\n",
              "  1.0435616970062256,\n",
              "  0.9704291820526123,\n",
              "  1.0480707883834839,\n",
              "  0.9728209972381592,\n",
              "  1.0664557218551636,\n",
              "  0.9892239570617676,\n",
              "  1.0393118858337402,\n",
              "  1.0368330478668213,\n",
              "  1.0204358100891113,\n",
              "  1.0099598169326782,\n",
              "  1.072033405303955,\n",
              "  0.9789926409721375,\n",
              "  1.0308754444122314,\n",
              "  0.9430131912231445,\n",
              "  1.0039219856262207,\n",
              "  1.0342423915863037,\n",
              "  1.0647705793380737,\n",
              "  1.0121468305587769,\n",
              "  1.0084011554718018,\n",
              "  1.0065172910690308,\n",
              "  1.0520949363708496,\n",
              "  1.0546869039535522,\n",
              "  1.0271694660186768,\n",
              "  1.0611399412155151,\n",
              "  1.0439966917037964,\n",
              "  0.9906455874443054,\n",
              "  0.9458630681037903,\n",
              "  0.9776231050491333,\n",
              "  1.0417835712432861,\n",
              "  1.0596281290054321,\n",
              "  0.9702937006950378,\n",
              "  1.0154469013214111,\n",
              "  1.0374819040298462,\n",
              "  1.0081050395965576,\n",
              "  1.0348968505859375,\n",
              "  0.9522435069084167,\n",
              "  1.0022541284561157,\n",
              "  0.9951735138893127,\n",
              "  1.086228847503662,\n",
              "  1.016377568244934,\n",
              "  0.9922095537185669,\n",
              "  0.9831680655479431,\n",
              "  1.0150209665298462,\n",
              "  1.0135102272033691,\n",
              "  0.9802630543708801,\n",
              "  0.9882287383079529,\n",
              "  0.9756938219070435,\n",
              "  0.951851487159729,\n",
              "  1.0700500011444092,\n",
              "  1.0069304704666138,\n",
              "  0.995994508266449,\n",
              "  1.0376758575439453,\n",
              "  1.010248064994812,\n",
              "  1.0222042798995972,\n",
              "  1.0153563022613525,\n",
              "  0.9787247776985168,\n",
              "  1.0118049383163452,\n",
              "  0.981023371219635,\n",
              "  1.0291526317596436,\n",
              "  1.08665132522583,\n",
              "  1.0266903638839722,\n",
              "  0.9700555801391602,\n",
              "  1.0493035316467285,\n",
              "  1.0755817890167236,\n",
              "  0.9989504218101501,\n",
              "  1.0652647018432617,\n",
              "  0.9987125396728516,\n",
              "  1.043422818183899,\n",
              "  1.052507758140564,\n",
              "  0.9334520101547241,\n",
              "  1.058432698249817,\n",
              "  0.9656774401664734,\n",
              "  1.0416114330291748,\n",
              "  1.0103411674499512,\n",
              "  0.9868810772895813,\n",
              "  0.9452237486839294,\n",
              "  0.934980571269989,\n",
              "  1.021875023841858,\n",
              "  0.9869697093963623,\n",
              "  1.0015143156051636,\n",
              "  1.0200581550598145,\n",
              "  1.0164107084274292,\n",
              "  1.0391106605529785,\n",
              "  0.9797006249427795,\n",
              "  0.9637745022773743,\n",
              "  0.9695002436637878,\n",
              "  0.9504019618034363,\n",
              "  0.9849600195884705,\n",
              "  0.9316598176956177,\n",
              "  0.918806254863739,\n",
              "  1.0113085508346558,\n",
              "  0.9450644254684448,\n",
              "  1.0196690559387207,\n",
              "  1.0078961849212646,\n",
              "  0.9523476958274841,\n",
              "  0.9549199938774109,\n",
              "  0.9747081995010376,\n",
              "  0.9620256423950195,\n",
              "  0.9912707209587097,\n",
              "  1.0106685161590576,\n",
              "  0.9336820840835571,\n",
              "  0.9794850945472717,\n",
              "  0.9848153591156006,\n",
              "  1.0100159645080566,\n",
              "  1.0184388160705566,\n",
              "  0.9574622511863708,\n",
              "  0.984332799911499,\n",
              "  1.0656414031982422,\n",
              "  1.031977653503418,\n",
              "  0.9309800863265991,\n",
              "  1.0010520219802856,\n",
              "  1.0113977193832397,\n",
              "  1.0102097988128662,\n",
              "  0.9191017746925354,\n",
              "  0.9590246677398682,\n",
              "  0.9868930578231812,\n",
              "  1.0623925924301147,\n",
              "  0.9743090867996216,\n",
              "  0.9557478427886963,\n",
              "  0.9450868964195251,\n",
              "  0.9902411699295044,\n",
              "  0.9865626692771912,\n",
              "  0.991856575012207,\n",
              "  0.9673078656196594,\n",
              "  1.0384894609451294,\n",
              "  1.0133827924728394,\n",
              "  0.9534493684768677,\n",
              "  0.8938711881637573,\n",
              "  0.9859055876731873,\n",
              "  0.9067225456237793,\n",
              "  0.9789044260978699,\n",
              "  0.9919641017913818,\n",
              "  1.003862738609314,\n",
              "  0.9879968762397766,\n",
              "  0.9740023612976074,\n",
              "  1.0335180759429932,\n",
              "  0.9947603344917297,\n",
              "  0.9271950721740723,\n",
              "  1.0048415660858154,\n",
              "  0.9666140675544739,\n",
              "  0.9751414060592651,\n",
              "  0.9934698343276978,\n",
              "  0.9645929336547852,\n",
              "  0.9014156460762024,\n",
              "  1.0095114707946777,\n",
              "  0.9142354726791382,\n",
              "  0.9991483092308044,\n",
              "  0.9863112568855286,\n",
              "  0.9800845384597778,\n",
              "  0.9630522727966309,\n",
              "  0.9844409227371216,\n",
              "  0.9224504828453064,\n",
              "  0.9910013675689697,\n",
              "  0.9629959464073181,\n",
              "  0.9996392130851746,\n",
              "  0.9513098001480103,\n",
              "  0.9661604762077332,\n",
              "  0.9605368375778198,\n",
              "  0.9164285063743591,\n",
              "  0.9686772227287292,\n",
              "  1.0115569829940796,\n",
              "  0.935329794883728,\n",
              "  0.9375657439231873,\n",
              "  0.9152121543884277,\n",
              "  0.92308509349823,\n",
              "  0.9894172549247742,\n",
              "  0.9857782125473022,\n",
              "  0.9983657002449036,\n",
              "  0.9992935061454773,\n",
              "  0.9702039361000061,\n",
              "  0.9727108478546143,\n",
              "  0.9441284537315369,\n",
              "  0.9844330549240112,\n",
              "  0.9736015796661377,\n",
              "  0.9581196904182434,\n",
              "  0.8620463013648987,\n",
              "  0.9858777523040771,\n",
              "  0.9550309181213379,\n",
              "  0.9890108704566956,\n",
              "  0.9445872902870178,\n",
              "  0.9818938970565796,\n",
              "  0.9682459831237793,\n",
              "  0.9331795573234558,\n",
              "  0.9424266815185547,\n",
              "  0.9389071464538574,\n",
              "  0.9709649682044983,\n",
              "  0.9603094458580017,\n",
              "  0.9902302026748657,\n",
              "  0.9196901321411133,\n",
              "  0.9029046297073364,\n",
              "  0.9712900519371033,\n",
              "  0.9810696840286255,\n",
              "  0.9406891465187073,\n",
              "  1.02882719039917,\n",
              "  0.95693039894104,\n",
              "  0.94479900598526,\n",
              "  0.995103657245636,\n",
              "  0.9567192196846008,\n",
              "  0.9709610939025879,\n",
              "  0.9644570350646973,\n",
              "  0.9345002174377441,\n",
              "  0.9583523869514465,\n",
              "  0.9672639966011047,\n",
              "  0.9446977972984314,\n",
              "  0.9324871897697449,\n",
              "  0.9589637517929077,\n",
              "  0.9530922770500183,\n",
              "  0.9195265173912048,\n",
              "  0.941523015499115,\n",
              "  0.9795787334442139,\n",
              "  0.9934260249137878,\n",
              "  0.9399288892745972,\n",
              "  0.8984780311584473,\n",
              "  0.9374116659164429,\n",
              "  0.9207983613014221,\n",
              "  0.9376084208488464,\n",
              "  0.9820500612258911,\n",
              "  0.9298643469810486,\n",
              "  0.976606011390686,\n",
              "  0.893096387386322,\n",
              "  0.9805271029472351,\n",
              "  0.9057705998420715,\n",
              "  0.9463768601417542,\n",
              "  0.9785730242729187,\n",
              "  0.9371541142463684,\n",
              "  0.9332438707351685,\n",
              "  0.8846181631088257,\n",
              "  0.926933228969574,\n",
              "  0.9327269792556763,\n",
              "  0.9528988003730774,\n",
              "  0.9523118734359741,\n",
              "  0.8999584913253784,\n",
              "  0.9277756214141846,\n",
              "  0.9195414185523987,\n",
              "  0.896035373210907,\n",
              "  0.9448153376579285,\n",
              "  0.9226756691932678,\n",
              "  0.8926986455917358,\n",
              "  0.9067360162734985,\n",
              "  0.9892526268959045,\n",
              "  0.9338261485099792,\n",
              "  0.9825683236122131,\n",
              "  0.9337368607521057,\n",
              "  0.9440845847129822,\n",
              "  0.9521112442016602,\n",
              "  0.9052188992500305,\n",
              "  0.923845112323761,\n",
              "  0.9428507685661316,\n",
              "  0.9820501208305359,\n",
              "  0.9890426397323608,\n",
              "  0.9413648247718811,\n",
              "  0.8955859541893005,\n",
              "  0.9294840693473816,\n",
              "  0.8713974952697754,\n",
              "  0.9127304553985596,\n",
              "  0.9333586692810059,\n",
              "  0.9162634015083313,\n",
              "  0.8563873767852783,\n",
              "  0.9845756888389587,\n",
              "  0.9229482412338257,\n",
              "  0.9622392654418945,\n",
              "  0.9164331555366516,\n",
              "  0.916047990322113,\n",
              "  0.9621120691299438,\n",
              "  0.92808997631073,\n",
              "  0.9110648036003113,\n",
              "  0.9215992093086243,\n",
              "  0.8688726425170898,\n",
              "  0.8779154419898987,\n",
              "  0.9465001225471497,\n",
              "  0.957338273525238,\n",
              "  0.9625945091247559,\n",
              "  0.9500282406806946,\n",
              "  0.8842256665229797,\n",
              "  0.9369272589683533,\n",
              "  0.9403765797615051,\n",
              "  0.8903266191482544,\n",
              "  0.8766506314277649,\n",
              "  0.8590348958969116,\n",
              "  0.8577172160148621,\n",
              "  0.9426873922348022,\n",
              "  0.9193654656410217,\n",
              "  0.9014745950698853,\n",
              "  0.8925275802612305,\n",
              "  0.9421802163124084,\n",
              "  0.8501520156860352,\n",
              "  0.8983259201049805,\n",
              "  0.9174613952636719,\n",
              "  0.949628472328186,\n",
              "  0.9353647232055664,\n",
              "  0.8634263873100281,\n",
              "  0.878220796585083,\n",
              "  0.8892973065376282,\n",
              "  0.922759473323822,\n",
              "  0.9469320178031921,\n",
              "  0.8940402865409851,\n",
              "  0.9019587635993958,\n",
              "  0.8913238644599915,\n",
              "  0.8938539028167725,\n",
              "  0.8555448651313782,\n",
              "  0.9103454947471619,\n",
              "  0.9410881400108337,\n",
              "  0.9240293502807617,\n",
              "  0.956745445728302,\n",
              "  0.9394682049751282,\n",
              "  0.946080207824707,\n",
              "  0.8432411551475525,\n",
              "  0.9235888719558716,\n",
              "  0.8696996569633484,\n",
              "  0.8955147862434387,\n",
              "  0.9137755632400513,\n",
              "  0.9557315111160278,\n",
              "  0.8794933557510376,\n",
              "  0.9149184226989746,\n",
              "  0.9106819033622742,\n",
              "  0.9221999049186707,\n",
              "  0.9787757992744446,\n",
              "  0.8943848013877869,\n",
              "  0.9057281017303467,\n",
              "  0.9040403366088867,\n",
              "  0.9099699258804321,\n",
              "  0.9105564951896667,\n",
              "  0.9087960720062256,\n",
              "  0.9151743650436401,\n",
              "  0.8964443802833557,\n",
              "  0.8803107738494873,\n",
              "  0.874281644821167,\n",
              "  0.9260297417640686,\n",
              "  0.8874166011810303,\n",
              "  0.9113504886627197,\n",
              "  0.9487508535385132,\n",
              "  0.9091516733169556,\n",
              "  0.9106538891792297,\n",
              "  0.8538559675216675,\n",
              "  0.8614165782928467,\n",
              "  0.893375039100647,\n",
              "  0.9291448593139648,\n",
              "  0.8454586267471313,\n",
              "  0.9587459564208984,\n",
              "  0.8835445046424866,\n",
              "  0.8786171078681946,\n",
              "  0.9342440962791443,\n",
              "  0.8954122066497803,\n",
              "  0.8821600675582886,\n",
              "  0.900827169418335,\n",
              "  0.8912400603294373,\n",
              "  0.912880539894104,\n",
              "  0.9136887788772583,\n",
              "  0.9219843745231628,\n",
              "  0.8598578572273254,\n",
              "  0.9053806066513062,\n",
              "  0.9229796528816223,\n",
              "  0.8836108446121216,\n",
              "  0.9024608135223389,\n",
              "  0.8831353783607483,\n",
              "  0.8872566223144531,\n",
              "  0.859796941280365,\n",
              "  0.9049506783485413,\n",
              "  0.913411021232605,\n",
              "  0.9183828830718994,\n",
              "  0.8670529127120972,\n",
              "  0.9282575249671936,\n",
              "  0.8671090602874756,\n",
              "  0.8857871890068054,\n",
              "  0.918540358543396,\n",
              "  0.9100159406661987,\n",
              "  0.8973385691642761,\n",
              "  0.8833410143852234,\n",
              "  0.928447961807251,\n",
              "  0.901668906211853,\n",
              "  0.8438498973846436,\n",
              "  0.9368273615837097,\n",
              "  0.892738401889801,\n",
              "  0.855033278465271,\n",
              "  0.9202970266342163,\n",
              "  0.8716223835945129,\n",
              "  0.876079261302948,\n",
              "  0.8889371752738953,\n",
              "  0.9999852180480957,\n",
              "  0.9039044976234436,\n",
              "  0.8748635649681091,\n",
              "  0.892145574092865,\n",
              "  0.8840470314025879,\n",
              "  0.877293586730957,\n",
              "  0.9166357517242432,\n",
              "  0.8991702198982239,\n",
              "  0.9359070658683777,\n",
              "  0.9173674583435059,\n",
              "  0.9071760177612305,\n",
              "  0.8529166579246521,\n",
              "  0.8404723405838013,\n",
              "  0.9006510972976685,\n",
              "  0.8903818130493164,\n",
              "  0.9510970711708069,\n",
              "  0.943680465221405,\n",
              "  0.8868352174758911,\n",
              "  0.9080216288566589,\n",
              "  0.8705935478210449,\n",
              "  0.8878770470619202,\n",
              "  0.9505388140678406,\n",
              "  0.8831208944320679,\n",
              "  0.8731368780136108,\n",
              "  0.8519070744514465,\n",
              "  0.8611361384391785,\n",
              "  0.8824436664581299,\n",
              "  0.9523484110832214,\n",
              "  0.8722078204154968,\n",
              "  0.8767251372337341,\n",
              "  0.8432623147964478,\n",
              "  0.890414834022522,\n",
              "  0.8888438940048218,\n",
              "  0.8659399747848511,\n",
              "  0.9441505670547485,\n",
              "  0.8776161670684814,\n",
              "  0.895408570766449,\n",
              "  0.890694797039032,\n",
              "  0.8064098954200745,\n",
              "  0.9055191278457642,\n",
              "  0.8547660112380981,\n",
              "  0.958756148815155,\n",
              "  0.8839418292045593,\n",
              "  0.8647576570510864,\n",
              "  0.8658992052078247,\n",
              "  0.8937886357307434,\n",
              "  0.8944686651229858,\n",
              "  0.8921659588813782,\n",
              "  0.8729889988899231,\n",
              "  0.8199529051780701,\n",
              "  0.8278185725212097,\n",
              "  0.9283955693244934,\n",
              "  0.8290512561798096,\n",
              "  0.9171461462974548,\n",
              "  0.9218881726264954,\n",
              "  0.8902361989021301,\n",
              "  0.8547905683517456,\n",
              "  0.88590008020401,\n",
              "  0.8555294275283813,\n",
              "  0.889495313167572,\n",
              "  0.9337366223335266,\n",
              "  0.84432452917099,\n",
              "  0.9220344424247742,\n",
              "  0.8460639715194702,\n",
              "  0.8828084468841553,\n",
              "  0.86326664686203,\n",
              "  0.8468302488327026,\n",
              "  0.8709104657173157,\n",
              "  0.8572841882705688,\n",
              "  0.8421095609664917,\n",
              "  0.8674495816230774,\n",
              "  0.871419370174408,\n",
              "  0.9284408688545227,\n",
              "  0.8750985264778137,\n",
              "  0.9150387644767761,\n",
              "  0.8903017044067383,\n",
              "  0.8358753323554993,\n",
              "  0.8559756875038147,\n",
              "  0.8639923334121704,\n",
              "  0.9003338813781738,\n",
              "  0.9258055090904236,\n",
              "  0.8523774743080139,\n",
              "  0.8396411538124084,\n",
              "  0.8852356672286987,\n",
              "  0.9142404198646545,\n",
              "  0.8764700293540955,\n",
              "  0.8569430112838745,\n",
              "  0.8708186149597168,\n",
              "  0.8294815421104431,\n",
              "  0.8364578485488892,\n",
              "  0.9010406732559204,\n",
              "  0.8575789332389832,\n",
              "  0.8637979030609131,\n",
              "  0.8651005029678345,\n",
              "  0.8903733491897583,\n",
              "  0.8624160885810852,\n",
              "  0.845723032951355,\n",
              "  0.8457383513450623,\n",
              "  0.9025279879570007,\n",
              "  0.8559635877609253,\n",
              "  0.7991849780082703,\n",
              "  0.8062843084335327,\n",
              "  0.8551157712936401,\n",
              "  0.8648976683616638,\n",
              "  0.8750820755958557,\n",
              "  0.8121282458305359,\n",
              "  0.8928303122520447,\n",
              "  0.8657290935516357,\n",
              "  0.8344605565071106,\n",
              "  0.8969209790229797,\n",
              "  0.8625044822692871,\n",
              "  0.8188038468360901,\n",
              "  0.8732861876487732,\n",
              "  0.8965138792991638,\n",
              "  0.8662086725234985,\n",
              "  0.8553617000579834,\n",
              "  0.851795494556427,\n",
              "  0.8884763717651367,\n",
              "  0.8180979490280151,\n",
              "  0.8509753346443176,\n",
              "  0.8336783647537231,\n",
              "  0.859778106212616,\n",
              "  0.901886522769928,\n",
              "  0.9226476550102234,\n",
              "  0.8919268250465393,\n",
              "  0.9128559827804565,\n",
              "  0.8349540829658508,\n",
              "  0.8777042031288147,\n",
              "  0.8638682961463928,\n",
              "  0.8810777068138123,\n",
              "  0.9071855545043945,\n",
              "  0.9162400960922241,\n",
              "  0.850219190120697,\n",
              "  0.8542436957359314,\n",
              "  0.7881606817245483,\n",
              "  0.8524154424667358,\n",
              "  0.8276323080062866,\n",
              "  0.8606610298156738,\n",
              "  0.8612768054008484,\n",
              "  0.8541475534439087,\n",
              "  0.8225287199020386,\n",
              "  0.8569623231887817,\n",
              "  0.8797662854194641,\n",
              "  0.8148449063301086,\n",
              "  0.8560612201690674,\n",
              "  0.88050377368927,\n",
              "  0.8709874153137207,\n",
              "  0.8959211111068726,\n",
              "  0.8238281011581421,\n",
              "  0.8629240393638611,\n",
              "  0.8451287746429443,\n",
              "  0.8513363003730774,\n",
              "  0.8722826838493347,\n",
              "  0.8672159314155579,\n",
              "  0.8187463283538818,\n",
              "  0.8873799443244934,\n",
              "  0.8033511638641357,\n",
              "  0.8046619296073914,\n",
              "  0.8163331151008606,\n",
              "  0.8712525963783264,\n",
              "  0.8319963216781616,\n",
              "  0.8673056364059448,\n",
              "  0.8447524905204773,\n",
              "  0.8804264068603516,\n",
              "  0.9100376963615417,\n",
              "  0.8083356618881226,\n",
              "  0.8314765691757202,\n",
              "  0.8078765869140625,\n",
              "  0.8376309871673584,\n",
              "  0.8150578737258911,\n",
              "  0.8270658254623413,\n",
              "  0.8547546863555908,\n",
              "  0.8493740558624268,\n",
              "  0.9246271252632141,\n",
              "  0.8391825556755066,\n",
              "  0.8882341384887695,\n",
              "  0.8621246218681335,\n",
              "  0.8827701210975647,\n",
              "  0.8322221636772156,\n",
              "  0.8077326416969299,\n",
              "  0.8514488935470581,\n",
              "  0.8247529864311218,\n",
              "  0.8512462377548218,\n",
              "  0.8610556125640869,\n",
              "  0.810404360294342,\n",
              "  0.8749523758888245,\n",
              "  0.885133683681488,\n",
              "  0.8626407384872437,\n",
              "  0.7851857542991638,\n",
              "  0.8103930354118347,\n",
              "  0.8716050386428833,\n",
              "  0.8228552341461182,\n",
              "  0.8275284171104431,\n",
              "  0.8716737031936646,\n",
              "  0.8230850100517273,\n",
              "  0.8615466356277466,\n",
              "  0.8451725244522095,\n",
              "  0.8433330655097961,\n",
              "  0.8741318583488464,\n",
              "  0.8576121926307678,\n",
              "  0.8752158284187317,\n",
              "  0.8613287210464478,\n",
              "  0.803589403629303,\n",
              "  0.8578228950500488,\n",
              "  0.8388155698776245,\n",
              "  0.80536288022995,\n",
              "  0.8535230755805969,\n",
              "  0.775789737701416,\n",
              "  0.8376632928848267,\n",
              "  0.8204218745231628,\n",
              "  0.813336968421936,\n",
              "  0.7953331470489502,\n",
              "  0.8782143592834473,\n",
              "  0.8447921872138977,\n",
              "  0.8332487344741821,\n",
              "  0.7943664789199829,\n",
              "  0.8151714205741882,\n",
              "  0.795076847076416,\n",
              "  0.7923968434333801,\n",
              "  0.8706982135772705,\n",
              "  0.848518967628479,\n",
              "  0.827961802482605,\n",
              "  0.8888048529624939,\n",
              "  0.8211221694946289,\n",
              "  0.8118313550949097,\n",
              "  0.8540525436401367,\n",
              "  0.8534429669380188,\n",
              "  0.8443963527679443,\n",
              "  0.8311441540718079,\n",
              "  0.8086379170417786,\n",
              "  0.8385471105575562,\n",
              "  0.8497646450996399,\n",
              "  0.8068532347679138,\n",
              "  0.837206244468689,\n",
              "  0.840753972530365],\n",
              " 'acc': [0.22525596618652344,\n",
              "  0.27474403381347656,\n",
              "  0.266211599111557,\n",
              "  0.2696245610713959,\n",
              "  0.30546075105667114,\n",
              "  0.319112628698349,\n",
              "  0.31740614771842957,\n",
              "  0.3310580253601074,\n",
              "  0.35836178064346313,\n",
              "  0.3890784978866577,\n",
              "  0.3498293459415436,\n",
              "  0.3924914598464966,\n",
              "  0.38566553592681885,\n",
              "  0.36348122358322144,\n",
              "  0.3668941855430603,\n",
              "  0.38566553592681885,\n",
              "  0.4390625059604645,\n",
              "  0.4112628102302551,\n",
              "  0.3968749940395355,\n",
              "  0.3873720169067383,\n",
              "  0.4505119323730469,\n",
              "  0.4112628102302551,\n",
              "  0.42150169610977173,\n",
              "  0.40273037552833557,\n",
              "  0.3993174135684967,\n",
              "  0.42150169610977173,\n",
              "  0.4163822531700134,\n",
              "  0.4351535737514496,\n",
              "  0.40102389454841614,\n",
              "  0.38054606318473816,\n",
              "  0.4197952151298523,\n",
              "  0.38566553592681885,\n",
              "  0.4351535737514496,\n",
              "  0.4515624940395355,\n",
              "  0.4266211688518524,\n",
              "  0.46587032079696655,\n",
              "  0.40784981846809387,\n",
              "  0.4163822531700134,\n",
              "  0.4112628102302551,\n",
              "  0.43856656551361084,\n",
              "  0.4546875059604645,\n",
              "  0.4266211688518524,\n",
              "  0.457337886095047,\n",
              "  0.41808873414993286,\n",
              "  0.436860054731369,\n",
              "  0.4095562994480133,\n",
              "  0.4203124940395355,\n",
              "  0.4163822531700134,\n",
              "  0.41808873414993286,\n",
              "  0.4692832827568054,\n",
              "  0.45392492413520813,\n",
              "  0.4419795274734497,\n",
              "  0.4468750059604645,\n",
              "  0.45563140511512756,\n",
              "  0.44062501192092896,\n",
              "  0.4266211688518524,\n",
              "  0.45392492413520813,\n",
              "  0.4317406117916107,\n",
              "  0.4197952151298523,\n",
              "  0.43856656551361084,\n",
              "  0.447098970413208,\n",
              "  0.4112628102302551,\n",
              "  0.447098970413208,\n",
              "  0.4402730464935303,\n",
              "  0.447098970413208,\n",
              "  0.43856656551361084,\n",
              "  0.44368600845336914,\n",
              "  0.457337886095047,\n",
              "  0.467576801776886,\n",
              "  0.49146756529808044,\n",
              "  0.45904436707496643,\n",
              "  0.46075084805488586,\n",
              "  0.45392492413520813,\n",
              "  0.4744027256965637,\n",
              "  0.4625000059604645,\n",
              "  0.4726962447166443,\n",
              "  0.4624573290348053,\n",
              "  0.4846416413784027,\n",
              "  0.4624573290348053,\n",
              "  0.4726962447166443,\n",
              "  0.4692832827568054,\n",
              "  0.46075084805488586,\n",
              "  0.414675772190094,\n",
              "  0.4402730464935303,\n",
              "  0.4829351603984833,\n",
              "  0.45904436707496643,\n",
              "  0.43344709277153015,\n",
              "  0.46416381001472473,\n",
              "  0.457337886095047,\n",
              "  0.44368600845336914,\n",
              "  0.4726962447166443,\n",
              "  0.45904436707496643,\n",
              "  0.457337886095047,\n",
              "  0.43437498807907104,\n",
              "  0.4419795274734497,\n",
              "  0.4880546033382416,\n",
              "  0.46075084805488586,\n",
              "  0.4505119323730469,\n",
              "  0.47098976373672485,\n",
              "  0.4624573290348053,\n",
              "  0.4880546033382416,\n",
              "  0.47098976373672485,\n",
              "  0.4744027256965637,\n",
              "  0.457337886095047,\n",
              "  0.4726962447166443,\n",
              "  0.4692832827568054,\n",
              "  0.45563140511512756,\n",
              "  0.4692832827568054,\n",
              "  0.48122867941856384,\n",
              "  0.4300341308116913,\n",
              "  0.49829351902008057,\n",
              "  0.4778156876564026,\n",
              "  0.47098976373672485,\n",
              "  0.48634812235832214,\n",
              "  0.4846416413784027,\n",
              "  0.47098976373672485,\n",
              "  0.49317407608032227,\n",
              "  0.49146756529808044,\n",
              "  0.49146756529808044,\n",
              "  0.4795221984386444,\n",
              "  0.46587032079696655,\n",
              "  0.4778156876564026,\n",
              "  0.4522184431552887,\n",
              "  0.46075084805488586,\n",
              "  0.4692832827568054,\n",
              "  0.4781250059604645,\n",
              "  0.4744027256965637,\n",
              "  0.46075084805488586,\n",
              "  0.4726962447166443,\n",
              "  0.4795221984386444,\n",
              "  0.4846416413784027,\n",
              "  0.47610920667648315,\n",
              "  0.4744027256965637,\n",
              "  0.5017064809799194,\n",
              "  0.46075084805488586,\n",
              "  0.4948805570602417,\n",
              "  0.49829351902008057,\n",
              "  0.4859375059604645,\n",
              "  0.4829351603984833,\n",
              "  0.46416381001472473,\n",
              "  0.4948805570602417,\n",
              "  0.5034129619598389,\n",
              "  0.49829351902008057,\n",
              "  0.5238907933235168,\n",
              "  0.47098976373672485,\n",
              "  0.48122867941856384,\n",
              "  0.48634812235832214,\n",
              "  0.4948805570602417,\n",
              "  0.47098976373672485,\n",
              "  0.47610920667648315,\n",
              "  0.4829351603984833,\n",
              "  0.48122867941856384,\n",
              "  0.49658703804016113,\n",
              "  0.48124998807907104,\n",
              "  0.48122867941856384,\n",
              "  0.48634812235832214,\n",
              "  0.4921875,\n",
              "  0.4829351603984833,\n",
              "  0.49658703804016113,\n",
              "  0.5093749761581421,\n",
              "  0.5017064809799194,\n",
              "  0.48906248807907104,\n",
              "  0.5153583884239197,\n",
              "  0.5,\n",
              "  0.5153583884239197,\n",
              "  0.511945366859436,\n",
              "  0.5,\n",
              "  0.5153583884239197,\n",
              "  0.4880546033382416,\n",
              "  0.5221843123435974,\n",
              "  0.5238907933235168,\n",
              "  0.4692832827568054,\n",
              "  0.5051194429397583,\n",
              "  0.4505119323730469,\n",
              "  0.5238907933235168,\n",
              "  0.47098976373672485,\n",
              "  0.49829351902008057,\n",
              "  0.5136518478393555,\n",
              "  0.4880546033382416,\n",
              "  0.5102388858795166,\n",
              "  0.4859375059604645,\n",
              "  0.5,\n",
              "  0.5187713503837585,\n",
              "  0.5093749761581421,\n",
              "  0.4829351603984833,\n",
              "  0.5255972743034363,\n",
              "  0.4692832827568054,\n",
              "  0.5051194429397583,\n",
              "  0.5170648694038391,\n",
              "  0.489761084318161,\n",
              "  0.46562498807907104,\n",
              "  0.4948805570602417,\n",
              "  0.5102388858795166,\n",
              "  0.4984374940395355,\n",
              "  0.5068259239196777,\n",
              "  0.532423198223114,\n",
              "  0.5511945486068726,\n",
              "  0.4880546033382416,\n",
              "  0.5170648694038391,\n",
              "  0.5051194429397583,\n",
              "  0.5153583884239197,\n",
              "  0.5273037552833557,\n",
              "  0.5255972743034363,\n",
              "  0.5273037552833557,\n",
              "  0.510937511920929,\n",
              "  0.5153583884239197,\n",
              "  0.49146756529808044,\n",
              "  0.520477831363678,\n",
              "  0.5443686246871948,\n",
              "  0.49146756529808044,\n",
              "  0.5078125,\n",
              "  0.47343748807907104,\n",
              "  0.48634812235832214,\n",
              "  0.5153583884239197,\n",
              "  0.5136518478393555,\n",
              "  0.5017064809799194,\n",
              "  0.49829351902008057,\n",
              "  0.5068259239196777,\n",
              "  0.5017064809799194,\n",
              "  0.489761084318161,\n",
              "  0.5085324048995972,\n",
              "  0.5034129619598389,\n",
              "  0.5443686246871948,\n",
              "  0.49317407608032227,\n",
              "  0.515625,\n",
              "  0.5085324048995972,\n",
              "  0.5273037552833557,\n",
              "  0.5255972743034363,\n",
              "  0.5102388858795166,\n",
              "  0.5307167172431946,\n",
              "  0.5273037552833557,\n",
              "  0.49317407608032227,\n",
              "  0.5307167172431946,\n",
              "  0.5234375,\n",
              "  0.5784983038902283,\n",
              "  0.5102388858795166,\n",
              "  0.4829351603984833,\n",
              "  0.5238907933235168,\n",
              "  0.46416381001472473,\n",
              "  0.5494880676269531,\n",
              "  0.5221843123435974,\n",
              "  0.5170648694038391,\n",
              "  0.5341296792030334,\n",
              "  0.5221843123435974,\n",
              "  0.534375011920929,\n",
              "  0.5085324048995972,\n",
              "  0.5614334344863892,\n",
              "  0.5034129619598389,\n",
              "  0.5068259239196777,\n",
              "  0.5273037552833557,\n",
              "  0.528124988079071,\n",
              "  0.552901029586792,\n",
              "  0.5136518478393555,\n",
              "  0.5358361601829529,\n",
              "  0.5341296792030334,\n",
              "  0.5341296792030334,\n",
              "  0.5358361601829529,\n",
              "  0.5426621437072754,\n",
              "  0.5341296792030334,\n",
              "  0.5358361601829529,\n",
              "  0.5563139915466309,\n",
              "  0.5273037552833557,\n",
              "  0.5017064809799194,\n",
              "  0.5296875238418579,\n",
              "  0.564846396446228,\n",
              "  0.526562511920929,\n",
              "  0.5307167172431946,\n",
              "  0.5136518478393555,\n",
              "  0.5477815866470337,\n",
              "  0.5068259239196777,\n",
              "  0.5341296792030334,\n",
              "  0.5494880676269531,\n",
              "  0.5170648694038391,\n",
              "  0.5375426411628723,\n",
              "  0.49146756529808044,\n",
              "  0.5443686246871948,\n",
              "  0.5068259239196777,\n",
              "  0.5341296792030334,\n",
              "  0.5375426411628723,\n",
              "  0.520477831363678,\n",
              "  0.5221843123435974,\n",
              "  0.5563139915466309,\n",
              "  0.49829351902008057,\n",
              "  0.5102388858795166,\n",
              "  0.49658703804016113,\n",
              "  0.520477831363678,\n",
              "  0.5017064809799194,\n",
              "  0.5221843123435974,\n",
              "  0.5307167172431946,\n",
              "  0.45904436707496643,\n",
              "  0.5614334344863892,\n",
              "  0.5255972743034363,\n",
              "  0.520477831363678,\n",
              "  0.5426621437072754,\n",
              "  0.532423198223114,\n",
              "  0.526562511920929,\n",
              "  0.5358361601829529,\n",
              "  0.5665528774261475,\n",
              "  0.5614334344863892,\n",
              "  0.5392491221427917,\n",
              "  0.5187713503837585,\n",
              "  0.5392491221427917,\n",
              "  0.5170648694038391,\n",
              "  0.5494880676269531,\n",
              "  0.5580204725265503,\n",
              "  0.5392491221427917,\n",
              "  0.5341296792030334,\n",
              "  0.4880546033382416,\n",
              "  0.5255972743034363,\n",
              "  0.5477815866470337,\n",
              "  0.5460751056671143,\n",
              "  0.4906249940395355,\n",
              "  0.5392491221427917,\n",
              "  0.5477815866470337,\n",
              "  0.5221843123435974,\n",
              "  0.5221843123435974,\n",
              "  0.5443686246871948,\n",
              "  0.5443686246871948,\n",
              "  0.532423198223114,\n",
              "  0.5511945486068726,\n",
              "  0.5443686246871948,\n",
              "  0.5597269535064697,\n",
              "  0.5221843123435974,\n",
              "  0.5563139915466309,\n",
              "  0.5580204725265503,\n",
              "  0.49829351902008057,\n",
              "  0.5375426411628723,\n",
              "  0.5409556031227112,\n",
              "  0.5392491221427917,\n",
              "  0.5409556031227112,\n",
              "  0.5443686246871948,\n",
              "  0.5341296792030334,\n",
              "  0.5767918229103088,\n",
              "  0.5887371897697449,\n",
              "  0.5358361601829529,\n",
              "  0.532423198223114,\n",
              "  0.5238907933235168,\n",
              "  0.5597269535064697,\n",
              "  0.5426621437072754,\n",
              "  0.520477831363678,\n",
              "  0.5015624761581421,\n",
              "  0.5409556031227112,\n",
              "  0.5784983038902283,\n",
              "  0.5631399154663086,\n",
              "  0.559374988079071,\n",
              "  0.5580204725265503,\n",
              "  0.5273037552833557,\n",
              "  0.564846396446228,\n",
              "  0.5631399154663086,\n",
              "  0.5511945486068726,\n",
              "  0.5767918229103088,\n",
              "  0.5358361601829529,\n",
              "  0.552901029586792,\n",
              "  0.5784983038902283,\n",
              "  0.5406249761581421,\n",
              "  0.5358361601829529,\n",
              "  0.5682593584060669,\n",
              "  0.5716723799705505,\n",
              "  0.5358361601829529,\n",
              "  0.5580204725265503,\n",
              "  0.5580204725265503,\n",
              "  0.5443686246871948,\n",
              "  0.5665528774261475,\n",
              "  0.5392491221427917,\n",
              "  0.5341296792030334,\n",
              "  0.5682593584060669,\n",
              "  0.5328124761581421,\n",
              "  0.5341296792030334,\n",
              "  0.5392491221427917,\n",
              "  0.546875,\n",
              "  0.5460751056671143,\n",
              "  0.5699658989906311,\n",
              "  0.57337886095047,\n",
              "  0.5460751056671143,\n",
              "  0.5819112658500671,\n",
              "  0.5426621437072754,\n",
              "  0.5392491221427917,\n",
              "  0.5546075105667114,\n",
              "  0.5187713503837585,\n",
              "  0.5546075105667114,\n",
              "  0.5750853419303894,\n",
              "  0.5390625,\n",
              "  0.5255972743034363,\n",
              "  0.5836177468299866,\n",
              "  0.5443686246871948,\n",
              "  0.552901029586792,\n",
              "  0.5409556031227112,\n",
              "  0.5563139915466309,\n",
              "  0.5955631136894226,\n",
              "  0.5375426411628723,\n",
              "  0.5784983038902283,\n",
              "  0.5409556031227112,\n",
              "  0.564846396446228,\n",
              "  0.5460751056671143,\n",
              "  0.5563139915466309,\n",
              "  0.5460751056671143,\n",
              "  0.542187511920929,\n",
              "  0.5443686246871948,\n",
              "  0.552901029586792,\n",
              "  0.564846396446228,\n",
              "  0.61774742603302,\n",
              "  0.5477815866470337,\n",
              "  0.5716723799705505,\n",
              "  0.5238907933235168,\n",
              "  0.5682593584060669,\n",
              "  0.5802047848701477,\n",
              "  0.57337886095047,\n",
              "  0.5494880676269531,\n",
              "  0.5494880676269531,\n",
              "  0.5665528774261475,\n",
              "  0.5409556031227112,\n",
              "  0.5443686246871948,\n",
              "  0.57337886095047,\n",
              "  0.6006826162338257,\n",
              "  0.5870307087898254,\n",
              "  0.5409556031227112,\n",
              "  0.5375426411628723,\n",
              "  0.5819112658500671,\n",
              "  0.5631399154663086,\n",
              "  0.5750853419303894,\n",
              "  0.5580204725265503,\n",
              "  0.5426621437072754,\n",
              "  0.5870307087898254,\n",
              "  0.552901029586792,\n",
              "  0.57337886095047,\n",
              "  0.5221843123435974,\n",
              "  0.5699658989906311,\n",
              "  0.5665528774261475,\n",
              "  0.57337886095047,\n",
              "  0.5665528774261475,\n",
              "  0.5665528774261475,\n",
              "  0.5836177468299866,\n",
              "  0.564846396446228,\n",
              "  0.5874999761581421,\n",
              "  0.5921501517295837,\n",
              "  0.5221843123435974,\n",
              "  0.5597269535064697,\n",
              "  0.5614334344863892,\n",
              "  0.5494880676269531,\n",
              "  0.5546075105667114,\n",
              "  0.5597269535064697,\n",
              "  0.5494880676269531,\n",
              "  0.5802047848701477,\n",
              "  0.5665528774261475,\n",
              "  0.5750853419303894,\n",
              "  0.550000011920929,\n",
              "  0.53125,\n",
              "  0.5802047848701477,\n",
              "  0.5921875238418579,\n",
              "  0.564846396446228,\n",
              "  0.5563139915466309,\n",
              "  0.5836177468299866,\n",
              "  0.5443686246871948,\n",
              "  0.5699658989906311,\n",
              "  0.5341296792030334,\n",
              "  0.564846396446228,\n",
              "  0.5921501517295837,\n",
              "  0.552901029586792,\n",
              "  0.5819112658500671,\n",
              "  0.5614334344863892,\n",
              "  0.564846396446228,\n",
              "  0.5796874761581421,\n",
              "  0.57337886095047,\n",
              "  0.5819112658500671,\n",
              "  0.5614334344863892,\n",
              "  0.5716723799705505,\n",
              "  0.5938566327095032,\n",
              "  0.5580204725265503,\n",
              "  0.5802047848701477,\n",
              "  0.5699658989906311,\n",
              "  0.5767918229103088,\n",
              "  0.5819112658500671,\n",
              "  0.5767918229103088,\n",
              "  0.5819112658500671,\n",
              "  0.5665528774261475,\n",
              "  0.6015625,\n",
              "  0.5904436707496643,\n",
              "  0.5597269535064697,\n",
              "  0.5796874761581421,\n",
              "  0.5563139915466309,\n",
              "  0.5580204725265503,\n",
              "  0.5767918229103088,\n",
              "  0.5819112658500671,\n",
              "  0.5580204725265503,\n",
              "  0.5836177468299866,\n",
              "  0.5874999761581421,\n",
              "  0.5716723799705505,\n",
              "  0.582812488079071,\n",
              "  0.5802047848701477,\n",
              "  0.5699658989906311,\n",
              "  0.5682593584060669,\n",
              "  0.5819112658500671,\n",
              "  0.5375426411628723,\n",
              "  0.5870307087898254,\n",
              "  0.5341296792030334,\n",
              "  0.5631399154663086,\n",
              "  0.5904436707496643,\n",
              "  0.5614334344863892,\n",
              "  0.5614334344863892,\n",
              "  0.5531250238418579,\n",
              "  0.5887371897697449,\n",
              "  0.5904436707496643,\n",
              "  0.5682593584060669,\n",
              "  0.5221843123435974,\n",
              "  0.5750853419303894,\n",
              "  0.5716723799705505,\n",
              "  0.5836177468299866,\n",
              "  0.5358361601829529,\n",
              "  0.5870307087898254,\n",
              "  0.5511945486068726,\n",
              "  0.5614334344863892,\n",
              "  0.5699658989906311,\n",
              "  0.5665528774261475,\n",
              "  0.5938566327095032,\n",
              "  0.6092150211334229,\n",
              "  0.57337886095047,\n",
              "  0.5904436707496643,\n",
              "  0.5870307087898254,\n",
              "  0.5716723799705505,\n",
              "  0.5716723799705505,\n",
              "  0.5699658989906311,\n",
              "  0.5938566327095032,\n",
              "  0.5625,\n",
              "  0.5597269535064697,\n",
              "  0.5921501517295837,\n",
              "  0.5631399154663086,\n",
              "  0.5767918229103088,\n",
              "  0.5836177468299866,\n",
              "  0.5546075105667114,\n",
              "  0.5784983038902283,\n",
              "  0.6279863715171814,\n",
              "  0.5443686246871948,\n",
              "  0.5972696542739868,\n",
              "  0.5699658989906311,\n",
              "  0.5716723799705505,\n",
              "  0.5665528774261475,\n",
              "  0.5699658989906311,\n",
              "  0.5631399154663086,\n",
              "  0.5802047848701477,\n",
              "  0.5699658989906311,\n",
              "  0.5716723799705505,\n",
              "  0.5699658989906311,\n",
              "  0.585324227809906,\n",
              "  0.5665528774261475,\n",
              "  0.5750853419303894,\n",
              "  0.5699658989906311,\n",
              "  0.5716723799705505,\n",
              "  0.5921501517295837,\n",
              "  0.5870307087898254,\n",
              "  0.5921501517295837,\n",
              "  0.6092150211334229,\n",
              "  0.5859375,\n",
              "  0.5631399154663086,\n",
              "  0.5767918229103088,\n",
              "  0.5443686246871948,\n",
              "  0.5597269535064697,\n",
              "  0.5802047848701477,\n",
              "  0.5819112658500671,\n",
              "  0.5870307087898254,\n",
              "  0.5718749761581421,\n",
              "  0.5784983038902283,\n",
              "  0.5784983038902283,\n",
              "  0.6450511813163757,\n",
              "  0.585324227809906,\n",
              "  0.5750853419303894,\n",
              "  0.5989761352539062,\n",
              "  0.6006826162338257,\n",
              "  0.5767918229103088,\n",
              "  0.5750853419303894,\n",
              "  0.5874999761581421,\n",
              "  0.5836177468299866,\n",
              "  0.5802047848701477,\n",
              "  0.57337886095047,\n",
              "  0.5921501517295837,\n",
              "  0.5870307087898254,\n",
              "  0.5938566327095032,\n",
              "  0.6006826162338257,\n",
              "  0.5699658989906311,\n",
              "  0.5874999761581421,\n",
              "  0.6023890972137451,\n",
              "  0.574999988079071,\n",
              "  0.5972696542739868,\n",
              "  0.5802047848701477,\n",
              "  0.5870307087898254,\n",
              "  0.5699658989906311,\n",
              "  0.5938566327095032,\n",
              "  0.6031249761581421,\n",
              "  0.5870307087898254,\n",
              "  0.582812488079071,\n",
              "  0.5938566327095032,\n",
              "  0.585324227809906,\n",
              "  0.5836177468299866,\n",
              "  0.5836177468299866,\n",
              "  0.5699658989906311,\n",
              "  0.5887371897697449,\n",
              "  0.5631399154663086,\n",
              "  0.5750853419303894,\n",
              "  0.585324227809906,\n",
              "  0.6040955781936646,\n",
              "  0.6211603879928589,\n",
              "  0.6126279830932617,\n",
              "  0.6143344640731812,\n",
              "  0.5750853419303894,\n",
              "  0.5921501517295837,\n",
              "  0.5802047848701477,\n",
              "  0.585324227809906,\n",
              "  0.6023890972137451,\n",
              "  0.57337886095047,\n",
              "  0.6006826162338257,\n",
              "  0.5836177468299866,\n",
              "  0.5836177468299866,\n",
              "  0.5819112658500671,\n",
              "  0.5870307087898254,\n",
              "  0.6228668689727783,\n",
              "  0.5767918229103088,\n",
              "  0.6092150211334229,\n",
              "  0.5887371897697449,\n",
              "  0.585324227809906,\n",
              "  0.5767918229103088,\n",
              "  0.5989761352539062,\n",
              "  0.585324227809906,\n",
              "  0.6092150211334229,\n",
              "  0.5802047848701477,\n",
              "  0.5716723799705505,\n",
              "  0.5955631136894226,\n",
              "  0.61774742603302,\n",
              "  0.5750853419303894,\n",
              "  0.6023890972137451,\n",
              "  0.5802047848701477,\n",
              "  0.5904436707496643,\n",
              "  0.605802059173584,\n",
              "  0.5938566327095032,\n",
              "  0.5843750238418579,\n",
              "  0.5921501517295837,\n",
              "  0.5784983038902283,\n",
              "  0.5750853419303894,\n",
              "  0.5784983038902283,\n",
              "  0.5784983038902283,\n",
              "  0.6143344640731812,\n",
              "  0.5870307087898254,\n",
              "  0.6331058144569397,\n",
              "  0.6075085401535034,\n",
              "  0.5887371897697449,\n",
              "  0.5836177468299866,\n",
              "  0.6092150211334229,\n",
              "  0.5784983038902283,\n",
              "  0.6023890972137451,\n",
              "  0.6006826162338257,\n",
              "  0.6075085401535034,\n",
              "  0.6228668689727783,\n",
              "  0.5750853419303894,\n",
              "  0.6023890972137451,\n",
              "  0.6126279830932617,\n",
              "  0.59375,\n",
              "  0.6279863715171814,\n",
              "  0.6075085401535034,\n",
              "  0.598437488079071,\n",
              "  0.5802047848701477,\n",
              "  0.5750853419303894,\n",
              "  0.5989761352539062,\n",
              "  0.6313993334770203,\n",
              "  0.605802059173584,\n",
              "  0.5989761352539062,\n",
              "  0.6075085401535034,\n",
              "  0.6399317383766174,\n",
              "  0.6160409450531006,\n",
              "  0.6211603879928589,\n",
              "  0.5989761352539062,\n",
              "  0.6109215021133423,\n",
              "  0.5904436707496643,\n",
              "  0.5921501517295837,\n",
              "  0.5870307087898254,\n",
              "  0.6433447003364563,\n",
              "  0.5904436707496643,\n",
              "  0.612500011920929,\n",
              "  0.5972696542739868,\n",
              "  0.5767918229103088,\n",
              "  0.6433447003364563,\n",
              "  0.6143344640731812,\n",
              "  0.5972696542739868,\n",
              "  0.5989761352539062,\n",
              "  0.5989761352539062,\n",
              "  0.6006826162338257,\n",
              "  0.61774742603302,\n",
              "  0.6211603879928589,\n",
              "  0.6279863715171814,\n",
              "  0.61774742603302,\n",
              "  0.605802059173584,\n",
              "  0.6194539070129395,\n",
              "  0.6109215021133423,\n",
              "  0.6126279830932617,\n",
              "  0.5819112658500671,\n",
              "  0.5887371897697449,\n",
              "  0.6228668689727783,\n",
              "  0.5989761352539062,\n",
              "  0.6040955781936646,\n",
              "  0.61774742603302,\n",
              "  0.6092150211334229,\n",
              "  0.5938566327095032,\n",
              "  0.6399317383766174,\n",
              "  0.5870307087898254,\n",
              "  0.605802059173584,\n",
              "  0.6031249761581421,\n",
              "  0.57337886095047,\n",
              "  0.626279890537262,\n",
              "  0.6006826162338257,\n",
              "  0.5972696542739868,\n",
              "  0.6234375238418579,\n",
              "  0.585324227809906,\n",
              "  0.6040955781936646,\n",
              "  0.6109215021133423,\n",
              "  0.6143344640731812,\n",
              "  0.6194539070129395,\n",
              "  0.6194539070129395,\n",
              "  0.6075085401535034,\n",
              "  0.61774742603302,\n",
              "  0.6160409450531006,\n",
              "  0.5802047848701477,\n",
              "  0.6143344640731812,\n",
              "  0.5870307087898254,\n",
              "  0.605802059173584,\n",
              "  0.6143344640731812,\n",
              "  0.614062488079071,\n",
              "  0.6143344640731812,\n",
              "  0.6331058144569397,\n",
              "  0.581250011920929,\n",
              "  0.6109215021133423,\n",
              "  0.6075085401535034,\n",
              "  0.5819112658500671,\n",
              "  0.6023890972137451,\n",
              "  0.5989761352539062,\n",
              "  0.626279890537262,\n",
              "  0.6228668689727783,\n",
              "  0.5904436707496643,\n",
              "  0.6006826162338257,\n",
              "  0.605802059173584,\n",
              "  0.626279890537262,\n",
              "  0.605802059173584,\n",
              "  0.6126279830932617,\n",
              "  0.5972696542739868,\n",
              "  0.6279863715171814,\n",
              "  0.6143344640731812,\n",
              "  0.5972696542739868,\n",
              "  0.6092150211334229,\n",
              "  0.6023890972137451,\n",
              "  0.6023890972137451,\n",
              "  0.6006826162338257,\n",
              "  0.6194539070129395,\n",
              "  0.5989761352539062,\n",
              "  0.626279890537262,\n",
              "  0.6092150211334229,\n",
              "  0.5870307087898254,\n",
              "  0.5870307087898254,\n",
              "  0.6313993334770203,\n",
              "  0.6348122954368591,\n",
              "  0.626279890537262,\n",
              "  0.6075085401535034,\n",
              "  0.6535836458206177,\n",
              "  0.5904436707496643,\n",
              "  0.6006826162338257,\n",
              "  0.626279890537262,\n",
              "  0.5972696542739868,\n",
              "  0.606249988079071,\n",
              "  0.6143344640731812,\n",
              "  0.6126279830932617,\n",
              "  0.5784983038902283,\n",
              "  0.6143344640731812,\n",
              "  0.5784983038902283,\n",
              "  0.6194539070129395,\n",
              "  0.6109215021133423,\n",
              "  0.605802059173584,\n",
              "  0.605802059173584,\n",
              "  0.605802059173584,\n",
              "  0.5887371897697449,\n",
              "  0.6075085401535034,\n",
              "  0.5955631136894226,\n",
              "  0.6348122954368591,\n",
              "  0.6109215021133423,\n",
              "  0.5938566327095032,\n",
              "  0.6023890972137451,\n",
              "  0.5682593584060669,\n",
              "  0.5989761352539062,\n",
              "  0.6279863715171814,\n",
              "  0.596875011920929,\n",
              "  0.61774742603302,\n",
              "  0.626279890537262,\n",
              "  0.6040955781936646,\n",
              "  0.626279890537262,\n",
              "  0.6228668689727783,\n",
              "  0.6194539070129395,\n",
              "  0.6211603879928589,\n",
              "  0.5802047848701477,\n",
              "  0.5887371897697449,\n",
              "  0.6194539070129395,\n",
              "  0.6313993334770203,\n",
              "  0.6365187764167786,\n",
              "  0.5955631136894226,\n",
              "  0.6343749761581421,\n",
              "  0.6040955781936646,\n",
              "  0.5784983038902283,\n",
              "  0.6023890972137451,\n",
              "  0.6006826162338257,\n",
              "  0.5750853419303894,\n",
              "  0.6433447003364563,\n",
              "  0.6023890972137451,\n",
              "  0.638225257396698,\n",
              "  0.5938566327095032,\n",
              "  0.6075085401535034,\n",
              "  0.6416382193565369,\n",
              "  0.6313993334770203,\n",
              "  0.6279863715171814,\n",
              "  0.5802047848701477,\n",
              "  0.6109215021133423,\n",
              "  0.6331058144569397,\n",
              "  0.6399317383766174,\n",
              "  0.6296928524971008,\n",
              "  0.6228668689727783,\n",
              "  0.6484641432762146,\n",
              "  0.5682593584060669,\n",
              "  0.6015625,\n",
              "  0.6040955781936646,\n",
              "  0.6228668689727783,\n",
              "  0.6194539070129395,\n",
              "  0.6228668689727783,\n",
              "  0.5904436707496643,\n",
              "  0.6006826162338257,\n",
              "  0.626279890537262,\n",
              "  0.6023890972137451,\n",
              "  0.6265624761581421,\n",
              "  0.5938566327095032,\n",
              "  0.6279863715171814,\n",
              "  0.6331058144569397,\n",
              "  0.605802059173584,\n",
              "  0.638225257396698,\n",
              "  0.5938566327095032,\n",
              "  0.6023890972137451,\n",
              "  0.5938566327095032,\n",
              "  0.5921501517295837,\n",
              "  0.6331058144569397,\n",
              "  0.5938566327095032,\n",
              "  0.6194539070129395,\n",
              "  0.6421874761581421,\n",
              "  0.6092150211334229,\n",
              "  0.6160409450531006,\n",
              "  0.5938566327095032,\n",
              "  0.5989761352539062,\n",
              "  0.6194539070129395,\n",
              "  0.6313993334770203,\n",
              "  0.6126279830932617,\n",
              "  0.5921501517295837,\n",
              "  0.5989761352539062,\n",
              "  0.6194539070129395,\n",
              "  0.6006826162338257,\n",
              "  0.6313993334770203,\n",
              "  0.6092150211334229,\n",
              "  0.6040955781936646,\n",
              "  0.629687488079071,\n",
              "  0.626279890537262,\n",
              "  0.6092150211334229,\n",
              "  0.6075085401535034,\n",
              "  0.606249988079071,\n",
              "  0.6092150211334229,\n",
              "  0.6348122954368591,\n",
              "  0.605802059173584,\n",
              "  0.6328125,\n",
              "  0.6433447003364563,\n",
              "  0.6279863715171814,\n",
              "  0.6160409450531006,\n",
              "  0.6126279830932617,\n",
              "  0.6228668689727783,\n",
              "  0.6399317383766174,\n",
              "  0.6040955781936646,\n",
              "  0.61774742603302,\n",
              "  0.6365187764167786,\n",
              "  0.6245733499526978,\n",
              "  0.6143344640731812,\n",
              "  0.6365187764167786,\n",
              "  0.6092150211334229,\n",
              "  0.605802059173584,\n",
              "  0.6109215021133423,\n",
              "  0.658703088760376,\n",
              "  0.6312500238418579,\n",
              "  0.6348122954368591,\n",
              "  0.6279863715171814,\n",
              "  0.61774742603302,\n",
              "  0.6245733499526978,\n",
              "  0.6040955781936646,\n",
              "  0.605802059173584,\n",
              "  0.5955631136894226,\n",
              "  0.626279890537262,\n",
              "  0.5870307087898254,\n",
              "  0.6416382193565369,\n",
              "  0.61774742603302,\n",
              "  0.6228668689727783,\n",
              "  0.6006826162338257,\n",
              "  0.5955631136894226,\n",
              "  0.6092150211334229,\n",
              "  0.61774742603302,\n",
              "  0.6313993334770203,\n",
              "  0.6535836458206177,\n",
              "  0.6160409450531006,\n",
              "  0.6348122954368591,\n",
              "  0.605802059173584,\n",
              "  0.6126279830932617,\n",
              "  0.6279863715171814,\n",
              "  0.6569966077804565,\n",
              "  0.6228668689727783,\n",
              "  0.6092150211334229,\n",
              "  0.6484641432762146,\n",
              "  0.6245733499526978,\n",
              "  0.626279890537262,\n",
              "  0.6109374761581421,\n",
              "  0.6006826162338257,\n",
              "  0.6484641432762146,\n",
              "  0.638225257396698,\n",
              "  0.6228668689727783,\n",
              "  0.6365187764167786,\n",
              "  0.6279863715171814,\n",
              "  0.6331058144569397,\n",
              "  0.6569966077804565,\n",
              "  0.6313993334770203,\n",
              "  0.6348122954368591,\n",
              "  0.650170624256134,\n",
              "  0.6578124761581421,\n",
              "  0.6143344640731812,\n",
              "  0.6313993334770203,\n",
              "  0.629687488079071,\n",
              "  0.6313993334770203,\n",
              "  0.626279890537262,\n",
              "  0.6006826162338257,\n",
              "  0.6348122954368591,\n",
              "  0.6234375238418579,\n",
              "  0.6348122954368591,\n",
              "  0.650170624256134,\n",
              "  0.6348122954368591,\n",
              "  0.6109215021133423,\n",
              "  0.6023890972137451,\n",
              "  0.6416382193565369,\n",
              "  0.5784983038902283,\n",
              "  0.6467576622962952,\n",
              "  0.6075085401535034,\n",
              "  0.6211603879928589,\n",
              "  0.5972696542739868,\n",
              "  0.6328125,\n",
              "  0.626279890537262,\n",
              "  0.6228668689727783,\n",
              "  0.6416382193565369,\n",
              "  0.6194539070129395,\n",
              "  0.61774742603302,\n",
              "  0.6655290126800537,\n",
              "  0.5938566327095032,\n",
              "  0.626279890537262,\n",
              "  0.6484641432762146,\n",
              "  0.6655290126800537,\n",
              "  0.6569966077804565,\n",
              "  0.6245733499526978,\n",
              "  0.6365187764167786,\n",
              "  0.6518771052360535,\n",
              "  0.6331058144569397,\n",
              "  0.6484641432762146,\n",
              "  0.6194539070129395,\n",
              "  0.638225257396698,\n",
              "  0.61774742603302,\n",
              "  0.6126279830932617,\n",
              "  0.6194539070129395,\n",
              "  0.6023890972137451,\n",
              "  0.6092150211334229,\n",
              "  0.626279890537262,\n",
              "  0.6006826162338257,\n",
              "  0.6313993334770203,\n",
              "  0.640625,\n",
              "  0.637499988079071,\n",
              "  0.6518771052360535,\n",
              "  0.6484641432762146,\n",
              "  0.6450511813163757,\n",
              "  0.6194539070129395,\n",
              "  0.6638225317001343,\n",
              "  0.6279863715171814,\n",
              "  0.6331058144569397,\n",
              "  0.6365187764167786,\n",
              "  0.6328125,\n",
              "  0.650170624256134,\n",
              "  0.6467576622962952,\n",
              "  0.6569966077804565,\n",
              "  0.6211603879928589,\n",
              "  0.6160409450531006,\n",
              "  0.6365187764167786,\n",
              "  0.6194539070129395,\n",
              "  0.6552901268005371,\n",
              "  0.6569966077804565,\n",
              "  0.6211603879928589,\n",
              "  0.6228668689727783,\n",
              "  0.61774742603302,\n",
              "  0.643750011920929,\n",
              "  0.6757678985595703,\n",
              "  0.6348122954368591,\n",
              "  0.6399317383766174,\n",
              "  0.6467576622962952,\n",
              "  0.6365187764167786,\n",
              "  0.6211603879928589],\n",
              " 'val_loss': [1.9417868852615356,\n",
              "  1.9535062313079834,\n",
              "  1.7739849090576172,\n",
              "  1.630119800567627,\n",
              "  1.5994980335235596,\n",
              "  1.5923130512237549,\n",
              "  1.5722367763519287,\n",
              "  1.5318138599395752,\n",
              "  1.4951090812683105,\n",
              "  1.5132832527160645,\n",
              "  1.5144509077072144,\n",
              "  1.4666242599487305,\n",
              "  1.4735944271087646,\n",
              "  1.3919391632080078,\n",
              "  1.5426567792892456,\n",
              "  1.5357221364974976,\n",
              "  1.5333560705184937,\n",
              "  1.5263702869415283,\n",
              "  1.528568148612976,\n",
              "  1.5202646255493164,\n",
              "  1.5333614349365234,\n",
              "  1.5440399646759033,\n",
              "  1.4535892009735107,\n",
              "  1.4625790119171143,\n",
              "  1.4374499320983887,\n",
              "  1.4991381168365479,\n",
              "  1.4872276782989502,\n",
              "  1.5066357851028442,\n",
              "  1.5266669988632202,\n",
              "  1.548001766204834,\n",
              "  1.5290175676345825,\n",
              "  1.475325584411621,\n",
              "  1.5435913801193237,\n",
              "  1.4402172565460205,\n",
              "  1.4613395929336548,\n",
              "  1.5044077634811401,\n",
              "  1.4421446323394775,\n",
              "  1.4548875093460083,\n",
              "  1.5113749504089355,\n",
              "  1.4457261562347412,\n",
              "  1.532942295074463,\n",
              "  1.5333435535430908,\n",
              "  1.3830474615097046,\n",
              "  1.3982913494110107,\n",
              "  1.450129747390747,\n",
              "  1.4413642883300781,\n",
              "  1.542128086090088,\n",
              "  1.5078237056732178,\n",
              "  1.5442290306091309,\n",
              "  1.454193115234375,\n",
              "  1.5040535926818848,\n",
              "  1.495265007019043,\n",
              "  1.5046547651290894,\n",
              "  1.548874855041504,\n",
              "  1.5284497737884521,\n",
              "  1.5023925304412842,\n",
              "  1.577437400817871,\n",
              "  1.5801341533660889,\n",
              "  1.541565179824829,\n",
              "  1.4872455596923828,\n",
              "  1.473503828048706,\n",
              "  1.4331929683685303,\n",
              "  1.575114369392395,\n",
              "  1.5257611274719238,\n",
              "  1.5665159225463867,\n",
              "  1.5805668830871582,\n",
              "  1.5734045505523682,\n",
              "  1.4658981561660767,\n",
              "  1.434678554534912,\n",
              "  1.524033784866333,\n",
              "  1.5340349674224854,\n",
              "  1.5750789642333984,\n",
              "  1.5231385231018066,\n",
              "  1.5850732326507568,\n",
              "  1.5017918348312378,\n",
              "  1.4620403051376343,\n",
              "  1.527631402015686,\n",
              "  1.47645902633667,\n",
              "  1.6135406494140625,\n",
              "  1.515082836151123,\n",
              "  1.492437720298767,\n",
              "  1.5801316499710083,\n",
              "  1.517334222793579,\n",
              "  1.5667378902435303,\n",
              "  1.5510917901992798,\n",
              "  1.5638340711593628,\n",
              "  1.6114916801452637,\n",
              "  1.4577062129974365,\n",
              "  1.534099817276001,\n",
              "  1.5055779218673706,\n",
              "  1.5387934446334839,\n",
              "  1.5203545093536377,\n",
              "  1.5643234252929688,\n",
              "  1.4248710870742798,\n",
              "  1.497765064239502,\n",
              "  1.5582562685012817,\n",
              "  1.5565462112426758,\n",
              "  1.6352359056472778,\n",
              "  1.6276687383651733,\n",
              "  1.5927876234054565,\n",
              "  1.6045081615447998,\n",
              "  1.501468539237976,\n",
              "  1.5595054626464844,\n",
              "  1.5680488348007202,\n",
              "  1.5324523448944092,\n",
              "  1.5058739185333252,\n",
              "  1.618678331375122,\n",
              "  1.5146615505218506,\n",
              "  1.4346396923065186,\n",
              "  1.478722333908081,\n",
              "  1.5261616706848145,\n",
              "  1.5754268169403076,\n",
              "  1.4747064113616943,\n",
              "  1.5186305046081543,\n",
              "  1.5765568017959595,\n",
              "  1.4980183839797974,\n",
              "  1.5319020748138428,\n",
              "  1.535144567489624,\n",
              "  1.5932819843292236,\n",
              "  1.5785229206085205,\n",
              "  1.511372685432434,\n",
              "  1.5441336631774902,\n",
              "  1.547413945198059,\n",
              "  1.6078983545303345,\n",
              "  1.470735788345337,\n",
              "  1.5546386241912842,\n",
              "  1.535989761352539,\n",
              "  1.6107386350631714,\n",
              "  1.5259974002838135,\n",
              "  1.507044792175293,\n",
              "  1.51318359375,\n",
              "  1.6118428707122803,\n",
              "  1.5499218702316284,\n",
              "  1.5507863759994507,\n",
              "  1.631453037261963,\n",
              "  1.5410594940185547,\n",
              "  1.56318998336792,\n",
              "  1.5133661031723022,\n",
              "  1.606707215309143,\n",
              "  1.5696390867233276,\n",
              "  1.6019866466522217,\n",
              "  1.5185573101043701,\n",
              "  1.5730912685394287,\n",
              "  1.5511326789855957,\n",
              "  1.5054378509521484,\n",
              "  1.5497366189956665,\n",
              "  1.5607783794403076,\n",
              "  1.5555684566497803,\n",
              "  1.523690938949585,\n",
              "  1.5655055046081543,\n",
              "  1.5518826246261597,\n",
              "  1.4676504135131836,\n",
              "  1.5314204692840576,\n",
              "  1.501926064491272,\n",
              "  1.435002088546753,\n",
              "  1.5109519958496094,\n",
              "  1.524263858795166,\n",
              "  1.5690876245498657,\n",
              "  1.5244717597961426,\n",
              "  1.4814295768737793,\n",
              "  1.5824394226074219,\n",
              "  1.5452854633331299,\n",
              "  1.5459604263305664,\n",
              "  1.590804100036621,\n",
              "  1.616729974746704,\n",
              "  1.5751092433929443,\n",
              "  1.5675691366195679,\n",
              "  1.5929126739501953,\n",
              "  1.607229232788086,\n",
              "  1.6583317518234253,\n",
              "  1.4936983585357666,\n",
              "  1.508162498474121,\n",
              "  1.5804345607757568,\n",
              "  1.4232478141784668,\n",
              "  1.5489320755004883,\n",
              "  1.5154157876968384,\n",
              "  1.5780274868011475,\n",
              "  1.5409331321716309,\n",
              "  1.579753041267395,\n",
              "  1.5618705749511719,\n",
              "  1.6105009317398071,\n",
              "  1.5035680532455444,\n",
              "  1.6477410793304443,\n",
              "  1.6476454734802246,\n",
              "  1.6286842823028564,\n",
              "  1.570326805114746,\n",
              "  1.6419076919555664,\n",
              "  1.5330793857574463,\n",
              "  1.6519135236740112,\n",
              "  1.5513218641281128,\n",
              "  1.5703601837158203,\n",
              "  1.5785623788833618,\n",
              "  1.5693507194519043,\n",
              "  1.6093378067016602,\n",
              "  1.5946273803710938,\n",
              "  1.558567762374878,\n",
              "  1.5872111320495605,\n",
              "  1.570190668106079,\n",
              "  1.4719011783599854,\n",
              "  1.6044846773147583,\n",
              "  1.609149694442749,\n",
              "  1.5962822437286377,\n",
              "  1.5681614875793457,\n",
              "  1.5216799974441528,\n",
              "  1.6076326370239258,\n",
              "  1.5835752487182617,\n",
              "  1.5787030458450317,\n",
              "  1.5742385387420654,\n",
              "  1.5304818153381348,\n",
              "  1.5172631740570068,\n",
              "  1.5723447799682617,\n",
              "  1.5672460794448853,\n",
              "  1.6143019199371338,\n",
              "  1.6620123386383057,\n",
              "  1.5490000247955322,\n",
              "  1.6331926584243774,\n",
              "  1.5612143278121948,\n",
              "  1.5645527839660645,\n",
              "  1.5436159372329712,\n",
              "  1.6146409511566162,\n",
              "  1.6381752490997314,\n",
              "  1.6053122282028198,\n",
              "  1.623600959777832,\n",
              "  1.6011099815368652,\n",
              "  1.5280743837356567,\n",
              "  1.5982695817947388,\n",
              "  1.6261301040649414,\n",
              "  1.6290638446807861,\n",
              "  1.6141889095306396,\n",
              "  1.610262155532837,\n",
              "  1.6380292177200317,\n",
              "  1.5648139715194702,\n",
              "  1.5730687379837036,\n",
              "  1.5971341133117676,\n",
              "  1.5899629592895508,\n",
              "  1.5451838970184326,\n",
              "  1.544627070426941,\n",
              "  1.581686019897461,\n",
              "  1.4942213296890259,\n",
              "  1.4835255146026611,\n",
              "  1.625539779663086,\n",
              "  1.6118042469024658,\n",
              "  1.6219313144683838,\n",
              "  1.6283835172653198,\n",
              "  1.6226840019226074,\n",
              "  1.607347011566162,\n",
              "  1.6196398735046387,\n",
              "  1.5008220672607422,\n",
              "  1.499107837677002,\n",
              "  1.5490411520004272,\n",
              "  1.5104115009307861,\n",
              "  1.5891616344451904,\n",
              "  1.618281602859497,\n",
              "  1.5909125804901123,\n",
              "  1.595714807510376,\n",
              "  1.6052794456481934,\n",
              "  1.547668218612671,\n",
              "  1.5863264799118042,\n",
              "  1.6398131847381592,\n",
              "  1.624617576599121,\n",
              "  1.595231056213379,\n",
              "  1.6062825918197632,\n",
              "  1.5755383968353271,\n",
              "  1.5248826742172241,\n",
              "  1.566828727722168,\n",
              "  1.6217705011367798,\n",
              "  1.560654878616333,\n",
              "  1.542174220085144,\n",
              "  1.6266992092132568,\n",
              "  1.5037555694580078,\n",
              "  1.6694972515106201,\n",
              "  1.5770307779312134,\n",
              "  1.608216643333435,\n",
              "  1.6087615489959717,\n",
              "  1.562014102935791,\n",
              "  1.601746916770935,\n",
              "  1.5948004722595215,\n",
              "  1.5858958959579468,\n",
              "  1.5137195587158203,\n",
              "  1.569452166557312,\n",
              "  1.676626205444336,\n",
              "  1.6553525924682617,\n",
              "  1.6118767261505127,\n",
              "  1.567962408065796,\n",
              "  1.6154183149337769,\n",
              "  1.558241367340088,\n",
              "  1.5234113931655884,\n",
              "  1.5550782680511475,\n",
              "  1.6562902927398682,\n",
              "  1.58201003074646,\n",
              "  1.5688116550445557,\n",
              "  1.584852933883667,\n",
              "  1.611465334892273,\n",
              "  1.6033347845077515,\n",
              "  1.571162223815918,\n",
              "  1.6206891536712646,\n",
              "  1.6864380836486816,\n",
              "  1.5673553943634033,\n",
              "  1.5223581790924072,\n",
              "  1.5704567432403564,\n",
              "  1.605600118637085,\n",
              "  1.5621261596679688,\n",
              "  1.5957462787628174,\n",
              "  1.5880391597747803,\n",
              "  1.5317511558532715,\n",
              "  1.5700023174285889,\n",
              "  1.5580158233642578,\n",
              "  1.6242778301239014,\n",
              "  1.675064206123352,\n",
              "  1.5437980890274048,\n",
              "  1.5336368083953857,\n",
              "  1.5531256198883057,\n",
              "  1.5913169384002686,\n",
              "  1.5924408435821533,\n",
              "  1.5769630670547485,\n",
              "  1.5508967638015747,\n",
              "  1.6198079586029053,\n",
              "  1.6011451482772827,\n",
              "  1.6110332012176514,\n",
              "  1.5817854404449463,\n",
              "  1.571035385131836,\n",
              "  1.4470492601394653,\n",
              "  1.510221004486084,\n",
              "  1.5678901672363281,\n",
              "  1.6257998943328857,\n",
              "  1.55034601688385,\n",
              "  1.615844488143921,\n",
              "  1.6737873554229736,\n",
              "  1.6536738872528076,\n",
              "  1.5602350234985352,\n",
              "  1.5740643739700317,\n",
              "  1.5382357835769653,\n",
              "  1.5031276941299438,\n",
              "  1.555182933807373,\n",
              "  1.5191583633422852,\n",
              "  1.5937542915344238,\n",
              "  1.6497981548309326,\n",
              "  1.5214883089065552,\n",
              "  1.5355048179626465,\n",
              "  1.6104985475540161,\n",
              "  1.6540992259979248,\n",
              "  1.5858579874038696,\n",
              "  1.5119366645812988,\n",
              "  1.598629117012024,\n",
              "  1.5716609954833984,\n",
              "  1.5162034034729004,\n",
              "  1.6506911516189575,\n",
              "  1.5591652393341064,\n",
              "  1.6113793849945068,\n",
              "  1.5966265201568604,\n",
              "  1.5669019222259521,\n",
              "  1.6265865564346313,\n",
              "  1.6246025562286377,\n",
              "  1.6786565780639648,\n",
              "  1.6476080417633057,\n",
              "  1.6033579111099243,\n",
              "  1.5630478858947754,\n",
              "  1.6172791719436646,\n",
              "  1.6859352588653564,\n",
              "  1.598639726638794,\n",
              "  1.6538114547729492,\n",
              "  1.6190884113311768,\n",
              "  1.6483633518218994,\n",
              "  1.677189826965332,\n",
              "  1.637209177017212,\n",
              "  1.554600477218628,\n",
              "  1.5754752159118652,\n",
              "  1.634920358657837,\n",
              "  1.6017221212387085,\n",
              "  1.5416011810302734,\n",
              "  1.4689844846725464,\n",
              "  1.5942387580871582,\n",
              "  1.5285152196884155,\n",
              "  1.5592896938323975,\n",
              "  1.6510818004608154,\n",
              "  1.5384180545806885,\n",
              "  1.519619107246399,\n",
              "  1.6052542924880981,\n",
              "  1.5715115070343018,\n",
              "  1.6358623504638672,\n",
              "  1.5338737964630127,\n",
              "  1.5353447198867798,\n",
              "  1.5278346538543701,\n",
              "  1.71011221408844,\n",
              "  1.4998129606246948,\n",
              "  1.6013110876083374,\n",
              "  1.636370301246643,\n",
              "  1.5873594284057617,\n",
              "  1.5592005252838135,\n",
              "  1.6077845096588135,\n",
              "  1.519252061843872,\n",
              "  1.6589431762695312,\n",
              "  1.5777065753936768,\n",
              "  1.5726633071899414,\n",
              "  1.543092966079712,\n",
              "  1.633436918258667,\n",
              "  1.5322785377502441,\n",
              "  1.5283215045928955,\n",
              "  1.6524860858917236,\n",
              "  1.6236001253128052,\n",
              "  1.6123446226119995,\n",
              "  1.5597460269927979,\n",
              "  1.5441687107086182,\n",
              "  1.648977518081665,\n",
              "  1.5988762378692627,\n",
              "  1.6090528964996338,\n",
              "  1.5509650707244873,\n",
              "  1.6975456476211548,\n",
              "  1.610904335975647,\n",
              "  1.5927472114562988,\n",
              "  1.6054935455322266,\n",
              "  1.6013877391815186,\n",
              "  1.5639138221740723,\n",
              "  1.529831886291504,\n",
              "  1.578080177307129,\n",
              "  1.56103515625,\n",
              "  1.5739703178405762,\n",
              "  1.5862386226654053,\n",
              "  1.6279104948043823,\n",
              "  1.6414568424224854,\n",
              "  1.6330890655517578,\n",
              "  1.6301082372665405,\n",
              "  1.5856196880340576,\n",
              "  1.5886659622192383,\n",
              "  1.5554583072662354,\n",
              "  1.6139769554138184,\n",
              "  1.607844352722168,\n",
              "  1.6627538204193115,\n",
              "  1.6204838752746582,\n",
              "  1.6112754344940186,\n",
              "  1.5896528959274292,\n",
              "  1.670556902885437,\n",
              "  1.570590853691101,\n",
              "  1.5279815196990967,\n",
              "  1.5589039325714111,\n",
              "  1.5728678703308105,\n",
              "  1.5228756666183472,\n",
              "  1.5548824071884155,\n",
              "  1.5862950086593628,\n",
              "  1.5049524307250977,\n",
              "  1.540661334991455,\n",
              "  1.5146766901016235,\n",
              "  1.5550665855407715,\n",
              "  1.5441920757293701,\n",
              "  1.5626442432403564,\n",
              "  1.6075646877288818,\n",
              "  1.5588550567626953,\n",
              "  1.5380605459213257,\n",
              "  1.5021295547485352,\n",
              "  1.624147891998291,\n",
              "  1.590993046760559,\n",
              "  1.6327329874038696,\n",
              "  1.5422077178955078,\n",
              "  1.5715765953063965,\n",
              "  1.5828349590301514,\n",
              "  1.523643970489502,\n",
              "  1.5351202487945557,\n",
              "  1.5906939506530762,\n",
              "  1.5708439350128174,\n",
              "  1.531799077987671,\n",
              "  1.4796863794326782,\n",
              "  1.627015233039856,\n",
              "  1.6497597694396973,\n",
              "  1.5493013858795166,\n",
              "  1.5252485275268555,\n",
              "  1.6360427141189575,\n",
              "  1.6086485385894775,\n",
              "  1.5695236921310425,\n",
              "  1.5861378908157349,\n",
              "  1.669783115386963,\n",
              "  1.5920017957687378,\n",
              "  1.5508885383605957,\n",
              "  1.5539371967315674,\n",
              "  1.4941037893295288,\n",
              "  1.5105680227279663,\n",
              "  1.4967410564422607,\n",
              "  1.6488116979599,\n",
              "  1.5835065841674805,\n",
              "  1.5179170370101929,\n",
              "  1.6275427341461182,\n",
              "  1.636213779449463,\n",
              "  1.7036750316619873,\n",
              "  1.6642258167266846,\n",
              "  1.510565996170044,\n",
              "  1.5413422584533691,\n",
              "  1.5987085103988647,\n",
              "  1.536150574684143,\n",
              "  1.621656894683838,\n",
              "  1.5607759952545166,\n",
              "  1.626215934753418,\n",
              "  1.6115227937698364,\n",
              "  1.6545445919036865,\n",
              "  1.6059224605560303,\n",
              "  1.5830076932907104,\n",
              "  1.5157679319381714,\n",
              "  1.5850677490234375,\n",
              "  1.5633877515792847,\n",
              "  1.6064085960388184,\n",
              "  1.6681702136993408,\n",
              "  1.5055115222930908,\n",
              "  1.6299309730529785,\n",
              "  1.502081274986267,\n",
              "  1.4813388586044312,\n",
              "  1.5410653352737427,\n",
              "  1.5096546411514282,\n",
              "  1.5154117345809937,\n",
              "  1.5369367599487305,\n",
              "  1.5453503131866455,\n",
              "  1.6284880638122559,\n",
              "  1.572567105293274,\n",
              "  1.6395552158355713,\n",
              "  1.546230673789978,\n",
              "  1.5491628646850586,\n",
              "  1.6385869979858398,\n",
              "  1.6187658309936523,\n",
              "  1.599747657775879,\n",
              "  1.6559960842132568,\n",
              "  1.593680500984192,\n",
              "  1.5669065713882446,\n",
              "  1.5637431144714355,\n",
              "  1.5533008575439453,\n",
              "  1.5893670320510864,\n",
              "  1.6047377586364746,\n",
              "  1.5679469108581543,\n",
              "  1.5790410041809082,\n",
              "  1.548365592956543,\n",
              "  1.6873927116394043,\n",
              "  1.5495514869689941,\n",
              "  1.6485871076583862,\n",
              "  1.6221301555633545,\n",
              "  1.6595458984375,\n",
              "  1.607432246208191,\n",
              "  1.5586795806884766,\n",
              "  1.6329197883605957,\n",
              "  1.571886658668518,\n",
              "  1.6062474250793457,\n",
              "  1.4961556196212769,\n",
              "  1.5295634269714355,\n",
              "  1.6000310182571411,\n",
              "  1.5656492710113525,\n",
              "  1.6003128290176392,\n",
              "  1.5965967178344727,\n",
              "  1.615686058998108,\n",
              "  1.568036437034607,\n",
              "  1.5714530944824219,\n",
              "  1.6003193855285645,\n",
              "  1.626889705657959,\n",
              "  1.5989339351654053,\n",
              "  1.6997146606445312,\n",
              "  1.56816565990448,\n",
              "  1.6113064289093018,\n",
              "  1.6804587841033936,\n",
              "  1.620621919631958,\n",
              "  1.5240654945373535,\n",
              "  1.5216989517211914,\n",
              "  1.5756256580352783,\n",
              "  1.5083460807800293,\n",
              "  1.5593169927597046,\n",
              "  1.5861711502075195,\n",
              "  1.6386749744415283,\n",
              "  1.5774145126342773,\n",
              "  1.6543477773666382,\n",
              "  1.5760818719863892,\n",
              "  1.5728871822357178,\n",
              "  1.6232635974884033,\n",
              "  1.5907955169677734,\n",
              "  1.6127665042877197,\n",
              "  1.5791456699371338,\n",
              "  1.5398423671722412,\n",
              "  1.5757890939712524,\n",
              "  1.61104154586792,\n",
              "  1.6053478717803955,\n",
              "  1.6173670291900635,\n",
              "  1.602557897567749,\n",
              "  1.5299744606018066,\n",
              "  1.5956952571868896,\n",
              "  1.5956244468688965,\n",
              "  1.594905138015747,\n",
              "  1.6150509119033813,\n",
              "  1.5276949405670166,\n",
              "  1.6592986583709717,\n",
              "  1.6407496929168701,\n",
              "  1.5694003105163574,\n",
              "  1.596062183380127,\n",
              "  1.5796144008636475,\n",
              "  1.7263069152832031,\n",
              "  1.6332707405090332,\n",
              "  1.629258155822754,\n",
              "  1.6387320756912231,\n",
              "  1.5679538249969482,\n",
              "  1.5955066680908203,\n",
              "  1.5848166942596436,\n",
              "  1.5846483707427979,\n",
              "  1.5899064540863037,\n",
              "  1.5848479270935059,\n",
              "  1.6634447574615479,\n",
              "  1.6179147958755493,\n",
              "  1.5179314613342285,\n",
              "  1.5657179355621338,\n",
              "  1.5473119020462036,\n",
              "  1.6253877878189087,\n",
              "  1.6798981428146362,\n",
              "  1.6227710247039795,\n",
              "  1.577019214630127,\n",
              "  1.6796867847442627,\n",
              "  1.5649645328521729,\n",
              "  1.5085420608520508,\n",
              "  1.6306118965148926,\n",
              "  1.5349476337432861,\n",
              "  1.5119011402130127,\n",
              "  1.5873795747756958,\n",
              "  1.586487054824829,\n",
              "  1.5471956729888916,\n",
              "  1.581037998199463,\n",
              "  1.5507948398590088,\n",
              "  1.5748132467269897,\n",
              "  1.53384268283844,\n",
              "  1.5971211194992065,\n",
              "  1.5557281970977783,\n",
              "  1.6157786846160889,\n",
              "  1.573706030845642,\n",
              "  1.625413417816162,\n",
              "  1.5314242839813232,\n",
              "  1.601987361907959,\n",
              "  1.6134400367736816,\n",
              "  1.5959711074829102,\n",
              "  1.5786592960357666,\n",
              "  1.654665231704712,\n",
              "  1.6104817390441895,\n",
              "  1.5585558414459229,\n",
              "  1.5920771360397339,\n",
              "  1.6399394273757935,\n",
              "  1.6435493230819702,\n",
              "  1.6343950033187866,\n",
              "  1.5790281295776367,\n",
              "  1.6190871000289917,\n",
              "  1.5811257362365723,\n",
              "  1.5663654804229736,\n",
              "  1.5452923774719238,\n",
              "  1.5312063694000244,\n",
              "  1.6494898796081543,\n",
              "  1.5936825275421143,\n",
              "  1.5444607734680176,\n",
              "  1.562657117843628,\n",
              "  1.5351107120513916,\n",
              "  1.4791436195373535,\n",
              "  1.543239951133728,\n",
              "  1.6236963272094727,\n",
              "  1.622602939605713,\n",
              "  1.5533838272094727,\n",
              "  1.5554569959640503,\n",
              "  1.567143201828003,\n",
              "  1.5892326831817627,\n",
              "  1.524376392364502,\n",
              "  1.595362901687622,\n",
              "  1.5375399589538574,\n",
              "  1.522348165512085,\n",
              "  1.5011487007141113,\n",
              "  1.5290675163269043,\n",
              "  1.5900657176971436,\n",
              "  1.6070443391799927,\n",
              "  1.5210895538330078,\n",
              "  1.5577595233917236,\n",
              "  1.5896846055984497,\n",
              "  1.6236382722854614,\n",
              "  1.5349785089492798,\n",
              "  1.576962947845459,\n",
              "  1.5801249742507935,\n",
              "  1.5679450035095215,\n",
              "  1.5914528369903564,\n",
              "  1.655369758605957,\n",
              "  1.5705195665359497,\n",
              "  1.6307812929153442,\n",
              "  1.595813274383545,\n",
              "  1.6246306896209717,\n",
              "  1.622710943222046,\n",
              "  1.6060858964920044,\n",
              "  1.6211687326431274,\n",
              "  1.5255794525146484,\n",
              "  1.561416506767273,\n",
              "  1.546482801437378,\n",
              "  1.6666138172149658,\n",
              "  1.6698088645935059,\n",
              "  1.6741950511932373,\n",
              "  1.6426421403884888,\n",
              "  1.6106274127960205,\n",
              "  1.6347099542617798,\n",
              "  1.5285272598266602,\n",
              "  1.5750396251678467,\n",
              "  1.5145375728607178,\n",
              "  1.5333528518676758,\n",
              "  1.5438151359558105,\n",
              "  1.5852711200714111,\n",
              "  1.5800013542175293,\n",
              "  1.6352916955947876,\n",
              "  1.515824317932129,\n",
              "  1.519085168838501,\n",
              "  1.6300500631332397,\n",
              "  1.6586203575134277,\n",
              "  1.625243902206421,\n",
              "  1.5702333450317383,\n",
              "  1.563890814781189,\n",
              "  1.5368508100509644,\n",
              "  1.5297223329544067,\n",
              "  1.580863118171692,\n",
              "  1.5823609828948975,\n",
              "  1.5124664306640625,\n",
              "  1.5510821342468262,\n",
              "  1.5710563659667969,\n",
              "  1.6091759204864502,\n",
              "  1.5835751295089722,\n",
              "  1.6841602325439453,\n",
              "  1.6211462020874023,\n",
              "  1.6775985956192017,\n",
              "  1.7190650701522827,\n",
              "  1.661498785018921,\n",
              "  1.6029176712036133,\n",
              "  1.6275922060012817,\n",
              "  1.618863821029663,\n",
              "  1.612633466720581,\n",
              "  1.673438549041748,\n",
              "  1.6575665473937988,\n",
              "  1.564635992050171,\n",
              "  1.6276803016662598,\n",
              "  1.5653091669082642,\n",
              "  1.5126004219055176,\n",
              "  1.5795278549194336,\n",
              "  1.5495344400405884,\n",
              "  1.584700584411621,\n",
              "  1.6619060039520264,\n",
              "  1.5285837650299072,\n",
              "  1.5782495737075806,\n",
              "  1.5242807865142822,\n",
              "  1.6396863460540771,\n",
              "  1.5715469121932983,\n",
              "  1.6010777950286865,\n",
              "  1.5183790922164917,\n",
              "  1.5435922145843506,\n",
              "  1.5639965534210205,\n",
              "  1.615907907485962,\n",
              "  1.5504504442214966,\n",
              "  1.6324894428253174,\n",
              "  1.6178897619247437,\n",
              "  1.6099406480789185,\n",
              "  1.5925827026367188,\n",
              "  1.5982341766357422,\n",
              "  1.5650105476379395,\n",
              "  1.5988818407058716,\n",
              "  1.544447898864746,\n",
              "  1.6222214698791504,\n",
              "  1.5525118112564087,\n",
              "  1.5710830688476562,\n",
              "  1.5647917985916138,\n",
              "  1.6371474266052246,\n",
              "  1.508298635482788,\n",
              "  1.4209972620010376,\n",
              "  1.6375911235809326,\n",
              "  1.6053543090820312,\n",
              "  1.599178433418274,\n",
              "  1.6070892810821533,\n",
              "  1.6391215324401855,\n",
              "  1.5912916660308838,\n",
              "  1.6839923858642578,\n",
              "  1.6702806949615479,\n",
              "  1.6052416563034058,\n",
              "  1.6088007688522339,\n",
              "  1.4779454469680786,\n",
              "  1.615415334701538,\n",
              "  1.6094753742218018,\n",
              "  1.6107169389724731,\n",
              "  1.583813190460205,\n",
              "  1.610581874847412,\n",
              "  1.5574431419372559,\n",
              "  1.6262876987457275,\n",
              "  1.579450011253357,\n",
              "  1.6265465021133423,\n",
              "  1.6444251537322998,\n",
              "  1.5941513776779175,\n",
              "  1.574225664138794,\n",
              "  1.5001835823059082,\n",
              "  1.637821078300476,\n",
              "  1.5546700954437256,\n",
              "  1.6320140361785889,\n",
              "  1.6194770336151123,\n",
              "  1.6379382610321045,\n",
              "  1.6428433656692505,\n",
              "  1.637223720550537,\n",
              "  1.6707093715667725,\n",
              "  1.6016557216644287,\n",
              "  1.5680071115493774,\n",
              "  1.4851038455963135,\n",
              "  1.5269689559936523,\n",
              "  1.5517346858978271,\n",
              "  1.5208799839019775,\n",
              "  1.521432638168335,\n",
              "  1.5281970500946045,\n",
              "  1.5571845769882202,\n",
              "  1.5859417915344238,\n",
              "  1.5908946990966797,\n",
              "  1.628833293914795,\n",
              "  1.6117608547210693,\n",
              "  1.5887491703033447,\n",
              "  1.5351808071136475,\n",
              "  1.6112427711486816,\n",
              "  1.597361445426941,\n",
              "  1.5089242458343506,\n",
              "  1.5072298049926758,\n",
              "  1.6182163953781128,\n",
              "  1.5775738954544067,\n",
              "  1.5551445484161377,\n",
              "  1.5087604522705078,\n",
              "  1.541219711303711,\n",
              "  1.553701400756836,\n",
              "  1.538080096244812,\n",
              "  1.54922616481781,\n",
              "  1.591825008392334,\n",
              "  1.5944236516952515,\n",
              "  1.6349977254867554,\n",
              "  1.6092958450317383,\n",
              "  1.5811727046966553,\n",
              "  1.5704460144042969,\n",
              "  1.535336971282959,\n",
              "  1.5954854488372803,\n",
              "  1.6076934337615967,\n",
              "  1.5809996128082275,\n",
              "  1.52535879611969,\n",
              "  1.5311707258224487,\n",
              "  1.5126785039901733,\n",
              "  1.5418099164962769,\n",
              "  1.57874596118927,\n",
              "  1.5433778762817383,\n",
              "  1.568091869354248,\n",
              "  1.5262442827224731,\n",
              "  1.507643699645996,\n",
              "  1.5812638998031616,\n",
              "  1.5936899185180664,\n",
              "  1.6099207401275635,\n",
              "  1.6275362968444824,\n",
              "  1.5158971548080444,\n",
              "  1.5618563890457153,\n",
              "  1.5394248962402344,\n",
              "  1.5427038669586182,\n",
              "  1.5259920358657837,\n",
              "  1.4861332178115845,\n",
              "  1.5636872053146362,\n",
              "  1.497023105621338,\n",
              "  1.5066766738891602,\n",
              "  1.553246021270752,\n",
              "  1.611051321029663,\n",
              "  1.5651416778564453,\n",
              "  1.4954496622085571,\n",
              "  1.5569039583206177,\n",
              "  1.5696804523468018,\n",
              "  1.5512593984603882,\n",
              "  1.5772626399993896,\n",
              "  1.5231568813323975,\n",
              "  1.5906214714050293,\n",
              "  1.5433411598205566,\n",
              "  1.5149747133255005,\n",
              "  1.4957916736602783,\n",
              "  1.5868453979492188,\n",
              "  1.5172948837280273,\n",
              "  1.5058634281158447,\n",
              "  1.5201250314712524,\n",
              "  1.5049073696136475,\n",
              "  1.471082091331482,\n",
              "  1.563227653503418,\n",
              "  1.5030767917633057,\n",
              "  1.537832498550415,\n",
              "  1.5702611207962036,\n",
              "  1.518160104751587,\n",
              "  1.4778776168823242,\n",
              "  1.4628069400787354,\n",
              "  1.5579787492752075,\n",
              "  1.5576000213623047,\n",
              "  1.5145918130874634,\n",
              "  1.5197834968566895,\n",
              "  1.55826997756958,\n",
              "  1.5912030935287476,\n",
              "  1.44765043258667,\n",
              "  1.5331217050552368,\n",
              "  1.5486464500427246,\n",
              "  1.5849220752716064,\n",
              "  1.5756757259368896,\n",
              "  1.5149911642074585,\n",
              "  1.597983956336975,\n",
              "  1.552980661392212,\n",
              "  1.4736449718475342,\n",
              "  1.4693859815597534,\n",
              "  1.5459049940109253,\n",
              "  1.5996880531311035,\n",
              "  1.530093789100647,\n",
              "  1.5526304244995117,\n",
              "  1.5514519214630127,\n",
              "  1.5454013347625732,\n",
              "  1.5799586772918701,\n",
              "  1.607153058052063,\n",
              "  1.6469073295593262,\n",
              "  1.550215482711792,\n",
              "  1.6024055480957031,\n",
              "  1.620692253112793,\n",
              "  1.6460793018341064,\n",
              "  1.5951263904571533,\n",
              "  1.521283507347107,\n",
              "  1.5512957572937012,\n",
              "  1.4894896745681763,\n",
              "  1.522749662399292,\n",
              "  1.5883567333221436,\n",
              "  1.5099893808364868,\n",
              "  1.5825870037078857,\n",
              "  1.5014734268188477,\n",
              "  1.563150405883789,\n",
              "  1.5183075666427612,\n",
              "  1.5885552167892456,\n",
              "  1.5938866138458252,\n",
              "  1.579722285270691,\n",
              "  1.617323398590088,\n",
              "  1.5646729469299316,\n",
              "  1.5244927406311035,\n",
              "  1.5832037925720215,\n",
              "  1.629774808883667,\n",
              "  1.5729751586914062,\n",
              "  1.5042898654937744,\n",
              "  1.5804773569107056,\n",
              "  1.5291588306427002,\n",
              "  1.560183048248291,\n",
              "  1.539884090423584,\n",
              "  1.5572682619094849,\n",
              "  1.5719627141952515,\n",
              "  1.5455321073532104,\n",
              "  1.4946069717407227,\n",
              "  1.609179973602295,\n",
              "  1.4826648235321045,\n",
              "  1.5005666017532349,\n",
              "  1.5112543106079102,\n",
              "  1.454499363899231,\n",
              "  1.4574508666992188,\n",
              "  1.5818665027618408,\n",
              "  1.5308237075805664,\n",
              "  1.5038061141967773,\n",
              "  1.486682415008545,\n",
              "  1.5467073917388916,\n",
              "  1.499617099761963,\n",
              "  1.5338125228881836,\n",
              "  1.5691416263580322,\n",
              "  1.5243154764175415,\n",
              "  1.5607385635375977,\n",
              "  1.5365784168243408,\n",
              "  1.5916035175323486,\n",
              "  1.5470428466796875,\n",
              "  1.5653547048568726,\n",
              "  1.5424238443374634,\n",
              "  1.5332216024398804,\n",
              "  1.5341782569885254,\n",
              "  1.5665003061294556,\n",
              "  1.6370232105255127,\n",
              "  1.5734219551086426,\n",
              "  1.5799601078033447,\n",
              "  1.5287384986877441,\n",
              "  1.5190187692642212,\n",
              "  1.4503880739212036,\n",
              "  1.579845905303955,\n",
              "  1.5364278554916382,\n",
              "  1.4874224662780762,\n",
              "  1.4972572326660156,\n",
              "  1.5491117238998413,\n",
              "  1.5309356451034546,\n",
              "  1.5334705114364624,\n",
              "  1.654750108718872,\n",
              "  1.6423382759094238,\n",
              "  1.6250560283660889,\n",
              "  1.5294222831726074,\n",
              "  1.5834369659423828,\n",
              "  1.5605645179748535,\n",
              "  1.5883177518844604,\n",
              "  1.6457632780075073,\n",
              "  1.6473917961120605,\n",
              "  1.6590864658355713,\n",
              "  1.4998180866241455,\n",
              "  1.5024220943450928,\n",
              "  1.6157463788986206,\n",
              "  1.6059577465057373,\n",
              "  1.6510155200958252,\n",
              "  1.577826738357544,\n",
              "  1.474810242652893,\n",
              "  1.5500047206878662,\n",
              "  1.5287775993347168,\n",
              "  1.5036065578460693,\n",
              "  1.5966672897338867,\n",
              "  1.5993671417236328,\n",
              "  1.5609172582626343,\n",
              "  1.5170496702194214,\n",
              "  1.5166175365447998,\n",
              "  1.543304443359375,\n",
              "  1.507436752319336,\n",
              "  1.6201666593551636,\n",
              "  1.5227246284484863,\n",
              "  1.5443140268325806,\n",
              "  1.5831494331359863,\n",
              "  1.5776309967041016],\n",
              " 'val_acc': [0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.125,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.125,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.140625,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.15625,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.25,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.21875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.15625,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.15625,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.15625,\n",
              "  0.1875,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.21875,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.171875,\n",
              "  0.1875,\n",
              "  0.203125,\n",
              "  0.21875,\n",
              "  0.171875,\n",
              "  0.1875]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "241c1f97-a9fb-4ae0-92d0-ba52f8db9d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "30a790c0-e2ec-4662-f1b4-13d324ec0d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e055d764-b9ee-4609-85f6-747555025dbd\", \"newdata_SEM1.h5\", 16615536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}