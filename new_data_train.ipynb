{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv3lVx83RFM8yAwQXDvUmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "f8c7da55-8959-494c-c911-001d77fa6a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "4a6fa26d-0c10-4d02-c7e4-99bc71da991a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "840  841  1-s2.0-S2095268622000210-main   \n",
              "841  842  1-s2.0-S2095268622000210-main   \n",
              "842  843  1-s2.0-S2095268622000210-main   \n",
              "843  844  1-s2.0-S2095268622000210-main   \n",
              "844  845  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "840  Integration of preparation of K, Na-embedded a...   \n",
              "841  Integration of preparation of K, Na-embedded a...   \n",
              "842  Integration of preparation of K, Na-embedded a...   \n",
              "843  Integration of preparation of K, Na-embedded a...   \n",
              "844  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "840  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "841  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "842  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "843  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "844  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture    detail  Class  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...  original  0-500   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom1  0-500   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom2  0-500   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom3  0-500   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...     zoom4  0-500   \n",
              "..                                                 ...       ...    ...   \n",
              "840  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom21  0-500   \n",
              "841  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom22  0-500   \n",
              "842  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom23  0-500   \n",
              "843  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom24  0-500   \n",
              "844  /content/drive/My Drive/new train/1-s2.0-S2095...    zoom25  0-500   \n",
              "\n",
              "        BET  Size(mico)  \n",
              "0    135.06           5  \n",
              "1    135.06          10  \n",
              "2    135.06          10  \n",
              "3    135.06          10  \n",
              "4    135.06          10  \n",
              "..      ...         ...  \n",
              "840  301.70          10  \n",
              "841  301.70          10  \n",
              "842  301.70          10  \n",
              "843  301.70          10  \n",
              "844  301.70          10  \n",
              "\n",
              "[845 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06bde244-e8a4-4993-8a70-319fc7263a7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>original</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>841</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>842</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>843</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>844</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>845</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>845 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06bde244-e8a4-4993-8a70-319fc7263a7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06bde244-e8a4-4993-8a70-319fc7263a7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06bde244-e8a4-4993-8a70-319fc7263a7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "070315c3-19de-47a9-8948-4639efe8c501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "bcf36f02-7a03-4c6a-9e17-fbb1a48d53d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "822dd7ef-472d-4c86-e371-59982f08959e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "1ad19726-d724-4917-d846-ebf7cc2e17bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(682,764)]\n",
        "train = df[df['No'].between(1,681)]\n",
        "test = df[df['No'].between(756,845)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "95dabf77-dfe6-44c2-cdbb-b45ff5d2a4ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 68\n",
            "total validation 2 images: 27\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 62\n",
            "total test 2 images: 6\n",
            "total test 3 images: 15 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 300  # จำนวนภาพ Train\n",
        "NUM_TEST = 20 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "5cbbe8ee-e933-4dcc-b451-765e67405265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)            (None, 75, 75, 32)   864         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 75, 75, 32)  128         ['conv2d_130[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_98 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_32 (Depthwise  (None, 75, 75, 32)  288         ['swish_98[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 75, 75, 32)  128         ['depthwise_conv2d_32[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_99 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_32 (Lambda)             (None, 1, 1, 32)     0           ['swish_99[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)            (None, 1, 1, 8)      264         ['lambda_32[0][0]']              \n",
            "                                                                                                  \n",
            " swish_100 (Swish)              (None, 1, 1, 8)      0           ['conv2d_131[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)            (None, 1, 1, 32)     288         ['swish_100[0][0]']              \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 1, 1, 32)     0           ['conv2d_132[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_32 (Multiply)         (None, 75, 75, 32)   0           ['activation_32[0][0]',          \n",
            "                                                                  'swish_99[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)            (None, 75, 75, 16)   512         ['multiply_32[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_100 (Batch  (None, 75, 75, 16)  64          ['conv2d_133[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)            (None, 75, 75, 96)   1536        ['batch_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_101 (Batch  (None, 75, 75, 96)  384         ['conv2d_134[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_101 (Swish)              (None, 75, 75, 96)   0           ['batch_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_33 (Depthwise  (None, 38, 38, 96)  864         ['swish_101[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_102 (Batch  (None, 38, 38, 96)  384         ['depthwise_conv2d_33[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_102 (Swish)              (None, 38, 38, 96)   0           ['batch_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " lambda_33 (Lambda)             (None, 1, 1, 96)     0           ['swish_102[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)            (None, 1, 1, 4)      388         ['lambda_33[0][0]']              \n",
            "                                                                                                  \n",
            " swish_103 (Swish)              (None, 1, 1, 4)      0           ['conv2d_135[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)            (None, 1, 1, 96)     480         ['swish_103[0][0]']              \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 1, 1, 96)     0           ['conv2d_136[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_33 (Multiply)         (None, 38, 38, 96)   0           ['activation_33[0][0]',          \n",
            "                                                                  'swish_102[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)            (None, 38, 38, 24)   2304        ['multiply_33[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_103 (Batch  (None, 38, 38, 24)  96          ['conv2d_137[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)            (None, 38, 38, 144)  3456        ['batch_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_104 (Batch  (None, 38, 38, 144)  576        ['conv2d_138[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_104 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_34 (Depthwise  (None, 38, 38, 144)  1296       ['swish_104[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_105 (Batch  (None, 38, 38, 144)  576        ['depthwise_conv2d_34[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_105 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " lambda_34 (Lambda)             (None, 1, 1, 144)    0           ['swish_105[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_34[0][0]']              \n",
            "                                                                                                  \n",
            " swish_106 (Swish)              (None, 1, 1, 6)      0           ['conv2d_139[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_106[0][0]']              \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 1, 1, 144)    0           ['conv2d_140[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_34 (Multiply)         (None, 38, 38, 144)  0           ['activation_34[0][0]',          \n",
            "                                                                  'swish_105[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)            (None, 38, 38, 24)   3456        ['multiply_34[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_106 (Batch  (None, 38, 38, 24)  96          ['conv2d_141[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_18 (DropConnect)  (None, 38, 38, 24)   0           ['batch_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 38, 38, 24)   0           ['drop_connect_18[0][0]',        \n",
            "                                                                  'batch_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)            (None, 38, 38, 144)  3456        ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 38, 38, 144)  576        ['conv2d_142[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_107 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_35 (Depthwise  (None, 19, 19, 144)  3600       ['swish_107[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 19, 19, 144)  576        ['depthwise_conv2d_35[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_108 (Swish)              (None, 19, 19, 144)  0           ['batch_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " lambda_35 (Lambda)             (None, 1, 1, 144)    0           ['swish_108[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_35[0][0]']              \n",
            "                                                                                                  \n",
            " swish_109 (Swish)              (None, 1, 1, 6)      0           ['conv2d_143[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_109[0][0]']              \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 1, 1, 144)    0           ['conv2d_144[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_35 (Multiply)         (None, 19, 19, 144)  0           ['activation_35[0][0]',          \n",
            "                                                                  'swish_108[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 19, 19, 40)   5760        ['multiply_35[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 19, 19, 40)  160         ['conv2d_145[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 19, 19, 240)  9600        ['batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 19, 19, 240)  960        ['conv2d_146[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_110 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_36 (Depthwise  (None, 19, 19, 240)  6000       ['swish_110[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 19, 19, 240)  960        ['depthwise_conv2d_36[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_111 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " lambda_36 (Lambda)             (None, 1, 1, 240)    0           ['swish_111[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_36[0][0]']              \n",
            "                                                                                                  \n",
            " swish_112 (Swish)              (None, 1, 1, 10)     0           ['conv2d_147[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_112[0][0]']              \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 1, 1, 240)    0           ['conv2d_148[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_36 (Multiply)         (None, 19, 19, 240)  0           ['activation_36[0][0]',          \n",
            "                                                                  'swish_111[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 19, 19, 40)   9600        ['multiply_36[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_112 (Batch  (None, 19, 19, 40)  160         ['conv2d_149[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_19 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_19[0][0]',        \n",
            "                                                                  'batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 19, 19, 240)  9600        ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_113 (Batch  (None, 19, 19, 240)  960        ['conv2d_150[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_113 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_37 (Depthwise  (None, 10, 10, 240)  2160       ['swish_113[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_114 (Batch  (None, 10, 10, 240)  960        ['depthwise_conv2d_37[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_114 (Swish)              (None, 10, 10, 240)  0           ['batch_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " lambda_37 (Lambda)             (None, 1, 1, 240)    0           ['swish_114[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_37[0][0]']              \n",
            "                                                                                                  \n",
            " swish_115 (Swish)              (None, 1, 1, 10)     0           ['conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_115[0][0]']              \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 1, 1, 240)    0           ['conv2d_152[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_37 (Multiply)         (None, 10, 10, 240)  0           ['activation_37[0][0]',          \n",
            "                                                                  'swish_114[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 10, 10, 80)   19200       ['multiply_37[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_115 (Batch  (None, 10, 10, 80)  320         ['conv2d_153[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 10, 10, 480)  38400       ['batch_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_116 (Batch  (None, 10, 10, 480)  1920       ['conv2d_154[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_116 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_38 (Depthwise  (None, 10, 10, 480)  4320       ['swish_116[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_117 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_38[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_117 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " lambda_38 (Lambda)             (None, 1, 1, 480)    0           ['swish_117[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_38[0][0]']              \n",
            "                                                                                                  \n",
            " swish_118 (Swish)              (None, 1, 1, 20)     0           ['conv2d_155[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_118[0][0]']              \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 1, 1, 480)    0           ['conv2d_156[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_38 (Multiply)         (None, 10, 10, 480)  0           ['activation_38[0][0]',          \n",
            "                                                                  'swish_117[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_38[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_118 (Batch  (None, 10, 10, 80)  320         ['conv2d_157[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_20 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_20[0][0]',        \n",
            "                                                                  'batch_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 10, 10, 480)  38400       ['add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_119 (Batch  (None, 10, 10, 480)  1920       ['conv2d_158[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_119 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_39 (Depthwise  (None, 10, 10, 480)  4320       ['swish_119[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_120 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_39[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_120 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " lambda_39 (Lambda)             (None, 1, 1, 480)    0           ['swish_120[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_39[0][0]']              \n",
            "                                                                                                  \n",
            " swish_121 (Swish)              (None, 1, 1, 20)     0           ['conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_121[0][0]']              \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 1, 1, 480)    0           ['conv2d_160[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_39 (Multiply)         (None, 10, 10, 480)  0           ['activation_39[0][0]',          \n",
            "                                                                  'swish_120[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_161 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_39[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_121 (Batch  (None, 10, 10, 80)  320         ['conv2d_161[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_21 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_21[0][0]',        \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)            (None, 10, 10, 480)  38400       ['add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_122 (Batch  (None, 10, 10, 480)  1920       ['conv2d_162[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_122 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_40 (Depthwise  (None, 10, 10, 480)  12000      ['swish_122[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_123 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_40[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_123 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " lambda_40 (Lambda)             (None, 1, 1, 480)    0           ['swish_123[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_40[0][0]']              \n",
            "                                                                                                  \n",
            " swish_124 (Swish)              (None, 1, 1, 20)     0           ['conv2d_163[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_124[0][0]']              \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 1, 1, 480)    0           ['conv2d_164[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_40 (Multiply)         (None, 10, 10, 480)  0           ['activation_40[0][0]',          \n",
            "                                                                  'swish_123[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_40[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_124 (Batch  (None, 10, 10, 112)  448        ['conv2d_165[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_125 (Batch  (None, 10, 10, 672)  2688       ['conv2d_166[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_125 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_41 (Depthwise  (None, 10, 10, 672)  16800      ['swish_125[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_126 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_41[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_126 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_126[0][0]']\n",
            "                                                                                                  \n",
            " lambda_41 (Lambda)             (None, 1, 1, 672)    0           ['swish_126[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_41[0][0]']              \n",
            "                                                                                                  \n",
            " swish_127 (Swish)              (None, 1, 1, 28)     0           ['conv2d_167[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_168 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_127[0][0]']              \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 1, 1, 672)    0           ['conv2d_168[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_41 (Multiply)         (None, 10, 10, 672)  0           ['activation_41[0][0]',          \n",
            "                                                                  'swish_126[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_169 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_41[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_127 (Batch  (None, 10, 10, 112)  448        ['conv2d_169[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_22 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_22[0][0]',        \n",
            "                                                                  'batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_170 (Conv2D)            (None, 10, 10, 672)  75264       ['add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_128 (Batch  (None, 10, 10, 672)  2688       ['conv2d_170[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_128 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_128[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_42 (Depthwise  (None, 10, 10, 672)  16800      ['swish_128[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_129 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_42[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_129 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " lambda_42 (Lambda)             (None, 1, 1, 672)    0           ['swish_129[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_171 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_42[0][0]']              \n",
            "                                                                                                  \n",
            " swish_130 (Swish)              (None, 1, 1, 28)     0           ['conv2d_171[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_172 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_130[0][0]']              \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 1, 1, 672)    0           ['conv2d_172[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_42 (Multiply)         (None, 10, 10, 672)  0           ['activation_42[0][0]',          \n",
            "                                                                  'swish_129[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_173 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_42[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_130 (Batch  (None, 10, 10, 112)  448        ['conv2d_173[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_23 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_23[0][0]',        \n",
            "                                                                  'add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_174 (Conv2D)            (None, 10, 10, 672)  75264       ['add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_131 (Batch  (None, 10, 10, 672)  2688       ['conv2d_174[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_131 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_43 (Depthwise  (None, 5, 5, 672)   16800       ['swish_131[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_132 (Batch  (None, 5, 5, 672)   2688        ['depthwise_conv2d_43[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_132 (Swish)              (None, 5, 5, 672)    0           ['batch_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " lambda_43 (Lambda)             (None, 1, 1, 672)    0           ['swish_132[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_175 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_43[0][0]']              \n",
            "                                                                                                  \n",
            " swish_133 (Swish)              (None, 1, 1, 28)     0           ['conv2d_175[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_176 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_133[0][0]']              \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 1, 1, 672)    0           ['conv2d_176[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_43 (Multiply)         (None, 5, 5, 672)    0           ['activation_43[0][0]',          \n",
            "                                                                  'swish_132[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_177 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_43[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_133 (Batch  (None, 5, 5, 192)   768         ['conv2d_177[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_178 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_134 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_178[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_134 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_44 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_134[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_44[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_135 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " lambda_44 (Lambda)             (None, 1, 1, 1152)   0           ['swish_135[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_179 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_44[0][0]']              \n",
            "                                                                                                  \n",
            " swish_136 (Swish)              (None, 1, 1, 48)     0           ['conv2d_179[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_136[0][0]']              \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_180[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_44 (Multiply)         (None, 5, 5, 1152)   0           ['activation_44[0][0]',          \n",
            "                                                                  'swish_135[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_44[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_136 (Batch  (None, 5, 5, 192)   768         ['conv2d_181[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_24 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_24[0][0]',        \n",
            "                                                                  'batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_137 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_182[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_137 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_45 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_137[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_138 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_45[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_138 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " lambda_45 (Lambda)             (None, 1, 1, 1152)   0           ['swish_138[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_45[0][0]']              \n",
            "                                                                                                  \n",
            " swish_139 (Swish)              (None, 1, 1, 48)     0           ['conv2d_183[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_139[0][0]']              \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_184[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_45 (Multiply)         (None, 5, 5, 1152)   0           ['activation_45[0][0]',          \n",
            "                                                                  'swish_138[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_45[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_139 (Batch  (None, 5, 5, 192)   768         ['conv2d_185[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_25 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_25[0][0]',        \n",
            "                                                                  'add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_140 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_186[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_140 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_46 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_140[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_141 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_46[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_141 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " lambda_46 (Lambda)             (None, 1, 1, 1152)   0           ['swish_141[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_46[0][0]']              \n",
            "                                                                                                  \n",
            " swish_142 (Swish)              (None, 1, 1, 48)     0           ['conv2d_187[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_188 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_142[0][0]']              \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_188[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_46 (Multiply)         (None, 5, 5, 1152)   0           ['activation_46[0][0]',          \n",
            "                                                                  'swish_141[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_189 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_46[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_142 (Batch  (None, 5, 5, 192)   768         ['conv2d_189[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_26 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_26[0][0]',        \n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_190 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_143 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_190[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_143 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_47 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_143[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_144 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_47[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_144 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " lambda_47 (Lambda)             (None, 1, 1, 1152)   0           ['swish_144[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_191 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_47[0][0]']              \n",
            "                                                                                                  \n",
            " swish_145 (Swish)              (None, 1, 1, 48)     0           ['conv2d_191[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_192 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_145[0][0]']              \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_192[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_47 (Multiply)         (None, 5, 5, 1152)   0           ['activation_47[0][0]',          \n",
            "                                                                  'swish_144[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_193 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_47[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_145 (Batch  (None, 5, 5, 320)   1280        ['conv2d_193[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_194 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_146 (Batch  (None, 5, 5, 1280)  5120        ['conv2d_194[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_146 (Swish)              (None, 5, 5, 1280)   0           ['batch_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "56191a19-d366-42ae-823d-d9d06201fa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "643dbe83-d37d-45c1-c931-adbb8a21cc58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "64c8aa11-1e4d-413c-b866-5aaced6910d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 107 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492342f5-8073-4d9d-f837-4bc9be3e2ed7"
      },
      "execution_count": 166,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-166-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "15/15 [==============================] - 23s 951ms/step - loss: 2.2794 - acc: 0.2533 - val_loss: 1.6266 - val_acc: 0.3000\n",
            "Epoch 2/1000\n",
            "15/15 [==============================] - 15s 854ms/step - loss: 2.1053 - acc: 0.2667 - val_loss: 1.7386 - val_acc: 0.2000\n",
            "Epoch 3/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 1.8641 - acc: 0.3100 - val_loss: 1.7743 - val_acc: 0.2500\n",
            "Epoch 4/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 1.8358 - acc: 0.3133 - val_loss: 1.6006 - val_acc: 0.1500\n",
            "Epoch 5/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 1.7286 - acc: 0.3274 - val_loss: 1.5269 - val_acc: 0.2500\n",
            "Epoch 6/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.6724 - acc: 0.3345 - val_loss: 1.8201 - val_acc: 0.1500\n",
            "Epoch 7/1000\n",
            "15/15 [==============================] - 12s 710ms/step - loss: 1.6122 - acc: 0.3630 - val_loss: 1.6571 - val_acc: 0.1500\n",
            "Epoch 8/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.6473 - acc: 0.3267 - val_loss: 1.6125 - val_acc: 0.1500\n",
            "Epoch 9/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 1.7122 - acc: 0.3100 - val_loss: 2.1444 - val_acc: 0.0500\n",
            "Epoch 10/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.5448 - acc: 0.3701 - val_loss: 1.3371 - val_acc: 0.3500\n",
            "Epoch 11/1000\n",
            "15/15 [==============================] - 12s 714ms/step - loss: 1.5010 - acc: 0.3701 - val_loss: 1.7241 - val_acc: 0.2000\n",
            "Epoch 12/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 1.5375 - acc: 0.3737 - val_loss: 1.6107 - val_acc: 0.2000\n",
            "Epoch 13/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 1.5998 - acc: 0.3600 - val_loss: 1.9110 - val_acc: 0.0500\n",
            "Epoch 14/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.5758 - acc: 0.3594 - val_loss: 1.9124 - val_acc: 0.1500\n",
            "Epoch 15/1000\n",
            "15/15 [==============================] - 13s 762ms/step - loss: 1.4216 - acc: 0.3900 - val_loss: 1.9722 - val_acc: 0.1500\n",
            "Epoch 16/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.6181 - acc: 0.3533 - val_loss: 1.8014 - val_acc: 0.2000\n",
            "Epoch 17/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 1.5234 - acc: 0.3733 - val_loss: 1.8991 - val_acc: 0.2000\n",
            "Epoch 18/1000\n",
            "15/15 [==============================] - 13s 748ms/step - loss: 1.5080 - acc: 0.3900 - val_loss: 1.4334 - val_acc: 0.1500\n",
            "Epoch 19/1000\n",
            "15/15 [==============================] - 12s 712ms/step - loss: 1.4992 - acc: 0.3915 - val_loss: 1.5229 - val_acc: 0.2000\n",
            "Epoch 20/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.5108 - acc: 0.3879 - val_loss: 1.6084 - val_acc: 0.3000\n",
            "Epoch 21/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 1.6413 - acc: 0.4021 - val_loss: 1.8341 - val_acc: 0.1500\n",
            "Epoch 22/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 1.5096 - acc: 0.4021 - val_loss: 2.0545 - val_acc: 0.1500\n",
            "Epoch 23/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 1.5136 - acc: 0.3767 - val_loss: 2.1743 - val_acc: 0.0500\n",
            "Epoch 24/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 1.4711 - acc: 0.3833 - val_loss: 2.1943 - val_acc: 0.0500\n",
            "Epoch 25/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 1.6629 - acc: 0.3986 - val_loss: 1.9397 - val_acc: 0.1500\n",
            "Epoch 26/1000\n",
            "15/15 [==============================] - 12s 706ms/step - loss: 1.5631 - acc: 0.3523 - val_loss: 1.6913 - val_acc: 0.2000\n",
            "Epoch 27/1000\n",
            "15/15 [==============================] - 13s 748ms/step - loss: 1.5183 - acc: 0.3733 - val_loss: 1.8337 - val_acc: 0.2500\n",
            "Epoch 28/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 1.5496 - acc: 0.4333 - val_loss: 1.9504 - val_acc: 0.2500\n",
            "Epoch 29/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 1.4869 - acc: 0.4200 - val_loss: 2.0147 - val_acc: 0.2500\n",
            "Epoch 30/1000\n",
            "15/15 [==============================] - 13s 710ms/step - loss: 1.4960 - acc: 0.3808 - val_loss: 1.4528 - val_acc: 0.2000\n",
            "Epoch 31/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.5626 - acc: 0.3986 - val_loss: 1.5364 - val_acc: 0.1000\n",
            "Epoch 32/1000\n",
            "15/15 [==============================] - 13s 717ms/step - loss: 1.5634 - acc: 0.3523 - val_loss: 1.7133 - val_acc: 0.2500\n",
            "Epoch 33/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 1.5286 - acc: 0.3772 - val_loss: 1.4576 - val_acc: 0.2500\n",
            "Epoch 34/1000\n",
            "15/15 [==============================] - 13s 769ms/step - loss: 1.4457 - acc: 0.4267 - val_loss: 1.8874 - val_acc: 0.0500\n",
            "Epoch 35/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 1.6678 - acc: 0.3567 - val_loss: 1.5880 - val_acc: 0.1500\n",
            "Epoch 36/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 1.5712 - acc: 0.3433 - val_loss: 1.5484 - val_acc: 0.3000\n",
            "Epoch 37/1000\n",
            "15/15 [==============================] - 13s 729ms/step - loss: 1.4955 - acc: 0.3950 - val_loss: 1.5571 - val_acc: 0.3000\n",
            "Epoch 38/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 1.4455 - acc: 0.4033 - val_loss: 1.7171 - val_acc: 0.0500\n",
            "Epoch 39/1000\n",
            "15/15 [==============================] - 13s 735ms/step - loss: 1.3901 - acc: 0.4377 - val_loss: 1.8055 - val_acc: 0.1500\n",
            "Epoch 40/1000\n",
            "15/15 [==============================] - 13s 746ms/step - loss: 1.4633 - acc: 0.4367 - val_loss: 1.5246 - val_acc: 0.3000\n",
            "Epoch 41/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 1.4444 - acc: 0.4164 - val_loss: 1.5706 - val_acc: 0.3000\n",
            "Epoch 42/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.4756 - acc: 0.3915 - val_loss: 1.8075 - val_acc: 0.2000\n",
            "Epoch 43/1000\n",
            "15/15 [==============================] - 13s 891ms/step - loss: 1.4119 - acc: 0.4128 - val_loss: 1.9512 - val_acc: 0.1000\n",
            "Epoch 44/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.4897 - acc: 0.4021 - val_loss: 1.6761 - val_acc: 0.1500\n",
            "Epoch 45/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.5198 - acc: 0.3986 - val_loss: 1.4525 - val_acc: 0.3500\n",
            "Epoch 46/1000\n",
            "15/15 [==============================] - 12s 851ms/step - loss: 1.4668 - acc: 0.4199 - val_loss: 1.8321 - val_acc: 0.2000\n",
            "Epoch 47/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.4365 - acc: 0.4000 - val_loss: 1.5849 - val_acc: 0.3000\n",
            "Epoch 48/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 1.3895 - acc: 0.4333 - val_loss: 1.7052 - val_acc: 0.2500\n",
            "Epoch 49/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.5679 - acc: 0.3533 - val_loss: 1.4627 - val_acc: 0.3500\n",
            "Epoch 50/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 1.4973 - acc: 0.3900 - val_loss: 1.5240 - val_acc: 0.3500\n",
            "Epoch 51/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 1.3430 - acc: 0.4267 - val_loss: 2.1920 - val_acc: 0.1500\n",
            "Epoch 52/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 1.5253 - acc: 0.3900 - val_loss: 1.6204 - val_acc: 0.2500\n",
            "Epoch 53/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.5889 - acc: 0.4033 - val_loss: 1.8022 - val_acc: 0.2000\n",
            "Epoch 54/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 1.4994 - acc: 0.3967 - val_loss: 1.5156 - val_acc: 0.2500\n",
            "Epoch 55/1000\n",
            "15/15 [==============================] - 13s 745ms/step - loss: 1.5395 - acc: 0.3416 - val_loss: 1.2211 - val_acc: 0.4500\n",
            "Epoch 56/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 1.4808 - acc: 0.4377 - val_loss: 1.6951 - val_acc: 0.3000\n",
            "Epoch 57/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 1.5134 - acc: 0.3879 - val_loss: 1.7440 - val_acc: 0.2000\n",
            "Epoch 58/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 1.4485 - acc: 0.4306 - val_loss: 1.5988 - val_acc: 0.2000\n",
            "Epoch 59/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 1.3482 - acc: 0.3915 - val_loss: 1.6247 - val_acc: 0.1500\n",
            "Epoch 60/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 1.3970 - acc: 0.4367 - val_loss: 1.5644 - val_acc: 0.3000\n",
            "Epoch 61/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 1.4381 - acc: 0.4000 - val_loss: 1.4920 - val_acc: 0.1000\n",
            "Epoch 62/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 1.2961 - acc: 0.4235 - val_loss: 1.7488 - val_acc: 0.2000\n",
            "Epoch 63/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 1.3806 - acc: 0.4400 - val_loss: 1.9454 - val_acc: 0.3000\n",
            "Epoch 64/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 1.4709 - acc: 0.4167 - val_loss: 1.6189 - val_acc: 0.3000\n",
            "Epoch 65/1000\n",
            "15/15 [==============================] - 14s 787ms/step - loss: 1.4591 - acc: 0.3559 - val_loss: 1.8301 - val_acc: 0.3000\n",
            "Epoch 66/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.3821 - acc: 0.3967 - val_loss: 1.3358 - val_acc: 0.4000\n",
            "Epoch 67/1000\n",
            "15/15 [==============================] - 13s 730ms/step - loss: 1.3941 - acc: 0.4413 - val_loss: 1.6351 - val_acc: 0.2500\n",
            "Epoch 68/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 1.3895 - acc: 0.4267 - val_loss: 1.9167 - val_acc: 0.1000\n",
            "Epoch 69/1000\n",
            "15/15 [==============================] - 14s 817ms/step - loss: 1.5864 - acc: 0.3667 - val_loss: 1.7919 - val_acc: 0.2000\n",
            "Epoch 70/1000\n",
            "15/15 [==============================] - 13s 758ms/step - loss: 1.4014 - acc: 0.4433 - val_loss: 1.1109 - val_acc: 0.3500\n",
            "Epoch 71/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 1.4421 - acc: 0.3767 - val_loss: 1.9640 - val_acc: 0.2000\n",
            "Epoch 72/1000\n",
            "15/15 [==============================] - 13s 717ms/step - loss: 1.5672 - acc: 0.3772 - val_loss: 1.8172 - val_acc: 0.1500\n",
            "Epoch 73/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 1.3997 - acc: 0.3767 - val_loss: 1.5575 - val_acc: 0.2000\n",
            "Epoch 74/1000\n",
            "15/15 [==============================] - 14s 798ms/step - loss: 1.3196 - acc: 0.4533 - val_loss: 1.8558 - val_acc: 0.2000\n",
            "Epoch 75/1000\n",
            "15/15 [==============================] - 14s 776ms/step - loss: 1.3642 - acc: 0.4167 - val_loss: 1.6981 - val_acc: 0.3000\n",
            "Epoch 76/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.4630 - acc: 0.4267 - val_loss: 1.8069 - val_acc: 0.2500\n",
            "Epoch 77/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 1.4513 - acc: 0.4033 - val_loss: 1.8875 - val_acc: 0.2000\n",
            "Epoch 78/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 1.4979 - acc: 0.4199 - val_loss: 1.5734 - val_acc: 0.1000\n",
            "Epoch 79/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.3301 - acc: 0.4300 - val_loss: 1.3699 - val_acc: 0.4000\n",
            "Epoch 80/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.5181 - acc: 0.3867 - val_loss: 1.3020 - val_acc: 0.4500\n",
            "Epoch 81/1000\n",
            "15/15 [==============================] - 13s 733ms/step - loss: 1.4154 - acc: 0.4167 - val_loss: 1.5533 - val_acc: 0.2500\n",
            "Epoch 82/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 1.4635 - acc: 0.4164 - val_loss: 1.6562 - val_acc: 0.2000\n",
            "Epoch 83/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.3562 - acc: 0.4342 - val_loss: 1.6719 - val_acc: 0.2500\n",
            "Epoch 84/1000\n",
            "15/15 [==============================] - 13s 705ms/step - loss: 1.5073 - acc: 0.4057 - val_loss: 1.4063 - val_acc: 0.3000\n",
            "Epoch 85/1000\n",
            "15/15 [==============================] - 12s 711ms/step - loss: 1.3586 - acc: 0.4448 - val_loss: 1.6362 - val_acc: 0.2000\n",
            "Epoch 86/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 1.3163 - acc: 0.4342 - val_loss: 1.8033 - val_acc: 0.1000\n",
            "Epoch 87/1000\n",
            "15/15 [==============================] - 13s 701ms/step - loss: 1.3548 - acc: 0.4021 - val_loss: 1.5851 - val_acc: 0.3000\n",
            "Epoch 88/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 1.3919 - acc: 0.4300 - val_loss: 1.8248 - val_acc: 0.1000\n",
            "Epoch 89/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.4485 - acc: 0.4033 - val_loss: 1.3725 - val_acc: 0.3000\n",
            "Epoch 90/1000\n",
            "15/15 [==============================] - 12s 703ms/step - loss: 1.3930 - acc: 0.4093 - val_loss: 1.5894 - val_acc: 0.2000\n",
            "Epoch 91/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.4063 - acc: 0.4167 - val_loss: 1.7144 - val_acc: 0.3000\n",
            "Epoch 92/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 1.4852 - acc: 0.3833 - val_loss: 1.6740 - val_acc: 0.1500\n",
            "Epoch 93/1000\n",
            "15/15 [==============================] - 12s 704ms/step - loss: 1.5023 - acc: 0.3915 - val_loss: 1.5608 - val_acc: 0.2000\n",
            "Epoch 94/1000\n",
            "15/15 [==============================] - 13s 726ms/step - loss: 1.4835 - acc: 0.3733 - val_loss: 1.3597 - val_acc: 0.2500\n",
            "Epoch 95/1000\n",
            "15/15 [==============================] - 12s 701ms/step - loss: 1.2736 - acc: 0.4520 - val_loss: 1.9169 - val_acc: 0.1500\n",
            "Epoch 96/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 1.3930 - acc: 0.4233 - val_loss: 1.6333 - val_acc: 0.2000\n",
            "Epoch 97/1000\n",
            "15/15 [==============================] - 13s 726ms/step - loss: 1.3018 - acc: 0.4533 - val_loss: 0.9746 - val_acc: 0.5000\n",
            "Epoch 98/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 1.4196 - acc: 0.4267 - val_loss: 1.7502 - val_acc: 0.2500\n",
            "Epoch 99/1000\n",
            "15/15 [==============================] - 12s 841ms/step - loss: 1.4306 - acc: 0.4377 - val_loss: 1.7621 - val_acc: 0.1500\n",
            "Epoch 100/1000\n",
            "15/15 [==============================] - 13s 725ms/step - loss: 1.3760 - acc: 0.4267 - val_loss: 1.4414 - val_acc: 0.3000\n",
            "Epoch 101/1000\n",
            "15/15 [==============================] - 13s 733ms/step - loss: 1.5231 - acc: 0.3300 - val_loss: 1.4451 - val_acc: 0.3000\n",
            "Epoch 102/1000\n",
            "15/15 [==============================] - 13s 729ms/step - loss: 1.3306 - acc: 0.3933 - val_loss: 1.3707 - val_acc: 0.3000\n",
            "Epoch 103/1000\n",
            "15/15 [==============================] - 13s 720ms/step - loss: 1.4567 - acc: 0.4300 - val_loss: 1.4605 - val_acc: 0.2000\n",
            "Epoch 104/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.3626 - acc: 0.4400 - val_loss: 1.3186 - val_acc: 0.1500\n",
            "Epoch 105/1000\n",
            "15/15 [==============================] - 12s 704ms/step - loss: 1.3240 - acc: 0.4164 - val_loss: 1.6529 - val_acc: 0.3500\n",
            "Epoch 106/1000\n",
            "15/15 [==============================] - 13s 718ms/step - loss: 1.2698 - acc: 0.4933 - val_loss: 1.2395 - val_acc: 0.4000\n",
            "Epoch 107/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.3659 - acc: 0.4067 - val_loss: 1.3116 - val_acc: 0.4000\n",
            "Epoch 108/1000\n",
            "15/15 [==============================] - 12s 696ms/step - loss: 1.2514 - acc: 0.4591 - val_loss: 1.3469 - val_acc: 0.2000\n",
            "Epoch 109/1000\n",
            "15/15 [==============================] - 13s 715ms/step - loss: 1.3385 - acc: 0.3900 - val_loss: 1.7730 - val_acc: 0.2000\n",
            "Epoch 110/1000\n",
            "15/15 [==============================] - 13s 730ms/step - loss: 1.2770 - acc: 0.4633 - val_loss: 1.5550 - val_acc: 0.2000\n",
            "Epoch 111/1000\n",
            "15/15 [==============================] - 12s 707ms/step - loss: 1.2680 - acc: 0.4300 - val_loss: 1.2655 - val_acc: 0.3500\n",
            "Epoch 112/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.4242 - acc: 0.3467 - val_loss: 1.6865 - val_acc: 0.1000\n",
            "Epoch 113/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.3048 - acc: 0.4133 - val_loss: 1.5460 - val_acc: 0.1000\n",
            "Epoch 114/1000\n",
            "15/15 [==============================] - 12s 682ms/step - loss: 1.3564 - acc: 0.4306 - val_loss: 1.3897 - val_acc: 0.2500\n",
            "Epoch 115/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 1.2737 - acc: 0.4500 - val_loss: 1.3572 - val_acc: 0.3000\n",
            "Epoch 116/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 1.3433 - acc: 0.4567 - val_loss: 1.8710 - val_acc: 0.1000\n",
            "Epoch 117/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.2895 - acc: 0.4333 - val_loss: 1.7415 - val_acc: 0.2000\n",
            "Epoch 118/1000\n",
            "15/15 [==============================] - 12s 691ms/step - loss: 1.3690 - acc: 0.4128 - val_loss: 1.6040 - val_acc: 0.3000\n",
            "Epoch 119/1000\n",
            "15/15 [==============================] - 13s 718ms/step - loss: 1.3012 - acc: 0.4200 - val_loss: 1.4689 - val_acc: 0.2500\n",
            "Epoch 120/1000\n",
            "15/15 [==============================] - 12s 688ms/step - loss: 1.3653 - acc: 0.4555 - val_loss: 1.2379 - val_acc: 0.3000\n",
            "Epoch 121/1000\n",
            "15/15 [==============================] - 12s 699ms/step - loss: 1.3691 - acc: 0.4342 - val_loss: 1.7765 - val_acc: 0.1000\n",
            "Epoch 122/1000\n",
            "15/15 [==============================] - 13s 715ms/step - loss: 1.4013 - acc: 0.4067 - val_loss: 1.3484 - val_acc: 0.2500\n",
            "Epoch 123/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.1749 - acc: 0.4867 - val_loss: 1.5610 - val_acc: 0.3000\n",
            "Epoch 124/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.3530 - acc: 0.3933 - val_loss: 1.6550 - val_acc: 0.1500\n",
            "Epoch 125/1000\n",
            "15/15 [==============================] - 12s 849ms/step - loss: 1.3169 - acc: 0.4448 - val_loss: 1.6646 - val_acc: 0.3000\n",
            "Epoch 126/1000\n",
            "15/15 [==============================] - 13s 726ms/step - loss: 1.2514 - acc: 0.4600 - val_loss: 1.8885 - val_acc: 0.2500\n",
            "Epoch 127/1000\n",
            "15/15 [==============================] - 12s 690ms/step - loss: 1.2944 - acc: 0.4128 - val_loss: 1.6972 - val_acc: 0.3000\n",
            "Epoch 128/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 1.2401 - acc: 0.4667 - val_loss: 1.8284 - val_acc: 0.1000\n",
            "Epoch 129/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 1.3923 - acc: 0.4467 - val_loss: 1.4598 - val_acc: 0.2000\n",
            "Epoch 130/1000\n",
            "15/15 [==============================] - 13s 726ms/step - loss: 1.3066 - acc: 0.4833 - val_loss: 1.3298 - val_acc: 0.1500\n",
            "Epoch 131/1000\n",
            "15/15 [==============================] - 12s 692ms/step - loss: 1.3972 - acc: 0.3986 - val_loss: 1.6492 - val_acc: 0.2500\n",
            "Epoch 132/1000\n",
            "15/15 [==============================] - 13s 727ms/step - loss: 1.3131 - acc: 0.4067 - val_loss: 1.6913 - val_acc: 0.2000\n",
            "Epoch 133/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 1.2350 - acc: 0.4600 - val_loss: 1.4117 - val_acc: 0.3000\n",
            "Epoch 134/1000\n",
            "15/15 [==============================] - 12s 724ms/step - loss: 1.3551 - acc: 0.4367 - val_loss: 1.5612 - val_acc: 0.2000\n",
            "Epoch 135/1000\n",
            "15/15 [==============================] - 12s 693ms/step - loss: 1.3300 - acc: 0.4342 - val_loss: 1.2510 - val_acc: 0.2500\n",
            "Epoch 136/1000\n",
            "15/15 [==============================] - 12s 696ms/step - loss: 1.3108 - acc: 0.4698 - val_loss: 1.6525 - val_acc: 0.2500\n",
            "Epoch 137/1000\n",
            "15/15 [==============================] - 12s 691ms/step - loss: 1.2639 - acc: 0.4306 - val_loss: 1.5660 - val_acc: 0.2000\n",
            "Epoch 138/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 1.2982 - acc: 0.4533 - val_loss: 1.4503 - val_acc: 0.1500\n",
            "Epoch 139/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 1.2590 - acc: 0.4533 - val_loss: 1.4449 - val_acc: 0.2500\n",
            "Epoch 140/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 1.3637 - acc: 0.4200 - val_loss: 1.4539 - val_acc: 0.3500\n",
            "Epoch 141/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 1.3434 - acc: 0.4567 - val_loss: 1.3912 - val_acc: 0.2500\n",
            "Epoch 142/1000\n",
            "15/15 [==============================] - 13s 758ms/step - loss: 1.3390 - acc: 0.4100 - val_loss: 1.6635 - val_acc: 0.3000\n",
            "Epoch 143/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 1.2616 - acc: 0.4467 - val_loss: 1.5870 - val_acc: 0.2000\n",
            "Epoch 144/1000\n",
            "15/15 [==============================] - 13s 720ms/step - loss: 1.3392 - acc: 0.4164 - val_loss: 1.3582 - val_acc: 0.2500\n",
            "Epoch 145/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.1971 - acc: 0.4233 - val_loss: 1.3318 - val_acc: 0.2000\n",
            "Epoch 146/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.3120 - acc: 0.4633 - val_loss: 1.3922 - val_acc: 0.2000\n",
            "Epoch 147/1000\n",
            "15/15 [==============================] - 13s 748ms/step - loss: 1.2874 - acc: 0.4633 - val_loss: 1.2316 - val_acc: 0.3000\n",
            "Epoch 148/1000\n",
            "15/15 [==============================] - 12s 714ms/step - loss: 1.2643 - acc: 0.4662 - val_loss: 1.5848 - val_acc: 0.2000\n",
            "Epoch 149/1000\n",
            "15/15 [==============================] - 12s 712ms/step - loss: 1.1701 - acc: 0.5053 - val_loss: 1.6332 - val_acc: 0.3500\n",
            "Epoch 150/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 1.1565 - acc: 0.4833 - val_loss: 1.6267 - val_acc: 0.3000\n",
            "Epoch 151/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 1.2031 - acc: 0.4900 - val_loss: 1.3722 - val_acc: 0.2500\n",
            "Epoch 152/1000\n",
            "15/15 [==============================] - 13s 740ms/step - loss: 1.2459 - acc: 0.4767 - val_loss: 1.7921 - val_acc: 0.1000\n",
            "Epoch 153/1000\n",
            "15/15 [==============================] - 12s 703ms/step - loss: 1.2594 - acc: 0.4413 - val_loss: 1.9327 - val_acc: 0.2000\n",
            "Epoch 154/1000\n",
            "15/15 [==============================] - 12s 703ms/step - loss: 1.2759 - acc: 0.4662 - val_loss: 1.2192 - val_acc: 0.3000\n",
            "Epoch 155/1000\n",
            "15/15 [==============================] - 13s 736ms/step - loss: 1.2744 - acc: 0.4500 - val_loss: 1.4603 - val_acc: 0.2500\n",
            "Epoch 156/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 1.1179 - acc: 0.5100 - val_loss: 1.3765 - val_acc: 0.3500\n",
            "Epoch 157/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 1.2371 - acc: 0.4733 - val_loss: 1.5564 - val_acc: 0.2500\n",
            "Epoch 158/1000\n",
            "15/15 [==============================] - 12s 703ms/step - loss: 1.2738 - acc: 0.4804 - val_loss: 1.5332 - val_acc: 0.1500\n",
            "Epoch 159/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 1.2705 - acc: 0.4367 - val_loss: 1.4094 - val_acc: 0.1000\n",
            "Epoch 160/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.2767 - acc: 0.4533 - val_loss: 1.7323 - val_acc: 0.1500\n",
            "Epoch 161/1000\n",
            "15/15 [==============================] - 13s 884ms/step - loss: 1.3405 - acc: 0.4484 - val_loss: 1.4850 - val_acc: 0.2000\n",
            "Epoch 162/1000\n",
            "15/15 [==============================] - 12s 702ms/step - loss: 1.2148 - acc: 0.4626 - val_loss: 1.4253 - val_acc: 0.3000\n",
            "Epoch 163/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.2377 - acc: 0.4767 - val_loss: 1.7227 - val_acc: 0.1000\n",
            "Epoch 164/1000\n",
            "15/15 [==============================] - 12s 704ms/step - loss: 1.3138 - acc: 0.4698 - val_loss: 1.6152 - val_acc: 0.2500\n",
            "Epoch 165/1000\n",
            "15/15 [==============================] - 13s 715ms/step - loss: 1.2089 - acc: 0.4867 - val_loss: 1.6421 - val_acc: 0.2000\n",
            "Epoch 166/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.2318 - acc: 0.4800 - val_loss: 1.3653 - val_acc: 0.3500\n",
            "Epoch 167/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 1.2651 - acc: 0.4600 - val_loss: 1.4249 - val_acc: 0.2000\n",
            "Epoch 168/1000\n",
            "15/15 [==============================] - 13s 735ms/step - loss: 1.2453 - acc: 0.4767 - val_loss: 1.4905 - val_acc: 0.2500\n",
            "Epoch 169/1000\n",
            "15/15 [==============================] - 13s 735ms/step - loss: 1.2453 - acc: 0.4633 - val_loss: 1.4447 - val_acc: 0.3000\n",
            "Epoch 170/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 1.3475 - acc: 0.4033 - val_loss: 1.4434 - val_acc: 0.2500\n",
            "Epoch 171/1000\n",
            "15/15 [==============================] - 12s 691ms/step - loss: 1.1881 - acc: 0.4875 - val_loss: 1.5605 - val_acc: 0.4000\n",
            "Epoch 172/1000\n",
            "15/15 [==============================] - 13s 737ms/step - loss: 1.2014 - acc: 0.4567 - val_loss: 1.6572 - val_acc: 0.2500\n",
            "Epoch 173/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.2800 - acc: 0.4467 - val_loss: 1.1744 - val_acc: 0.3500\n",
            "Epoch 174/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 1.3326 - acc: 0.4033 - val_loss: 1.7757 - val_acc: 0.2000\n",
            "Epoch 175/1000\n",
            "15/15 [==============================] - 12s 685ms/step - loss: 1.2197 - acc: 0.4733 - val_loss: 1.7018 - val_acc: 0.1500\n",
            "Epoch 176/1000\n",
            "15/15 [==============================] - 12s 712ms/step - loss: 1.2596 - acc: 0.4769 - val_loss: 1.3602 - val_acc: 0.2000\n",
            "Epoch 177/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 1.1946 - acc: 0.4633 - val_loss: 1.5942 - val_acc: 0.1500\n",
            "Epoch 178/1000\n",
            "15/15 [==============================] - 13s 771ms/step - loss: 1.1913 - acc: 0.4900 - val_loss: 1.4111 - val_acc: 0.4500\n",
            "Epoch 179/1000\n",
            "15/15 [==============================] - 13s 733ms/step - loss: 1.1618 - acc: 0.4875 - val_loss: 1.6055 - val_acc: 0.2000\n",
            "Epoch 180/1000\n",
            "15/15 [==============================] - 13s 736ms/step - loss: 1.2871 - acc: 0.4633 - val_loss: 1.4369 - val_acc: 0.2000\n",
            "Epoch 181/1000\n",
            "15/15 [==============================] - 13s 769ms/step - loss: 1.2130 - acc: 0.4433 - val_loss: 1.3608 - val_acc: 0.4000\n",
            "Epoch 182/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 1.2305 - acc: 0.4500 - val_loss: 1.3771 - val_acc: 0.2000\n",
            "Epoch 183/1000\n",
            "15/15 [==============================] - 13s 768ms/step - loss: 1.2887 - acc: 0.4533 - val_loss: 1.6477 - val_acc: 0.3000\n",
            "Epoch 184/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1742 - acc: 0.5018 - val_loss: 1.3572 - val_acc: 0.3500\n",
            "Epoch 185/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 1.2088 - acc: 0.4633 - val_loss: 1.4735 - val_acc: 0.3500\n",
            "Epoch 186/1000\n",
            "15/15 [==============================] - 13s 729ms/step - loss: 1.2320 - acc: 0.4769 - val_loss: 1.4748 - val_acc: 0.2500\n",
            "Epoch 187/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 1.1135 - acc: 0.5133 - val_loss: 1.2225 - val_acc: 0.2500\n",
            "Epoch 188/1000\n",
            "15/15 [==============================] - 13s 752ms/step - loss: 1.2322 - acc: 0.4867 - val_loss: 1.5873 - val_acc: 0.1500\n",
            "Epoch 189/1000\n",
            "15/15 [==============================] - 13s 725ms/step - loss: 1.2725 - acc: 0.4377 - val_loss: 1.6772 - val_acc: 0.2000\n",
            "Epoch 190/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 1.2237 - acc: 0.4698 - val_loss: 1.5699 - val_acc: 0.3500\n",
            "Epoch 191/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 1.2539 - acc: 0.4767 - val_loss: 1.7279 - val_acc: 0.2500\n",
            "Epoch 192/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.2703 - acc: 0.4500 - val_loss: 1.1316 - val_acc: 0.3000\n",
            "Epoch 193/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 1.2661 - acc: 0.4767 - val_loss: 1.9107 - val_acc: 0.2000\n",
            "Epoch 194/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 1.2341 - acc: 0.4698 - val_loss: 1.8125 - val_acc: 0.1000\n",
            "Epoch 195/1000\n",
            "15/15 [==============================] - 13s 745ms/step - loss: 1.1180 - acc: 0.5018 - val_loss: 1.4494 - val_acc: 0.3000\n",
            "Epoch 196/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 1.2267 - acc: 0.4800 - val_loss: 1.2004 - val_acc: 0.3500\n",
            "Epoch 197/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.1587 - acc: 0.4947 - val_loss: 1.5820 - val_acc: 0.1500\n",
            "Epoch 198/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 1.2549 - acc: 0.4367 - val_loss: 1.3915 - val_acc: 0.2500\n",
            "Epoch 199/1000\n",
            "15/15 [==============================] - 13s 767ms/step - loss: 1.2383 - acc: 0.4600 - val_loss: 1.4070 - val_acc: 0.2000\n",
            "Epoch 200/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.2259 - acc: 0.4698 - val_loss: 1.7162 - val_acc: 0.1500\n",
            "Epoch 201/1000\n",
            "15/15 [==============================] - 17s 1s/step - loss: 1.1918 - acc: 0.4875 - val_loss: 1.4362 - val_acc: 0.3500\n",
            "Epoch 202/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 1.2439 - acc: 0.4733 - val_loss: 1.4188 - val_acc: 0.2500\n",
            "Epoch 203/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 1.1728 - acc: 0.4967 - val_loss: 1.6138 - val_acc: 0.2000\n",
            "Epoch 204/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 1.2157 - acc: 0.4698 - val_loss: 1.3150 - val_acc: 0.3000\n",
            "Epoch 205/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 1.1169 - acc: 0.5133 - val_loss: 1.5551 - val_acc: 0.3000\n",
            "Epoch 206/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 1.1915 - acc: 0.4767 - val_loss: 1.7351 - val_acc: 0.2000\n",
            "Epoch 207/1000\n",
            "15/15 [==============================] - 13s 773ms/step - loss: 1.2327 - acc: 0.4500 - val_loss: 1.3773 - val_acc: 0.3500\n",
            "Epoch 208/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.1275 - acc: 0.5100 - val_loss: 1.3923 - val_acc: 0.2500\n",
            "Epoch 209/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.1996 - acc: 0.4804 - val_loss: 1.5128 - val_acc: 0.2000\n",
            "Epoch 210/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 1.2108 - acc: 0.4667 - val_loss: 1.5377 - val_acc: 0.3500\n",
            "Epoch 211/1000\n",
            "15/15 [==============================] - 13s 752ms/step - loss: 1.1772 - acc: 0.5231 - val_loss: 1.2511 - val_acc: 0.2500\n",
            "Epoch 212/1000\n",
            "15/15 [==============================] - 14s 831ms/step - loss: 1.2096 - acc: 0.5167 - val_loss: 1.6205 - val_acc: 0.2500\n",
            "Epoch 213/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 1.1468 - acc: 0.5267 - val_loss: 1.8381 - val_acc: 0.1500\n",
            "Epoch 214/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 1.2448 - acc: 0.4733 - val_loss: 1.3567 - val_acc: 0.3000\n",
            "Epoch 215/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 1.1544 - acc: 0.5167 - val_loss: 1.6569 - val_acc: 0.2000\n",
            "Epoch 216/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 1.2762 - acc: 0.4667 - val_loss: 1.6097 - val_acc: 0.2000\n",
            "Epoch 217/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 1.1965 - acc: 0.4600 - val_loss: 1.3148 - val_acc: 0.2500\n",
            "Epoch 218/1000\n",
            "15/15 [==============================] - 14s 785ms/step - loss: 1.0852 - acc: 0.5333 - val_loss: 1.4865 - val_acc: 0.2500\n",
            "Epoch 219/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 1.1012 - acc: 0.5053 - val_loss: 1.5757 - val_acc: 0.2000\n",
            "Epoch 220/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.2046 - acc: 0.4667 - val_loss: 1.6152 - val_acc: 0.2000\n",
            "Epoch 221/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 1.2132 - acc: 0.4982 - val_loss: 1.4982 - val_acc: 0.2000\n",
            "Epoch 222/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.0850 - acc: 0.4900 - val_loss: 1.2736 - val_acc: 0.2000\n",
            "Epoch 223/1000\n",
            "15/15 [==============================] - 14s 775ms/step - loss: 1.3106 - acc: 0.4467 - val_loss: 1.4136 - val_acc: 0.3000\n",
            "Epoch 224/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.2030 - acc: 0.4698 - val_loss: 1.4371 - val_acc: 0.2500\n",
            "Epoch 225/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.3296 - acc: 0.4067 - val_loss: 1.4647 - val_acc: 0.3500\n",
            "Epoch 226/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.2109 - acc: 0.5089 - val_loss: 1.2067 - val_acc: 0.3500\n",
            "Epoch 227/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 1.2487 - acc: 0.4591 - val_loss: 1.3058 - val_acc: 0.2500\n",
            "Epoch 228/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 1.2410 - acc: 0.4875 - val_loss: 1.2859 - val_acc: 0.3000\n",
            "Epoch 229/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 1.1168 - acc: 0.5196 - val_loss: 1.7409 - val_acc: 0.1000\n",
            "Epoch 230/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 1.1008 - acc: 0.5267 - val_loss: 1.0994 - val_acc: 0.3500\n",
            "Epoch 231/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 1.1897 - acc: 0.4867 - val_loss: 1.6238 - val_acc: 0.2000\n",
            "Epoch 232/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 1.2531 - acc: 0.4698 - val_loss: 1.3271 - val_acc: 0.1500\n",
            "Epoch 233/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 1.1681 - acc: 0.4733 - val_loss: 1.3430 - val_acc: 0.2500\n",
            "Epoch 234/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 1.2310 - acc: 0.4733 - val_loss: 1.4702 - val_acc: 0.1500\n",
            "Epoch 235/1000\n",
            "15/15 [==============================] - 13s 767ms/step - loss: 1.2219 - acc: 0.4767 - val_loss: 1.5946 - val_acc: 0.2000\n",
            "Epoch 236/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 1.1007 - acc: 0.5300 - val_loss: 1.5102 - val_acc: 0.4000\n",
            "Epoch 237/1000\n",
            "15/15 [==============================] - 13s 909ms/step - loss: 1.1283 - acc: 0.4982 - val_loss: 1.7372 - val_acc: 0.2500\n",
            "Epoch 238/1000\n",
            "15/15 [==============================] - 13s 916ms/step - loss: 1.1504 - acc: 0.5231 - val_loss: 1.5114 - val_acc: 0.3000\n",
            "Epoch 239/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 1.2295 - acc: 0.4555 - val_loss: 1.4378 - val_acc: 0.3000\n",
            "Epoch 240/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 1.1685 - acc: 0.4867 - val_loss: 1.6362 - val_acc: 0.3000\n",
            "Epoch 241/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1687 - acc: 0.5302 - val_loss: 1.4846 - val_acc: 0.3000\n",
            "Epoch 242/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.1955 - acc: 0.4804 - val_loss: 1.4273 - val_acc: 0.3000\n",
            "Epoch 243/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 1.2059 - acc: 0.4967 - val_loss: 1.2555 - val_acc: 0.3000\n",
            "Epoch 244/1000\n",
            "15/15 [==============================] - 14s 773ms/step - loss: 1.1226 - acc: 0.5167 - val_loss: 1.6484 - val_acc: 0.2500\n",
            "Epoch 245/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1327 - acc: 0.5018 - val_loss: 1.8591 - val_acc: 0.0500\n",
            "Epoch 246/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 1.1117 - acc: 0.5480 - val_loss: 1.1607 - val_acc: 0.5000\n",
            "Epoch 247/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 1.1727 - acc: 0.4700 - val_loss: 1.3523 - val_acc: 0.2500\n",
            "Epoch 248/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 1.1316 - acc: 0.4520 - val_loss: 1.5932 - val_acc: 0.3500\n",
            "Epoch 249/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 1.2141 - acc: 0.5053 - val_loss: 1.5827 - val_acc: 0.3000\n",
            "Epoch 250/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.1275 - acc: 0.5067 - val_loss: 1.6515 - val_acc: 0.1500\n",
            "Epoch 251/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.2280 - acc: 0.4698 - val_loss: 1.5656 - val_acc: 0.3000\n",
            "Epoch 252/1000\n",
            "15/15 [==============================] - 14s 775ms/step - loss: 1.1343 - acc: 0.5067 - val_loss: 1.5981 - val_acc: 0.1500\n",
            "Epoch 253/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 1.0891 - acc: 0.5445 - val_loss: 1.6527 - val_acc: 0.2000\n",
            "Epoch 254/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 1.1745 - acc: 0.4733 - val_loss: 1.5990 - val_acc: 0.2000\n",
            "Epoch 255/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.1139 - acc: 0.5338 - val_loss: 1.2993 - val_acc: 0.3500\n",
            "Epoch 256/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 1.1956 - acc: 0.4567 - val_loss: 1.5074 - val_acc: 0.3000\n",
            "Epoch 257/1000\n",
            "15/15 [==============================] - 13s 746ms/step - loss: 1.1776 - acc: 0.4982 - val_loss: 1.3258 - val_acc: 0.3500\n",
            "Epoch 258/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.2494 - acc: 0.5018 - val_loss: 1.4478 - val_acc: 0.3000\n",
            "Epoch 259/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 1.1274 - acc: 0.4900 - val_loss: 1.5135 - val_acc: 0.2000\n",
            "Epoch 260/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 1.0907 - acc: 0.4967 - val_loss: 1.5646 - val_acc: 0.2000\n",
            "Epoch 261/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 1.0905 - acc: 0.5100 - val_loss: 1.5171 - val_acc: 0.2000\n",
            "Epoch 262/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1924 - acc: 0.5089 - val_loss: 1.2791 - val_acc: 0.2500\n",
            "Epoch 263/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.1898 - acc: 0.5067 - val_loss: 1.2530 - val_acc: 0.3500\n",
            "Epoch 264/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 1.1982 - acc: 0.4700 - val_loss: 1.6313 - val_acc: 0.2000\n",
            "Epoch 265/1000\n",
            "15/15 [==============================] - 14s 785ms/step - loss: 1.1558 - acc: 0.4700 - val_loss: 1.7940 - val_acc: 0.0500\n",
            "Epoch 266/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 1.1203 - acc: 0.5267 - val_loss: 1.4933 - val_acc: 0.2000\n",
            "Epoch 267/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 1.2620 - acc: 0.4235 - val_loss: 1.5775 - val_acc: 0.1000\n",
            "Epoch 268/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.1394 - acc: 0.5233 - val_loss: 1.7041 - val_acc: 0.2000\n",
            "Epoch 269/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 1.2822 - acc: 0.4833 - val_loss: 1.5148 - val_acc: 0.2000\n",
            "Epoch 270/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 1.2318 - acc: 0.4342 - val_loss: 1.5125 - val_acc: 0.3500\n",
            "Epoch 271/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.1012 - acc: 0.5200 - val_loss: 1.5401 - val_acc: 0.2000\n",
            "Epoch 272/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1518 - acc: 0.5018 - val_loss: 1.5431 - val_acc: 0.1500\n",
            "Epoch 273/1000\n",
            "15/15 [==============================] - 13s 770ms/step - loss: 1.1007 - acc: 0.5196 - val_loss: 1.4703 - val_acc: 0.1500\n",
            "Epoch 274/1000\n",
            "15/15 [==============================] - 13s 772ms/step - loss: 1.1379 - acc: 0.5018 - val_loss: 1.1089 - val_acc: 0.3000\n",
            "Epoch 275/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 1.1708 - acc: 0.4840 - val_loss: 1.6097 - val_acc: 0.2500\n",
            "Epoch 276/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 1.2761 - acc: 0.4467 - val_loss: 1.3517 - val_acc: 0.3000\n",
            "Epoch 277/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 1.1705 - acc: 0.4947 - val_loss: 1.5223 - val_acc: 0.3000\n",
            "Epoch 278/1000\n",
            "15/15 [==============================] - 14s 787ms/step - loss: 1.0634 - acc: 0.5300 - val_loss: 1.6767 - val_acc: 0.3000\n",
            "Epoch 279/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 1.1251 - acc: 0.4911 - val_loss: 1.6716 - val_acc: 0.2000\n",
            "Epoch 280/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.2085 - acc: 0.4867 - val_loss: 1.8065 - val_acc: 0.2000\n",
            "Epoch 281/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 1.1771 - acc: 0.5000 - val_loss: 1.5059 - val_acc: 0.2000\n",
            "Epoch 282/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 1.2217 - acc: 0.4804 - val_loss: 1.5607 - val_acc: 0.2500\n",
            "Epoch 283/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.1207 - acc: 0.4833 - val_loss: 1.7842 - val_acc: 0.1500\n",
            "Epoch 284/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 1.1923 - acc: 0.4698 - val_loss: 1.4898 - val_acc: 0.1000\n",
            "Epoch 285/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 1.2238 - acc: 0.4733 - val_loss: 1.6741 - val_acc: 0.1500\n",
            "Epoch 286/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 1.1455 - acc: 0.4840 - val_loss: 1.3247 - val_acc: 0.2500\n",
            "Epoch 287/1000\n",
            "15/15 [==============================] - 13s 736ms/step - loss: 1.1550 - acc: 0.4875 - val_loss: 1.3853 - val_acc: 0.4000\n",
            "Epoch 288/1000\n",
            "15/15 [==============================] - 13s 777ms/step - loss: 1.0437 - acc: 0.5533 - val_loss: 1.4037 - val_acc: 0.1500\n",
            "Epoch 289/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 1.1113 - acc: 0.4967 - val_loss: 1.3741 - val_acc: 0.3000\n",
            "Epoch 290/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 1.1604 - acc: 0.4967 - val_loss: 1.5616 - val_acc: 0.1500\n",
            "Epoch 291/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 1.1397 - acc: 0.4982 - val_loss: 1.2518 - val_acc: 0.5000\n",
            "Epoch 292/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 1.0903 - acc: 0.5231 - val_loss: 1.4613 - val_acc: 0.2000\n",
            "Epoch 293/1000\n",
            "15/15 [==============================] - 13s 772ms/step - loss: 1.0324 - acc: 0.5516 - val_loss: 1.3904 - val_acc: 0.2500\n",
            "Epoch 294/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 1.0312 - acc: 0.5445 - val_loss: 1.5926 - val_acc: 0.2000\n",
            "Epoch 295/1000\n",
            "15/15 [==============================] - 15s 880ms/step - loss: 1.1401 - acc: 0.5267 - val_loss: 1.2949 - val_acc: 0.4000\n",
            "Epoch 296/1000\n",
            "15/15 [==============================] - 15s 834ms/step - loss: 1.0842 - acc: 0.5160 - val_loss: 1.3272 - val_acc: 0.3000\n",
            "Epoch 297/1000\n",
            "15/15 [==============================] - 15s 817ms/step - loss: 1.0879 - acc: 0.5516 - val_loss: 1.7900 - val_acc: 0.1500\n",
            "Epoch 298/1000\n",
            "15/15 [==============================] - 15s 853ms/step - loss: 1.0806 - acc: 0.5338 - val_loss: 1.3128 - val_acc: 0.2500\n",
            "Epoch 299/1000\n",
            "15/15 [==============================] - 16s 899ms/step - loss: 1.1137 - acc: 0.5467 - val_loss: 1.2092 - val_acc: 0.3000\n",
            "Epoch 300/1000\n",
            "15/15 [==============================] - 16s 894ms/step - loss: 1.1270 - acc: 0.5233 - val_loss: 1.6804 - val_acc: 0.2000\n",
            "Epoch 301/1000\n",
            "15/15 [==============================] - 16s 910ms/step - loss: 1.1631 - acc: 0.5067 - val_loss: 1.8503 - val_acc: 0.2000\n",
            "Epoch 302/1000\n",
            "15/15 [==============================] - 16s 892ms/step - loss: 1.1201 - acc: 0.5267 - val_loss: 1.3561 - val_acc: 0.3000\n",
            "Epoch 303/1000\n",
            "15/15 [==============================] - 16s 898ms/step - loss: 1.0851 - acc: 0.5100 - val_loss: 1.4950 - val_acc: 0.2500\n",
            "Epoch 304/1000\n",
            "15/15 [==============================] - 16s 892ms/step - loss: 1.0197 - acc: 0.5700 - val_loss: 1.5276 - val_acc: 0.2000\n",
            "Epoch 305/1000\n",
            "15/15 [==============================] - 16s 893ms/step - loss: 1.0544 - acc: 0.5367 - val_loss: 1.5908 - val_acc: 0.1500\n",
            "Epoch 306/1000\n",
            "15/15 [==============================] - 15s 880ms/step - loss: 1.1123 - acc: 0.5100 - val_loss: 1.1907 - val_acc: 0.3500\n",
            "Epoch 307/1000\n",
            "15/15 [==============================] - 16s 907ms/step - loss: 1.1313 - acc: 0.5033 - val_loss: 1.3864 - val_acc: 0.2500\n",
            "Epoch 308/1000\n",
            "15/15 [==============================] - 15s 853ms/step - loss: 1.1597 - acc: 0.5018 - val_loss: 1.3315 - val_acc: 0.3000\n",
            "Epoch 309/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 1.0998 - acc: 0.5100 - val_loss: 1.1267 - val_acc: 0.3500\n",
            "Epoch 310/1000\n",
            "15/15 [==============================] - 15s 865ms/step - loss: 1.0671 - acc: 0.5300 - val_loss: 1.3150 - val_acc: 0.2000\n",
            "Epoch 311/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.2465 - acc: 0.4733 - val_loss: 1.4309 - val_acc: 0.2500\n",
            "Epoch 312/1000\n",
            "15/15 [==============================] - 16s 900ms/step - loss: 1.1334 - acc: 0.5067 - val_loss: 1.7096 - val_acc: 0.1500\n",
            "Epoch 313/1000\n",
            "15/15 [==============================] - 15s 871ms/step - loss: 1.0921 - acc: 0.5300 - val_loss: 1.2428 - val_acc: 0.4000\n",
            "Epoch 314/1000\n",
            "15/15 [==============================] - 15s 839ms/step - loss: 1.1595 - acc: 0.4947 - val_loss: 1.3785 - val_acc: 0.3000\n",
            "Epoch 315/1000\n",
            "15/15 [==============================] - 15s 866ms/step - loss: 1.0720 - acc: 0.5300 - val_loss: 1.4007 - val_acc: 0.3000\n",
            "Epoch 316/1000\n",
            "15/15 [==============================] - 15s 860ms/step - loss: 1.2321 - acc: 0.5196 - val_loss: 1.7052 - val_acc: 0.0500\n",
            "Epoch 317/1000\n",
            "15/15 [==============================] - 16s 903ms/step - loss: 1.1115 - acc: 0.5100 - val_loss: 1.3183 - val_acc: 0.2000\n",
            "Epoch 318/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 0.9714 - acc: 0.5667 - val_loss: 1.2560 - val_acc: 0.3500\n",
            "Epoch 319/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 1.1276 - acc: 0.4967 - val_loss: 1.0740 - val_acc: 0.3500\n",
            "Epoch 320/1000\n",
            "15/15 [==============================] - 15s 1s/step - loss: 1.0375 - acc: 0.5552 - val_loss: 1.0356 - val_acc: 0.3500\n",
            "Epoch 321/1000\n",
            "15/15 [==============================] - 16s 891ms/step - loss: 1.1297 - acc: 0.5133 - val_loss: 1.3663 - val_acc: 0.2000\n",
            "Epoch 322/1000\n",
            "15/15 [==============================] - 15s 828ms/step - loss: 1.1115 - acc: 0.5231 - val_loss: 1.4025 - val_acc: 0.3500\n",
            "Epoch 323/1000\n",
            "15/15 [==============================] - 15s 878ms/step - loss: 1.1632 - acc: 0.5133 - val_loss: 1.7182 - val_acc: 0.2500\n",
            "Epoch 324/1000\n",
            "15/15 [==============================] - 15s 832ms/step - loss: 1.0636 - acc: 0.5623 - val_loss: 1.2492 - val_acc: 0.3000\n",
            "Epoch 325/1000\n",
            "15/15 [==============================] - 16s 878ms/step - loss: 1.0563 - acc: 0.5433 - val_loss: 1.4414 - val_acc: 0.2000\n",
            "Epoch 326/1000\n",
            "15/15 [==============================] - 15s 850ms/step - loss: 1.0922 - acc: 0.5552 - val_loss: 1.1712 - val_acc: 0.4000\n",
            "Epoch 327/1000\n",
            "15/15 [==============================] - 16s 877ms/step - loss: 1.0798 - acc: 0.4800 - val_loss: 1.3754 - val_acc: 0.2500\n",
            "Epoch 328/1000\n",
            "15/15 [==============================] - 16s 890ms/step - loss: 1.0386 - acc: 0.5400 - val_loss: 1.3866 - val_acc: 0.2000\n",
            "Epoch 329/1000\n",
            "15/15 [==============================] - 16s 892ms/step - loss: 1.1362 - acc: 0.5000 - val_loss: 1.4070 - val_acc: 0.3000\n",
            "Epoch 330/1000\n",
            "15/15 [==============================] - 15s 842ms/step - loss: 1.1738 - acc: 0.4555 - val_loss: 1.4663 - val_acc: 0.2500\n",
            "Epoch 331/1000\n",
            "15/15 [==============================] - 16s 886ms/step - loss: 1.0792 - acc: 0.5367 - val_loss: 1.8557 - val_acc: 0.1500\n",
            "Epoch 332/1000\n",
            "15/15 [==============================] - 15s 856ms/step - loss: 1.1059 - acc: 0.5267 - val_loss: 1.5505 - val_acc: 0.3000\n",
            "Epoch 333/1000\n",
            "15/15 [==============================] - 16s 893ms/step - loss: 1.0842 - acc: 0.5267 - val_loss: 1.4819 - val_acc: 0.4500\n",
            "Epoch 334/1000\n",
            "15/15 [==============================] - 16s 898ms/step - loss: 1.0667 - acc: 0.5400 - val_loss: 1.5299 - val_acc: 0.2500\n",
            "Epoch 335/1000\n",
            "15/15 [==============================] - 15s 881ms/step - loss: 1.0204 - acc: 0.5367 - val_loss: 1.5990 - val_acc: 0.2500\n",
            "Epoch 336/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 1.1259 - acc: 0.5400 - val_loss: 1.6428 - val_acc: 0.2500\n",
            "Epoch 337/1000\n",
            "15/15 [==============================] - 15s 867ms/step - loss: 1.2509 - acc: 0.4433 - val_loss: 1.4966 - val_acc: 0.3000\n",
            "Epoch 338/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.1491 - acc: 0.4933 - val_loss: 1.3826 - val_acc: 0.4000\n",
            "Epoch 339/1000\n",
            "15/15 [==============================] - 15s 871ms/step - loss: 1.0700 - acc: 0.5125 - val_loss: 1.4008 - val_acc: 0.3000\n",
            "Epoch 340/1000\n",
            "15/15 [==============================] - 15s 867ms/step - loss: 1.1572 - acc: 0.4967 - val_loss: 1.2669 - val_acc: 0.3500\n",
            "Epoch 341/1000\n",
            "15/15 [==============================] - 15s 857ms/step - loss: 1.0275 - acc: 0.5767 - val_loss: 1.4043 - val_acc: 0.3000\n",
            "Epoch 342/1000\n",
            "15/15 [==============================] - 15s 856ms/step - loss: 1.0439 - acc: 0.5400 - val_loss: 1.6070 - val_acc: 0.3500\n",
            "Epoch 343/1000\n",
            "15/15 [==============================] - 15s 887ms/step - loss: 1.0220 - acc: 0.5600 - val_loss: 1.4563 - val_acc: 0.4000\n",
            "Epoch 344/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0411 - acc: 0.5133 - val_loss: 1.3400 - val_acc: 0.3000\n",
            "Epoch 345/1000\n",
            "15/15 [==============================] - 15s 862ms/step - loss: 1.0694 - acc: 0.5233 - val_loss: 1.3412 - val_acc: 0.3000\n",
            "Epoch 346/1000\n",
            "15/15 [==============================] - 15s 878ms/step - loss: 1.0289 - acc: 0.5300 - val_loss: 1.4573 - val_acc: 0.2000\n",
            "Epoch 347/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 1.0121 - acc: 0.5267 - val_loss: 1.4548 - val_acc: 0.3000\n",
            "Epoch 348/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 1.1015 - acc: 0.4900 - val_loss: 1.2181 - val_acc: 0.3000\n",
            "Epoch 349/1000\n",
            "15/15 [==============================] - 15s 849ms/step - loss: 1.1433 - acc: 0.5033 - val_loss: 1.4161 - val_acc: 0.2500\n",
            "Epoch 350/1000\n",
            "15/15 [==============================] - 15s 867ms/step - loss: 1.1029 - acc: 0.4867 - val_loss: 1.5491 - val_acc: 0.3000\n",
            "Epoch 351/1000\n",
            "15/15 [==============================] - 15s 834ms/step - loss: 1.1025 - acc: 0.5018 - val_loss: 1.6215 - val_acc: 0.2500\n",
            "Epoch 352/1000\n",
            "15/15 [==============================] - 15s 870ms/step - loss: 1.1236 - acc: 0.5433 - val_loss: 1.5736 - val_acc: 0.1500\n",
            "Epoch 353/1000\n",
            "15/15 [==============================] - 15s 835ms/step - loss: 1.1077 - acc: 0.5018 - val_loss: 1.5949 - val_acc: 0.1500\n",
            "Epoch 354/1000\n",
            "15/15 [==============================] - 15s 871ms/step - loss: 1.1695 - acc: 0.5200 - val_loss: 1.6602 - val_acc: 0.2000\n",
            "Epoch 355/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.0833 - acc: 0.5333 - val_loss: 1.2268 - val_acc: 0.3000\n",
            "Epoch 356/1000\n",
            "15/15 [==============================] - 16s 891ms/step - loss: 1.0421 - acc: 0.5233 - val_loss: 1.3738 - val_acc: 0.3000\n",
            "Epoch 357/1000\n",
            "15/15 [==============================] - 15s 849ms/step - loss: 1.1154 - acc: 0.5125 - val_loss: 1.1420 - val_acc: 0.3500\n",
            "Epoch 358/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.0363 - acc: 0.5800 - val_loss: 1.3609 - val_acc: 0.2500\n",
            "Epoch 359/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 0.9641 - acc: 0.5658 - val_loss: 1.7674 - val_acc: 0.2000\n",
            "Epoch 360/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 1.0930 - acc: 0.5480 - val_loss: 1.4314 - val_acc: 0.2500\n",
            "Epoch 361/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 1.1725 - acc: 0.4800 - val_loss: 1.3418 - val_acc: 0.1500\n",
            "Epoch 362/1000\n",
            "15/15 [==============================] - 15s 882ms/step - loss: 1.1505 - acc: 0.5067 - val_loss: 1.4888 - val_acc: 0.2000\n",
            "Epoch 363/1000\n",
            "15/15 [==============================] - 15s 889ms/step - loss: 1.0144 - acc: 0.5600 - val_loss: 1.2170 - val_acc: 0.4000\n",
            "Epoch 364/1000\n",
            "15/15 [==============================] - 15s 876ms/step - loss: 1.1274 - acc: 0.4667 - val_loss: 1.4418 - val_acc: 0.3500\n",
            "Epoch 365/1000\n",
            "15/15 [==============================] - 15s 880ms/step - loss: 1.0810 - acc: 0.5433 - val_loss: 1.4069 - val_acc: 0.1500\n",
            "Epoch 366/1000\n",
            "15/15 [==============================] - 15s 876ms/step - loss: 1.0543 - acc: 0.5200 - val_loss: 1.2776 - val_acc: 0.3500\n",
            "Epoch 367/1000\n",
            "15/15 [==============================] - 15s 827ms/step - loss: 1.1211 - acc: 0.4947 - val_loss: 1.3067 - val_acc: 0.3500\n",
            "Epoch 368/1000\n",
            "15/15 [==============================] - 15s 814ms/step - loss: 1.0117 - acc: 0.5053 - val_loss: 1.2836 - val_acc: 0.4000\n",
            "Epoch 369/1000\n",
            "15/15 [==============================] - 15s 861ms/step - loss: 1.0316 - acc: 0.5533 - val_loss: 1.3558 - val_acc: 0.2000\n",
            "Epoch 370/1000\n",
            "15/15 [==============================] - 15s 863ms/step - loss: 1.1341 - acc: 0.5567 - val_loss: 1.4311 - val_acc: 0.2000\n",
            "Epoch 371/1000\n",
            "15/15 [==============================] - 15s 862ms/step - loss: 1.0878 - acc: 0.5600 - val_loss: 1.3700 - val_acc: 0.2000\n",
            "Epoch 372/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0905 - acc: 0.4667 - val_loss: 1.4696 - val_acc: 0.2000\n",
            "Epoch 373/1000\n",
            "15/15 [==============================] - 15s 858ms/step - loss: 1.0242 - acc: 0.5700 - val_loss: 1.6137 - val_acc: 0.1500\n",
            "Epoch 374/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0264 - acc: 0.5333 - val_loss: 1.3989 - val_acc: 0.4000\n",
            "Epoch 375/1000\n",
            "15/15 [==============================] - 15s 873ms/step - loss: 1.1427 - acc: 0.5067 - val_loss: 1.4031 - val_acc: 0.2500\n",
            "Epoch 376/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 1.0244 - acc: 0.5533 - val_loss: 1.7741 - val_acc: 0.2500\n",
            "Epoch 377/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 1.1549 - acc: 0.5200 - val_loss: 1.4566 - val_acc: 0.4000\n",
            "Epoch 378/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.1436 - acc: 0.4767 - val_loss: 1.4050 - val_acc: 0.3500\n",
            "Epoch 379/1000\n",
            "15/15 [==============================] - 15s 876ms/step - loss: 1.1037 - acc: 0.5300 - val_loss: 1.3653 - val_acc: 0.2500\n",
            "Epoch 380/1000\n",
            "15/15 [==============================] - 16s 889ms/step - loss: 1.1438 - acc: 0.4567 - val_loss: 1.4751 - val_acc: 0.3000\n",
            "Epoch 381/1000\n",
            "15/15 [==============================] - 15s 846ms/step - loss: 1.0294 - acc: 0.5374 - val_loss: 1.1052 - val_acc: 0.4000\n",
            "Epoch 382/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 1.0497 - acc: 0.4982 - val_loss: 1.6270 - val_acc: 0.1500\n",
            "Epoch 383/1000\n",
            "15/15 [==============================] - 15s 845ms/step - loss: 1.0296 - acc: 0.5516 - val_loss: 1.5439 - val_acc: 0.2000\n",
            "Epoch 384/1000\n",
            "15/15 [==============================] - 15s 832ms/step - loss: 0.9491 - acc: 0.5872 - val_loss: 1.4466 - val_acc: 0.2500\n",
            "Epoch 385/1000\n",
            "15/15 [==============================] - 15s 855ms/step - loss: 1.0555 - acc: 0.5445 - val_loss: 1.4057 - val_acc: 0.3500\n",
            "Epoch 386/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 1.1553 - acc: 0.5133 - val_loss: 1.3291 - val_acc: 0.3000\n",
            "Epoch 387/1000\n",
            "15/15 [==============================] - 16s 885ms/step - loss: 1.1085 - acc: 0.5333 - val_loss: 1.6175 - val_acc: 0.2500\n",
            "Epoch 388/1000\n",
            "15/15 [==============================] - 15s 866ms/step - loss: 1.0379 - acc: 0.6121 - val_loss: 1.2168 - val_acc: 0.4000\n",
            "Epoch 389/1000\n",
            "15/15 [==============================] - 15s 870ms/step - loss: 1.1164 - acc: 0.5233 - val_loss: 1.1574 - val_acc: 0.3500\n",
            "Epoch 390/1000\n",
            "15/15 [==============================] - 15s 861ms/step - loss: 1.0916 - acc: 0.5467 - val_loss: 1.3764 - val_acc: 0.3000\n",
            "Epoch 391/1000\n",
            "15/15 [==============================] - 15s 856ms/step - loss: 1.0185 - acc: 0.5567 - val_loss: 1.3018 - val_acc: 0.3000\n",
            "Epoch 392/1000\n",
            "15/15 [==============================] - 16s 897ms/step - loss: 1.0873 - acc: 0.5267 - val_loss: 1.3583 - val_acc: 0.3000\n",
            "Epoch 393/1000\n",
            "15/15 [==============================] - 16s 890ms/step - loss: 1.0980 - acc: 0.5067 - val_loss: 1.4336 - val_acc: 0.2500\n",
            "Epoch 394/1000\n",
            "15/15 [==============================] - 16s 896ms/step - loss: 1.1632 - acc: 0.4833 - val_loss: 1.4674 - val_acc: 0.2500\n",
            "Epoch 395/1000\n",
            "15/15 [==============================] - 15s 849ms/step - loss: 1.0148 - acc: 0.5587 - val_loss: 1.1870 - val_acc: 0.4000\n",
            "Epoch 396/1000\n",
            "15/15 [==============================] - 15s 867ms/step - loss: 1.0429 - acc: 0.5767 - val_loss: 1.4687 - val_acc: 0.1500\n",
            "Epoch 397/1000\n",
            "15/15 [==============================] - 15s 830ms/step - loss: 0.9796 - acc: 0.5836 - val_loss: 1.5160 - val_acc: 0.3500\n",
            "Epoch 398/1000\n",
            "15/15 [==============================] - 15s 876ms/step - loss: 1.1077 - acc: 0.5167 - val_loss: 1.1051 - val_acc: 0.4500\n",
            "Epoch 399/1000\n",
            "15/15 [==============================] - 15s 880ms/step - loss: 1.0168 - acc: 0.5733 - val_loss: 1.2101 - val_acc: 0.3500\n",
            "Epoch 400/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0659 - acc: 0.5480 - val_loss: 1.3847 - val_acc: 0.3000\n",
            "Epoch 401/1000\n",
            "15/15 [==============================] - 16s 891ms/step - loss: 1.1128 - acc: 0.5000 - val_loss: 1.0915 - val_acc: 0.4000\n",
            "Epoch 402/1000\n",
            "15/15 [==============================] - 15s 863ms/step - loss: 1.0149 - acc: 0.5767 - val_loss: 1.3881 - val_acc: 0.2000\n",
            "Epoch 403/1000\n",
            "15/15 [==============================] - 15s 870ms/step - loss: 1.0284 - acc: 0.5667 - val_loss: 1.2343 - val_acc: 0.3500\n",
            "Epoch 404/1000\n",
            "15/15 [==============================] - 15s 831ms/step - loss: 1.0106 - acc: 0.5658 - val_loss: 1.2290 - val_acc: 0.2000\n",
            "Epoch 405/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.0309 - acc: 0.5433 - val_loss: 1.3836 - val_acc: 0.2000\n",
            "Epoch 406/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 1.0048 - acc: 0.5445 - val_loss: 1.3657 - val_acc: 0.3000\n",
            "Epoch 407/1000\n",
            "15/15 [==============================] - 15s 872ms/step - loss: 1.0715 - acc: 0.5233 - val_loss: 1.2642 - val_acc: 0.3500\n",
            "Epoch 408/1000\n",
            "15/15 [==============================] - 15s 874ms/step - loss: 1.0981 - acc: 0.5367 - val_loss: 1.4475 - val_acc: 0.4000\n",
            "Epoch 409/1000\n",
            "15/15 [==============================] - 15s 817ms/step - loss: 1.0742 - acc: 0.5160 - val_loss: 1.6710 - val_acc: 0.1000\n",
            "Epoch 410/1000\n",
            "15/15 [==============================] - 15s 848ms/step - loss: 1.0533 - acc: 0.5367 - val_loss: 1.3091 - val_acc: 0.3000\n",
            "Epoch 411/1000\n",
            "15/15 [==============================] - 14s 823ms/step - loss: 1.0591 - acc: 0.5516 - val_loss: 1.7033 - val_acc: 0.2500\n",
            "Epoch 412/1000\n",
            "15/15 [==============================] - 15s 886ms/step - loss: 1.0684 - acc: 0.5433 - val_loss: 1.6151 - val_acc: 0.3000\n",
            "Epoch 413/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.0455 - acc: 0.5133 - val_loss: 1.3488 - val_acc: 0.2500\n",
            "Epoch 414/1000\n",
            "15/15 [==============================] - 15s 850ms/step - loss: 1.1555 - acc: 0.4911 - val_loss: 1.2997 - val_acc: 0.4000\n",
            "Epoch 415/1000\n",
            "15/15 [==============================] - 16s 910ms/step - loss: 1.1054 - acc: 0.5267 - val_loss: 1.2418 - val_acc: 0.3500\n",
            "Epoch 416/1000\n",
            "15/15 [==============================] - 16s 912ms/step - loss: 1.0495 - acc: 0.5433 - val_loss: 1.8610 - val_acc: 0.1500\n",
            "Epoch 417/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.0181 - acc: 0.5500 - val_loss: 1.3987 - val_acc: 0.2500\n",
            "Epoch 418/1000\n",
            "15/15 [==============================] - 15s 871ms/step - loss: 1.0249 - acc: 0.5333 - val_loss: 1.4901 - val_acc: 0.2000\n",
            "Epoch 419/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 1.0459 - acc: 0.5374 - val_loss: 1.6022 - val_acc: 0.3500\n",
            "Epoch 420/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.0096 - acc: 0.5200 - val_loss: 1.6184 - val_acc: 0.2000\n",
            "Epoch 421/1000\n",
            "15/15 [==============================] - 15s 864ms/step - loss: 1.0897 - acc: 0.5587 - val_loss: 1.3971 - val_acc: 0.2000\n",
            "Epoch 422/1000\n",
            "15/15 [==============================] - 16s 885ms/step - loss: 1.1022 - acc: 0.4967 - val_loss: 1.3987 - val_acc: 0.3000\n",
            "Epoch 423/1000\n",
            "15/15 [==============================] - 15s 848ms/step - loss: 0.9878 - acc: 0.5872 - val_loss: 1.4154 - val_acc: 0.2500\n",
            "Epoch 424/1000\n",
            "15/15 [==============================] - 16s 881ms/step - loss: 0.9908 - acc: 0.5367 - val_loss: 1.4016 - val_acc: 0.3000\n",
            "Epoch 425/1000\n",
            "15/15 [==============================] - 15s 871ms/step - loss: 1.0900 - acc: 0.5333 - val_loss: 1.3529 - val_acc: 0.3500\n",
            "Epoch 426/1000\n",
            "15/15 [==============================] - 15s 836ms/step - loss: 0.9528 - acc: 0.5623 - val_loss: 1.4692 - val_acc: 0.3000\n",
            "Epoch 427/1000\n",
            "15/15 [==============================] - 16s 892ms/step - loss: 0.9481 - acc: 0.6000 - val_loss: 1.3443 - val_acc: 0.3500\n",
            "Epoch 428/1000\n",
            "15/15 [==============================] - 16s 918ms/step - loss: 0.9870 - acc: 0.5367 - val_loss: 1.3738 - val_acc: 0.2000\n",
            "Epoch 429/1000\n",
            "15/15 [==============================] - 15s 863ms/step - loss: 0.9997 - acc: 0.5516 - val_loss: 1.3565 - val_acc: 0.2500\n",
            "Epoch 430/1000\n",
            "15/15 [==============================] - 15s 853ms/step - loss: 1.0268 - acc: 0.5552 - val_loss: 1.5255 - val_acc: 0.3000\n",
            "Epoch 431/1000\n",
            "15/15 [==============================] - 15s 854ms/step - loss: 1.0475 - acc: 0.5231 - val_loss: 1.1648 - val_acc: 0.4500\n",
            "Epoch 432/1000\n",
            "15/15 [==============================] - 16s 888ms/step - loss: 0.9639 - acc: 0.5600 - val_loss: 1.3237 - val_acc: 0.3000\n",
            "Epoch 433/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0642 - acc: 0.5623 - val_loss: 1.3456 - val_acc: 0.2500\n",
            "Epoch 434/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 1.0289 - acc: 0.5587 - val_loss: 1.4492 - val_acc: 0.3500\n",
            "Epoch 435/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 1.0460 - acc: 0.5567 - val_loss: 1.2345 - val_acc: 0.3000\n",
            "Epoch 436/1000\n",
            "15/15 [==============================] - 15s 837ms/step - loss: 1.0408 - acc: 0.5409 - val_loss: 1.4876 - val_acc: 0.3000\n",
            "Epoch 437/1000\n",
            "15/15 [==============================] - 15s 864ms/step - loss: 1.0139 - acc: 0.5567 - val_loss: 1.4483 - val_acc: 0.3500\n",
            "Epoch 438/1000\n",
            "15/15 [==============================] - 15s 878ms/step - loss: 1.0704 - acc: 0.5200 - val_loss: 1.3668 - val_acc: 0.2500\n",
            "Epoch 439/1000\n",
            "15/15 [==============================] - 16s 887ms/step - loss: 0.9334 - acc: 0.6233 - val_loss: 1.3717 - val_acc: 0.4500\n",
            "Epoch 440/1000\n",
            "15/15 [==============================] - 15s 829ms/step - loss: 1.0403 - acc: 0.5231 - val_loss: 1.5156 - val_acc: 0.3000\n",
            "Epoch 441/1000\n",
            "15/15 [==============================] - 15s 869ms/step - loss: 1.0612 - acc: 0.5500 - val_loss: 1.2929 - val_acc: 0.5000\n",
            "Epoch 442/1000\n",
            "15/15 [==============================] - 15s 869ms/step - loss: 1.0153 - acc: 0.5467 - val_loss: 1.2327 - val_acc: 0.4500\n",
            "Epoch 443/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 1.0792 - acc: 0.5100 - val_loss: 1.4692 - val_acc: 0.2500\n",
            "Epoch 444/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 0.9897 - acc: 0.5480 - val_loss: 1.3429 - val_acc: 0.2500\n",
            "Epoch 445/1000\n",
            "15/15 [==============================] - 15s 843ms/step - loss: 1.0976 - acc: 0.5374 - val_loss: 1.4672 - val_acc: 0.2500\n",
            "Epoch 446/1000\n",
            "15/15 [==============================] - 15s 836ms/step - loss: 1.0343 - acc: 0.5480 - val_loss: 1.1358 - val_acc: 0.5000\n",
            "Epoch 447/1000\n",
            "15/15 [==============================] - 15s 849ms/step - loss: 1.0440 - acc: 0.5367 - val_loss: 1.1637 - val_acc: 0.5000\n",
            "Epoch 448/1000\n",
            "15/15 [==============================] - 15s 836ms/step - loss: 1.0164 - acc: 0.5552 - val_loss: 1.2593 - val_acc: 0.4500\n",
            "Epoch 449/1000\n",
            "15/15 [==============================] - 16s 889ms/step - loss: 1.0126 - acc: 0.5767 - val_loss: 1.4484 - val_acc: 0.4000\n",
            "Epoch 450/1000\n",
            "15/15 [==============================] - 15s 845ms/step - loss: 0.8687 - acc: 0.6085 - val_loss: 1.6831 - val_acc: 0.2000\n",
            "Epoch 451/1000\n",
            "15/15 [==============================] - 15s 851ms/step - loss: 0.9981 - acc: 0.5765 - val_loss: 1.1973 - val_acc: 0.4500\n",
            "Epoch 452/1000\n",
            "15/15 [==============================] - 16s 892ms/step - loss: 1.0758 - acc: 0.5367 - val_loss: 1.1715 - val_acc: 0.3000\n",
            "Epoch 453/1000\n",
            "15/15 [==============================] - 16s 903ms/step - loss: 1.0330 - acc: 0.5433 - val_loss: 1.6002 - val_acc: 0.2500\n",
            "Epoch 454/1000\n",
            "15/15 [==============================] - 15s 864ms/step - loss: 1.0284 - acc: 0.5400 - val_loss: 1.4088 - val_acc: 0.4500\n",
            "Epoch 455/1000\n",
            "15/15 [==============================] - 15s 857ms/step - loss: 1.0727 - acc: 0.5133 - val_loss: 1.5844 - val_acc: 0.3000\n",
            "Epoch 456/1000\n",
            "15/15 [==============================] - 15s 823ms/step - loss: 1.0279 - acc: 0.5500 - val_loss: 1.5714 - val_acc: 0.2500\n",
            "Epoch 457/1000\n",
            "15/15 [==============================] - 15s 810ms/step - loss: 1.0665 - acc: 0.5196 - val_loss: 1.3194 - val_acc: 0.3500\n",
            "Epoch 458/1000\n",
            "15/15 [==============================] - 15s 835ms/step - loss: 1.0714 - acc: 0.5433 - val_loss: 1.3760 - val_acc: 0.3000\n",
            "Epoch 459/1000\n",
            "15/15 [==============================] - 15s 836ms/step - loss: 1.0289 - acc: 0.5467 - val_loss: 1.3011 - val_acc: 0.3500\n",
            "Epoch 460/1000\n",
            "15/15 [==============================] - 15s 851ms/step - loss: 1.0036 - acc: 0.5667 - val_loss: 1.1713 - val_acc: 0.4000\n",
            "Epoch 461/1000\n",
            "15/15 [==============================] - 15s 877ms/step - loss: 0.9737 - acc: 0.5600 - val_loss: 1.7514 - val_acc: 0.1000\n",
            "Epoch 462/1000\n",
            "15/15 [==============================] - 15s 841ms/step - loss: 0.9821 - acc: 0.5943 - val_loss: 1.5749 - val_acc: 0.4000\n",
            "Epoch 463/1000\n",
            "15/15 [==============================] - 15s 877ms/step - loss: 0.9828 - acc: 0.5900 - val_loss: 1.4064 - val_acc: 0.2000\n",
            "Epoch 464/1000\n",
            "15/15 [==============================] - 15s 882ms/step - loss: 1.0735 - acc: 0.5200 - val_loss: 1.3381 - val_acc: 0.3000\n",
            "Epoch 465/1000\n",
            "15/15 [==============================] - 16s 886ms/step - loss: 0.9773 - acc: 0.5600 - val_loss: 1.5677 - val_acc: 0.3500\n",
            "Epoch 466/1000\n",
            "15/15 [==============================] - 15s 861ms/step - loss: 1.0573 - acc: 0.5200 - val_loss: 1.3680 - val_acc: 0.2500\n",
            "Epoch 467/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 1.0883 - acc: 0.5300 - val_loss: 1.3894 - val_acc: 0.4000\n",
            "Epoch 468/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 0.9391 - acc: 0.5667 - val_loss: 1.2472 - val_acc: 0.2500\n",
            "Epoch 469/1000\n",
            "15/15 [==============================] - 15s 865ms/step - loss: 0.9853 - acc: 0.5833 - val_loss: 1.6654 - val_acc: 0.2000\n",
            "Epoch 470/1000\n",
            "15/15 [==============================] - 15s 834ms/step - loss: 0.9771 - acc: 0.5730 - val_loss: 1.6931 - val_acc: 0.1500\n",
            "Epoch 471/1000\n",
            "15/15 [==============================] - 15s 888ms/step - loss: 0.9211 - acc: 0.5700 - val_loss: 1.4215 - val_acc: 0.3000\n",
            "Epoch 472/1000\n",
            "15/15 [==============================] - 15s 877ms/step - loss: 1.0738 - acc: 0.5633 - val_loss: 1.4410 - val_acc: 0.3000\n",
            "Epoch 473/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 0.9618 - acc: 0.5667 - val_loss: 1.0716 - val_acc: 0.5000\n",
            "Epoch 474/1000\n",
            "15/15 [==============================] - 15s 845ms/step - loss: 1.0005 - acc: 0.5733 - val_loss: 1.3718 - val_acc: 0.4500\n",
            "Epoch 475/1000\n",
            "15/15 [==============================] - 14s 811ms/step - loss: 0.9811 - acc: 0.5836 - val_loss: 1.0841 - val_acc: 0.3000\n",
            "Epoch 476/1000\n",
            "15/15 [==============================] - 15s 880ms/step - loss: 0.9316 - acc: 0.5700 - val_loss: 1.2356 - val_acc: 0.4500\n",
            "Epoch 477/1000\n",
            "15/15 [==============================] - 15s 832ms/step - loss: 1.0176 - acc: 0.5730 - val_loss: 1.6203 - val_acc: 0.2500\n",
            "Epoch 478/1000\n",
            "15/15 [==============================] - 16s 875ms/step - loss: 1.0235 - acc: 0.5500 - val_loss: 1.6367 - val_acc: 0.3000\n",
            "Epoch 479/1000\n",
            "15/15 [==============================] - 15s 1s/step - loss: 1.0775 - acc: 0.5338 - val_loss: 1.2425 - val_acc: 0.4000\n",
            "Epoch 480/1000\n",
            "15/15 [==============================] - 15s 837ms/step - loss: 1.0263 - acc: 0.5302 - val_loss: 1.4027 - val_acc: 0.3500\n",
            "Epoch 481/1000\n",
            "15/15 [==============================] - 15s 831ms/step - loss: 1.0328 - acc: 0.5480 - val_loss: 1.4415 - val_acc: 0.3500\n",
            "Epoch 482/1000\n",
            "15/15 [==============================] - 15s 828ms/step - loss: 0.9937 - acc: 0.5552 - val_loss: 1.5080 - val_acc: 0.3500\n",
            "Epoch 483/1000\n",
            "15/15 [==============================] - 15s 884ms/step - loss: 1.0058 - acc: 0.5633 - val_loss: 1.3374 - val_acc: 0.3500\n",
            "Epoch 484/1000\n",
            "15/15 [==============================] - 16s 882ms/step - loss: 1.0174 - acc: 0.5700 - val_loss: 1.2805 - val_acc: 0.4000\n",
            "Epoch 485/1000\n",
            "15/15 [==============================] - 15s 845ms/step - loss: 0.9814 - acc: 0.5836 - val_loss: 1.5954 - val_acc: 0.2500\n",
            "Epoch 486/1000\n",
            "15/15 [==============================] - 16s 899ms/step - loss: 1.0395 - acc: 0.5267 - val_loss: 1.3079 - val_acc: 0.4500\n",
            "Epoch 487/1000\n",
            "15/15 [==============================] - 15s 854ms/step - loss: 1.0086 - acc: 0.5552 - val_loss: 1.5611 - val_acc: 0.2500\n",
            "Epoch 488/1000\n",
            "15/15 [==============================] - 15s 833ms/step - loss: 1.0513 - acc: 0.5160 - val_loss: 1.5839 - val_acc: 0.3500\n",
            "Epoch 489/1000\n",
            "15/15 [==============================] - 15s 860ms/step - loss: 0.9558 - acc: 0.5800 - val_loss: 1.1421 - val_acc: 0.5000\n",
            "Epoch 490/1000\n",
            "15/15 [==============================] - 15s 838ms/step - loss: 1.0375 - acc: 0.5374 - val_loss: 1.4213 - val_acc: 0.1500\n",
            "Epoch 491/1000\n",
            "15/15 [==============================] - 15s 836ms/step - loss: 1.0061 - acc: 0.5374 - val_loss: 1.2969 - val_acc: 0.4000\n",
            "Epoch 492/1000\n",
            "15/15 [==============================] - 15s 823ms/step - loss: 0.9867 - acc: 0.5467 - val_loss: 1.3502 - val_acc: 0.2500\n",
            "Epoch 493/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 1.0296 - acc: 0.5800 - val_loss: 1.0417 - val_acc: 0.5500\n",
            "Epoch 494/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.9910 - acc: 0.5767 - val_loss: 1.2257 - val_acc: 0.5000\n",
            "Epoch 495/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 1.0309 - acc: 0.5633 - val_loss: 1.6347 - val_acc: 0.3000\n",
            "Epoch 496/1000\n",
            "15/15 [==============================] - 15s 856ms/step - loss: 1.0343 - acc: 0.5433 - val_loss: 1.4191 - val_acc: 0.2500\n",
            "Epoch 497/1000\n",
            "15/15 [==============================] - 15s 864ms/step - loss: 0.9750 - acc: 0.5733 - val_loss: 1.1681 - val_acc: 0.5000\n",
            "Epoch 498/1000\n",
            "15/15 [==============================] - 15s 843ms/step - loss: 1.0831 - acc: 0.5623 - val_loss: 1.3723 - val_acc: 0.3000\n",
            "Epoch 499/1000\n",
            "15/15 [==============================] - 15s 862ms/step - loss: 1.1188 - acc: 0.5100 - val_loss: 1.3556 - val_acc: 0.2000\n",
            "Epoch 500/1000\n",
            "15/15 [==============================] - 15s 852ms/step - loss: 1.0028 - acc: 0.5400 - val_loss: 1.2100 - val_acc: 0.3500\n",
            "Epoch 501/1000\n",
            "15/15 [==============================] - 15s 846ms/step - loss: 1.0066 - acc: 0.5400 - val_loss: 1.1037 - val_acc: 0.5000\n",
            "Epoch 502/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.0244 - acc: 0.5800 - val_loss: 1.5584 - val_acc: 0.3000\n",
            "Epoch 503/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 1.0167 - acc: 0.5552 - val_loss: 1.6518 - val_acc: 0.2500\n",
            "Epoch 504/1000\n",
            "15/15 [==============================] - 15s 866ms/step - loss: 1.0278 - acc: 0.5367 - val_loss: 1.5689 - val_acc: 0.3500\n",
            "Epoch 505/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.9365 - acc: 0.5765 - val_loss: 1.1696 - val_acc: 0.4500\n",
            "Epoch 506/1000\n",
            "15/15 [==============================] - 15s 849ms/step - loss: 1.0639 - acc: 0.5600 - val_loss: 1.2746 - val_acc: 0.5000\n",
            "Epoch 507/1000\n",
            "15/15 [==============================] - 14s 818ms/step - loss: 1.0164 - acc: 0.5445 - val_loss: 1.6417 - val_acc: 0.2000\n",
            "Epoch 508/1000\n",
            "15/15 [==============================] - 15s 864ms/step - loss: 1.0289 - acc: 0.5533 - val_loss: 1.3898 - val_acc: 0.3500\n",
            "Epoch 509/1000\n",
            "15/15 [==============================] - 15s 858ms/step - loss: 1.0390 - acc: 0.5433 - val_loss: 1.3859 - val_acc: 0.3000\n",
            "Epoch 510/1000\n",
            "15/15 [==============================] - 15s 826ms/step - loss: 0.9244 - acc: 0.5623 - val_loss: 1.5006 - val_acc: 0.3500\n",
            "Epoch 511/1000\n",
            "15/15 [==============================] - 16s 890ms/step - loss: 0.9789 - acc: 0.5800 - val_loss: 1.4230 - val_acc: 0.3000\n",
            "Epoch 512/1000\n",
            "15/15 [==============================] - 15s 868ms/step - loss: 0.9653 - acc: 0.5500 - val_loss: 1.4464 - val_acc: 0.2500\n",
            "Epoch 513/1000\n",
            "15/15 [==============================] - 15s 837ms/step - loss: 0.9452 - acc: 0.5733 - val_loss: 1.1936 - val_acc: 0.4500\n",
            "Epoch 514/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 1.0134 - acc: 0.5552 - val_loss: 1.5123 - val_acc: 0.3500\n",
            "Epoch 515/1000\n",
            "15/15 [==============================] - 15s 855ms/step - loss: 0.9979 - acc: 0.5433 - val_loss: 1.4605 - val_acc: 0.3500\n",
            "Epoch 516/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 1.0116 - acc: 0.5516 - val_loss: 1.4854 - val_acc: 0.1500\n",
            "Epoch 517/1000\n",
            "15/15 [==============================] - 15s 859ms/step - loss: 0.9442 - acc: 0.6033 - val_loss: 1.2998 - val_acc: 0.4000\n",
            "Epoch 518/1000\n",
            "15/15 [==============================] - 14s 991ms/step - loss: 0.9613 - acc: 0.6050 - val_loss: 1.5202 - val_acc: 0.3000\n",
            "Epoch 519/1000\n",
            "15/15 [==============================] - 15s 853ms/step - loss: 1.0270 - acc: 0.5400 - val_loss: 1.6677 - val_acc: 0.2500\n",
            "Epoch 520/1000\n",
            "15/15 [==============================] - 14s 824ms/step - loss: 1.0219 - acc: 0.5302 - val_loss: 1.5022 - val_acc: 0.3000\n",
            "Epoch 521/1000\n",
            "15/15 [==============================] - 15s 861ms/step - loss: 1.0421 - acc: 0.5133 - val_loss: 1.5432 - val_acc: 0.2500\n",
            "Epoch 522/1000\n",
            "15/15 [==============================] - 15s 828ms/step - loss: 0.9595 - acc: 0.5800 - val_loss: 1.8627 - val_acc: 0.1500\n",
            "Epoch 523/1000\n",
            "15/15 [==============================] - 15s 843ms/step - loss: 0.9874 - acc: 0.5467 - val_loss: 1.4392 - val_acc: 0.3000\n",
            "Epoch 524/1000\n",
            "15/15 [==============================] - 15s 839ms/step - loss: 1.0243 - acc: 0.5302 - val_loss: 1.2201 - val_acc: 0.5500\n",
            "Epoch 525/1000\n",
            "15/15 [==============================] - 14s 787ms/step - loss: 0.9100 - acc: 0.5872 - val_loss: 1.2020 - val_acc: 0.4000\n",
            "Epoch 526/1000\n",
            "15/15 [==============================] - 15s 850ms/step - loss: 1.1019 - acc: 0.5533 - val_loss: 1.3320 - val_acc: 0.3000\n",
            "Epoch 527/1000\n",
            "15/15 [==============================] - 15s 875ms/step - loss: 1.0495 - acc: 0.5067 - val_loss: 1.4227 - val_acc: 0.3500\n",
            "Epoch 528/1000\n",
            "15/15 [==============================] - 15s 835ms/step - loss: 1.0193 - acc: 0.5067 - val_loss: 1.4010 - val_acc: 0.2500\n",
            "Epoch 529/1000\n",
            "15/15 [==============================] - 15s 873ms/step - loss: 0.8991 - acc: 0.6300 - val_loss: 1.3997 - val_acc: 0.4000\n",
            "Epoch 530/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 0.9156 - acc: 0.6067 - val_loss: 1.2984 - val_acc: 0.4500\n",
            "Epoch 531/1000\n",
            "15/15 [==============================] - 15s 843ms/step - loss: 0.9605 - acc: 0.5933 - val_loss: 1.5208 - val_acc: 0.2500\n",
            "Epoch 532/1000\n",
            "15/15 [==============================] - 14s 816ms/step - loss: 1.0383 - acc: 0.5694 - val_loss: 1.3160 - val_acc: 0.4000\n",
            "Epoch 533/1000\n",
            "15/15 [==============================] - 15s 838ms/step - loss: 0.9944 - acc: 0.5667 - val_loss: 1.3114 - val_acc: 0.4500\n",
            "Epoch 534/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 0.9436 - acc: 0.5979 - val_loss: 1.5937 - val_acc: 0.3000\n",
            "Epoch 535/1000\n",
            "15/15 [==============================] - 14s 819ms/step - loss: 0.9451 - acc: 0.6133 - val_loss: 1.4624 - val_acc: 0.4000\n",
            "Epoch 536/1000\n",
            "15/15 [==============================] - 14s 830ms/step - loss: 1.0099 - acc: 0.5900 - val_loss: 1.3011 - val_acc: 0.4000\n",
            "Epoch 537/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.9576 - acc: 0.5872 - val_loss: 1.2575 - val_acc: 0.4000\n",
            "Epoch 538/1000\n",
            "15/15 [==============================] - 14s 812ms/step - loss: 0.9787 - acc: 0.5633 - val_loss: 1.4285 - val_acc: 0.3000\n",
            "Epoch 539/1000\n",
            "15/15 [==============================] - 14s 770ms/step - loss: 0.9380 - acc: 0.5730 - val_loss: 1.1484 - val_acc: 0.4500\n",
            "Epoch 540/1000\n",
            "15/15 [==============================] - 14s 772ms/step - loss: 0.9399 - acc: 0.6014 - val_loss: 1.1692 - val_acc: 0.3500\n",
            "Epoch 541/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 0.9759 - acc: 0.5409 - val_loss: 1.5100 - val_acc: 0.3000\n",
            "Epoch 542/1000\n",
            "15/15 [==============================] - 14s 804ms/step - loss: 0.8697 - acc: 0.6233 - val_loss: 1.2942 - val_acc: 0.3500\n",
            "Epoch 543/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.9787 - acc: 0.5633 - val_loss: 1.5051 - val_acc: 0.3500\n",
            "Epoch 544/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 0.9832 - acc: 0.5587 - val_loss: 1.2947 - val_acc: 0.3500\n",
            "Epoch 545/1000\n",
            "15/15 [==============================] - 14s 767ms/step - loss: 1.0194 - acc: 0.5730 - val_loss: 1.5097 - val_acc: 0.3000\n",
            "Epoch 546/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.9404 - acc: 0.5733 - val_loss: 1.2870 - val_acc: 0.4500\n",
            "Epoch 547/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.9737 - acc: 0.5900 - val_loss: 1.4312 - val_acc: 0.2500\n",
            "Epoch 548/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 0.9361 - acc: 0.5933 - val_loss: 1.8495 - val_acc: 0.2000\n",
            "Epoch 549/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.9753 - acc: 0.5567 - val_loss: 1.3362 - val_acc: 0.3500\n",
            "Epoch 550/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9610 - acc: 0.5800 - val_loss: 1.3508 - val_acc: 0.3000\n",
            "Epoch 551/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 1.0777 - acc: 0.5000 - val_loss: 1.1060 - val_acc: 0.4000\n",
            "Epoch 552/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.9591 - acc: 0.6033 - val_loss: 1.3829 - val_acc: 0.3500\n",
            "Epoch 553/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 0.9533 - acc: 0.5943 - val_loss: 1.3568 - val_acc: 0.4000\n",
            "Epoch 554/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 0.9682 - acc: 0.5801 - val_loss: 1.6896 - val_acc: 0.3500\n",
            "Epoch 555/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.9196 - acc: 0.5765 - val_loss: 1.3909 - val_acc: 0.3500\n",
            "Epoch 556/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.9906 - acc: 0.5567 - val_loss: 1.0680 - val_acc: 0.4500\n",
            "Epoch 557/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.9705 - acc: 0.5967 - val_loss: 1.4220 - val_acc: 0.3000\n",
            "Epoch 558/1000\n",
            "15/15 [==============================] - 14s 775ms/step - loss: 0.9886 - acc: 0.5587 - val_loss: 1.4493 - val_acc: 0.2500\n",
            "Epoch 559/1000\n",
            "15/15 [==============================] - 14s 770ms/step - loss: 1.0048 - acc: 0.5658 - val_loss: 1.3736 - val_acc: 0.3500\n",
            "Epoch 560/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.9717 - acc: 0.5500 - val_loss: 1.6323 - val_acc: 0.3000\n",
            "Epoch 561/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 1.0089 - acc: 0.5733 - val_loss: 1.1478 - val_acc: 0.4000\n",
            "Epoch 562/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.9014 - acc: 0.6085 - val_loss: 1.3869 - val_acc: 0.3500\n",
            "Epoch 563/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 0.9729 - acc: 0.5533 - val_loss: 1.4837 - val_acc: 0.2000\n",
            "Epoch 564/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 0.9543 - acc: 0.5933 - val_loss: 1.2721 - val_acc: 0.4000\n",
            "Epoch 565/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 1.0372 - acc: 0.5445 - val_loss: 1.3169 - val_acc: 0.4000\n",
            "Epoch 566/1000\n",
            "15/15 [==============================] - 13s 928ms/step - loss: 0.9165 - acc: 0.6085 - val_loss: 1.3941 - val_acc: 0.3000\n",
            "Epoch 567/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 1.0147 - acc: 0.5567 - val_loss: 1.2051 - val_acc: 0.4000\n",
            "Epoch 568/1000\n",
            "15/15 [==============================] - 13s 926ms/step - loss: 1.0198 - acc: 0.5694 - val_loss: 1.3028 - val_acc: 0.4500\n",
            "Epoch 569/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 1.0265 - acc: 0.5700 - val_loss: 1.6502 - val_acc: 0.3000\n",
            "Epoch 570/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 0.9630 - acc: 0.5623 - val_loss: 1.5965 - val_acc: 0.2500\n",
            "Epoch 571/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 0.9248 - acc: 0.5667 - val_loss: 1.5266 - val_acc: 0.3500\n",
            "Epoch 572/1000\n",
            "15/15 [==============================] - 14s 787ms/step - loss: 0.9409 - acc: 0.5943 - val_loss: 1.5053 - val_acc: 0.3000\n",
            "Epoch 573/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 1.0013 - acc: 0.5633 - val_loss: 1.4651 - val_acc: 0.3500\n",
            "Epoch 574/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 1.0732 - acc: 0.5233 - val_loss: 1.5896 - val_acc: 0.3000\n",
            "Epoch 575/1000\n",
            "15/15 [==============================] - 14s 821ms/step - loss: 0.8985 - acc: 0.6133 - val_loss: 1.5975 - val_acc: 0.2500\n",
            "Epoch 576/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 1.0325 - acc: 0.5233 - val_loss: 1.6712 - val_acc: 0.3000\n",
            "Epoch 577/1000\n",
            "15/15 [==============================] - 14s 803ms/step - loss: 0.9857 - acc: 0.5567 - val_loss: 1.5188 - val_acc: 0.3500\n",
            "Epoch 578/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9395 - acc: 0.6085 - val_loss: 1.4120 - val_acc: 0.3500\n",
            "Epoch 579/1000\n",
            "15/15 [==============================] - 15s 826ms/step - loss: 0.9375 - acc: 0.5800 - val_loss: 1.3370 - val_acc: 0.3000\n",
            "Epoch 580/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9780 - acc: 0.5800 - val_loss: 1.4211 - val_acc: 0.3000\n",
            "Epoch 581/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 1.0684 - acc: 0.5433 - val_loss: 1.6704 - val_acc: 0.2500\n",
            "Epoch 582/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.9805 - acc: 0.5633 - val_loss: 1.3560 - val_acc: 0.4000\n",
            "Epoch 583/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 1.0408 - acc: 0.5400 - val_loss: 1.2610 - val_acc: 0.3500\n",
            "Epoch 584/1000\n",
            "15/15 [==============================] - 14s 798ms/step - loss: 0.9300 - acc: 0.5900 - val_loss: 1.4393 - val_acc: 0.3500\n",
            "Epoch 585/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.9717 - acc: 0.5667 - val_loss: 1.4548 - val_acc: 0.2500\n",
            "Epoch 586/1000\n",
            "15/15 [==============================] - 14s 804ms/step - loss: 0.9517 - acc: 0.5633 - val_loss: 1.6757 - val_acc: 0.1500\n",
            "Epoch 587/1000\n",
            "15/15 [==============================] - 13s 782ms/step - loss: 1.0068 - acc: 0.5196 - val_loss: 1.1850 - val_acc: 0.4500\n",
            "Epoch 588/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 0.9368 - acc: 0.5867 - val_loss: 1.3914 - val_acc: 0.3000\n",
            "Epoch 589/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9273 - acc: 0.5667 - val_loss: 1.2105 - val_acc: 0.4500\n",
            "Epoch 590/1000\n",
            "15/15 [==============================] - 14s 819ms/step - loss: 0.9975 - acc: 0.5633 - val_loss: 1.3601 - val_acc: 0.2500\n",
            "Epoch 591/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.9352 - acc: 0.5800 - val_loss: 1.2782 - val_acc: 0.4000\n",
            "Epoch 592/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.9128 - acc: 0.5587 - val_loss: 1.4905 - val_acc: 0.2500\n",
            "Epoch 593/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.8671 - acc: 0.6067 - val_loss: 1.5556 - val_acc: 0.1500\n",
            "Epoch 594/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.9286 - acc: 0.5867 - val_loss: 1.3370 - val_acc: 0.3500\n",
            "Epoch 595/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.9675 - acc: 0.5967 - val_loss: 1.6005 - val_acc: 0.3000\n",
            "Epoch 596/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 0.9878 - acc: 0.5623 - val_loss: 1.5345 - val_acc: 0.4000\n",
            "Epoch 597/1000\n",
            "15/15 [==============================] - 15s 828ms/step - loss: 0.8766 - acc: 0.5867 - val_loss: 1.4753 - val_acc: 0.2500\n",
            "Epoch 598/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9739 - acc: 0.5733 - val_loss: 1.4970 - val_acc: 0.4000\n",
            "Epoch 599/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.9992 - acc: 0.5765 - val_loss: 1.4352 - val_acc: 0.3500\n",
            "Epoch 600/1000\n",
            "15/15 [==============================] - 14s 944ms/step - loss: 0.9421 - acc: 0.5587 - val_loss: 1.3208 - val_acc: 0.4500\n",
            "Epoch 601/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9916 - acc: 0.5767 - val_loss: 1.2503 - val_acc: 0.4500\n",
            "Epoch 602/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 1.0136 - acc: 0.5500 - val_loss: 1.2812 - val_acc: 0.4000\n",
            "Epoch 603/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 1.0011 - acc: 0.5667 - val_loss: 1.6052 - val_acc: 0.2500\n",
            "Epoch 604/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 1.0364 - acc: 0.5100 - val_loss: 1.6600 - val_acc: 0.3000\n",
            "Epoch 605/1000\n",
            "15/15 [==============================] - 13s 933ms/step - loss: 0.9361 - acc: 0.5979 - val_loss: 1.5163 - val_acc: 0.3500\n",
            "Epoch 606/1000\n",
            "15/15 [==============================] - 14s 951ms/step - loss: 0.9207 - acc: 0.6085 - val_loss: 1.6465 - val_acc: 0.3000\n",
            "Epoch 607/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9249 - acc: 0.5767 - val_loss: 1.3621 - val_acc: 0.5000\n",
            "Epoch 608/1000\n",
            "15/15 [==============================] - 14s 772ms/step - loss: 0.9113 - acc: 0.5979 - val_loss: 1.5600 - val_acc: 0.2500\n",
            "Epoch 609/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 1.0967 - acc: 0.5400 - val_loss: 1.3817 - val_acc: 0.3500\n",
            "Epoch 610/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9189 - acc: 0.5867 - val_loss: 1.2810 - val_acc: 0.3000\n",
            "Epoch 611/1000\n",
            "15/15 [==============================] - 14s 804ms/step - loss: 0.9902 - acc: 0.5500 - val_loss: 1.2242 - val_acc: 0.5500\n",
            "Epoch 612/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 0.9908 - acc: 0.5567 - val_loss: 1.6797 - val_acc: 0.2500\n",
            "Epoch 613/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 0.9283 - acc: 0.6085 - val_loss: 1.4421 - val_acc: 0.3000\n",
            "Epoch 614/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 0.9294 - acc: 0.5933 - val_loss: 1.4176 - val_acc: 0.3500\n",
            "Epoch 615/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 0.9654 - acc: 0.5633 - val_loss: 1.5115 - val_acc: 0.4000\n",
            "Epoch 616/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 0.9809 - acc: 0.6121 - val_loss: 1.4872 - val_acc: 0.3500\n",
            "Epoch 617/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9674 - acc: 0.5967 - val_loss: 1.3484 - val_acc: 0.4000\n",
            "Epoch 618/1000\n",
            "15/15 [==============================] - 14s 798ms/step - loss: 0.9172 - acc: 0.6200 - val_loss: 1.2727 - val_acc: 0.4000\n",
            "Epoch 619/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 1.0082 - acc: 0.5733 - val_loss: 1.3722 - val_acc: 0.3500\n",
            "Epoch 620/1000\n",
            "15/15 [==============================] - 13s 924ms/step - loss: 1.0765 - acc: 0.5160 - val_loss: 1.0620 - val_acc: 0.4500\n",
            "Epoch 621/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 1.0261 - acc: 0.5409 - val_loss: 1.3705 - val_acc: 0.4000\n",
            "Epoch 622/1000\n",
            "15/15 [==============================] - 13s 762ms/step - loss: 0.9478 - acc: 0.5801 - val_loss: 1.4197 - val_acc: 0.3000\n",
            "Epoch 623/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 1.0653 - acc: 0.5552 - val_loss: 1.7033 - val_acc: 0.2000\n",
            "Epoch 624/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.9995 - acc: 0.5667 - val_loss: 1.3404 - val_acc: 0.3500\n",
            "Epoch 625/1000\n",
            "15/15 [==============================] - 14s 820ms/step - loss: 0.9105 - acc: 0.5933 - val_loss: 1.4578 - val_acc: 0.3000\n",
            "Epoch 626/1000\n",
            "15/15 [==============================] - 14s 777ms/step - loss: 0.9739 - acc: 0.5833 - val_loss: 1.2472 - val_acc: 0.3500\n",
            "Epoch 627/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 1.0118 - acc: 0.5567 - val_loss: 1.1038 - val_acc: 0.4500\n",
            "Epoch 628/1000\n",
            "15/15 [==============================] - 14s 768ms/step - loss: 0.9908 - acc: 0.5587 - val_loss: 1.3484 - val_acc: 0.3000\n",
            "Epoch 629/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 0.9697 - acc: 0.5943 - val_loss: 1.4655 - val_acc: 0.3500\n",
            "Epoch 630/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.9370 - acc: 0.5733 - val_loss: 1.2022 - val_acc: 0.3500\n",
            "Epoch 631/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8362 - acc: 0.6477 - val_loss: 1.6194 - val_acc: 0.3000\n",
            "Epoch 632/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.9380 - acc: 0.6033 - val_loss: 1.4358 - val_acc: 0.3500\n",
            "Epoch 633/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.9126 - acc: 0.6233 - val_loss: 1.2018 - val_acc: 0.4000\n",
            "Epoch 634/1000\n",
            "15/15 [==============================] - 13s 930ms/step - loss: 0.9026 - acc: 0.5979 - val_loss: 1.4783 - val_acc: 0.2000\n",
            "Epoch 635/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.9520 - acc: 0.5967 - val_loss: 1.3148 - val_acc: 0.3500\n",
            "Epoch 636/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.9601 - acc: 0.5967 - val_loss: 1.7158 - val_acc: 0.2000\n",
            "Epoch 637/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9587 - acc: 0.5533 - val_loss: 1.3156 - val_acc: 0.4500\n",
            "Epoch 638/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 0.9212 - acc: 0.5801 - val_loss: 1.3804 - val_acc: 0.3500\n",
            "Epoch 639/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 0.9338 - acc: 0.5872 - val_loss: 1.4781 - val_acc: 0.2500\n",
            "Epoch 640/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.9134 - acc: 0.5867 - val_loss: 1.3889 - val_acc: 0.5000\n",
            "Epoch 641/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 0.9810 - acc: 0.5267 - val_loss: 1.3940 - val_acc: 0.3500\n",
            "Epoch 642/1000\n",
            "15/15 [==============================] - 14s 800ms/step - loss: 0.9575 - acc: 0.5900 - val_loss: 1.4477 - val_acc: 0.3500\n",
            "Epoch 643/1000\n",
            "15/15 [==============================] - 14s 772ms/step - loss: 0.9595 - acc: 0.5836 - val_loss: 1.3044 - val_acc: 0.4000\n",
            "Epoch 644/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.8932 - acc: 0.5943 - val_loss: 1.6073 - val_acc: 0.2500\n",
            "Epoch 645/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 0.9715 - acc: 0.5767 - val_loss: 1.5655 - val_acc: 0.2500\n",
            "Epoch 646/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.9633 - acc: 0.5943 - val_loss: 1.4116 - val_acc: 0.4000\n",
            "Epoch 647/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 0.9458 - acc: 0.5907 - val_loss: 1.5369 - val_acc: 0.1500\n",
            "Epoch 648/1000\n",
            "15/15 [==============================] - 14s 777ms/step - loss: 0.9641 - acc: 0.5801 - val_loss: 1.5997 - val_acc: 0.1500\n",
            "Epoch 649/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.9308 - acc: 0.5800 - val_loss: 1.2183 - val_acc: 0.5000\n",
            "Epoch 650/1000\n",
            "15/15 [==============================] - 14s 811ms/step - loss: 0.9825 - acc: 0.5533 - val_loss: 1.3379 - val_acc: 0.4500\n",
            "Epoch 651/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9357 - acc: 0.5933 - val_loss: 1.1674 - val_acc: 0.4000\n",
            "Epoch 652/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 1.0044 - acc: 0.5400 - val_loss: 1.6564 - val_acc: 0.2500\n",
            "Epoch 653/1000\n",
            "15/15 [==============================] - 13s 929ms/step - loss: 0.9441 - acc: 0.5872 - val_loss: 1.2131 - val_acc: 0.3500\n",
            "Epoch 654/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 0.9939 - acc: 0.5833 - val_loss: 1.4570 - val_acc: 0.4000\n",
            "Epoch 655/1000\n",
            "15/15 [==============================] - 14s 775ms/step - loss: 0.9485 - acc: 0.5623 - val_loss: 1.3899 - val_acc: 0.3000\n",
            "Epoch 656/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9412 - acc: 0.5967 - val_loss: 1.6719 - val_acc: 0.3500\n",
            "Epoch 657/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.9537 - acc: 0.6085 - val_loss: 1.3528 - val_acc: 0.3000\n",
            "Epoch 658/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.9408 - acc: 0.6033 - val_loss: 1.8153 - val_acc: 0.1000\n",
            "Epoch 659/1000\n",
            "15/15 [==============================] - 13s 771ms/step - loss: 0.8959 - acc: 0.5801 - val_loss: 1.5864 - val_acc: 0.3000\n",
            "Epoch 660/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.9348 - acc: 0.5801 - val_loss: 1.4850 - val_acc: 0.3500\n",
            "Epoch 661/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 0.8755 - acc: 0.6299 - val_loss: 1.6217 - val_acc: 0.2500\n",
            "Epoch 662/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 0.9457 - acc: 0.5836 - val_loss: 1.4870 - val_acc: 0.3000\n",
            "Epoch 663/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.9672 - acc: 0.5700 - val_loss: 1.3701 - val_acc: 0.2500\n",
            "Epoch 664/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 1.0022 - acc: 0.5907 - val_loss: 1.5887 - val_acc: 0.2500\n",
            "Epoch 665/1000\n",
            "15/15 [==============================] - 14s 825ms/step - loss: 0.9306 - acc: 0.5600 - val_loss: 1.5410 - val_acc: 0.3000\n",
            "Epoch 666/1000\n",
            "15/15 [==============================] - 15s 835ms/step - loss: 0.9712 - acc: 0.5833 - val_loss: 1.4720 - val_acc: 0.2500\n",
            "Epoch 667/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.9156 - acc: 0.5979 - val_loss: 1.6345 - val_acc: 0.2500\n",
            "Epoch 668/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.9038 - acc: 0.6100 - val_loss: 1.2363 - val_acc: 0.3500\n",
            "Epoch 669/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 0.9107 - acc: 0.5907 - val_loss: 1.3132 - val_acc: 0.3500\n",
            "Epoch 670/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.8390 - acc: 0.6533 - val_loss: 0.8876 - val_acc: 0.6500\n",
            "Epoch 671/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9442 - acc: 0.6014 - val_loss: 1.6445 - val_acc: 0.2000\n",
            "Epoch 672/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.9319 - acc: 0.5867 - val_loss: 1.6266 - val_acc: 0.2500\n",
            "Epoch 673/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 0.9146 - acc: 0.5516 - val_loss: 1.4029 - val_acc: 0.3000\n",
            "Epoch 674/1000\n",
            "15/15 [==============================] - 14s 785ms/step - loss: 0.9460 - acc: 0.5872 - val_loss: 1.3260 - val_acc: 0.4500\n",
            "Epoch 675/1000\n",
            "15/15 [==============================] - 15s 826ms/step - loss: 1.0018 - acc: 0.5633 - val_loss: 1.4331 - val_acc: 0.2500\n",
            "Epoch 676/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9564 - acc: 0.5733 - val_loss: 1.4325 - val_acc: 0.3500\n",
            "Epoch 677/1000\n",
            "15/15 [==============================] - 14s 819ms/step - loss: 0.9442 - acc: 0.5800 - val_loss: 1.1850 - val_acc: 0.4500\n",
            "Epoch 678/1000\n",
            "15/15 [==============================] - 14s 814ms/step - loss: 0.9937 - acc: 0.6033 - val_loss: 1.1574 - val_acc: 0.4000\n",
            "Epoch 679/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.9546 - acc: 0.5943 - val_loss: 1.4867 - val_acc: 0.3000\n",
            "Epoch 680/1000\n",
            "15/15 [==============================] - 15s 833ms/step - loss: 0.9095 - acc: 0.5967 - val_loss: 1.2304 - val_acc: 0.4500\n",
            "Epoch 681/1000\n",
            "15/15 [==============================] - 15s 844ms/step - loss: 0.9026 - acc: 0.6233 - val_loss: 1.2417 - val_acc: 0.3000\n",
            "Epoch 682/1000\n",
            "15/15 [==============================] - 14s 785ms/step - loss: 0.9615 - acc: 0.5836 - val_loss: 1.2002 - val_acc: 0.4500\n",
            "Epoch 683/1000\n",
            "15/15 [==============================] - 14s 821ms/step - loss: 0.9667 - acc: 0.5733 - val_loss: 1.5019 - val_acc: 0.3500\n",
            "Epoch 684/1000\n",
            "15/15 [==============================] - 14s 790ms/step - loss: 0.9203 - acc: 0.6014 - val_loss: 1.2091 - val_acc: 0.4000\n",
            "Epoch 685/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.8876 - acc: 0.6121 - val_loss: 1.5615 - val_acc: 0.2500\n",
            "Epoch 686/1000\n",
            "15/15 [==============================] - 15s 845ms/step - loss: 0.9289 - acc: 0.5833 - val_loss: 1.5872 - val_acc: 0.3000\n",
            "Epoch 687/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.8765 - acc: 0.6050 - val_loss: 1.2323 - val_acc: 0.4500\n",
            "Epoch 688/1000\n",
            "15/15 [==============================] - 15s 838ms/step - loss: 0.9392 - acc: 0.5833 - val_loss: 1.2212 - val_acc: 0.5500\n",
            "Epoch 689/1000\n",
            "15/15 [==============================] - 14s 773ms/step - loss: 0.8432 - acc: 0.6192 - val_loss: 1.2846 - val_acc: 0.3500\n",
            "Epoch 690/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 0.8441 - acc: 0.6335 - val_loss: 1.2131 - val_acc: 0.3500\n",
            "Epoch 691/1000\n",
            "15/15 [==============================] - 14s 817ms/step - loss: 0.9528 - acc: 0.5633 - val_loss: 1.2483 - val_acc: 0.4000\n",
            "Epoch 692/1000\n",
            "15/15 [==============================] - 15s 834ms/step - loss: 0.9618 - acc: 0.5667 - val_loss: 1.1058 - val_acc: 0.5000\n",
            "Epoch 693/1000\n",
            "15/15 [==============================] - 15s 830ms/step - loss: 0.9447 - acc: 0.5767 - val_loss: 1.3028 - val_acc: 0.2500\n",
            "Epoch 694/1000\n",
            "15/15 [==============================] - 15s 846ms/step - loss: 0.8791 - acc: 0.6100 - val_loss: 1.0543 - val_acc: 0.5500\n",
            "Epoch 695/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 0.9076 - acc: 0.6192 - val_loss: 1.3473 - val_acc: 0.4500\n",
            "Epoch 696/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 1.0286 - acc: 0.5409 - val_loss: 1.3415 - val_acc: 0.3500\n",
            "Epoch 697/1000\n",
            "15/15 [==============================] - 14s 765ms/step - loss: 0.9538 - acc: 0.5658 - val_loss: 1.2155 - val_acc: 0.4000\n",
            "Epoch 698/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 0.9512 - acc: 0.6000 - val_loss: 1.3547 - val_acc: 0.5000\n",
            "Epoch 699/1000\n",
            "15/15 [==============================] - 14s 814ms/step - loss: 0.9530 - acc: 0.5767 - val_loss: 1.5333 - val_acc: 0.3000\n",
            "Epoch 700/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9704 - acc: 0.5467 - val_loss: 1.4438 - val_acc: 0.4500\n",
            "Epoch 701/1000\n",
            "15/15 [==============================] - 14s 774ms/step - loss: 0.8720 - acc: 0.6050 - val_loss: 1.3532 - val_acc: 0.3500\n",
            "Epoch 702/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.7973 - acc: 0.6548 - val_loss: 1.5805 - val_acc: 0.3000\n",
            "Epoch 703/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 0.9502 - acc: 0.5867 - val_loss: 1.5015 - val_acc: 0.3500\n",
            "Epoch 704/1000\n",
            "15/15 [==============================] - 14s 787ms/step - loss: 1.0454 - acc: 0.4804 - val_loss: 1.1946 - val_acc: 0.5000\n",
            "Epoch 705/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.8868 - acc: 0.5800 - val_loss: 1.6208 - val_acc: 0.2500\n",
            "Epoch 706/1000\n",
            "15/15 [==============================] - 13s 770ms/step - loss: 0.9500 - acc: 0.5694 - val_loss: 1.5154 - val_acc: 0.2500\n",
            "Epoch 707/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.9142 - acc: 0.5667 - val_loss: 1.4160 - val_acc: 0.4000\n",
            "Epoch 708/1000\n",
            "15/15 [==============================] - 14s 812ms/step - loss: 0.9159 - acc: 0.5833 - val_loss: 1.4337 - val_acc: 0.2500\n",
            "Epoch 709/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.9632 - acc: 0.5658 - val_loss: 1.3864 - val_acc: 0.3000\n",
            "Epoch 710/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.8978 - acc: 0.5700 - val_loss: 1.2499 - val_acc: 0.5000\n",
            "Epoch 711/1000\n",
            "15/15 [==============================] - 14s 755ms/step - loss: 0.9156 - acc: 0.5943 - val_loss: 1.6001 - val_acc: 0.2500\n",
            "Epoch 712/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.9670 - acc: 0.6014 - val_loss: 1.2590 - val_acc: 0.5000\n",
            "Epoch 713/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 1.0409 - acc: 0.5658 - val_loss: 1.4546 - val_acc: 0.3000\n",
            "Epoch 714/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.9387 - acc: 0.5800 - val_loss: 1.4549 - val_acc: 0.4500\n",
            "Epoch 715/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9400 - acc: 0.5900 - val_loss: 1.1954 - val_acc: 0.3500\n",
            "Epoch 716/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.9499 - acc: 0.5300 - val_loss: 1.4587 - val_acc: 0.3000\n",
            "Epoch 717/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.8990 - acc: 0.6333 - val_loss: 1.3799 - val_acc: 0.2500\n",
            "Epoch 718/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 0.9023 - acc: 0.5943 - val_loss: 1.3115 - val_acc: 0.3500\n",
            "Epoch 719/1000\n",
            "15/15 [==============================] - 14s 811ms/step - loss: 0.8672 - acc: 0.6167 - val_loss: 1.2909 - val_acc: 0.4500\n",
            "Epoch 720/1000\n",
            "15/15 [==============================] - 14s 812ms/step - loss: 0.9197 - acc: 0.6133 - val_loss: 1.0802 - val_acc: 0.5000\n",
            "Epoch 721/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 1.0019 - acc: 0.5765 - val_loss: 1.4730 - val_acc: 0.2000\n",
            "Epoch 722/1000\n",
            "15/15 [==============================] - 14s 767ms/step - loss: 0.8852 - acc: 0.6263 - val_loss: 1.1757 - val_acc: 0.4000\n",
            "Epoch 723/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.9490 - acc: 0.5733 - val_loss: 1.2723 - val_acc: 0.3000\n",
            "Epoch 724/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.9513 - acc: 0.5767 - val_loss: 1.3827 - val_acc: 0.3500\n",
            "Epoch 725/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.9019 - acc: 0.6133 - val_loss: 1.4382 - val_acc: 0.3000\n",
            "Epoch 726/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 0.9363 - acc: 0.6067 - val_loss: 1.5908 - val_acc: 0.3000\n",
            "Epoch 727/1000\n",
            "15/15 [==============================] - 13s 929ms/step - loss: 0.8939 - acc: 0.6121 - val_loss: 1.1389 - val_acc: 0.5500\n",
            "Epoch 728/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 0.9399 - acc: 0.6121 - val_loss: 1.4532 - val_acc: 0.3000\n",
            "Epoch 729/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 0.9303 - acc: 0.5979 - val_loss: 1.5797 - val_acc: 0.3000\n",
            "Epoch 730/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 0.9603 - acc: 0.5979 - val_loss: 1.6771 - val_acc: 0.3000\n",
            "Epoch 731/1000\n",
            "15/15 [==============================] - 14s 812ms/step - loss: 0.9575 - acc: 0.6100 - val_loss: 1.1041 - val_acc: 0.5000\n",
            "Epoch 732/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9346 - acc: 0.5833 - val_loss: 1.2195 - val_acc: 0.4000\n",
            "Epoch 733/1000\n",
            "15/15 [==============================] - 14s 778ms/step - loss: 0.9648 - acc: 0.5907 - val_loss: 1.6534 - val_acc: 0.3500\n",
            "Epoch 734/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 0.8782 - acc: 0.6533 - val_loss: 1.1643 - val_acc: 0.3500\n",
            "Epoch 735/1000\n",
            "15/15 [==============================] - 14s 804ms/step - loss: 0.9123 - acc: 0.5900 - val_loss: 1.6236 - val_acc: 0.0500\n",
            "Epoch 736/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.8464 - acc: 0.6400 - val_loss: 1.4246 - val_acc: 0.2000\n",
            "Epoch 737/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 0.9723 - acc: 0.5480 - val_loss: 1.1362 - val_acc: 0.4000\n",
            "Epoch 738/1000\n",
            "15/15 [==============================] - 14s 773ms/step - loss: 1.0563 - acc: 0.5409 - val_loss: 1.3001 - val_acc: 0.3000\n",
            "Epoch 739/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9760 - acc: 0.5800 - val_loss: 1.2711 - val_acc: 0.4000\n",
            "Epoch 740/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 0.9097 - acc: 0.6085 - val_loss: 1.2470 - val_acc: 0.4000\n",
            "Epoch 741/1000\n",
            "15/15 [==============================] - 14s 765ms/step - loss: 0.9418 - acc: 0.5872 - val_loss: 1.3376 - val_acc: 0.3000\n",
            "Epoch 742/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 0.9050 - acc: 0.5733 - val_loss: 1.7113 - val_acc: 0.2500\n",
            "Epoch 743/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 0.8664 - acc: 0.6157 - val_loss: 1.3932 - val_acc: 0.4000\n",
            "Epoch 744/1000\n",
            "15/15 [==============================] - 14s 823ms/step - loss: 0.9568 - acc: 0.5533 - val_loss: 1.6023 - val_acc: 0.1500\n",
            "Epoch 745/1000\n",
            "15/15 [==============================] - 14s 811ms/step - loss: 0.9049 - acc: 0.6233 - val_loss: 1.6097 - val_acc: 0.2000\n",
            "Epoch 746/1000\n",
            "15/15 [==============================] - 14s 803ms/step - loss: 0.8839 - acc: 0.6167 - val_loss: 0.9034 - val_acc: 0.4500\n",
            "Epoch 747/1000\n",
            "15/15 [==============================] - 13s 770ms/step - loss: 0.8359 - acc: 0.6299 - val_loss: 1.0445 - val_acc: 0.6000\n",
            "Epoch 748/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 0.9380 - acc: 0.5979 - val_loss: 1.4177 - val_acc: 0.3000\n",
            "Epoch 749/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.9464 - acc: 0.5867 - val_loss: 1.3810 - val_acc: 0.4000\n",
            "Epoch 750/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 0.9576 - acc: 0.5867 - val_loss: 1.2669 - val_acc: 0.4000\n",
            "Epoch 751/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 0.8621 - acc: 0.6370 - val_loss: 1.0801 - val_acc: 0.5000\n",
            "Epoch 752/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 0.9608 - acc: 0.5516 - val_loss: 1.2069 - val_acc: 0.4500\n",
            "Epoch 753/1000\n",
            "15/15 [==============================] - 14s 768ms/step - loss: 0.9127 - acc: 0.6085 - val_loss: 1.2849 - val_acc: 0.5000\n",
            "Epoch 754/1000\n",
            "15/15 [==============================] - 14s 792ms/step - loss: 0.8701 - acc: 0.6267 - val_loss: 1.4850 - val_acc: 0.2000\n",
            "Epoch 755/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 0.9674 - acc: 0.5700 - val_loss: 1.4370 - val_acc: 0.3500\n",
            "Epoch 756/1000\n",
            "15/15 [==============================] - 14s 804ms/step - loss: 0.9077 - acc: 0.5800 - val_loss: 1.3498 - val_acc: 0.4000\n",
            "Epoch 757/1000\n",
            "15/15 [==============================] - 14s 814ms/step - loss: 0.9146 - acc: 0.6233 - val_loss: 1.4174 - val_acc: 0.3500\n",
            "Epoch 758/1000\n",
            "15/15 [==============================] - 14s 776ms/step - loss: 0.8234 - acc: 0.6512 - val_loss: 1.5356 - val_acc: 0.3500\n",
            "Epoch 759/1000\n",
            "15/15 [==============================] - 14s 788ms/step - loss: 0.9334 - acc: 0.5767 - val_loss: 1.5122 - val_acc: 0.3000\n",
            "Epoch 760/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 0.8519 - acc: 0.6233 - val_loss: 1.1921 - val_acc: 0.3500\n",
            "Epoch 761/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 0.9267 - acc: 0.5765 - val_loss: 1.4296 - val_acc: 0.3500\n",
            "Epoch 762/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.9475 - acc: 0.5567 - val_loss: 1.4235 - val_acc: 0.2500\n",
            "Epoch 763/1000\n",
            "15/15 [==============================] - 14s 938ms/step - loss: 0.8635 - acc: 0.5907 - val_loss: 1.5934 - val_acc: 0.2000\n",
            "Epoch 764/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 0.9318 - acc: 0.6167 - val_loss: 1.2462 - val_acc: 0.5000\n",
            "Epoch 765/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 0.9070 - acc: 0.5967 - val_loss: 1.5428 - val_acc: 0.3000\n",
            "Epoch 766/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9445 - acc: 0.5700 - val_loss: 1.6388 - val_acc: 0.2000\n",
            "Epoch 767/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.8896 - acc: 0.6000 - val_loss: 1.5498 - val_acc: 0.3000\n",
            "Epoch 768/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.8778 - acc: 0.6367 - val_loss: 1.2679 - val_acc: 0.3500\n",
            "Epoch 769/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.8906 - acc: 0.6233 - val_loss: 1.3676 - val_acc: 0.3500\n",
            "Epoch 770/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.9776 - acc: 0.5302 - val_loss: 1.3442 - val_acc: 0.5000\n",
            "Epoch 771/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.9483 - acc: 0.6000 - val_loss: 1.5378 - val_acc: 0.2500\n",
            "Epoch 772/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 0.9146 - acc: 0.5872 - val_loss: 1.2768 - val_acc: 0.4000\n",
            "Epoch 773/1000\n",
            "15/15 [==============================] - 13s 768ms/step - loss: 0.9690 - acc: 0.5623 - val_loss: 1.3740 - val_acc: 0.3500\n",
            "Epoch 774/1000\n",
            "15/15 [==============================] - 14s 810ms/step - loss: 0.8946 - acc: 0.5800 - val_loss: 1.2000 - val_acc: 0.4000\n",
            "Epoch 775/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 0.8934 - acc: 0.6000 - val_loss: 1.3914 - val_acc: 0.3500\n",
            "Epoch 776/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.9033 - acc: 0.6300 - val_loss: 1.6409 - val_acc: 0.3000\n",
            "Epoch 777/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9321 - acc: 0.5800 - val_loss: 1.0352 - val_acc: 0.4500\n",
            "Epoch 778/1000\n",
            "15/15 [==============================] - 14s 770ms/step - loss: 0.9237 - acc: 0.6050 - val_loss: 1.1756 - val_acc: 0.4000\n",
            "Epoch 779/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.8380 - acc: 0.6200 - val_loss: 1.3024 - val_acc: 0.3500\n",
            "Epoch 780/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 0.8878 - acc: 0.6133 - val_loss: 1.6412 - val_acc: 0.2000\n",
            "Epoch 781/1000\n",
            "15/15 [==============================] - 14s 803ms/step - loss: 0.8488 - acc: 0.6567 - val_loss: 1.3423 - val_acc: 0.3500\n",
            "Epoch 782/1000\n",
            "15/15 [==============================] - 14s 772ms/step - loss: 0.8408 - acc: 0.6263 - val_loss: 1.1981 - val_acc: 0.3500\n",
            "Epoch 783/1000\n",
            "15/15 [==============================] - 14s 815ms/step - loss: 0.8904 - acc: 0.5833 - val_loss: 1.2249 - val_acc: 0.4000\n",
            "Epoch 784/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.8886 - acc: 0.5694 - val_loss: 1.4187 - val_acc: 0.3000\n",
            "Epoch 785/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 0.9391 - acc: 0.6157 - val_loss: 1.7208 - val_acc: 0.2500\n",
            "Epoch 786/1000\n",
            "15/15 [==============================] - 14s 801ms/step - loss: 0.8415 - acc: 0.6267 - val_loss: 1.5453 - val_acc: 0.3500\n",
            "Epoch 787/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 0.8637 - acc: 0.5979 - val_loss: 1.6280 - val_acc: 0.3000\n",
            "Epoch 788/1000\n",
            "15/15 [==============================] - 13s 755ms/step - loss: 0.8886 - acc: 0.6263 - val_loss: 1.3882 - val_acc: 0.3500\n",
            "Epoch 789/1000\n",
            "15/15 [==============================] - 14s 794ms/step - loss: 0.8896 - acc: 0.6100 - val_loss: 1.6182 - val_acc: 0.3500\n",
            "Epoch 790/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 0.8659 - acc: 0.6033 - val_loss: 1.5121 - val_acc: 0.3000\n",
            "Epoch 791/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.8971 - acc: 0.6367 - val_loss: 1.4794 - val_acc: 0.4000\n",
            "Epoch 792/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 0.9936 - acc: 0.5765 - val_loss: 1.6001 - val_acc: 0.3500\n",
            "Epoch 793/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.9092 - acc: 0.5658 - val_loss: 1.4113 - val_acc: 0.4000\n",
            "Epoch 794/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 0.9619 - acc: 0.6000 - val_loss: 1.6856 - val_acc: 0.1500\n",
            "Epoch 795/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.9243 - acc: 0.5979 - val_loss: 1.1998 - val_acc: 0.3500\n",
            "Epoch 796/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.9460 - acc: 0.5907 - val_loss: 1.2688 - val_acc: 0.3000\n",
            "Epoch 797/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 0.8803 - acc: 0.5867 - val_loss: 1.3341 - val_acc: 0.4000\n",
            "Epoch 798/1000\n",
            "15/15 [==============================] - 13s 758ms/step - loss: 0.9826 - acc: 0.5872 - val_loss: 1.2178 - val_acc: 0.4500\n",
            "Epoch 799/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.9994 - acc: 0.5967 - val_loss: 1.1220 - val_acc: 0.4500\n",
            "Epoch 800/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 0.8624 - acc: 0.6267 - val_loss: 1.4601 - val_acc: 0.2000\n",
            "Epoch 801/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.9728 - acc: 0.5445 - val_loss: 1.5830 - val_acc: 0.3500\n",
            "Epoch 802/1000\n",
            "15/15 [==============================] - 14s 777ms/step - loss: 0.8593 - acc: 0.6200 - val_loss: 1.1824 - val_acc: 0.5000\n",
            "Epoch 803/1000\n",
            "15/15 [==============================] - 14s 805ms/step - loss: 0.8807 - acc: 0.5800 - val_loss: 1.2952 - val_acc: 0.4500\n",
            "Epoch 804/1000\n",
            "15/15 [==============================] - 14s 793ms/step - loss: 0.8547 - acc: 0.6233 - val_loss: 1.3196 - val_acc: 0.3500\n",
            "Epoch 805/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 0.9346 - acc: 0.6085 - val_loss: 1.4559 - val_acc: 0.3500\n",
            "Epoch 806/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 0.8749 - acc: 0.6085 - val_loss: 1.4173 - val_acc: 0.4500\n",
            "Epoch 807/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 0.9050 - acc: 0.5900 - val_loss: 1.2612 - val_acc: 0.3500\n",
            "Epoch 808/1000\n",
            "15/15 [==============================] - 14s 791ms/step - loss: 0.9615 - acc: 0.5567 - val_loss: 1.3385 - val_acc: 0.3500\n",
            "Epoch 809/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.8818 - acc: 0.6167 - val_loss: 1.7466 - val_acc: 0.0500\n",
            "Epoch 810/1000\n",
            "15/15 [==============================] - 13s 767ms/step - loss: 0.9727 - acc: 0.6121 - val_loss: 1.3669 - val_acc: 0.2000\n",
            "Epoch 811/1000\n",
            "15/15 [==============================] - 13s 776ms/step - loss: 0.9420 - acc: 0.5730 - val_loss: 1.4882 - val_acc: 0.2000\n",
            "Epoch 812/1000\n",
            "15/15 [==============================] - 14s 781ms/step - loss: 0.9485 - acc: 0.5800 - val_loss: 1.3318 - val_acc: 0.3000\n",
            "Epoch 813/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 0.8893 - acc: 0.6067 - val_loss: 1.7555 - val_acc: 0.2000\n",
            "Epoch 814/1000\n",
            "15/15 [==============================] - 13s 712ms/step - loss: 0.9029 - acc: 0.6228 - val_loss: 1.5402 - val_acc: 0.2500\n",
            "Epoch 815/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 0.8454 - acc: 0.6267 - val_loss: 1.6480 - val_acc: 0.2500\n",
            "Epoch 816/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 0.8779 - acc: 0.6085 - val_loss: 1.2826 - val_acc: 0.3500\n",
            "Epoch 817/1000\n",
            "15/15 [==============================] - 12s 710ms/step - loss: 0.9943 - acc: 0.5196 - val_loss: 1.1878 - val_acc: 0.4500\n",
            "Epoch 818/1000\n",
            "15/15 [==============================] - 13s 708ms/step - loss: 0.8633 - acc: 0.6085 - val_loss: 1.6463 - val_acc: 0.3000\n",
            "Epoch 819/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 0.8956 - acc: 0.6133 - val_loss: 1.4079 - val_acc: 0.4500\n",
            "Epoch 820/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 0.8871 - acc: 0.6157 - val_loss: 1.4472 - val_acc: 0.3000\n",
            "Epoch 821/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 0.9563 - acc: 0.5801 - val_loss: 1.7175 - val_acc: 0.2000\n",
            "Epoch 822/1000\n",
            "15/15 [==============================] - 13s 769ms/step - loss: 0.8546 - acc: 0.6233 - val_loss: 1.4141 - val_acc: 0.4000\n",
            "Epoch 823/1000\n",
            "15/15 [==============================] - 13s 762ms/step - loss: 0.9654 - acc: 0.5933 - val_loss: 1.2834 - val_acc: 0.3000\n",
            "Epoch 824/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 0.8527 - acc: 0.6121 - val_loss: 1.3025 - val_acc: 0.3000\n",
            "Epoch 825/1000\n",
            "15/15 [==============================] - 13s 718ms/step - loss: 0.9131 - acc: 0.5979 - val_loss: 1.4209 - val_acc: 0.2500\n",
            "Epoch 826/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 0.8807 - acc: 0.5733 - val_loss: 1.2571 - val_acc: 0.5000\n",
            "Epoch 827/1000\n",
            "15/15 [==============================] - 13s 723ms/step - loss: 0.8330 - acc: 0.6477 - val_loss: 1.3424 - val_acc: 0.3000\n",
            "Epoch 828/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.8466 - acc: 0.5833 - val_loss: 1.3863 - val_acc: 0.3500\n",
            "Epoch 829/1000\n",
            "15/15 [==============================] - 13s 768ms/step - loss: 0.8688 - acc: 0.6100 - val_loss: 1.7860 - val_acc: 0.2500\n",
            "Epoch 830/1000\n",
            "15/15 [==============================] - 13s 883ms/step - loss: 0.8396 - acc: 0.6335 - val_loss: 1.2573 - val_acc: 0.5000\n",
            "Epoch 831/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 0.9427 - acc: 0.6050 - val_loss: 1.0826 - val_acc: 0.4000\n",
            "Epoch 832/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 0.8317 - acc: 0.6406 - val_loss: 1.5197 - val_acc: 0.3000\n",
            "Epoch 833/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 0.8644 - acc: 0.6233 - val_loss: 1.3216 - val_acc: 0.3500\n",
            "Epoch 834/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 0.9027 - acc: 0.5933 - val_loss: 1.1394 - val_acc: 0.4000\n",
            "Epoch 835/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 0.9808 - acc: 0.5633 - val_loss: 1.1654 - val_acc: 0.4000\n",
            "Epoch 836/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 0.9189 - acc: 0.5730 - val_loss: 1.1805 - val_acc: 0.6000\n",
            "Epoch 837/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.8559 - acc: 0.5967 - val_loss: 1.3756 - val_acc: 0.3500\n",
            "Epoch 838/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.9130 - acc: 0.5967 - val_loss: 1.0163 - val_acc: 0.5500\n",
            "Epoch 839/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 0.9162 - acc: 0.5867 - val_loss: 1.2876 - val_acc: 0.4000\n",
            "Epoch 840/1000\n",
            "15/15 [==============================] - 13s 722ms/step - loss: 0.8501 - acc: 0.6548 - val_loss: 1.3791 - val_acc: 0.4000\n",
            "Epoch 841/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.9310 - acc: 0.5587 - val_loss: 1.5303 - val_acc: 0.2000\n",
            "Epoch 842/1000\n",
            "15/15 [==============================] - 13s 717ms/step - loss: 0.9487 - acc: 0.5836 - val_loss: 1.3391 - val_acc: 0.3500\n",
            "Epoch 843/1000\n",
            "15/15 [==============================] - 12s 701ms/step - loss: 0.9461 - acc: 0.6085 - val_loss: 1.0278 - val_acc: 0.5500\n",
            "Epoch 844/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 0.8669 - acc: 0.6167 - val_loss: 1.1805 - val_acc: 0.3500\n",
            "Epoch 845/1000\n",
            "15/15 [==============================] - 12s 715ms/step - loss: 0.8877 - acc: 0.6085 - val_loss: 1.3324 - val_acc: 0.3500\n",
            "Epoch 846/1000\n",
            "15/15 [==============================] - 13s 723ms/step - loss: 0.8605 - acc: 0.6335 - val_loss: 1.3912 - val_acc: 0.4000\n",
            "Epoch 847/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 0.8581 - acc: 0.6370 - val_loss: 1.3427 - val_acc: 0.4000\n",
            "Epoch 848/1000\n",
            "15/15 [==============================] - 13s 752ms/step - loss: 0.8213 - acc: 0.6267 - val_loss: 1.3660 - val_acc: 0.3500\n",
            "Epoch 849/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.9139 - acc: 0.5933 - val_loss: 1.5311 - val_acc: 0.3000\n",
            "Epoch 850/1000\n",
            "15/15 [==============================] - 13s 745ms/step - loss: 0.8845 - acc: 0.6000 - val_loss: 1.2220 - val_acc: 0.5500\n",
            "Epoch 851/1000\n",
            "15/15 [==============================] - 13s 715ms/step - loss: 0.9136 - acc: 0.5907 - val_loss: 1.7523 - val_acc: 0.1500\n",
            "Epoch 852/1000\n",
            "15/15 [==============================] - 13s 760ms/step - loss: 0.9032 - acc: 0.5833 - val_loss: 1.2221 - val_acc: 0.4500\n",
            "Epoch 853/1000\n",
            "15/15 [==============================] - 13s 753ms/step - loss: 0.9206 - acc: 0.6100 - val_loss: 1.5138 - val_acc: 0.3500\n",
            "Epoch 854/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 0.7979 - acc: 0.6433 - val_loss: 1.3678 - val_acc: 0.3500\n",
            "Epoch 855/1000\n",
            "15/15 [==============================] - 13s 736ms/step - loss: 0.9042 - acc: 0.6100 - val_loss: 1.6417 - val_acc: 0.2000\n",
            "Epoch 856/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8797 - acc: 0.6400 - val_loss: 1.5354 - val_acc: 0.2000\n",
            "Epoch 857/1000\n",
            "15/15 [==============================] - 12s 703ms/step - loss: 0.8048 - acc: 0.6619 - val_loss: 1.4275 - val_acc: 0.4000\n",
            "Epoch 858/1000\n",
            "15/15 [==============================] - 13s 720ms/step - loss: 0.8458 - acc: 0.6441 - val_loss: 1.3719 - val_acc: 0.3500\n",
            "Epoch 859/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 0.8743 - acc: 0.6433 - val_loss: 1.5299 - val_acc: 0.2000\n",
            "Epoch 860/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.8147 - acc: 0.6200 - val_loss: 1.1634 - val_acc: 0.5000\n",
            "Epoch 861/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 0.8991 - acc: 0.6200 - val_loss: 1.3929 - val_acc: 0.3500\n",
            "Epoch 862/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 0.9456 - acc: 0.6157 - val_loss: 1.4702 - val_acc: 0.3500\n",
            "Epoch 863/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 0.8834 - acc: 0.5979 - val_loss: 1.2263 - val_acc: 0.4000\n",
            "Epoch 864/1000\n",
            "15/15 [==============================] - 13s 710ms/step - loss: 0.8062 - acc: 0.6619 - val_loss: 1.0195 - val_acc: 0.5500\n",
            "Epoch 865/1000\n",
            "15/15 [==============================] - 13s 741ms/step - loss: 0.8286 - acc: 0.6600 - val_loss: 1.4715 - val_acc: 0.2500\n",
            "Epoch 866/1000\n",
            "15/15 [==============================] - 13s 878ms/step - loss: 0.8381 - acc: 0.6263 - val_loss: 1.5218 - val_acc: 0.3000\n",
            "Epoch 867/1000\n",
            "15/15 [==============================] - 12s 707ms/step - loss: 0.8543 - acc: 0.6619 - val_loss: 1.6819 - val_acc: 0.2500\n",
            "Epoch 868/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 0.9666 - acc: 0.5833 - val_loss: 1.7392 - val_acc: 0.2500\n",
            "Epoch 869/1000\n",
            "15/15 [==============================] - 13s 761ms/step - loss: 0.9267 - acc: 0.5867 - val_loss: 1.5217 - val_acc: 0.3000\n",
            "Epoch 870/1000\n",
            "15/15 [==============================] - 13s 734ms/step - loss: 0.8816 - acc: 0.6263 - val_loss: 1.4447 - val_acc: 0.3500\n",
            "Epoch 871/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 0.9474 - acc: 0.5900 - val_loss: 1.1972 - val_acc: 0.5000\n",
            "Epoch 872/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 0.9348 - acc: 0.6192 - val_loss: 1.3040 - val_acc: 0.4000\n",
            "Epoch 873/1000\n",
            "15/15 [==============================] - 13s 763ms/step - loss: 0.8339 - acc: 0.6400 - val_loss: 1.2119 - val_acc: 0.5000\n",
            "Epoch 874/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 0.9239 - acc: 0.5943 - val_loss: 1.2633 - val_acc: 0.3000\n",
            "Epoch 875/1000\n",
            "15/15 [==============================] - 13s 770ms/step - loss: 0.8790 - acc: 0.6267 - val_loss: 1.7067 - val_acc: 0.3000\n",
            "Epoch 876/1000\n",
            "15/15 [==============================] - 13s 736ms/step - loss: 0.8651 - acc: 0.6335 - val_loss: 1.0150 - val_acc: 0.5500\n",
            "Epoch 877/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 0.7801 - acc: 0.6335 - val_loss: 1.4969 - val_acc: 0.2500\n",
            "Epoch 878/1000\n",
            "15/15 [==============================] - 13s 732ms/step - loss: 0.8596 - acc: 0.6050 - val_loss: 1.3483 - val_acc: 0.4000\n",
            "Epoch 879/1000\n",
            "15/15 [==============================] - 13s 768ms/step - loss: 0.9272 - acc: 0.5700 - val_loss: 1.2191 - val_acc: 0.4000\n",
            "Epoch 880/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 0.8920 - acc: 0.5833 - val_loss: 1.5267 - val_acc: 0.2500\n",
            "Epoch 881/1000\n",
            "15/15 [==============================] - 13s 737ms/step - loss: 0.9001 - acc: 0.6100 - val_loss: 1.2276 - val_acc: 0.3500\n",
            "Epoch 882/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 0.9061 - acc: 0.6233 - val_loss: 1.5442 - val_acc: 0.3000\n",
            "Epoch 883/1000\n",
            "15/15 [==============================] - 12s 706ms/step - loss: 0.8602 - acc: 0.6512 - val_loss: 1.6884 - val_acc: 0.2000\n",
            "Epoch 884/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 0.9253 - acc: 0.5658 - val_loss: 1.2689 - val_acc: 0.4000\n",
            "Epoch 885/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 0.8905 - acc: 0.6000 - val_loss: 1.2431 - val_acc: 0.4000\n",
            "Epoch 886/1000\n",
            "15/15 [==============================] - 13s 911ms/step - loss: 0.8949 - acc: 0.5872 - val_loss: 1.3466 - val_acc: 0.4000\n",
            "Epoch 887/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.7839 - acc: 0.6567 - val_loss: 1.4181 - val_acc: 0.4500\n",
            "Epoch 888/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 0.8416 - acc: 0.6050 - val_loss: 1.1884 - val_acc: 0.4500\n",
            "Epoch 889/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 0.9255 - acc: 0.6000 - val_loss: 1.2022 - val_acc: 0.5000\n",
            "Epoch 890/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.8303 - acc: 0.6333 - val_loss: 1.4470 - val_acc: 0.3000\n",
            "Epoch 891/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 0.9052 - acc: 0.6014 - val_loss: 1.4785 - val_acc: 0.4000\n",
            "Epoch 892/1000\n",
            "15/15 [==============================] - 13s 765ms/step - loss: 0.8757 - acc: 0.6299 - val_loss: 1.3010 - val_acc: 0.4000\n",
            "Epoch 893/1000\n",
            "15/15 [==============================] - 14s 814ms/step - loss: 0.8554 - acc: 0.6067 - val_loss: 1.5783 - val_acc: 0.2000\n",
            "Epoch 894/1000\n",
            "15/15 [==============================] - 14s 770ms/step - loss: 0.8284 - acc: 0.6370 - val_loss: 1.4910 - val_acc: 0.3000\n",
            "Epoch 895/1000\n",
            "15/15 [==============================] - 14s 770ms/step - loss: 0.7645 - acc: 0.6548 - val_loss: 1.4462 - val_acc: 0.3500\n",
            "Epoch 896/1000\n",
            "15/15 [==============================] - 14s 800ms/step - loss: 0.8799 - acc: 0.5767 - val_loss: 1.2046 - val_acc: 0.4500\n",
            "Epoch 897/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.9213 - acc: 0.5967 - val_loss: 1.2647 - val_acc: 0.4500\n",
            "Epoch 898/1000\n",
            "15/15 [==============================] - 14s 775ms/step - loss: 0.8233 - acc: 0.6690 - val_loss: 1.6484 - val_acc: 0.2500\n",
            "Epoch 899/1000\n",
            "15/15 [==============================] - 14s 816ms/step - loss: 0.9011 - acc: 0.5667 - val_loss: 1.4858 - val_acc: 0.3500\n",
            "Epoch 900/1000\n",
            "15/15 [==============================] - 14s 782ms/step - loss: 0.9147 - acc: 0.6050 - val_loss: 1.3881 - val_acc: 0.4500\n",
            "Epoch 901/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.9225 - acc: 0.6167 - val_loss: 1.4440 - val_acc: 0.3500\n",
            "Epoch 902/1000\n",
            "15/15 [==============================] - 14s 799ms/step - loss: 0.7889 - acc: 0.6933 - val_loss: 1.5237 - val_acc: 0.2500\n",
            "Epoch 903/1000\n",
            "15/15 [==============================] - 14s 783ms/step - loss: 0.7851 - acc: 0.6762 - val_loss: 1.5007 - val_acc: 0.4000\n",
            "Epoch 904/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9433 - acc: 0.6100 - val_loss: 1.2796 - val_acc: 0.4500\n",
            "Epoch 905/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 0.8077 - acc: 0.6333 - val_loss: 1.2272 - val_acc: 0.4500\n",
            "Epoch 906/1000\n",
            "15/15 [==============================] - 15s 832ms/step - loss: 0.8315 - acc: 0.6100 - val_loss: 1.0765 - val_acc: 0.5000\n",
            "Epoch 907/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 0.8513 - acc: 0.6619 - val_loss: 1.2114 - val_acc: 0.3000\n",
            "Epoch 908/1000\n",
            "15/15 [==============================] - 14s 949ms/step - loss: 0.8592 - acc: 0.6228 - val_loss: 1.2902 - val_acc: 0.4500\n",
            "Epoch 909/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.9106 - acc: 0.6100 - val_loss: 1.3455 - val_acc: 0.4000\n",
            "Epoch 910/1000\n",
            "15/15 [==============================] - 14s 826ms/step - loss: 0.9519 - acc: 0.5900 - val_loss: 1.3575 - val_acc: 0.4000\n",
            "Epoch 911/1000\n",
            "15/15 [==============================] - 14s 786ms/step - loss: 0.8941 - acc: 0.5943 - val_loss: 1.3846 - val_acc: 0.4000\n",
            "Epoch 912/1000\n",
            "15/15 [==============================] - 14s 835ms/step - loss: 0.8362 - acc: 0.6033 - val_loss: 1.4016 - val_acc: 0.1500\n",
            "Epoch 913/1000\n",
            "15/15 [==============================] - 13s 764ms/step - loss: 0.7947 - acc: 0.6157 - val_loss: 1.2651 - val_acc: 0.3500\n",
            "Epoch 914/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.8409 - acc: 0.6200 - val_loss: 1.2338 - val_acc: 0.3500\n",
            "Epoch 915/1000\n",
            "15/15 [==============================] - 14s 800ms/step - loss: 0.8942 - acc: 0.6233 - val_loss: 1.4437 - val_acc: 0.3500\n",
            "Epoch 916/1000\n",
            "15/15 [==============================] - 14s 769ms/step - loss: 0.7934 - acc: 0.6477 - val_loss: 1.2091 - val_acc: 0.4000\n",
            "Epoch 917/1000\n",
            "15/15 [==============================] - 14s 776ms/step - loss: 0.8418 - acc: 0.6192 - val_loss: 1.5303 - val_acc: 0.2000\n",
            "Epoch 918/1000\n",
            "15/15 [==============================] - 14s 795ms/step - loss: 0.8587 - acc: 0.6050 - val_loss: 1.3960 - val_acc: 0.4000\n",
            "Epoch 919/1000\n",
            "15/15 [==============================] - 14s 807ms/step - loss: 0.9180 - acc: 0.5700 - val_loss: 1.3527 - val_acc: 0.3000\n",
            "Epoch 920/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.8726 - acc: 0.6033 - val_loss: 1.3362 - val_acc: 0.3000\n",
            "Epoch 921/1000\n",
            "15/15 [==============================] - 14s 768ms/step - loss: 0.7868 - acc: 0.6477 - val_loss: 1.4077 - val_acc: 0.3000\n",
            "Epoch 922/1000\n",
            "15/15 [==============================] - 14s 805ms/step - loss: 0.9208 - acc: 0.6133 - val_loss: 1.4159 - val_acc: 0.2000\n",
            "Epoch 923/1000\n",
            "15/15 [==============================] - 14s 809ms/step - loss: 0.8574 - acc: 0.6100 - val_loss: 1.0341 - val_acc: 0.3000\n",
            "Epoch 924/1000\n",
            "15/15 [==============================] - 14s 784ms/step - loss: 0.9090 - acc: 0.5967 - val_loss: 1.3877 - val_acc: 0.4000\n",
            "Epoch 925/1000\n",
            "15/15 [==============================] - 14s 808ms/step - loss: 0.8305 - acc: 0.6200 - val_loss: 1.3944 - val_acc: 0.3500\n",
            "Epoch 926/1000\n",
            "15/15 [==============================] - 14s 806ms/step - loss: 0.8535 - acc: 0.5900 - val_loss: 1.5798 - val_acc: 0.3500\n",
            "Epoch 927/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 0.8960 - acc: 0.6548 - val_loss: 1.9170 - val_acc: 0.0500\n",
            "Epoch 928/1000\n",
            "15/15 [==============================] - 14s 816ms/step - loss: 0.9027 - acc: 0.6133 - val_loss: 1.5503 - val_acc: 0.3000\n",
            "Epoch 929/1000\n",
            "15/15 [==============================] - 14s 802ms/step - loss: 0.8919 - acc: 0.6200 - val_loss: 1.1237 - val_acc: 0.4500\n",
            "Epoch 930/1000\n",
            "15/15 [==============================] - 14s 779ms/step - loss: 0.8544 - acc: 0.6228 - val_loss: 1.3354 - val_acc: 0.2500\n",
            "Epoch 931/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.7756 - acc: 0.6367 - val_loss: 1.4299 - val_acc: 0.2500\n",
            "Epoch 932/1000\n",
            "15/15 [==============================] - 14s 789ms/step - loss: 0.7716 - acc: 0.6690 - val_loss: 1.2708 - val_acc: 0.3500\n",
            "Epoch 933/1000\n",
            "15/15 [==============================] - 14s 796ms/step - loss: 0.7764 - acc: 0.6833 - val_loss: 1.5040 - val_acc: 0.2500\n",
            "Epoch 934/1000\n",
            "15/15 [==============================] - 14s 773ms/step - loss: 0.8198 - acc: 0.6406 - val_loss: 1.1830 - val_acc: 0.4000\n",
            "Epoch 935/1000\n",
            "15/15 [==============================] - 14s 813ms/step - loss: 0.8568 - acc: 0.6133 - val_loss: 1.6309 - val_acc: 0.3000\n",
            "Epoch 936/1000\n",
            "15/15 [==============================] - 14s 797ms/step - loss: 0.8761 - acc: 0.6267 - val_loss: 1.2272 - val_acc: 0.4500\n",
            "Epoch 937/1000\n",
            "15/15 [==============================] - 13s 776ms/step - loss: 0.9312 - acc: 0.5694 - val_loss: 1.5801 - val_acc: 0.2500\n",
            "Epoch 938/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 0.8650 - acc: 0.6157 - val_loss: 1.1329 - val_acc: 0.4000\n",
            "Epoch 939/1000\n",
            "15/15 [==============================] - 14s 780ms/step - loss: 0.9094 - acc: 0.6067 - val_loss: 1.3411 - val_acc: 0.4000\n",
            "Epoch 940/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 0.8163 - acc: 0.6263 - val_loss: 1.5450 - val_acc: 0.3500\n",
            "Epoch 941/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8603 - acc: 0.6400 - val_loss: 1.4536 - val_acc: 0.2000\n",
            "Epoch 942/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 0.8285 - acc: 0.6233 - val_loss: 1.1564 - val_acc: 0.4500\n",
            "Epoch 943/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 0.8960 - acc: 0.6367 - val_loss: 0.9116 - val_acc: 0.5500\n",
            "Epoch 944/1000\n",
            "15/15 [==============================] - 13s 745ms/step - loss: 0.8644 - acc: 0.6263 - val_loss: 1.1971 - val_acc: 0.3500\n",
            "Epoch 945/1000\n",
            "15/15 [==============================] - 13s 706ms/step - loss: 0.8972 - acc: 0.5801 - val_loss: 1.6781 - val_acc: 0.2000\n",
            "Epoch 946/1000\n",
            "15/15 [==============================] - 13s 719ms/step - loss: 0.8035 - acc: 0.6477 - val_loss: 1.1542 - val_acc: 0.4000\n",
            "Epoch 947/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8637 - acc: 0.6167 - val_loss: 1.2734 - val_acc: 0.4500\n",
            "Epoch 948/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8463 - acc: 0.6433 - val_loss: 1.4919 - val_acc: 0.3500\n",
            "Epoch 949/1000\n",
            "15/15 [==============================] - 13s 886ms/step - loss: 0.7971 - acc: 0.6690 - val_loss: 1.1992 - val_acc: 0.5000\n",
            "Epoch 950/1000\n",
            "15/15 [==============================] - 13s 759ms/step - loss: 0.8510 - acc: 0.6267 - val_loss: 1.1609 - val_acc: 0.4000\n",
            "Epoch 951/1000\n",
            "15/15 [==============================] - 13s 729ms/step - loss: 1.0029 - acc: 0.5907 - val_loss: 1.2894 - val_acc: 0.3000\n",
            "Epoch 952/1000\n",
            "15/15 [==============================] - 13s 757ms/step - loss: 0.8830 - acc: 0.5867 - val_loss: 1.5667 - val_acc: 0.2500\n",
            "Epoch 953/1000\n",
            "15/15 [==============================] - 13s 749ms/step - loss: 0.8050 - acc: 0.6467 - val_loss: 1.2922 - val_acc: 0.3000\n",
            "Epoch 954/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 0.8114 - acc: 0.6619 - val_loss: 1.3503 - val_acc: 0.3500\n",
            "Epoch 955/1000\n",
            "15/15 [==============================] - 13s 721ms/step - loss: 0.8638 - acc: 0.6512 - val_loss: 1.3130 - val_acc: 0.3500\n",
            "Epoch 956/1000\n",
            "15/15 [==============================] - 13s 740ms/step - loss: 0.8668 - acc: 0.6200 - val_loss: 1.3088 - val_acc: 0.2500\n",
            "Epoch 957/1000\n",
            "15/15 [==============================] - 12s 707ms/step - loss: 0.8833 - acc: 0.6192 - val_loss: 1.6112 - val_acc: 0.3000\n",
            "Epoch 958/1000\n",
            "15/15 [==============================] - 13s 752ms/step - loss: 0.9438 - acc: 0.5967 - val_loss: 1.2732 - val_acc: 0.4500\n",
            "Epoch 959/1000\n",
            "15/15 [==============================] - 13s 718ms/step - loss: 0.9176 - acc: 0.5979 - val_loss: 1.0575 - val_acc: 0.4500\n",
            "Epoch 960/1000\n",
            "15/15 [==============================] - 13s 727ms/step - loss: 0.8430 - acc: 0.6263 - val_loss: 1.4115 - val_acc: 0.4000\n",
            "Epoch 961/1000\n",
            "15/15 [==============================] - 13s 726ms/step - loss: 0.8806 - acc: 0.6299 - val_loss: 1.3058 - val_acc: 0.3500\n",
            "Epoch 962/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 0.8451 - acc: 0.6584 - val_loss: 1.4621 - val_acc: 0.3000\n",
            "Epoch 963/1000\n",
            "15/15 [==============================] - 13s 756ms/step - loss: 0.8641 - acc: 0.6200 - val_loss: 1.1726 - val_acc: 0.3500\n",
            "Epoch 964/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 0.8143 - acc: 0.6433 - val_loss: 1.5510 - val_acc: 0.2500\n",
            "Epoch 965/1000\n",
            "15/15 [==============================] - 13s 724ms/step - loss: 0.9184 - acc: 0.6014 - val_loss: 1.0888 - val_acc: 0.5000\n",
            "Epoch 966/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 0.7867 - acc: 0.6690 - val_loss: 1.4699 - val_acc: 0.2000\n",
            "Epoch 967/1000\n",
            "15/15 [==============================] - 13s 742ms/step - loss: 0.8766 - acc: 0.6200 - val_loss: 1.4050 - val_acc: 0.4500\n",
            "Epoch 968/1000\n",
            "15/15 [==============================] - 13s 766ms/step - loss: 0.8211 - acc: 0.6533 - val_loss: 0.9024 - val_acc: 0.4500\n",
            "Epoch 969/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 0.9040 - acc: 0.5900 - val_loss: 1.2563 - val_acc: 0.2500\n",
            "Epoch 970/1000\n",
            "15/15 [==============================] - 13s 738ms/step - loss: 0.8164 - acc: 0.6567 - val_loss: 1.0307 - val_acc: 0.5000\n",
            "Epoch 971/1000\n",
            "15/15 [==============================] - 13s 725ms/step - loss: 0.8185 - acc: 0.6584 - val_loss: 1.5356 - val_acc: 0.3000\n",
            "Epoch 972/1000\n",
            "15/15 [==============================] - 13s 767ms/step - loss: 0.9168 - acc: 0.5733 - val_loss: 1.4736 - val_acc: 0.2500\n",
            "Epoch 973/1000\n",
            "15/15 [==============================] - 13s 733ms/step - loss: 0.8479 - acc: 0.6050 - val_loss: 1.3692 - val_acc: 0.4500\n",
            "Epoch 974/1000\n",
            "15/15 [==============================] - 13s 751ms/step - loss: 0.8408 - acc: 0.6300 - val_loss: 1.2493 - val_acc: 0.4500\n",
            "Epoch 975/1000\n",
            "15/15 [==============================] - 13s 748ms/step - loss: 0.8790 - acc: 0.6433 - val_loss: 1.2344 - val_acc: 0.5000\n",
            "Epoch 976/1000\n",
            "15/15 [==============================] - 13s 718ms/step - loss: 0.9585 - acc: 0.5836 - val_loss: 1.5660 - val_acc: 0.4000\n",
            "Epoch 977/1000\n",
            "15/15 [==============================] - 13s 739ms/step - loss: 0.9220 - acc: 0.5833 - val_loss: 1.4970 - val_acc: 0.3500\n",
            "Epoch 978/1000\n",
            "15/15 [==============================] - 13s 739ms/step - loss: 0.9724 - acc: 0.5933 - val_loss: 1.7443 - val_acc: 0.1000\n",
            "Epoch 979/1000\n",
            "15/15 [==============================] - 13s 748ms/step - loss: 0.7552 - acc: 0.6767 - val_loss: 1.3482 - val_acc: 0.3500\n",
            "Epoch 980/1000\n",
            "15/15 [==============================] - 13s 740ms/step - loss: 0.8855 - acc: 0.6133 - val_loss: 1.5164 - val_acc: 0.3000\n",
            "Epoch 981/1000\n",
            "15/15 [==============================] - 13s 731ms/step - loss: 0.9043 - acc: 0.5836 - val_loss: 1.0999 - val_acc: 0.4500\n",
            "Epoch 982/1000\n",
            "15/15 [==============================] - 13s 728ms/step - loss: 0.8593 - acc: 0.6370 - val_loss: 1.2804 - val_acc: 0.4000\n",
            "Epoch 983/1000\n",
            "15/15 [==============================] - 13s 784ms/step - loss: 0.8346 - acc: 0.6200 - val_loss: 1.5928 - val_acc: 0.3500\n",
            "Epoch 984/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8583 - acc: 0.6333 - val_loss: 1.1593 - val_acc: 0.4000\n",
            "Epoch 985/1000\n",
            "15/15 [==============================] - 13s 881ms/step - loss: 0.8738 - acc: 0.6335 - val_loss: 1.6658 - val_acc: 0.3000\n",
            "Epoch 986/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 0.8655 - acc: 0.6067 - val_loss: 1.2959 - val_acc: 0.3000\n",
            "Epoch 987/1000\n",
            "15/15 [==============================] - 13s 885ms/step - loss: 0.9019 - acc: 0.6406 - val_loss: 1.4554 - val_acc: 0.3500\n",
            "Epoch 988/1000\n",
            "15/15 [==============================] - 13s 750ms/step - loss: 0.8488 - acc: 0.6233 - val_loss: 1.1785 - val_acc: 0.3000\n",
            "Epoch 989/1000\n",
            "15/15 [==============================] - 13s 713ms/step - loss: 0.7096 - acc: 0.6584 - val_loss: 1.7274 - val_acc: 0.3000\n",
            "Epoch 990/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 0.8495 - acc: 0.5933 - val_loss: 1.2916 - val_acc: 0.4500\n",
            "Epoch 991/1000\n",
            "15/15 [==============================] - 13s 720ms/step - loss: 0.8017 - acc: 0.6477 - val_loss: 1.8411 - val_acc: 0.1500\n",
            "Epoch 992/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 0.8257 - acc: 0.6400 - val_loss: 1.4173 - val_acc: 0.4000\n",
            "Epoch 993/1000\n",
            "15/15 [==============================] - 13s 754ms/step - loss: 0.8409 - acc: 0.6067 - val_loss: 1.6482 - val_acc: 0.2500\n",
            "Epoch 994/1000\n",
            "15/15 [==============================] - 13s 744ms/step - loss: 0.8161 - acc: 0.6167 - val_loss: 1.3126 - val_acc: 0.3500\n",
            "Epoch 995/1000\n",
            "15/15 [==============================] - 12s 706ms/step - loss: 0.8410 - acc: 0.6085 - val_loss: 1.3054 - val_acc: 0.2000\n",
            "Epoch 996/1000\n",
            "15/15 [==============================] - 13s 747ms/step - loss: 0.9349 - acc: 0.6100 - val_loss: 1.1463 - val_acc: 0.5000\n",
            "Epoch 997/1000\n",
            "15/15 [==============================] - 13s 712ms/step - loss: 0.8494 - acc: 0.6548 - val_loss: 1.2230 - val_acc: 0.5000\n",
            "Epoch 998/1000\n",
            "15/15 [==============================] - 13s 714ms/step - loss: 0.9065 - acc: 0.5979 - val_loss: 1.0856 - val_acc: 0.3000\n",
            "Epoch 999/1000\n",
            "15/15 [==============================] - 13s 716ms/step - loss: 0.8978 - acc: 0.5694 - val_loss: 1.7237 - val_acc: 0.1000\n",
            "Epoch 1000/1000\n",
            "15/15 [==============================] - 13s 743ms/step - loss: 0.8808 - acc: 0.5867 - val_loss: 1.4763 - val_acc: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "bef0187b-b8c1-4d57-89df-433b0fc67d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [2.279412031173706,\n",
              "  2.1053056716918945,\n",
              "  1.8641357421875,\n",
              "  1.8357763290405273,\n",
              "  1.7286432981491089,\n",
              "  1.6724038124084473,\n",
              "  1.6121551990509033,\n",
              "  1.6473079919815063,\n",
              "  1.7122350931167603,\n",
              "  1.5447980165481567,\n",
              "  1.5010076761245728,\n",
              "  1.5374534130096436,\n",
              "  1.5998128652572632,\n",
              "  1.5757882595062256,\n",
              "  1.4216269254684448,\n",
              "  1.618072748184204,\n",
              "  1.5234311819076538,\n",
              "  1.5079798698425293,\n",
              "  1.4992247819900513,\n",
              "  1.5107536315917969,\n",
              "  1.6413177251815796,\n",
              "  1.509568214416504,\n",
              "  1.5135650634765625,\n",
              "  1.471070408821106,\n",
              "  1.6629321575164795,\n",
              "  1.5630855560302734,\n",
              "  1.518343448638916,\n",
              "  1.5496219396591187,\n",
              "  1.4869312047958374,\n",
              "  1.4959828853607178,\n",
              "  1.5626006126403809,\n",
              "  1.5634026527404785,\n",
              "  1.5285627841949463,\n",
              "  1.4457283020019531,\n",
              "  1.6678361892700195,\n",
              "  1.5712101459503174,\n",
              "  1.4955123662948608,\n",
              "  1.4455434083938599,\n",
              "  1.3901458978652954,\n",
              "  1.4632917642593384,\n",
              "  1.444402813911438,\n",
              "  1.4755852222442627,\n",
              "  1.4119312763214111,\n",
              "  1.4896942377090454,\n",
              "  1.519814372062683,\n",
              "  1.466778039932251,\n",
              "  1.436498999595642,\n",
              "  1.3895177841186523,\n",
              "  1.56792151927948,\n",
              "  1.4972695112228394,\n",
              "  1.3430266380310059,\n",
              "  1.525327205657959,\n",
              "  1.5888793468475342,\n",
              "  1.4994454383850098,\n",
              "  1.5394915342330933,\n",
              "  1.480757236480713,\n",
              "  1.5133732557296753,\n",
              "  1.4484946727752686,\n",
              "  1.3481587171554565,\n",
              "  1.3969913721084595,\n",
              "  1.438092827796936,\n",
              "  1.2960774898529053,\n",
              "  1.3806133270263672,\n",
              "  1.4709378480911255,\n",
              "  1.4590907096862793,\n",
              "  1.3820589780807495,\n",
              "  1.3940761089324951,\n",
              "  1.389524221420288,\n",
              "  1.5864166021347046,\n",
              "  1.4013670682907104,\n",
              "  1.4421160221099854,\n",
              "  1.5671851634979248,\n",
              "  1.3997336626052856,\n",
              "  1.3196102380752563,\n",
              "  1.3642390966415405,\n",
              "  1.4629971981048584,\n",
              "  1.4512832164764404,\n",
              "  1.4979214668273926,\n",
              "  1.330127477645874,\n",
              "  1.518131971359253,\n",
              "  1.4153565168380737,\n",
              "  1.4635263681411743,\n",
              "  1.3562192916870117,\n",
              "  1.5072780847549438,\n",
              "  1.3585662841796875,\n",
              "  1.3163355588912964,\n",
              "  1.3547722101211548,\n",
              "  1.3919302225112915,\n",
              "  1.448468565940857,\n",
              "  1.3929803371429443,\n",
              "  1.406324028968811,\n",
              "  1.4851897954940796,\n",
              "  1.5022908449172974,\n",
              "  1.4835416078567505,\n",
              "  1.273576259613037,\n",
              "  1.3930476903915405,\n",
              "  1.3018063306808472,\n",
              "  1.419578194618225,\n",
              "  1.4305658340454102,\n",
              "  1.3760433197021484,\n",
              "  1.5231034755706787,\n",
              "  1.3305702209472656,\n",
              "  1.4566704034805298,\n",
              "  1.3625887632369995,\n",
              "  1.323988914489746,\n",
              "  1.2697646617889404,\n",
              "  1.36588716506958,\n",
              "  1.2513561248779297,\n",
              "  1.3385107517242432,\n",
              "  1.277045726776123,\n",
              "  1.2679592370986938,\n",
              "  1.4241541624069214,\n",
              "  1.3047782182693481,\n",
              "  1.3563965559005737,\n",
              "  1.2736762762069702,\n",
              "  1.3432835340499878,\n",
              "  1.2894504070281982,\n",
              "  1.3690197467803955,\n",
              "  1.3011621236801147,\n",
              "  1.3653484582901,\n",
              "  1.3690913915634155,\n",
              "  1.4012621641159058,\n",
              "  1.17487370967865,\n",
              "  1.3530083894729614,\n",
              "  1.3168538808822632,\n",
              "  1.2514450550079346,\n",
              "  1.2943543195724487,\n",
              "  1.2401074171066284,\n",
              "  1.39230215549469,\n",
              "  1.306578516960144,\n",
              "  1.397217869758606,\n",
              "  1.3131060600280762,\n",
              "  1.2350256443023682,\n",
              "  1.3550808429718018,\n",
              "  1.3299622535705566,\n",
              "  1.3108139038085938,\n",
              "  1.2639211416244507,\n",
              "  1.2982038259506226,\n",
              "  1.2589958906173706,\n",
              "  1.3637495040893555,\n",
              "  1.3434401750564575,\n",
              "  1.3390034437179565,\n",
              "  1.2615526914596558,\n",
              "  1.3391913175582886,\n",
              "  1.1971352100372314,\n",
              "  1.3120473623275757,\n",
              "  1.2873961925506592,\n",
              "  1.2642518281936646,\n",
              "  1.1701165437698364,\n",
              "  1.1565039157867432,\n",
              "  1.2031224966049194,\n",
              "  1.2459156513214111,\n",
              "  1.259358525276184,\n",
              "  1.2759426832199097,\n",
              "  1.274389386177063,\n",
              "  1.1178683042526245,\n",
              "  1.2370705604553223,\n",
              "  1.2737711668014526,\n",
              "  1.2704585790634155,\n",
              "  1.276736855506897,\n",
              "  1.3404508829116821,\n",
              "  1.2148182392120361,\n",
              "  1.2377086877822876,\n",
              "  1.3137809038162231,\n",
              "  1.2089451551437378,\n",
              "  1.2318458557128906,\n",
              "  1.2651329040527344,\n",
              "  1.2453120946884155,\n",
              "  1.2453163862228394,\n",
              "  1.347517967224121,\n",
              "  1.1881213188171387,\n",
              "  1.201380968093872,\n",
              "  1.280041217803955,\n",
              "  1.3326241970062256,\n",
              "  1.2197139263153076,\n",
              "  1.2596018314361572,\n",
              "  1.1945973634719849,\n",
              "  1.1912506818771362,\n",
              "  1.161819577217102,\n",
              "  1.2871463298797607,\n",
              "  1.2130303382873535,\n",
              "  1.2305110692977905,\n",
              "  1.2886543273925781,\n",
              "  1.1741528511047363,\n",
              "  1.208774447441101,\n",
              "  1.2320283651351929,\n",
              "  1.1134778261184692,\n",
              "  1.2321909666061401,\n",
              "  1.272505521774292,\n",
              "  1.2236697673797607,\n",
              "  1.2539265155792236,\n",
              "  1.270334005355835,\n",
              "  1.2661399841308594,\n",
              "  1.2341278791427612,\n",
              "  1.1180481910705566,\n",
              "  1.2267429828643799,\n",
              "  1.158707618713379,\n",
              "  1.2549169063568115,\n",
              "  1.2382639646530151,\n",
              "  1.225936770439148,\n",
              "  1.1918315887451172,\n",
              "  1.2438801527023315,\n",
              "  1.1727555990219116,\n",
              "  1.215684175491333,\n",
              "  1.1168948411941528,\n",
              "  1.1914855241775513,\n",
              "  1.232740879058838,\n",
              "  1.1275097131729126,\n",
              "  1.1996403932571411,\n",
              "  1.2108169794082642,\n",
              "  1.1772364377975464,\n",
              "  1.2095603942871094,\n",
              "  1.1468443870544434,\n",
              "  1.2447831630706787,\n",
              "  1.1544073820114136,\n",
              "  1.2761608362197876,\n",
              "  1.196529746055603,\n",
              "  1.0851573944091797,\n",
              "  1.1011688709259033,\n",
              "  1.2046090364456177,\n",
              "  1.2132351398468018,\n",
              "  1.0849878787994385,\n",
              "  1.3106374740600586,\n",
              "  1.2030078172683716,\n",
              "  1.329620599746704,\n",
              "  1.210894227027893,\n",
              "  1.2486552000045776,\n",
              "  1.2410366535186768,\n",
              "  1.1167923212051392,\n",
              "  1.1008055210113525,\n",
              "  1.1896580457687378,\n",
              "  1.253095030784607,\n",
              "  1.168123483657837,\n",
              "  1.231040358543396,\n",
              "  1.2218760251998901,\n",
              "  1.1006889343261719,\n",
              "  1.1283410787582397,\n",
              "  1.1504443883895874,\n",
              "  1.2295070886611938,\n",
              "  1.1685173511505127,\n",
              "  1.1686583757400513,\n",
              "  1.1954944133758545,\n",
              "  1.2058968544006348,\n",
              "  1.122586727142334,\n",
              "  1.132672905921936,\n",
              "  1.1116925477981567,\n",
              "  1.1727256774902344,\n",
              "  1.1315704584121704,\n",
              "  1.2140644788742065,\n",
              "  1.1274656057357788,\n",
              "  1.2279726266860962,\n",
              "  1.1343034505844116,\n",
              "  1.0890841484069824,\n",
              "  1.1744896173477173,\n",
              "  1.1138800382614136,\n",
              "  1.1955510377883911,\n",
              "  1.1775578260421753,\n",
              "  1.2494237422943115,\n",
              "  1.127370834350586,\n",
              "  1.0907210111618042,\n",
              "  1.0904549360275269,\n",
              "  1.1924387216567993,\n",
              "  1.189784288406372,\n",
              "  1.1982154846191406,\n",
              "  1.1558470726013184,\n",
              "  1.1202723979949951,\n",
              "  1.26204252243042,\n",
              "  1.1394333839416504,\n",
              "  1.2822165489196777,\n",
              "  1.2317883968353271,\n",
              "  1.1011897325515747,\n",
              "  1.1517906188964844,\n",
              "  1.1006782054901123,\n",
              "  1.137935757637024,\n",
              "  1.1708428859710693,\n",
              "  1.2761483192443848,\n",
              "  1.1705124378204346,\n",
              "  1.0634334087371826,\n",
              "  1.1251492500305176,\n",
              "  1.2085055112838745,\n",
              "  1.177075743675232,\n",
              "  1.2216731309890747,\n",
              "  1.1207473278045654,\n",
              "  1.1922857761383057,\n",
              "  1.2237963676452637,\n",
              "  1.1455316543579102,\n",
              "  1.154997706413269,\n",
              "  1.043743371963501,\n",
              "  1.1112608909606934,\n",
              "  1.1604374647140503,\n",
              "  1.1397359371185303,\n",
              "  1.0903112888336182,\n",
              "  1.0324363708496094,\n",
              "  1.0312494039535522,\n",
              "  1.1401134729385376,\n",
              "  1.0842355489730835,\n",
              "  1.0878777503967285,\n",
              "  1.0806363821029663,\n",
              "  1.1136690378189087,\n",
              "  1.1270054578781128,\n",
              "  1.1630977392196655,\n",
              "  1.1201249361038208,\n",
              "  1.0851128101348877,\n",
              "  1.019720196723938,\n",
              "  1.054423451423645,\n",
              "  1.1122647523880005,\n",
              "  1.1313209533691406,\n",
              "  1.1596705913543701,\n",
              "  1.099767804145813,\n",
              "  1.0671321153640747,\n",
              "  1.2464908361434937,\n",
              "  1.13339364528656,\n",
              "  1.0921229124069214,\n",
              "  1.1594953536987305,\n",
              "  1.0719789266586304,\n",
              "  1.2321383953094482,\n",
              "  1.1115413904190063,\n",
              "  0.9713624119758606,\n",
              "  1.127565860748291,\n",
              "  1.037514328956604,\n",
              "  1.1296803951263428,\n",
              "  1.1115458011627197,\n",
              "  1.1632122993469238,\n",
              "  1.0635793209075928,\n",
              "  1.0563308000564575,\n",
              "  1.0922125577926636,\n",
              "  1.0797773599624634,\n",
              "  1.0386141538619995,\n",
              "  1.136183261871338,\n",
              "  1.1737520694732666,\n",
              "  1.0792419910430908,\n",
              "  1.1059242486953735,\n",
              "  1.0842205286026,\n",
              "  1.0666872262954712,\n",
              "  1.0203708410263062,\n",
              "  1.1259132623672485,\n",
              "  1.2509092092514038,\n",
              "  1.1490596532821655,\n",
              "  1.07002854347229,\n",
              "  1.1572216749191284,\n",
              "  1.027485966682434,\n",
              "  1.0438568592071533,\n",
              "  1.0220022201538086,\n",
              "  1.04106867313385,\n",
              "  1.0693717002868652,\n",
              "  1.0289287567138672,\n",
              "  1.0121345520019531,\n",
              "  1.1014872789382935,\n",
              "  1.1433192491531372,\n",
              "  1.1029250621795654,\n",
              "  1.1024954319000244,\n",
              "  1.1235865354537964,\n",
              "  1.107737421989441,\n",
              "  1.1695207357406616,\n",
              "  1.083297610282898,\n",
              "  1.042089581489563,\n",
              "  1.1154357194900513,\n",
              "  1.0362638235092163,\n",
              "  0.9641187191009521,\n",
              "  1.0930466651916504,\n",
              "  1.172491431236267,\n",
              "  1.150468349456787,\n",
              "  1.0144383907318115,\n",
              "  1.1274161338806152,\n",
              "  1.0809674263000488,\n",
              "  1.0543044805526733,\n",
              "  1.121059536933899,\n",
              "  1.011715054512024,\n",
              "  1.0315605401992798,\n",
              "  1.1341015100479126,\n",
              "  1.0877699851989746,\n",
              "  1.090507984161377,\n",
              "  1.0241788625717163,\n",
              "  1.0263594388961792,\n",
              "  1.1426777839660645,\n",
              "  1.024440050125122,\n",
              "  1.1548856496810913,\n",
              "  1.143642544746399,\n",
              "  1.103685736656189,\n",
              "  1.1437684297561646,\n",
              "  1.029365062713623,\n",
              "  1.0497161149978638,\n",
              "  1.0296051502227783,\n",
              "  0.9491303563117981,\n",
              "  1.0555037260055542,\n",
              "  1.1553289890289307,\n",
              "  1.108544945716858,\n",
              "  1.0379302501678467,\n",
              "  1.1163777112960815,\n",
              "  1.0916224718093872,\n",
              "  1.018528699874878,\n",
              "  1.0873340368270874,\n",
              "  1.098044753074646,\n",
              "  1.1632062196731567,\n",
              "  1.0147634744644165,\n",
              "  1.0428516864776611,\n",
              "  0.9795731902122498,\n",
              "  1.1076666116714478,\n",
              "  1.0168315172195435,\n",
              "  1.0659151077270508,\n",
              "  1.1127629280090332,\n",
              "  1.0148582458496094,\n",
              "  1.0283771753311157,\n",
              "  1.0106450319290161,\n",
              "  1.0309146642684937,\n",
              "  1.0048235654830933,\n",
              "  1.0714523792266846,\n",
              "  1.0981433391571045,\n",
              "  1.0741719007492065,\n",
              "  1.0532903671264648,\n",
              "  1.0590589046478271,\n",
              "  1.0684260129928589,\n",
              "  1.0455082654953003,\n",
              "  1.1554542779922485,\n",
              "  1.105385184288025,\n",
              "  1.0495338439941406,\n",
              "  1.0180858373641968,\n",
              "  1.0249302387237549,\n",
              "  1.045924186706543,\n",
              "  1.0096083879470825,\n",
              "  1.0896674394607544,\n",
              "  1.102207899093628,\n",
              "  0.9878299236297607,\n",
              "  0.9908369779586792,\n",
              "  1.0899962186813354,\n",
              "  0.9527722597122192,\n",
              "  0.9481276273727417,\n",
              "  0.9869678020477295,\n",
              "  0.9997448921203613,\n",
              "  1.0267924070358276,\n",
              "  1.047537922859192,\n",
              "  0.963919997215271,\n",
              "  1.0642269849777222,\n",
              "  1.028855562210083,\n",
              "  1.045957088470459,\n",
              "  1.0408105850219727,\n",
              "  1.013903021812439,\n",
              "  1.070359230041504,\n",
              "  0.9334161281585693,\n",
              "  1.0403411388397217,\n",
              "  1.0612401962280273,\n",
              "  1.015251874923706,\n",
              "  1.0791677236557007,\n",
              "  0.9897089600563049,\n",
              "  1.0976462364196777,\n",
              "  1.034337043762207,\n",
              "  1.0440404415130615,\n",
              "  1.0163713693618774,\n",
              "  1.0125640630722046,\n",
              "  0.8686739206314087,\n",
              "  0.9980978965759277,\n",
              "  1.075803279876709,\n",
              "  1.0330387353897095,\n",
              "  1.0283949375152588,\n",
              "  1.0727033615112305,\n",
              "  1.027919888496399,\n",
              "  1.0664681196212769,\n",
              "  1.0713707208633423,\n",
              "  1.0289257764816284,\n",
              "  1.0035978555679321,\n",
              "  0.9736867547035217,\n",
              "  0.9820901155471802,\n",
              "  0.9827616214752197,\n",
              "  1.0735105276107788,\n",
              "  0.9773318767547607,\n",
              "  1.0572782754898071,\n",
              "  1.0882991552352905,\n",
              "  0.939126193523407,\n",
              "  0.9853202104568481,\n",
              "  0.9770704507827759,\n",
              "  0.9211341142654419,\n",
              "  1.0738104581832886,\n",
              "  0.9617931842803955,\n",
              "  1.0004982948303223,\n",
              "  0.981110155582428,\n",
              "  0.9316064715385437,\n",
              "  1.017562747001648,\n",
              "  1.0235233306884766,\n",
              "  1.0774774551391602,\n",
              "  1.026265263557434,\n",
              "  1.0327916145324707,\n",
              "  0.9936520457267761,\n",
              "  1.0057841539382935,\n",
              "  1.0173914432525635,\n",
              "  0.981449544429779,\n",
              "  1.039514183998108,\n",
              "  1.0086040496826172,\n",
              "  1.0513015985488892,\n",
              "  0.955767810344696,\n",
              "  1.037489652633667,\n",
              "  1.006081223487854,\n",
              "  0.9867291450500488,\n",
              "  1.0295637845993042,\n",
              "  0.990958034992218,\n",
              "  1.0308858156204224,\n",
              "  1.0343397855758667,\n",
              "  0.9749643206596375,\n",
              "  1.0831239223480225,\n",
              "  1.1188385486602783,\n",
              "  1.0027565956115723,\n",
              "  1.0066189765930176,\n",
              "  1.0243511199951172,\n",
              "  1.0167365074157715,\n",
              "  1.027806043624878,\n",
              "  0.936546802520752,\n",
              "  1.0638691186904907,\n",
              "  1.0163977146148682,\n",
              "  1.0289161205291748,\n",
              "  1.0389940738677979,\n",
              "  0.9244080781936646,\n",
              "  0.9789356589317322,\n",
              "  0.9652754664421082,\n",
              "  0.9451622366905212,\n",
              "  1.0134357213974,\n",
              "  0.9978968501091003,\n",
              "  1.0115940570831299,\n",
              "  0.9441702365875244,\n",
              "  0.9612641930580139,\n",
              "  1.0269724130630493,\n",
              "  1.0219355821609497,\n",
              "  1.042148470878601,\n",
              "  0.959510087966919,\n",
              "  0.9874027371406555,\n",
              "  1.0243476629257202,\n",
              "  0.9099841713905334,\n",
              "  1.101893663406372,\n",
              "  1.049543857574463,\n",
              "  1.0192763805389404,\n",
              "  0.8991138935089111,\n",
              "  0.9156007766723633,\n",
              "  0.9604635834693909,\n",
              "  1.0382721424102783,\n",
              "  0.9943763017654419,\n",
              "  0.9436194896697998,\n",
              "  0.9450659155845642,\n",
              "  1.0099087953567505,\n",
              "  0.9575751423835754,\n",
              "  0.9787246584892273,\n",
              "  0.9379590749740601,\n",
              "  0.9399250149726868,\n",
              "  0.9758814573287964,\n",
              "  0.8697083592414856,\n",
              "  0.9786985516548157,\n",
              "  0.9831759333610535,\n",
              "  1.0193721055984497,\n",
              "  0.9404177069664001,\n",
              "  0.9737014770507812,\n",
              "  0.9360685348510742,\n",
              "  0.9753156304359436,\n",
              "  0.9609533548355103,\n",
              "  1.0776880979537964,\n",
              "  0.9590559005737305,\n",
              "  0.9532814025878906,\n",
              "  0.9681826829910278,\n",
              "  0.9195706248283386,\n",
              "  0.9905930757522583,\n",
              "  0.9704877734184265,\n",
              "  0.988644003868103,\n",
              "  1.0047729015350342,\n",
              "  0.9717417359352112,\n",
              "  1.0088582038879395,\n",
              "  0.9014303088188171,\n",
              "  0.9728577733039856,\n",
              "  0.9543389081954956,\n",
              "  1.0372079610824585,\n",
              "  0.9165071845054626,\n",
              "  1.0147112607955933,\n",
              "  1.0197746753692627,\n",
              "  1.0264817476272583,\n",
              "  0.9630258083343506,\n",
              "  0.9247583746910095,\n",
              "  0.9409306645393372,\n",
              "  1.0013140439987183,\n",
              "  1.0731620788574219,\n",
              "  0.8985283374786377,\n",
              "  1.0324783325195312,\n",
              "  0.9857186675071716,\n",
              "  0.9394951462745667,\n",
              "  0.9375097751617432,\n",
              "  0.9779884815216064,\n",
              "  1.0684120655059814,\n",
              "  0.9805267453193665,\n",
              "  1.0407586097717285,\n",
              "  0.929995596408844,\n",
              "  0.9717320799827576,\n",
              "  0.9517063498497009,\n",
              "  1.006785273551941,\n",
              "  0.9367729425430298,\n",
              "  0.9272865056991577,\n",
              "  0.9975269436836243,\n",
              "  0.935167133808136,\n",
              "  0.9128425717353821,\n",
              "  0.8670732378959656,\n",
              "  0.9285749197006226,\n",
              "  0.9674871563911438,\n",
              "  0.9877927303314209,\n",
              "  0.8766473531723022,\n",
              "  0.9738795161247253,\n",
              "  0.9991711378097534,\n",
              "  0.9421467185020447,\n",
              "  0.9915534257888794,\n",
              "  1.0136401653289795,\n",
              "  1.0011121034622192,\n",
              "  1.0363869667053223,\n",
              "  0.9360766410827637,\n",
              "  0.9207096695899963,\n",
              "  0.9248642921447754,\n",
              "  0.9113368988037109,\n",
              "  1.0966678857803345,\n",
              "  0.918887197971344,\n",
              "  0.9901638031005859,\n",
              "  0.9908328056335449,\n",
              "  0.9283266067504883,\n",
              "  0.9293928146362305,\n",
              "  0.9654015302658081,\n",
              "  0.9809396266937256,\n",
              "  0.9674311280250549,\n",
              "  0.917163610458374,\n",
              "  1.0082004070281982,\n",
              "  1.0765153169631958,\n",
              "  1.02606201171875,\n",
              "  0.9477843046188354,\n",
              "  1.0652883052825928,\n",
              "  0.9994634985923767,\n",
              "  0.9105362892150879,\n",
              "  0.9738598465919495,\n",
              "  1.01176118850708,\n",
              "  0.9907512068748474,\n",
              "  0.9696780443191528,\n",
              "  0.9370332956314087,\n",
              "  0.8362250328063965,\n",
              "  0.9380268454551697,\n",
              "  0.9126216769218445,\n",
              "  0.9026410579681396,\n",
              "  0.9519848823547363,\n",
              "  0.9600964188575745,\n",
              "  0.9586766362190247,\n",
              "  0.9212334752082825,\n",
              "  0.9338001012802124,\n",
              "  0.9134304523468018,\n",
              "  0.9810094833374023,\n",
              "  0.9574933052062988,\n",
              "  0.9594501852989197,\n",
              "  0.893196702003479,\n",
              "  0.9714981317520142,\n",
              "  0.9633132815361023,\n",
              "  0.945823609828949,\n",
              "  0.9641426205635071,\n",
              "  0.9307680130004883,\n",
              "  0.9824709296226501,\n",
              "  0.9356735348701477,\n",
              "  1.004377007484436,\n",
              "  0.9440725445747375,\n",
              "  0.9938814043998718,\n",
              "  0.9484991431236267,\n",
              "  0.9412474632263184,\n",
              "  0.9536575675010681,\n",
              "  0.9408275485038757,\n",
              "  0.8958563804626465,\n",
              "  0.9347770810127258,\n",
              "  0.8754701018333435,\n",
              "  0.9457058906555176,\n",
              "  0.9671850800514221,\n",
              "  1.0022070407867432,\n",
              "  0.9305615425109863,\n",
              "  0.9711866974830627,\n",
              "  0.915614664554596,\n",
              "  0.903756320476532,\n",
              "  0.9107063412666321,\n",
              "  0.8390195369720459,\n",
              "  0.9442042112350464,\n",
              "  0.9318637847900391,\n",
              "  0.9146232008934021,\n",
              "  0.9460121393203735,\n",
              "  1.001827359199524,\n",
              "  0.956429660320282,\n",
              "  0.9441701173782349,\n",
              "  0.9936733841896057,\n",
              "  0.9546115398406982,\n",
              "  0.909467339515686,\n",
              "  0.9025872945785522,\n",
              "  0.9615285396575928,\n",
              "  0.9666828513145447,\n",
              "  0.9203383922576904,\n",
              "  0.8875802159309387,\n",
              "  0.9289364814758301,\n",
              "  0.8764984011650085,\n",
              "  0.9391921162605286,\n",
              "  0.843203604221344,\n",
              "  0.8440603613853455,\n",
              "  0.9528179168701172,\n",
              "  0.9617835283279419,\n",
              "  0.9447357058525085,\n",
              "  0.879104733467102,\n",
              "  0.9076382517814636,\n",
              "  1.0285626649856567,\n",
              "  0.9537819623947144,\n",
              "  0.9512428045272827,\n",
              "  0.9529735445976257,\n",
              "  0.9703981280326843,\n",
              "  0.8719930052757263,\n",
              "  0.7973034977912903,\n",
              "  0.9502451419830322,\n",
              "  1.0454074144363403,\n",
              "  0.8868018388748169,\n",
              "  0.9500439167022705,\n",
              "  0.9142489433288574,\n",
              "  0.9158673286437988,\n",
              "  0.9631592035293579,\n",
              "  0.8977872729301453,\n",
              "  0.9155512452125549,\n",
              "  0.9669532775878906,\n",
              "  1.0408802032470703,\n",
              "  0.9387034177780151,\n",
              "  0.940028190612793,\n",
              "  0.9499343037605286,\n",
              "  0.8989699482917786,\n",
              "  0.9023168087005615,\n",
              "  0.8672020435333252,\n",
              "  0.9196709990501404,\n",
              "  1.0019203424453735,\n",
              "  0.8852119445800781,\n",
              "  0.9490175247192383,\n",
              "  0.9513269066810608,\n",
              "  0.9018735885620117,\n",
              "  0.9362555146217346,\n",
              "  0.8938531875610352,\n",
              "  0.9398896098136902,\n",
              "  0.9303441047668457,\n",
              "  0.9602926969528198,\n",
              "  0.9574522972106934,\n",
              "  0.9346452951431274,\n",
              "  0.964810311794281,\n",
              "  0.87823885679245,\n",
              "  0.9123051762580872,\n",
              "  0.8464198112487793,\n",
              "  0.9722959995269775,\n",
              "  1.0563355684280396,\n",
              "  0.9759851098060608,\n",
              "  0.9097187519073486,\n",
              "  0.9417922496795654,\n",
              "  0.9049603343009949,\n",
              "  0.8664255738258362,\n",
              "  0.9567725658416748,\n",
              "  0.9048853516578674,\n",
              "  0.8838903903961182,\n",
              "  0.8359121680259705,\n",
              "  0.9380064010620117,\n",
              "  0.9463686347007751,\n",
              "  0.9576224684715271,\n",
              "  0.8620813488960266,\n",
              "  0.9608244299888611,\n",
              "  0.9127338528633118,\n",
              "  0.8701069355010986,\n",
              "  0.9673827290534973,\n",
              "  0.9076833128929138,\n",
              "  0.9145634174346924,\n",
              "  0.8233895301818848,\n",
              "  0.9334486126899719,\n",
              "  0.8519189953804016,\n",
              "  0.926688551902771,\n",
              "  0.9474603533744812,\n",
              "  0.8634668588638306,\n",
              "  0.9318381547927856,\n",
              "  0.9069830179214478,\n",
              "  0.9445498585700989,\n",
              "  0.8895506858825684,\n",
              "  0.8777638673782349,\n",
              "  0.8906002044677734,\n",
              "  0.9776401519775391,\n",
              "  0.9483094215393066,\n",
              "  0.9146049618721008,\n",
              "  0.9690083861351013,\n",
              "  0.8945600390434265,\n",
              "  0.8934497237205505,\n",
              "  0.9033182859420776,\n",
              "  0.9321179389953613,\n",
              "  0.9237396121025085,\n",
              "  0.8380079865455627,\n",
              "  0.8878259062767029,\n",
              "  0.8488448858261108,\n",
              "  0.8408019542694092,\n",
              "  0.8903898000717163,\n",
              "  0.8885643482208252,\n",
              "  0.9391403198242188,\n",
              "  0.8414806127548218,\n",
              "  0.8637095093727112,\n",
              "  0.8886373043060303,\n",
              "  0.8895877003669739,\n",
              "  0.865943193435669,\n",
              "  0.8970674872398376,\n",
              "  0.99358731508255,\n",
              "  0.9091625213623047,\n",
              "  0.9619473218917847,\n",
              "  0.9242687225341797,\n",
              "  0.9460486173629761,\n",
              "  0.8803053498268127,\n",
              "  0.9825929403305054,\n",
              "  0.9994103908538818,\n",
              "  0.8624243140220642,\n",
              "  0.9727970957756042,\n",
              "  0.8592539429664612,\n",
              "  0.8807275295257568,\n",
              "  0.8546518087387085,\n",
              "  0.9346179962158203,\n",
              "  0.8749452233314514,\n",
              "  0.9050494432449341,\n",
              "  0.9614535570144653,\n",
              "  0.8818492889404297,\n",
              "  0.9727405309677124,\n",
              "  0.9420375823974609,\n",
              "  0.9484703540802002,\n",
              "  0.8893066644668579,\n",
              "  0.9028679728507996,\n",
              "  0.8453792929649353,\n",
              "  0.8778530955314636,\n",
              "  0.9943420886993408,\n",
              "  0.8632626533508301,\n",
              "  0.8956459760665894,\n",
              "  0.8870559334754944,\n",
              "  0.9562803506851196,\n",
              "  0.854640781879425,\n",
              "  0.9654496312141418,\n",
              "  0.8526530265808105,\n",
              "  0.9131314158439636,\n",
              "  0.8806877732276917,\n",
              "  0.8330366015434265,\n",
              "  0.8466009497642517,\n",
              "  0.868803083896637,\n",
              "  0.8395566344261169,\n",
              "  0.9427491426467896,\n",
              "  0.831669270992279,\n",
              "  0.8643875122070312,\n",
              "  0.9026715159416199,\n",
              "  0.9807761907577515,\n",
              "  0.9189401865005493,\n",
              "  0.8559341430664062,\n",
              "  0.9129635691642761,\n",
              "  0.9161696434020996,\n",
              "  0.8501346111297607,\n",
              "  0.9310051798820496,\n",
              "  0.94873046875,\n",
              "  0.9460579752922058,\n",
              "  0.866913378238678,\n",
              "  0.8876771926879883,\n",
              "  0.8604649305343628,\n",
              "  0.8581233620643616,\n",
              "  0.821291446685791,\n",
              "  0.913886308670044,\n",
              "  0.8844525218009949,\n",
              "  0.9136051535606384,\n",
              "  0.9031959176063538,\n",
              "  0.9206477999687195,\n",
              "  0.7978976964950562,\n",
              "  0.9041832685470581,\n",
              "  0.8797032833099365,\n",
              "  0.8048387169837952,\n",
              "  0.8458340167999268,\n",
              "  0.8743174076080322,\n",
              "  0.8146724700927734,\n",
              "  0.8991495966911316,\n",
              "  0.9455509781837463,\n",
              "  0.8834206461906433,\n",
              "  0.8061793446540833,\n",
              "  0.8286053538322449,\n",
              "  0.8380821943283081,\n",
              "  0.8542892336845398,\n",
              "  0.9665550589561462,\n",
              "  0.9267417192459106,\n",
              "  0.881646990776062,\n",
              "  0.9473822116851807,\n",
              "  0.9347716569900513,\n",
              "  0.8339419960975647,\n",
              "  0.9238813519477844,\n",
              "  0.8789501190185547,\n",
              "  0.8650518655776978,\n",
              "  0.7801440954208374,\n",
              "  0.8596048355102539,\n",
              "  0.9271896481513977,\n",
              "  0.8919937014579773,\n",
              "  0.9001438617706299,\n",
              "  0.9061111211776733,\n",
              "  0.8602026104927063,\n",
              "  0.9252812266349792,\n",
              "  0.8904734253883362,\n",
              "  0.8948527574539185,\n",
              "  0.7838565707206726,\n",
              "  0.8416067957878113,\n",
              "  0.9254602193832397,\n",
              "  0.8302603960037231,\n",
              "  0.9051870107650757,\n",
              "  0.8757184147834778,\n",
              "  0.8553869724273682,\n",
              "  0.8283647298812866,\n",
              "  0.764474093914032,\n",
              "  0.8798927664756775,\n",
              "  0.921333909034729,\n",
              "  0.8233451247215271,\n",
              "  0.9010692238807678,\n",
              "  0.9146707057952881,\n",
              "  0.9224526882171631,\n",
              "  0.7888864874839783,\n",
              "  0.785068690776825,\n",
              "  0.9432967901229858,\n",
              "  0.8076707720756531,\n",
              "  0.8314983248710632,\n",
              "  0.8513128757476807,\n",
              "  0.8591565489768982,\n",
              "  0.9105961918830872,\n",
              "  0.9518502950668335,\n",
              "  0.8941450715065002,\n",
              "  0.8361640572547913,\n",
              "  0.794745922088623,\n",
              "  0.8409450054168701,\n",
              "  0.8941686749458313,\n",
              "  0.7933763861656189,\n",
              "  0.8418115377426147,\n",
              "  0.8587106466293335,\n",
              "  0.9179626703262329,\n",
              "  0.8726174831390381,\n",
              "  0.7867987155914307,\n",
              "  0.9207868576049805,\n",
              "  0.8574162721633911,\n",
              "  0.9089679718017578,\n",
              "  0.8305274248123169,\n",
              "  0.8534743189811707,\n",
              "  0.8959801197052002,\n",
              "  0.9026549458503723,\n",
              "  0.8918647170066833,\n",
              "  0.8543820381164551,\n",
              "  0.7755534052848816,\n",
              "  0.7715551257133484,\n",
              "  0.7763722538948059,\n",
              "  0.8198066353797913,\n",
              "  0.856814980506897,\n",
              "  0.8761131763458252,\n",
              "  0.9312182664871216,\n",
              "  0.8650497198104858,\n",
              "  0.9093732833862305,\n",
              "  0.81634122133255,\n",
              "  0.8602728247642517,\n",
              "  0.8285486698150635,\n",
              "  0.8960280418395996,\n",
              "  0.8644225597381592,\n",
              "  0.8971772789955139,\n",
              "  0.803523063659668,\n",
              "  0.863701581954956,\n",
              "  0.8462706804275513,\n",
              "  0.7971436381340027,\n",
              "  0.8510488867759705,\n",
              "  1.002917766571045,\n",
              "  0.8830271363258362,\n",
              "  0.805041491985321,\n",
              "  0.8114491105079651,\n",
              "  0.8637521266937256,\n",
              "  0.8667507767677307,\n",
              "  0.883261501789093,\n",
              "  0.9437896609306335,\n",
              "  0.9175775647163391,\n",
              "  0.8429807424545288,\n",
              "  0.8806266188621521,\n",
              "  0.8450590968132019,\n",
              "  0.8641321659088135,\n",
              "  0.8143476247787476,\n",
              "  0.9184135794639587,\n",
              "  0.7867451906204224,\n",
              "  0.8765899538993835,\n",
              "  0.8210549354553223,\n",
              "  0.9040022492408752,\n",
              "  0.816436231136322,\n",
              "  0.8185116052627563,\n",
              "  0.9167546629905701,\n",
              "  0.8479466438293457,\n",
              "  0.8407979607582092,\n",
              "  0.8790218234062195,\n",
              "  0.9584799408912659,\n",
              "  0.9219748973846436,\n",
              "  0.972372829914093,\n",
              "  0.755240261554718,\n",
              "  0.8855461478233337,\n",
              "  0.9042794108390808,\n",
              "  0.8592513799667358,\n",
              "  0.8345901370048523,\n",
              "  0.8583498001098633,\n",
              "  0.8738413453102112,\n",
              "  0.8654828667640686,\n",
              "  0.9018619656562805,\n",
              "  0.8487786054611206,\n",
              "  0.7095631957054138,\n",
              "  0.849537193775177,\n",
              "  0.8016838431358337,\n",
              "  0.8257073760032654,\n",
              "  0.8409131765365601,\n",
              "  0.8160646557807922,\n",
              "  0.8410167694091797,\n",
              "  0.9348880052566528,\n",
              "  0.8494235277175903,\n",
              "  0.9065450429916382,\n",
              "  0.8978049755096436,\n",
              "  0.8808454275131226],\n",
              " 'acc': [0.25333333015441895,\n",
              "  0.2666666805744171,\n",
              "  0.3100000023841858,\n",
              "  0.31333333253860474,\n",
              "  0.32740214467048645,\n",
              "  0.33451956510543823,\n",
              "  0.3629893362522125,\n",
              "  0.3266666531562805,\n",
              "  0.3100000023841858,\n",
              "  0.3701067566871643,\n",
              "  0.3701067566871643,\n",
              "  0.3736654818058014,\n",
              "  0.36000001430511475,\n",
              "  0.35943061113357544,\n",
              "  0.38999998569488525,\n",
              "  0.35333332419395447,\n",
              "  0.3733333349227905,\n",
              "  0.38999998569488525,\n",
              "  0.39145907759666443,\n",
              "  0.38790035247802734,\n",
              "  0.4021352231502533,\n",
              "  0.4021352231502533,\n",
              "  0.3766666650772095,\n",
              "  0.38333332538604736,\n",
              "  0.3985764980316162,\n",
              "  0.35231316089630127,\n",
              "  0.3733333349227905,\n",
              "  0.4333333373069763,\n",
              "  0.41999998688697815,\n",
              "  0.38078293204307556,\n",
              "  0.3985764980316162,\n",
              "  0.35231316089630127,\n",
              "  0.3772242069244385,\n",
              "  0.4266666769981384,\n",
              "  0.3566666543483734,\n",
              "  0.34333333373069763,\n",
              "  0.3950178027153015,\n",
              "  0.4033333361148834,\n",
              "  0.43772241473197937,\n",
              "  0.43666666746139526,\n",
              "  0.41637009382247925,\n",
              "  0.39145907759666443,\n",
              "  0.41281139850616455,\n",
              "  0.4021352231502533,\n",
              "  0.3985764980316162,\n",
              "  0.41992881894111633,\n",
              "  0.4000000059604645,\n",
              "  0.4333333373069763,\n",
              "  0.35333332419395447,\n",
              "  0.38999998569488525,\n",
              "  0.4266666769981384,\n",
              "  0.38999998569488525,\n",
              "  0.4033333361148834,\n",
              "  0.39666667580604553,\n",
              "  0.3416370153427124,\n",
              "  0.43772241473197937,\n",
              "  0.38790035247802734,\n",
              "  0.4306049942970276,\n",
              "  0.39145907759666443,\n",
              "  0.43666666746139526,\n",
              "  0.4000000059604645,\n",
              "  0.4234875440597534,\n",
              "  0.4399999976158142,\n",
              "  0.4166666567325592,\n",
              "  0.35587188601493835,\n",
              "  0.39666667580604553,\n",
              "  0.44128113985061646,\n",
              "  0.4266666769981384,\n",
              "  0.36666667461395264,\n",
              "  0.44333332777023315,\n",
              "  0.3766666650772095,\n",
              "  0.3772242069244385,\n",
              "  0.3766666650772095,\n",
              "  0.4533333480358124,\n",
              "  0.4166666567325592,\n",
              "  0.4266666769981384,\n",
              "  0.4033333361148834,\n",
              "  0.41992881894111633,\n",
              "  0.4300000071525574,\n",
              "  0.3866666555404663,\n",
              "  0.4166666567325592,\n",
              "  0.41637009382247925,\n",
              "  0.4341636896133423,\n",
              "  0.4056939482688904,\n",
              "  0.44483986496925354,\n",
              "  0.4341636896133423,\n",
              "  0.4021352231502533,\n",
              "  0.4300000071525574,\n",
              "  0.4033333361148834,\n",
              "  0.40925267338752747,\n",
              "  0.4166666567325592,\n",
              "  0.38333332538604736,\n",
              "  0.39145907759666443,\n",
              "  0.3733333349227905,\n",
              "  0.4519572854042053,\n",
              "  0.4233333468437195,\n",
              "  0.4533333480358124,\n",
              "  0.4266666769981384,\n",
              "  0.43772241473197937,\n",
              "  0.4266666769981384,\n",
              "  0.33000001311302185,\n",
              "  0.3933333456516266,\n",
              "  0.4300000071525574,\n",
              "  0.4399999976158142,\n",
              "  0.41637009382247925,\n",
              "  0.4933333396911621,\n",
              "  0.40666666626930237,\n",
              "  0.4590747356414795,\n",
              "  0.38999998569488525,\n",
              "  0.4633333384990692,\n",
              "  0.4300000071525574,\n",
              "  0.3466666638851166,\n",
              "  0.41333332657814026,\n",
              "  0.4306049942970276,\n",
              "  0.44999998807907104,\n",
              "  0.4566666781902313,\n",
              "  0.4333333373069763,\n",
              "  0.41281139850616455,\n",
              "  0.41999998688697815,\n",
              "  0.4555160105228424,\n",
              "  0.4341636896133423,\n",
              "  0.40666666626930237,\n",
              "  0.4866666793823242,\n",
              "  0.3933333456516266,\n",
              "  0.44483986496925354,\n",
              "  0.46000000834465027,\n",
              "  0.41281139850616455,\n",
              "  0.46666666865348816,\n",
              "  0.4466666579246521,\n",
              "  0.4833333194255829,\n",
              "  0.3985764980316162,\n",
              "  0.40666666626930237,\n",
              "  0.46000000834465027,\n",
              "  0.43666666746139526,\n",
              "  0.4341636896133423,\n",
              "  0.46975088119506836,\n",
              "  0.4306049942970276,\n",
              "  0.4533333480358124,\n",
              "  0.4533333480358124,\n",
              "  0.41999998688697815,\n",
              "  0.4566666781902313,\n",
              "  0.4099999964237213,\n",
              "  0.4466666579246521,\n",
              "  0.41637009382247925,\n",
              "  0.4233333468437195,\n",
              "  0.4633333384990692,\n",
              "  0.4633333384990692,\n",
              "  0.4661921560764313,\n",
              "  0.5053380727767944,\n",
              "  0.4833333194255829,\n",
              "  0.49000000953674316,\n",
              "  0.476666659116745,\n",
              "  0.44128113985061646,\n",
              "  0.4661921560764313,\n",
              "  0.44999998807907104,\n",
              "  0.5099999904632568,\n",
              "  0.47333332896232605,\n",
              "  0.4804270565509796,\n",
              "  0.43666666746139526,\n",
              "  0.4533333480358124,\n",
              "  0.4483985900878906,\n",
              "  0.4626334607601166,\n",
              "  0.476666659116745,\n",
              "  0.46975088119506836,\n",
              "  0.4866666793823242,\n",
              "  0.47999998927116394,\n",
              "  0.46000000834465027,\n",
              "  0.476666659116745,\n",
              "  0.4633333384990692,\n",
              "  0.4033333361148834,\n",
              "  0.4875444769859314,\n",
              "  0.4566666781902313,\n",
              "  0.4466666579246521,\n",
              "  0.4033333361148834,\n",
              "  0.47330960631370544,\n",
              "  0.47686833143234253,\n",
              "  0.4633333384990692,\n",
              "  0.49000000953674316,\n",
              "  0.4875444769859314,\n",
              "  0.4633333384990692,\n",
              "  0.44333332777023315,\n",
              "  0.44999998807907104,\n",
              "  0.4533333480358124,\n",
              "  0.5017793774604797,\n",
              "  0.4633333384990692,\n",
              "  0.47686833143234253,\n",
              "  0.5133333206176758,\n",
              "  0.4866666793823242,\n",
              "  0.43772241473197937,\n",
              "  0.46975088119506836,\n",
              "  0.476666659116745,\n",
              "  0.44999998807907104,\n",
              "  0.476666659116745,\n",
              "  0.46975088119506836,\n",
              "  0.5017793774604797,\n",
              "  0.47999998927116394,\n",
              "  0.49466192722320557,\n",
              "  0.43666666746139526,\n",
              "  0.46000000834465027,\n",
              "  0.46975088119506836,\n",
              "  0.4875444769859314,\n",
              "  0.47333332896232605,\n",
              "  0.49666666984558105,\n",
              "  0.46975088119506836,\n",
              "  0.5133333206176758,\n",
              "  0.476666659116745,\n",
              "  0.44999998807907104,\n",
              "  0.5099999904632568,\n",
              "  0.4804270565509796,\n",
              "  0.46666666865348816,\n",
              "  0.5231316685676575,\n",
              "  0.5166666507720947,\n",
              "  0.5266666412353516,\n",
              "  0.47333332896232605,\n",
              "  0.5166666507720947,\n",
              "  0.46666666865348816,\n",
              "  0.46000000834465027,\n",
              "  0.5333333611488342,\n",
              "  0.5053380727767944,\n",
              "  0.46666666865348816,\n",
              "  0.49822065234184265,\n",
              "  0.49000000953674316,\n",
              "  0.4466666579246521,\n",
              "  0.46975088119506836,\n",
              "  0.40666666626930237,\n",
              "  0.5088967680931091,\n",
              "  0.4590747356414795,\n",
              "  0.4875444769859314,\n",
              "  0.5195729732513428,\n",
              "  0.5266903638839722,\n",
              "  0.4866666793823242,\n",
              "  0.46975088119506836,\n",
              "  0.47333332896232605,\n",
              "  0.47330960631370544,\n",
              "  0.476666659116745,\n",
              "  0.5299999713897705,\n",
              "  0.49822065234184265,\n",
              "  0.5231316685676575,\n",
              "  0.4555160105228424,\n",
              "  0.4866666793823242,\n",
              "  0.5302491188049316,\n",
              "  0.4804270565509796,\n",
              "  0.49666666984558105,\n",
              "  0.5166666507720947,\n",
              "  0.5017793774604797,\n",
              "  0.5480427145957947,\n",
              "  0.4699999988079071,\n",
              "  0.4519572854042053,\n",
              "  0.5053380727767944,\n",
              "  0.5066666603088379,\n",
              "  0.46975088119506836,\n",
              "  0.5066666603088379,\n",
              "  0.5444839596748352,\n",
              "  0.47330960631370544,\n",
              "  0.5338078141212463,\n",
              "  0.4566666781902313,\n",
              "  0.49822065234184265,\n",
              "  0.5017793774604797,\n",
              "  0.49000000953674316,\n",
              "  0.49666666984558105,\n",
              "  0.5099999904632568,\n",
              "  0.5088967680931091,\n",
              "  0.5066666603088379,\n",
              "  0.4699999988079071,\n",
              "  0.4699999988079071,\n",
              "  0.5266666412353516,\n",
              "  0.4234875440597534,\n",
              "  0.5233333110809326,\n",
              "  0.4833333194255829,\n",
              "  0.4341636896133423,\n",
              "  0.5199999809265137,\n",
              "  0.5017793774604797,\n",
              "  0.5195729732513428,\n",
              "  0.5017793774604797,\n",
              "  0.4839857518672943,\n",
              "  0.4466666579246521,\n",
              "  0.49466192722320557,\n",
              "  0.5299999713897705,\n",
              "  0.4911032021045685,\n",
              "  0.4866666793823242,\n",
              "  0.5,\n",
              "  0.4804270565509796,\n",
              "  0.4833333194255829,\n",
              "  0.46975088119506836,\n",
              "  0.47333332896232605,\n",
              "  0.4839857518672943,\n",
              "  0.4875444769859314,\n",
              "  0.5533333420753479,\n",
              "  0.49666666984558105,\n",
              "  0.49666666984558105,\n",
              "  0.49822065234184265,\n",
              "  0.5231316685676575,\n",
              "  0.5516014099121094,\n",
              "  0.5444839596748352,\n",
              "  0.5266666412353516,\n",
              "  0.5160142183303833,\n",
              "  0.5516014099121094,\n",
              "  0.5338078141212463,\n",
              "  0.54666668176651,\n",
              "  0.5233333110809326,\n",
              "  0.5066666603088379,\n",
              "  0.5266666412353516,\n",
              "  0.5099999904632568,\n",
              "  0.5699999928474426,\n",
              "  0.5366666913032532,\n",
              "  0.5099999904632568,\n",
              "  0.503333330154419,\n",
              "  0.5017793774604797,\n",
              "  0.5099999904632568,\n",
              "  0.5299999713897705,\n",
              "  0.47333332896232605,\n",
              "  0.5066666603088379,\n",
              "  0.5299999713897705,\n",
              "  0.49466192722320557,\n",
              "  0.5299999713897705,\n",
              "  0.5195729732513428,\n",
              "  0.5099999904632568,\n",
              "  0.5666666626930237,\n",
              "  0.49666666984558105,\n",
              "  0.5551601648330688,\n",
              "  0.5133333206176758,\n",
              "  0.5231316685676575,\n",
              "  0.5133333206176758,\n",
              "  0.5622775554656982,\n",
              "  0.5433333516120911,\n",
              "  0.5551601648330688,\n",
              "  0.47999998927116394,\n",
              "  0.5400000214576721,\n",
              "  0.5,\n",
              "  0.4555160105228424,\n",
              "  0.5366666913032532,\n",
              "  0.5266903638839722,\n",
              "  0.5266666412353516,\n",
              "  0.5400000214576721,\n",
              "  0.5366666913032532,\n",
              "  0.5400000214576721,\n",
              "  0.44333332777023315,\n",
              "  0.4933333396911621,\n",
              "  0.5124555230140686,\n",
              "  0.49666666984558105,\n",
              "  0.5766666531562805,\n",
              "  0.5400000214576721,\n",
              "  0.5600000023841858,\n",
              "  0.5133333206176758,\n",
              "  0.5233333110809326,\n",
              "  0.5299999713897705,\n",
              "  0.5266666412353516,\n",
              "  0.49000000953674316,\n",
              "  0.503333330154419,\n",
              "  0.4866666793823242,\n",
              "  0.5017793774604797,\n",
              "  0.5433333516120911,\n",
              "  0.5017793774604797,\n",
              "  0.5199999809265137,\n",
              "  0.5333333611488342,\n",
              "  0.5233333110809326,\n",
              "  0.5124555230140686,\n",
              "  0.5799999833106995,\n",
              "  0.5658363103866577,\n",
              "  0.5480427145957947,\n",
              "  0.47999998927116394,\n",
              "  0.5066666603088379,\n",
              "  0.5600000023841858,\n",
              "  0.46666666865348816,\n",
              "  0.5433333516120911,\n",
              "  0.5199999809265137,\n",
              "  0.49466192722320557,\n",
              "  0.5053380727767944,\n",
              "  0.5533333420753479,\n",
              "  0.5566666722297668,\n",
              "  0.5600000023841858,\n",
              "  0.46666666865348816,\n",
              "  0.5699999928474426,\n",
              "  0.5333333611488342,\n",
              "  0.5066666603088379,\n",
              "  0.5533333420753479,\n",
              "  0.5199999809265137,\n",
              "  0.476666659116745,\n",
              "  0.5299999713897705,\n",
              "  0.4566666781902313,\n",
              "  0.5373665690422058,\n",
              "  0.49822065234184265,\n",
              "  0.5516014099121094,\n",
              "  0.5871886014938354,\n",
              "  0.5444839596748352,\n",
              "  0.5133333206176758,\n",
              "  0.5333333611488342,\n",
              "  0.6120996475219727,\n",
              "  0.5233333110809326,\n",
              "  0.54666668176651,\n",
              "  0.5566666722297668,\n",
              "  0.5266666412353516,\n",
              "  0.5066666603088379,\n",
              "  0.4833333194255829,\n",
              "  0.5587188601493835,\n",
              "  0.5766666531562805,\n",
              "  0.5836299061775208,\n",
              "  0.5166666507720947,\n",
              "  0.5733333230018616,\n",
              "  0.5480427145957947,\n",
              "  0.5,\n",
              "  0.5766666531562805,\n",
              "  0.5666666626930237,\n",
              "  0.5658363103866577,\n",
              "  0.5433333516120911,\n",
              "  0.5444839596748352,\n",
              "  0.5233333110809326,\n",
              "  0.5366666913032532,\n",
              "  0.5160142183303833,\n",
              "  0.5366666913032532,\n",
              "  0.5516014099121094,\n",
              "  0.5433333516120911,\n",
              "  0.5133333206176758,\n",
              "  0.4911032021045685,\n",
              "  0.5266666412353516,\n",
              "  0.5433333516120911,\n",
              "  0.550000011920929,\n",
              "  0.5333333611488342,\n",
              "  0.5373665690422058,\n",
              "  0.5199999809265137,\n",
              "  0.5587188601493835,\n",
              "  0.49666666984558105,\n",
              "  0.5871886014938354,\n",
              "  0.5366666913032532,\n",
              "  0.5333333611488342,\n",
              "  0.5622775554656982,\n",
              "  0.6000000238418579,\n",
              "  0.5366666913032532,\n",
              "  0.5516014099121094,\n",
              "  0.5551601648330688,\n",
              "  0.5231316685676575,\n",
              "  0.5600000023841858,\n",
              "  0.5622775554656982,\n",
              "  0.5587188601493835,\n",
              "  0.5566666722297668,\n",
              "  0.5409252643585205,\n",
              "  0.5566666722297668,\n",
              "  0.5199999809265137,\n",
              "  0.6233333349227905,\n",
              "  0.5231316685676575,\n",
              "  0.550000011920929,\n",
              "  0.54666668176651,\n",
              "  0.5099999904632568,\n",
              "  0.5480427145957947,\n",
              "  0.5373665690422058,\n",
              "  0.5480427145957947,\n",
              "  0.5366666913032532,\n",
              "  0.5551601648330688,\n",
              "  0.5766666531562805,\n",
              "  0.608540952205658,\n",
              "  0.5765124559402466,\n",
              "  0.5366666913032532,\n",
              "  0.5433333516120911,\n",
              "  0.5400000214576721,\n",
              "  0.5133333206176758,\n",
              "  0.550000011920929,\n",
              "  0.5195729732513428,\n",
              "  0.5433333516120911,\n",
              "  0.54666668176651,\n",
              "  0.5666666626930237,\n",
              "  0.5600000023841858,\n",
              "  0.5943060517311096,\n",
              "  0.5899999737739563,\n",
              "  0.5199999809265137,\n",
              "  0.5600000023841858,\n",
              "  0.5199999809265137,\n",
              "  0.5299999713897705,\n",
              "  0.5666666626930237,\n",
              "  0.5833333134651184,\n",
              "  0.5729537606239319,\n",
              "  0.5699999928474426,\n",
              "  0.5633333325386047,\n",
              "  0.5666666626930237,\n",
              "  0.5733333230018616,\n",
              "  0.5836299061775208,\n",
              "  0.5699999928474426,\n",
              "  0.5729537606239319,\n",
              "  0.550000011920929,\n",
              "  0.5338078141212463,\n",
              "  0.5302491188049316,\n",
              "  0.5480427145957947,\n",
              "  0.5551601648330688,\n",
              "  0.5633333325386047,\n",
              "  0.5699999928474426,\n",
              "  0.5836299061775208,\n",
              "  0.5266666412353516,\n",
              "  0.5551601648330688,\n",
              "  0.5160142183303833,\n",
              "  0.5799999833106995,\n",
              "  0.5373665690422058,\n",
              "  0.5373665690422058,\n",
              "  0.54666668176651,\n",
              "  0.5799999833106995,\n",
              "  0.5766666531562805,\n",
              "  0.5633333325386047,\n",
              "  0.5433333516120911,\n",
              "  0.5733333230018616,\n",
              "  0.5622775554656982,\n",
              "  0.5099999904632568,\n",
              "  0.5400000214576721,\n",
              "  0.5400000214576721,\n",
              "  0.5799999833106995,\n",
              "  0.5551601648330688,\n",
              "  0.5366666913032532,\n",
              "  0.5765124559402466,\n",
              "  0.5600000023841858,\n",
              "  0.5444839596748352,\n",
              "  0.5533333420753479,\n",
              "  0.5433333516120911,\n",
              "  0.5622775554656982,\n",
              "  0.5799999833106995,\n",
              "  0.550000011920929,\n",
              "  0.5733333230018616,\n",
              "  0.5551601648330688,\n",
              "  0.5433333516120911,\n",
              "  0.5516014099121094,\n",
              "  0.6033333539962769,\n",
              "  0.6049821972846985,\n",
              "  0.5400000214576721,\n",
              "  0.5302491188049316,\n",
              "  0.5133333206176758,\n",
              "  0.5799999833106995,\n",
              "  0.54666668176651,\n",
              "  0.5302491188049316,\n",
              "  0.5871886014938354,\n",
              "  0.5533333420753479,\n",
              "  0.5066666603088379,\n",
              "  0.5066666603088379,\n",
              "  0.6299999952316284,\n",
              "  0.6066666841506958,\n",
              "  0.5933333039283752,\n",
              "  0.5693950057029724,\n",
              "  0.5666666626930237,\n",
              "  0.5978647470474243,\n",
              "  0.6133333444595337,\n",
              "  0.5899999737739563,\n",
              "  0.5871886014938354,\n",
              "  0.5633333325386047,\n",
              "  0.5729537606239319,\n",
              "  0.6014235019683838,\n",
              "  0.5409252643585205,\n",
              "  0.6233333349227905,\n",
              "  0.5633333325386047,\n",
              "  0.5587188601493835,\n",
              "  0.5729537606239319,\n",
              "  0.5733333230018616,\n",
              "  0.5899999737739563,\n",
              "  0.5933333039283752,\n",
              "  0.5566666722297668,\n",
              "  0.5799999833106995,\n",
              "  0.5,\n",
              "  0.6033333539962769,\n",
              "  0.5943060517311096,\n",
              "  0.5800711512565613,\n",
              "  0.5765124559402466,\n",
              "  0.5566666722297668,\n",
              "  0.596666693687439,\n",
              "  0.5587188601493835,\n",
              "  0.5658363103866577,\n",
              "  0.550000011920929,\n",
              "  0.5733333230018616,\n",
              "  0.608540952205658,\n",
              "  0.5533333420753479,\n",
              "  0.5933333039283752,\n",
              "  0.5444839596748352,\n",
              "  0.608540952205658,\n",
              "  0.5566666722297668,\n",
              "  0.5693950057029724,\n",
              "  0.5699999928474426,\n",
              "  0.5622775554656982,\n",
              "  0.5666666626930237,\n",
              "  0.5943060517311096,\n",
              "  0.5633333325386047,\n",
              "  0.5233333110809326,\n",
              "  0.6133333444595337,\n",
              "  0.5233333110809326,\n",
              "  0.5566666722297668,\n",
              "  0.608540952205658,\n",
              "  0.5799999833106995,\n",
              "  0.5799999833106995,\n",
              "  0.5433333516120911,\n",
              "  0.5633333325386047,\n",
              "  0.5400000214576721,\n",
              "  0.5899999737739563,\n",
              "  0.5666666626930237,\n",
              "  0.5633333325386047,\n",
              "  0.5195729732513428,\n",
              "  0.5866666436195374,\n",
              "  0.5666666626930237,\n",
              "  0.5633333325386047,\n",
              "  0.5799999833106995,\n",
              "  0.5587188601493835,\n",
              "  0.6066666841506958,\n",
              "  0.5866666436195374,\n",
              "  0.596666693687439,\n",
              "  0.5622775554656982,\n",
              "  0.5866666436195374,\n",
              "  0.5733333230018616,\n",
              "  0.5765124559402466,\n",
              "  0.5587188601493835,\n",
              "  0.5766666531562805,\n",
              "  0.550000011920929,\n",
              "  0.5666666626930237,\n",
              "  0.5099999904632568,\n",
              "  0.5978647470474243,\n",
              "  0.608540952205658,\n",
              "  0.5766666531562805,\n",
              "  0.5978647470474243,\n",
              "  0.5400000214576721,\n",
              "  0.5866666436195374,\n",
              "  0.550000011920929,\n",
              "  0.5566666722297668,\n",
              "  0.608540952205658,\n",
              "  0.5933333039283752,\n",
              "  0.5633333325386047,\n",
              "  0.6120996475219727,\n",
              "  0.596666693687439,\n",
              "  0.6200000047683716,\n",
              "  0.5733333230018616,\n",
              "  0.5160142183303833,\n",
              "  0.5409252643585205,\n",
              "  0.5800711512565613,\n",
              "  0.5551601648330688,\n",
              "  0.5666666626930237,\n",
              "  0.5933333039283752,\n",
              "  0.5833333134651184,\n",
              "  0.5566666722297668,\n",
              "  0.5587188601493835,\n",
              "  0.5943060517311096,\n",
              "  0.5733333230018616,\n",
              "  0.6476868391036987,\n",
              "  0.6033333539962769,\n",
              "  0.6233333349227905,\n",
              "  0.5978647470474243,\n",
              "  0.596666693687439,\n",
              "  0.596666693687439,\n",
              "  0.5533333420753479,\n",
              "  0.5800711512565613,\n",
              "  0.5871886014938354,\n",
              "  0.5866666436195374,\n",
              "  0.5266903638839722,\n",
              "  0.5899999737739563,\n",
              "  0.5836299061775208,\n",
              "  0.5943060517311096,\n",
              "  0.5766666531562805,\n",
              "  0.5943060517311096,\n",
              "  0.5907473564147949,\n",
              "  0.5800711512565613,\n",
              "  0.5799999833106995,\n",
              "  0.5533333420753479,\n",
              "  0.5933333039283752,\n",
              "  0.5400000214576721,\n",
              "  0.5871886014938354,\n",
              "  0.5833333134651184,\n",
              "  0.5622775554656982,\n",
              "  0.596666693687439,\n",
              "  0.608540952205658,\n",
              "  0.6033333539962769,\n",
              "  0.5800711512565613,\n",
              "  0.5800711512565613,\n",
              "  0.6298932433128357,\n",
              "  0.5836299061775208,\n",
              "  0.5699999928474426,\n",
              "  0.5907473564147949,\n",
              "  0.5600000023841858,\n",
              "  0.5833333134651184,\n",
              "  0.5978647470474243,\n",
              "  0.6100000143051147,\n",
              "  0.5907473564147949,\n",
              "  0.653333306312561,\n",
              "  0.6014235019683838,\n",
              "  0.5866666436195374,\n",
              "  0.5516014099121094,\n",
              "  0.5871886014938354,\n",
              "  0.5633333325386047,\n",
              "  0.5733333230018616,\n",
              "  0.5799999833106995,\n",
              "  0.6033333539962769,\n",
              "  0.5943060517311096,\n",
              "  0.596666693687439,\n",
              "  0.6233333349227905,\n",
              "  0.5836299061775208,\n",
              "  0.5733333230018616,\n",
              "  0.6014235019683838,\n",
              "  0.6120996475219727,\n",
              "  0.5833333134651184,\n",
              "  0.6049821972846985,\n",
              "  0.5833333134651184,\n",
              "  0.6192170977592468,\n",
              "  0.6334519386291504,\n",
              "  0.5633333325386047,\n",
              "  0.5666666626930237,\n",
              "  0.5766666531562805,\n",
              "  0.6100000143051147,\n",
              "  0.6192170977592468,\n",
              "  0.5409252643585205,\n",
              "  0.5658363103866577,\n",
              "  0.6000000238418579,\n",
              "  0.5766666531562805,\n",
              "  0.54666668176651,\n",
              "  0.6049821972846985,\n",
              "  0.6548042893409729,\n",
              "  0.5866666436195374,\n",
              "  0.4804270565509796,\n",
              "  0.5799999833106995,\n",
              "  0.5693950057029724,\n",
              "  0.5666666626930237,\n",
              "  0.5833333134651184,\n",
              "  0.5658363103866577,\n",
              "  0.5699999928474426,\n",
              "  0.5943060517311096,\n",
              "  0.6014235019683838,\n",
              "  0.5658363103866577,\n",
              "  0.5799999833106995,\n",
              "  0.5899999737739563,\n",
              "  0.5299999713897705,\n",
              "  0.6333333253860474,\n",
              "  0.5943060517311096,\n",
              "  0.6166666746139526,\n",
              "  0.6133333444595337,\n",
              "  0.5765124559402466,\n",
              "  0.626334547996521,\n",
              "  0.5733333230018616,\n",
              "  0.5766666531562805,\n",
              "  0.6133333444595337,\n",
              "  0.6066666841506958,\n",
              "  0.6120996475219727,\n",
              "  0.6120996475219727,\n",
              "  0.5978647470474243,\n",
              "  0.5978647470474243,\n",
              "  0.6100000143051147,\n",
              "  0.5833333134651184,\n",
              "  0.5907473564147949,\n",
              "  0.653333306312561,\n",
              "  0.5899999737739563,\n",
              "  0.6399999856948853,\n",
              "  0.5480427145957947,\n",
              "  0.5409252643585205,\n",
              "  0.5799999833106995,\n",
              "  0.608540952205658,\n",
              "  0.5871886014938354,\n",
              "  0.5733333230018616,\n",
              "  0.6156583428382874,\n",
              "  0.5533333420753479,\n",
              "  0.6233333349227905,\n",
              "  0.6166666746139526,\n",
              "  0.6298932433128357,\n",
              "  0.5978647470474243,\n",
              "  0.5866666436195374,\n",
              "  0.5866666436195374,\n",
              "  0.6370106935501099,\n",
              "  0.5516014099121094,\n",
              "  0.608540952205658,\n",
              "  0.6266666650772095,\n",
              "  0.5699999928474426,\n",
              "  0.5799999833106995,\n",
              "  0.6233333349227905,\n",
              "  0.6512455344200134,\n",
              "  0.5766666531562805,\n",
              "  0.6233333349227905,\n",
              "  0.5765124559402466,\n",
              "  0.5566666722297668,\n",
              "  0.5907473564147949,\n",
              "  0.6166666746139526,\n",
              "  0.596666693687439,\n",
              "  0.5699999928474426,\n",
              "  0.6000000238418579,\n",
              "  0.6366666555404663,\n",
              "  0.6233333349227905,\n",
              "  0.5302491188049316,\n",
              "  0.6000000238418579,\n",
              "  0.5871886014938354,\n",
              "  0.5622775554656982,\n",
              "  0.5799999833106995,\n",
              "  0.6000000238418579,\n",
              "  0.6299999952316284,\n",
              "  0.5799999833106995,\n",
              "  0.6049821972846985,\n",
              "  0.6200000047683716,\n",
              "  0.6133333444595337,\n",
              "  0.6566666960716248,\n",
              "  0.626334547996521,\n",
              "  0.5833333134651184,\n",
              "  0.5693950057029724,\n",
              "  0.6156583428382874,\n",
              "  0.6266666650772095,\n",
              "  0.5978647470474243,\n",
              "  0.626334547996521,\n",
              "  0.6100000143051147,\n",
              "  0.6033333539962769,\n",
              "  0.6366666555404663,\n",
              "  0.5765124559402466,\n",
              "  0.5658363103866577,\n",
              "  0.6000000238418579,\n",
              "  0.5978647470474243,\n",
              "  0.5907473564147949,\n",
              "  0.5866666436195374,\n",
              "  0.5871886014938354,\n",
              "  0.596666693687439,\n",
              "  0.6266666650772095,\n",
              "  0.5444839596748352,\n",
              "  0.6200000047683716,\n",
              "  0.5799999833106995,\n",
              "  0.6233333349227905,\n",
              "  0.608540952205658,\n",
              "  0.608540952205658,\n",
              "  0.5899999737739563,\n",
              "  0.5566666722297668,\n",
              "  0.6166666746139526,\n",
              "  0.6120996475219727,\n",
              "  0.5729537606239319,\n",
              "  0.5799999833106995,\n",
              "  0.6066666841506958,\n",
              "  0.6227757930755615,\n",
              "  0.6266666650772095,\n",
              "  0.608540952205658,\n",
              "  0.5195729732513428,\n",
              "  0.608540952205658,\n",
              "  0.6133333444595337,\n",
              "  0.6156583428382874,\n",
              "  0.5800711512565613,\n",
              "  0.6233333349227905,\n",
              "  0.5933333039283752,\n",
              "  0.6120996475219727,\n",
              "  0.5978647470474243,\n",
              "  0.5733333230018616,\n",
              "  0.6476868391036987,\n",
              "  0.5833333134651184,\n",
              "  0.6100000143051147,\n",
              "  0.6334519386291504,\n",
              "  0.6049821972846985,\n",
              "  0.6405693888664246,\n",
              "  0.6233333349227905,\n",
              "  0.5933333039283752,\n",
              "  0.5633333325386047,\n",
              "  0.5729537606239319,\n",
              "  0.596666693687439,\n",
              "  0.596666693687439,\n",
              "  0.5866666436195374,\n",
              "  0.6548042893409729,\n",
              "  0.5587188601493835,\n",
              "  0.5836299061775208,\n",
              "  0.608540952205658,\n",
              "  0.6166666746139526,\n",
              "  0.608540952205658,\n",
              "  0.6334519386291504,\n",
              "  0.6370106935501099,\n",
              "  0.6266666650772095,\n",
              "  0.5933333039283752,\n",
              "  0.6000000238418579,\n",
              "  0.5907473564147949,\n",
              "  0.5833333134651184,\n",
              "  0.6100000143051147,\n",
              "  0.6433333158493042,\n",
              "  0.6100000143051147,\n",
              "  0.6399999856948853,\n",
              "  0.6619216799736023,\n",
              "  0.6441280841827393,\n",
              "  0.6433333158493042,\n",
              "  0.6200000047683716,\n",
              "  0.6200000047683716,\n",
              "  0.6156583428382874,\n",
              "  0.5978647470474243,\n",
              "  0.6619216799736023,\n",
              "  0.6600000262260437,\n",
              "  0.626334547996521,\n",
              "  0.6619216799736023,\n",
              "  0.5833333134651184,\n",
              "  0.5866666436195374,\n",
              "  0.626334547996521,\n",
              "  0.5899999737739563,\n",
              "  0.6192170977592468,\n",
              "  0.6399999856948853,\n",
              "  0.5943060517311096,\n",
              "  0.6266666650772095,\n",
              "  0.6334519386291504,\n",
              "  0.6334519386291504,\n",
              "  0.6049821972846985,\n",
              "  0.5699999928474426,\n",
              "  0.5833333134651184,\n",
              "  0.6100000143051147,\n",
              "  0.6233333349227905,\n",
              "  0.6512455344200134,\n",
              "  0.5658363103866577,\n",
              "  0.6000000238418579,\n",
              "  0.5871886014938354,\n",
              "  0.6566666960716248,\n",
              "  0.6049821972846985,\n",
              "  0.6000000238418579,\n",
              "  0.6333333253860474,\n",
              "  0.6014235019683838,\n",
              "  0.6298932433128357,\n",
              "  0.6066666841506958,\n",
              "  0.6370106935501099,\n",
              "  0.6548042893409729,\n",
              "  0.5766666531562805,\n",
              "  0.596666693687439,\n",
              "  0.6690391302108765,\n",
              "  0.5666666626930237,\n",
              "  0.6049821972846985,\n",
              "  0.6166666746139526,\n",
              "  0.6933333277702332,\n",
              "  0.6761565804481506,\n",
              "  0.6100000143051147,\n",
              "  0.6333333253860474,\n",
              "  0.6100000143051147,\n",
              "  0.6619216799736023,\n",
              "  0.6227757930755615,\n",
              "  0.6100000143051147,\n",
              "  0.5899999737739563,\n",
              "  0.5943060517311096,\n",
              "  0.6033333539962769,\n",
              "  0.6156583428382874,\n",
              "  0.6200000047683716,\n",
              "  0.6233333349227905,\n",
              "  0.6476868391036987,\n",
              "  0.6192170977592468,\n",
              "  0.6049821972846985,\n",
              "  0.5699999928474426,\n",
              "  0.6033333539962769,\n",
              "  0.6476868391036987,\n",
              "  0.6133333444595337,\n",
              "  0.6100000143051147,\n",
              "  0.596666693687439,\n",
              "  0.6200000047683716,\n",
              "  0.5899999737739563,\n",
              "  0.6548042893409729,\n",
              "  0.6133333444595337,\n",
              "  0.6200000047683716,\n",
              "  0.6227757930755615,\n",
              "  0.6366666555404663,\n",
              "  0.6690391302108765,\n",
              "  0.6833333373069763,\n",
              "  0.6405693888664246,\n",
              "  0.6133333444595337,\n",
              "  0.6266666650772095,\n",
              "  0.5693950057029724,\n",
              "  0.6156583428382874,\n",
              "  0.6066666841506958,\n",
              "  0.626334547996521,\n",
              "  0.6399999856948853,\n",
              "  0.6233333349227905,\n",
              "  0.6366666555404663,\n",
              "  0.626334547996521,\n",
              "  0.5800711512565613,\n",
              "  0.6476868391036987,\n",
              "  0.6166666746139526,\n",
              "  0.6433333158493042,\n",
              "  0.6690391302108765,\n",
              "  0.6266666650772095,\n",
              "  0.5907473564147949,\n",
              "  0.5866666436195374,\n",
              "  0.6466666460037231,\n",
              "  0.6619216799736023,\n",
              "  0.6512455344200134,\n",
              "  0.6200000047683716,\n",
              "  0.6192170977592468,\n",
              "  0.596666693687439,\n",
              "  0.5978647470474243,\n",
              "  0.626334547996521,\n",
              "  0.6298932433128357,\n",
              "  0.6583629846572876,\n",
              "  0.6200000047683716,\n",
              "  0.6433333158493042,\n",
              "  0.6014235019683838,\n",
              "  0.6690391302108765,\n",
              "  0.6200000047683716,\n",
              "  0.653333306312561,\n",
              "  0.5899999737739563,\n",
              "  0.6566666960716248,\n",
              "  0.6583629846572876,\n",
              "  0.5733333230018616,\n",
              "  0.6049821972846985,\n",
              "  0.6299999952316284,\n",
              "  0.6433333158493042,\n",
              "  0.5836299061775208,\n",
              "  0.5833333134651184,\n",
              "  0.5933333039283752,\n",
              "  0.6766666769981384,\n",
              "  0.6133333444595337,\n",
              "  0.5836299061775208,\n",
              "  0.6370106935501099,\n",
              "  0.6200000047683716,\n",
              "  0.6333333253860474,\n",
              "  0.6334519386291504,\n",
              "  0.6066666841506958,\n",
              "  0.6405693888664246,\n",
              "  0.6233333349227905,\n",
              "  0.6583629846572876,\n",
              "  0.5933333039283752,\n",
              "  0.6476868391036987,\n",
              "  0.6399999856948853,\n",
              "  0.6066666841506958,\n",
              "  0.6166666746139526,\n",
              "  0.608540952205658,\n",
              "  0.6100000143051147,\n",
              "  0.6548042893409729,\n",
              "  0.5978647470474243,\n",
              "  0.5693950057029724,\n",
              "  0.5866666436195374],\n",
              " 'val_loss': [1.6265580654144287,\n",
              "  1.7385571002960205,\n",
              "  1.7743476629257202,\n",
              "  1.6005895137786865,\n",
              "  1.526865839958191,\n",
              "  1.8201344013214111,\n",
              "  1.6571136713027954,\n",
              "  1.6124769449234009,\n",
              "  2.144350290298462,\n",
              "  1.3370637893676758,\n",
              "  1.7241222858428955,\n",
              "  1.610740303993225,\n",
              "  1.9110358953475952,\n",
              "  1.9124095439910889,\n",
              "  1.9722278118133545,\n",
              "  1.801398515701294,\n",
              "  1.8991386890411377,\n",
              "  1.4334160089492798,\n",
              "  1.5228843688964844,\n",
              "  1.6084076166152954,\n",
              "  1.834071159362793,\n",
              "  2.054468870162964,\n",
              "  2.1743481159210205,\n",
              "  2.194308280944824,\n",
              "  1.9397350549697876,\n",
              "  1.6912758350372314,\n",
              "  1.8336665630340576,\n",
              "  1.9503625631332397,\n",
              "  2.014692783355713,\n",
              "  1.4527899026870728,\n",
              "  1.536381483078003,\n",
              "  1.7132648229599,\n",
              "  1.4576019048690796,\n",
              "  1.887372612953186,\n",
              "  1.5880013704299927,\n",
              "  1.5483533143997192,\n",
              "  1.5571426153182983,\n",
              "  1.7171176671981812,\n",
              "  1.8054711818695068,\n",
              "  1.5245519876480103,\n",
              "  1.57062828540802,\n",
              "  1.8075320720672607,\n",
              "  1.9512386322021484,\n",
              "  1.676145315170288,\n",
              "  1.4524762630462646,\n",
              "  1.8321006298065186,\n",
              "  1.5849472284317017,\n",
              "  1.7051807641983032,\n",
              "  1.4627454280853271,\n",
              "  1.5240440368652344,\n",
              "  2.19197154045105,\n",
              "  1.6204293966293335,\n",
              "  1.8021808862686157,\n",
              "  1.515639066696167,\n",
              "  1.2210838794708252,\n",
              "  1.6950995922088623,\n",
              "  1.7439756393432617,\n",
              "  1.5987714529037476,\n",
              "  1.624664306640625,\n",
              "  1.5644153356552124,\n",
              "  1.4920110702514648,\n",
              "  1.748766303062439,\n",
              "  1.945383071899414,\n",
              "  1.6188790798187256,\n",
              "  1.830134391784668,\n",
              "  1.3357702493667603,\n",
              "  1.6351410150527954,\n",
              "  1.916746735572815,\n",
              "  1.7919471263885498,\n",
              "  1.1109209060668945,\n",
              "  1.9640426635742188,\n",
              "  1.817172646522522,\n",
              "  1.557539939880371,\n",
              "  1.8557687997817993,\n",
              "  1.6980817317962646,\n",
              "  1.8068516254425049,\n",
              "  1.887499451637268,\n",
              "  1.5733749866485596,\n",
              "  1.3699432611465454,\n",
              "  1.3020403385162354,\n",
              "  1.5533467531204224,\n",
              "  1.6561752557754517,\n",
              "  1.6719261407852173,\n",
              "  1.4063341617584229,\n",
              "  1.6361548900604248,\n",
              "  1.8032543659210205,\n",
              "  1.5850964784622192,\n",
              "  1.8247754573822021,\n",
              "  1.3724507093429565,\n",
              "  1.589365005493164,\n",
              "  1.7144489288330078,\n",
              "  1.6739561557769775,\n",
              "  1.5608389377593994,\n",
              "  1.3597493171691895,\n",
              "  1.916867971420288,\n",
              "  1.633265733718872,\n",
              "  0.9746047854423523,\n",
              "  1.7501869201660156,\n",
              "  1.7621161937713623,\n",
              "  1.4413514137268066,\n",
              "  1.4450581073760986,\n",
              "  1.3707342147827148,\n",
              "  1.460522174835205,\n",
              "  1.3186241388320923,\n",
              "  1.652897596359253,\n",
              "  1.2395484447479248,\n",
              "  1.3116117715835571,\n",
              "  1.3468735218048096,\n",
              "  1.7729705572128296,\n",
              "  1.5549900531768799,\n",
              "  1.2654937505722046,\n",
              "  1.686505913734436,\n",
              "  1.5460362434387207,\n",
              "  1.389679193496704,\n",
              "  1.3572282791137695,\n",
              "  1.871023178100586,\n",
              "  1.7415485382080078,\n",
              "  1.6039634943008423,\n",
              "  1.468931794166565,\n",
              "  1.237863302230835,\n",
              "  1.7765085697174072,\n",
              "  1.3484258651733398,\n",
              "  1.5610414743423462,\n",
              "  1.654968023300171,\n",
              "  1.6646302938461304,\n",
              "  1.888471245765686,\n",
              "  1.6971807479858398,\n",
              "  1.828412652015686,\n",
              "  1.4597994089126587,\n",
              "  1.3298248052597046,\n",
              "  1.649163842201233,\n",
              "  1.6913416385650635,\n",
              "  1.4116530418395996,\n",
              "  1.5612388849258423,\n",
              "  1.2510316371917725,\n",
              "  1.6524722576141357,\n",
              "  1.5659902095794678,\n",
              "  1.4503357410430908,\n",
              "  1.4449474811553955,\n",
              "  1.4539310932159424,\n",
              "  1.391218900680542,\n",
              "  1.6635491847991943,\n",
              "  1.58701491355896,\n",
              "  1.3582130670547485,\n",
              "  1.331831693649292,\n",
              "  1.3921632766723633,\n",
              "  1.2315889596939087,\n",
              "  1.5848323106765747,\n",
              "  1.6331946849822998,\n",
              "  1.626708745956421,\n",
              "  1.3721879720687866,\n",
              "  1.792121171951294,\n",
              "  1.9326732158660889,\n",
              "  1.2192271947860718,\n",
              "  1.4602992534637451,\n",
              "  1.376499056816101,\n",
              "  1.5564205646514893,\n",
              "  1.5332324504852295,\n",
              "  1.409425973892212,\n",
              "  1.732282280921936,\n",
              "  1.4850276708602905,\n",
              "  1.425257682800293,\n",
              "  1.7227064371109009,\n",
              "  1.6151679754257202,\n",
              "  1.642149567604065,\n",
              "  1.3652753829956055,\n",
              "  1.4248833656311035,\n",
              "  1.4904745817184448,\n",
              "  1.4446797370910645,\n",
              "  1.4434373378753662,\n",
              "  1.5605374574661255,\n",
              "  1.6572433710098267,\n",
              "  1.1744184494018555,\n",
              "  1.7756723165512085,\n",
              "  1.7017902135849,\n",
              "  1.360190749168396,\n",
              "  1.5941776037216187,\n",
              "  1.4111392498016357,\n",
              "  1.6054579019546509,\n",
              "  1.4369418621063232,\n",
              "  1.3607800006866455,\n",
              "  1.3771212100982666,\n",
              "  1.647719144821167,\n",
              "  1.35724675655365,\n",
              "  1.4734523296356201,\n",
              "  1.4748014211654663,\n",
              "  1.2224574089050293,\n",
              "  1.5872726440429688,\n",
              "  1.6772018671035767,\n",
              "  1.5699131488800049,\n",
              "  1.7278741598129272,\n",
              "  1.1316490173339844,\n",
              "  1.910713791847229,\n",
              "  1.8124721050262451,\n",
              "  1.449424386024475,\n",
              "  1.2004483938217163,\n",
              "  1.5820024013519287,\n",
              "  1.391499400138855,\n",
              "  1.4069979190826416,\n",
              "  1.7161709070205688,\n",
              "  1.4362266063690186,\n",
              "  1.4187853336334229,\n",
              "  1.613781213760376,\n",
              "  1.3149628639221191,\n",
              "  1.555127739906311,\n",
              "  1.7351188659667969,\n",
              "  1.3772923946380615,\n",
              "  1.3922803401947021,\n",
              "  1.5128068923950195,\n",
              "  1.5376653671264648,\n",
              "  1.2510626316070557,\n",
              "  1.6205377578735352,\n",
              "  1.8381006717681885,\n",
              "  1.3567250967025757,\n",
              "  1.6569125652313232,\n",
              "  1.6097259521484375,\n",
              "  1.3147870302200317,\n",
              "  1.486477255821228,\n",
              "  1.5757209062576294,\n",
              "  1.6151800155639648,\n",
              "  1.498246431350708,\n",
              "  1.2735955715179443,\n",
              "  1.4136301279067993,\n",
              "  1.4371271133422852,\n",
              "  1.4646624326705933,\n",
              "  1.2066729068756104,\n",
              "  1.3058242797851562,\n",
              "  1.2859288454055786,\n",
              "  1.740902304649353,\n",
              "  1.0994431972503662,\n",
              "  1.6237890720367432,\n",
              "  1.3271318674087524,\n",
              "  1.3430335521697998,\n",
              "  1.470215916633606,\n",
              "  1.5946248769760132,\n",
              "  1.510204553604126,\n",
              "  1.7372150421142578,\n",
              "  1.5114469528198242,\n",
              "  1.4378130435943604,\n",
              "  1.6362483501434326,\n",
              "  1.4845900535583496,\n",
              "  1.4272558689117432,\n",
              "  1.2555469274520874,\n",
              "  1.6484473943710327,\n",
              "  1.8590633869171143,\n",
              "  1.1607270240783691,\n",
              "  1.352315902709961,\n",
              "  1.5932352542877197,\n",
              "  1.5827020406723022,\n",
              "  1.6515419483184814,\n",
              "  1.5656369924545288,\n",
              "  1.598074197769165,\n",
              "  1.6526705026626587,\n",
              "  1.5990207195281982,\n",
              "  1.2993183135986328,\n",
              "  1.507436990737915,\n",
              "  1.3258217573165894,\n",
              "  1.4477713108062744,\n",
              "  1.5135343074798584,\n",
              "  1.564554214477539,\n",
              "  1.517103910446167,\n",
              "  1.2791252136230469,\n",
              "  1.2529994249343872,\n",
              "  1.631259560585022,\n",
              "  1.7940477132797241,\n",
              "  1.493268609046936,\n",
              "  1.577530860900879,\n",
              "  1.704109787940979,\n",
              "  1.5147603750228882,\n",
              "  1.5125013589859009,\n",
              "  1.5400803089141846,\n",
              "  1.5430927276611328,\n",
              "  1.470331072807312,\n",
              "  1.1089482307434082,\n",
              "  1.6097195148468018,\n",
              "  1.351741075515747,\n",
              "  1.5222597122192383,\n",
              "  1.6767094135284424,\n",
              "  1.6716034412384033,\n",
              "  1.806470513343811,\n",
              "  1.5059421062469482,\n",
              "  1.5606664419174194,\n",
              "  1.7841682434082031,\n",
              "  1.4897897243499756,\n",
              "  1.6740787029266357,\n",
              "  1.3246556520462036,\n",
              "  1.3852694034576416,\n",
              "  1.403682827949524,\n",
              "  1.3740860223770142,\n",
              "  1.5615713596343994,\n",
              "  1.2517733573913574,\n",
              "  1.4613341093063354,\n",
              "  1.3904377222061157,\n",
              "  1.5925778150558472,\n",
              "  1.294875144958496,\n",
              "  1.3271620273590088,\n",
              "  1.7900199890136719,\n",
              "  1.3128007650375366,\n",
              "  1.2092403173446655,\n",
              "  1.6804249286651611,\n",
              "  1.8502918481826782,\n",
              "  1.35611891746521,\n",
              "  1.495042085647583,\n",
              "  1.527576208114624,\n",
              "  1.5908150672912598,\n",
              "  1.1907092332839966,\n",
              "  1.3863648176193237,\n",
              "  1.3315492868423462,\n",
              "  1.1266926527023315,\n",
              "  1.3149726390838623,\n",
              "  1.4309208393096924,\n",
              "  1.7096004486083984,\n",
              "  1.242767572402954,\n",
              "  1.3784830570220947,\n",
              "  1.4006898403167725,\n",
              "  1.7052371501922607,\n",
              "  1.3183043003082275,\n",
              "  1.2560243606567383,\n",
              "  1.0740363597869873,\n",
              "  1.035640835762024,\n",
              "  1.3663183450698853,\n",
              "  1.4025394916534424,\n",
              "  1.718245506286621,\n",
              "  1.249175786972046,\n",
              "  1.4413912296295166,\n",
              "  1.1711633205413818,\n",
              "  1.375398874282837,\n",
              "  1.3866467475891113,\n",
              "  1.4070351123809814,\n",
              "  1.4662725925445557,\n",
              "  1.8557285070419312,\n",
              "  1.5504658222198486,\n",
              "  1.4819453954696655,\n",
              "  1.5298888683319092,\n",
              "  1.599007487297058,\n",
              "  1.6428169012069702,\n",
              "  1.4965518712997437,\n",
              "  1.3826100826263428,\n",
              "  1.4008270502090454,\n",
              "  1.266890525817871,\n",
              "  1.4042574167251587,\n",
              "  1.6069908142089844,\n",
              "  1.4562956094741821,\n",
              "  1.3400027751922607,\n",
              "  1.3411591053009033,\n",
              "  1.4573041200637817,\n",
              "  1.4547778367996216,\n",
              "  1.2180572748184204,\n",
              "  1.4161075353622437,\n",
              "  1.5490882396697998,\n",
              "  1.6215184926986694,\n",
              "  1.5735712051391602,\n",
              "  1.5949232578277588,\n",
              "  1.6602184772491455,\n",
              "  1.226814866065979,\n",
              "  1.3738453388214111,\n",
              "  1.1419622898101807,\n",
              "  1.3609473705291748,\n",
              "  1.767449140548706,\n",
              "  1.4313548803329468,\n",
              "  1.3417819738388062,\n",
              "  1.4887808561325073,\n",
              "  1.2170095443725586,\n",
              "  1.4417864084243774,\n",
              "  1.4069137573242188,\n",
              "  1.2775657176971436,\n",
              "  1.3067350387573242,\n",
              "  1.2835949659347534,\n",
              "  1.3557766675949097,\n",
              "  1.431145429611206,\n",
              "  1.3700170516967773,\n",
              "  1.4696357250213623,\n",
              "  1.6136553287506104,\n",
              "  1.3988574743270874,\n",
              "  1.4030557870864868,\n",
              "  1.77411687374115,\n",
              "  1.456602692604065,\n",
              "  1.4050031900405884,\n",
              "  1.365275263786316,\n",
              "  1.4751384258270264,\n",
              "  1.105159878730774,\n",
              "  1.6270192861557007,\n",
              "  1.5439320802688599,\n",
              "  1.4466304779052734,\n",
              "  1.405744194984436,\n",
              "  1.3290666341781616,\n",
              "  1.6174980401992798,\n",
              "  1.2168318033218384,\n",
              "  1.1573549509048462,\n",
              "  1.376404881477356,\n",
              "  1.3017637729644775,\n",
              "  1.358332872390747,\n",
              "  1.4335815906524658,\n",
              "  1.4673515558242798,\n",
              "  1.1869707107543945,\n",
              "  1.4687087535858154,\n",
              "  1.5160149335861206,\n",
              "  1.1051005125045776,\n",
              "  1.2101398706436157,\n",
              "  1.3847284317016602,\n",
              "  1.0915220975875854,\n",
              "  1.3881295919418335,\n",
              "  1.2342565059661865,\n",
              "  1.228959321975708,\n",
              "  1.3836095333099365,\n",
              "  1.365665316581726,\n",
              "  1.2642167806625366,\n",
              "  1.447486162185669,\n",
              "  1.670964002609253,\n",
              "  1.3091306686401367,\n",
              "  1.7032638788223267,\n",
              "  1.6151440143585205,\n",
              "  1.348750114440918,\n",
              "  1.299718976020813,\n",
              "  1.2418378591537476,\n",
              "  1.8609864711761475,\n",
              "  1.3987436294555664,\n",
              "  1.4901188611984253,\n",
              "  1.6022027730941772,\n",
              "  1.618429183959961,\n",
              "  1.3971259593963623,\n",
              "  1.3986518383026123,\n",
              "  1.4154080152511597,\n",
              "  1.4016175270080566,\n",
              "  1.3529330492019653,\n",
              "  1.469175934791565,\n",
              "  1.3443328142166138,\n",
              "  1.3737716674804688,\n",
              "  1.356454610824585,\n",
              "  1.5255306959152222,\n",
              "  1.1648459434509277,\n",
              "  1.3236627578735352,\n",
              "  1.3455504179000854,\n",
              "  1.44915771484375,\n",
              "  1.2344813346862793,\n",
              "  1.4876220226287842,\n",
              "  1.4483433961868286,\n",
              "  1.3667995929718018,\n",
              "  1.3716551065444946,\n",
              "  1.5155810117721558,\n",
              "  1.2928699254989624,\n",
              "  1.232710838317871,\n",
              "  1.4691956043243408,\n",
              "  1.3428707122802734,\n",
              "  1.4672315120697021,\n",
              "  1.1358497142791748,\n",
              "  1.1636567115783691,\n",
              "  1.2593172788619995,\n",
              "  1.448352336883545,\n",
              "  1.6831104755401611,\n",
              "  1.1973040103912354,\n",
              "  1.1714779138565063,\n",
              "  1.6001653671264648,\n",
              "  1.4088118076324463,\n",
              "  1.584437608718872,\n",
              "  1.5713655948638916,\n",
              "  1.3194184303283691,\n",
              "  1.3760199546813965,\n",
              "  1.3011311292648315,\n",
              "  1.1712520122528076,\n",
              "  1.7513774633407593,\n",
              "  1.5748980045318604,\n",
              "  1.4063640832901,\n",
              "  1.3380588293075562,\n",
              "  1.5676777362823486,\n",
              "  1.3680235147476196,\n",
              "  1.3893773555755615,\n",
              "  1.2471630573272705,\n",
              "  1.6653903722763062,\n",
              "  1.6931142807006836,\n",
              "  1.4214681386947632,\n",
              "  1.4409945011138916,\n",
              "  1.0715731382369995,\n",
              "  1.3718303442001343,\n",
              "  1.0841470956802368,\n",
              "  1.2355883121490479,\n",
              "  1.620274305343628,\n",
              "  1.6367018222808838,\n",
              "  1.2425116300582886,\n",
              "  1.402706503868103,\n",
              "  1.4414968490600586,\n",
              "  1.5079635381698608,\n",
              "  1.337400197982788,\n",
              "  1.2805020809173584,\n",
              "  1.5954397916793823,\n",
              "  1.3079125881195068,\n",
              "  1.5611116886138916,\n",
              "  1.5838510990142822,\n",
              "  1.1420745849609375,\n",
              "  1.4213416576385498,\n",
              "  1.29689621925354,\n",
              "  1.350199580192566,\n",
              "  1.0416854619979858,\n",
              "  1.2257111072540283,\n",
              "  1.6347377300262451,\n",
              "  1.4191490411758423,\n",
              "  1.1681020259857178,\n",
              "  1.372277855873108,\n",
              "  1.3556382656097412,\n",
              "  1.2099859714508057,\n",
              "  1.103694200515747,\n",
              "  1.5583529472351074,\n",
              "  1.6518199443817139,\n",
              "  1.56893789768219,\n",
              "  1.1696231365203857,\n",
              "  1.2746217250823975,\n",
              "  1.6417334079742432,\n",
              "  1.3897874355316162,\n",
              "  1.38587486743927,\n",
              "  1.5006109476089478,\n",
              "  1.4230157136917114,\n",
              "  1.446386694908142,\n",
              "  1.1935621500015259,\n",
              "  1.5123015642166138,\n",
              "  1.4605395793914795,\n",
              "  1.4853832721710205,\n",
              "  1.2997621297836304,\n",
              "  1.5202480554580688,\n",
              "  1.6677348613739014,\n",
              "  1.5021551847457886,\n",
              "  1.5432459115982056,\n",
              "  1.8627170324325562,\n",
              "  1.4391742944717407,\n",
              "  1.220136046409607,\n",
              "  1.2020127773284912,\n",
              "  1.3320392370224,\n",
              "  1.422666311264038,\n",
              "  1.4009573459625244,\n",
              "  1.399702787399292,\n",
              "  1.298404574394226,\n",
              "  1.5207891464233398,\n",
              "  1.315995216369629,\n",
              "  1.3114192485809326,\n",
              "  1.5937139987945557,\n",
              "  1.462416648864746,\n",
              "  1.3010871410369873,\n",
              "  1.2575029134750366,\n",
              "  1.4285222291946411,\n",
              "  1.1483631134033203,\n",
              "  1.1691925525665283,\n",
              "  1.5099973678588867,\n",
              "  1.2942254543304443,\n",
              "  1.5050523281097412,\n",
              "  1.2946844100952148,\n",
              "  1.5097272396087646,\n",
              "  1.2870149612426758,\n",
              "  1.4311950206756592,\n",
              "  1.8494908809661865,\n",
              "  1.3361799716949463,\n",
              "  1.3508460521697998,\n",
              "  1.1059833765029907,\n",
              "  1.3828984498977661,\n",
              "  1.3568321466445923,\n",
              "  1.6896244287490845,\n",
              "  1.3908970355987549,\n",
              "  1.0679980516433716,\n",
              "  1.4220144748687744,\n",
              "  1.4492906332015991,\n",
              "  1.3736375570297241,\n",
              "  1.6323134899139404,\n",
              "  1.1477686166763306,\n",
              "  1.3868601322174072,\n",
              "  1.4837372303009033,\n",
              "  1.2720855474472046,\n",
              "  1.316854476928711,\n",
              "  1.3941278457641602,\n",
              "  1.2051408290863037,\n",
              "  1.3027594089508057,\n",
              "  1.6502012014389038,\n",
              "  1.5965335369110107,\n",
              "  1.5266444683074951,\n",
              "  1.50531804561615,\n",
              "  1.4651117324829102,\n",
              "  1.5896116495132446,\n",
              "  1.5975415706634521,\n",
              "  1.6711984872817993,\n",
              "  1.518812894821167,\n",
              "  1.411967396736145,\n",
              "  1.3369868993759155,\n",
              "  1.4210773706436157,\n",
              "  1.6704151630401611,\n",
              "  1.3560340404510498,\n",
              "  1.2610158920288086,\n",
              "  1.4392679929733276,\n",
              "  1.4547604322433472,\n",
              "  1.6757147312164307,\n",
              "  1.18500816822052,\n",
              "  1.3914129734039307,\n",
              "  1.2105296850204468,\n",
              "  1.3600839376449585,\n",
              "  1.2781602144241333,\n",
              "  1.4905424118041992,\n",
              "  1.5555973052978516,\n",
              "  1.3370428085327148,\n",
              "  1.6004873514175415,\n",
              "  1.534518837928772,\n",
              "  1.4753446578979492,\n",
              "  1.4970381259918213,\n",
              "  1.4351768493652344,\n",
              "  1.3208332061767578,\n",
              "  1.2502926588058472,\n",
              "  1.281235694885254,\n",
              "  1.6051654815673828,\n",
              "  1.6599887609481812,\n",
              "  1.5162684917449951,\n",
              "  1.646501898765564,\n",
              "  1.362065076828003,\n",
              "  1.5600073337554932,\n",
              "  1.3816889524459839,\n",
              "  1.2809903621673584,\n",
              "  1.224176287651062,\n",
              "  1.6797459125518799,\n",
              "  1.442095398902893,\n",
              "  1.4176090955734253,\n",
              "  1.5114529132843018,\n",
              "  1.4872227907180786,\n",
              "  1.348401427268982,\n",
              "  1.2726818323135376,\n",
              "  1.372245192527771,\n",
              "  1.0619837045669556,\n",
              "  1.3705023527145386,\n",
              "  1.4197208881378174,\n",
              "  1.7032644748687744,\n",
              "  1.3403726816177368,\n",
              "  1.4578406810760498,\n",
              "  1.247204303741455,\n",
              "  1.1037629842758179,\n",
              "  1.348423719406128,\n",
              "  1.4655256271362305,\n",
              "  1.202231764793396,\n",
              "  1.6193650960922241,\n",
              "  1.4357702732086182,\n",
              "  1.2018396854400635,\n",
              "  1.4782798290252686,\n",
              "  1.3147926330566406,\n",
              "  1.7157684564590454,\n",
              "  1.315629243850708,\n",
              "  1.3804327249526978,\n",
              "  1.4781453609466553,\n",
              "  1.3888908624649048,\n",
              "  1.3940085172653198,\n",
              "  1.447692632675171,\n",
              "  1.304398536682129,\n",
              "  1.6072978973388672,\n",
              "  1.5654723644256592,\n",
              "  1.4116384983062744,\n",
              "  1.5369198322296143,\n",
              "  1.5996958017349243,\n",
              "  1.218285322189331,\n",
              "  1.3379402160644531,\n",
              "  1.1674197912216187,\n",
              "  1.656416893005371,\n",
              "  1.213135004043579,\n",
              "  1.4570205211639404,\n",
              "  1.389886498451233,\n",
              "  1.6718966960906982,\n",
              "  1.352827787399292,\n",
              "  1.815325140953064,\n",
              "  1.5864241123199463,\n",
              "  1.4849642515182495,\n",
              "  1.6216720342636108,\n",
              "  1.4870221614837646,\n",
              "  1.370100975036621,\n",
              "  1.588694453239441,\n",
              "  1.5410434007644653,\n",
              "  1.472011923789978,\n",
              "  1.634482979774475,\n",
              "  1.2362780570983887,\n",
              "  1.313214659690857,\n",
              "  0.8875755071640015,\n",
              "  1.6444692611694336,\n",
              "  1.6265599727630615,\n",
              "  1.4028795957565308,\n",
              "  1.3260302543640137,\n",
              "  1.4331028461456299,\n",
              "  1.4324517250061035,\n",
              "  1.185043215751648,\n",
              "  1.1573511362075806,\n",
              "  1.4867408275604248,\n",
              "  1.230395793914795,\n",
              "  1.2417402267456055,\n",
              "  1.200195550918579,\n",
              "  1.5019047260284424,\n",
              "  1.20909583568573,\n",
              "  1.561540961265564,\n",
              "  1.5872485637664795,\n",
              "  1.232290506362915,\n",
              "  1.2211560010910034,\n",
              "  1.284553050994873,\n",
              "  1.2130638360977173,\n",
              "  1.2482810020446777,\n",
              "  1.105844259262085,\n",
              "  1.3028205633163452,\n",
              "  1.0542738437652588,\n",
              "  1.3473016023635864,\n",
              "  1.3415194749832153,\n",
              "  1.215543508529663,\n",
              "  1.3547381162643433,\n",
              "  1.533332109451294,\n",
              "  1.4437756538391113,\n",
              "  1.3532127141952515,\n",
              "  1.580451250076294,\n",
              "  1.5015467405319214,\n",
              "  1.1945542097091675,\n",
              "  1.6208480596542358,\n",
              "  1.5154463052749634,\n",
              "  1.416028618812561,\n",
              "  1.433696985244751,\n",
              "  1.3864381313323975,\n",
              "  1.2499284744262695,\n",
              "  1.600114107131958,\n",
              "  1.2590043544769287,\n",
              "  1.4546269178390503,\n",
              "  1.4549033641815186,\n",
              "  1.195364236831665,\n",
              "  1.458693504333496,\n",
              "  1.3799240589141846,\n",
              "  1.3115112781524658,\n",
              "  1.290928602218628,\n",
              "  1.080223798751831,\n",
              "  1.4730262756347656,\n",
              "  1.1757376194000244,\n",
              "  1.2723032236099243,\n",
              "  1.3827224969863892,\n",
              "  1.438248634338379,\n",
              "  1.5908324718475342,\n",
              "  1.1388740539550781,\n",
              "  1.4531974792480469,\n",
              "  1.5796985626220703,\n",
              "  1.6770950555801392,\n",
              "  1.1041371822357178,\n",
              "  1.219522476196289,\n",
              "  1.6533654928207397,\n",
              "  1.1643152236938477,\n",
              "  1.6236276626586914,\n",
              "  1.4246212244033813,\n",
              "  1.1362240314483643,\n",
              "  1.300137996673584,\n",
              "  1.2710946798324585,\n",
              "  1.2470070123672485,\n",
              "  1.3376365900039673,\n",
              "  1.7112934589385986,\n",
              "  1.3931602239608765,\n",
              "  1.6022541522979736,\n",
              "  1.6097244024276733,\n",
              "  0.9033985137939453,\n",
              "  1.0444504022598267,\n",
              "  1.4176709651947021,\n",
              "  1.3809542655944824,\n",
              "  1.266853928565979,\n",
              "  1.0800833702087402,\n",
              "  1.206896424293518,\n",
              "  1.2848548889160156,\n",
              "  1.4849770069122314,\n",
              "  1.4370235204696655,\n",
              "  1.3498104810714722,\n",
              "  1.417435884475708,\n",
              "  1.535627841949463,\n",
              "  1.5121604204177856,\n",
              "  1.1921387910842896,\n",
              "  1.4296156167984009,\n",
              "  1.4234895706176758,\n",
              "  1.593441367149353,\n",
              "  1.2461504936218262,\n",
              "  1.5428272485733032,\n",
              "  1.6387676000595093,\n",
              "  1.5498334169387817,\n",
              "  1.267867088317871,\n",
              "  1.367595911026001,\n",
              "  1.344151258468628,\n",
              "  1.5377992391586304,\n",
              "  1.2767561674118042,\n",
              "  1.3739774227142334,\n",
              "  1.1999719142913818,\n",
              "  1.3913558721542358,\n",
              "  1.6409070491790771,\n",
              "  1.0352240800857544,\n",
              "  1.1755887269973755,\n",
              "  1.3024365901947021,\n",
              "  1.6411892175674438,\n",
              "  1.3422683477401733,\n",
              "  1.1981136798858643,\n",
              "  1.2249317169189453,\n",
              "  1.4187368154525757,\n",
              "  1.7208061218261719,\n",
              "  1.5452595949172974,\n",
              "  1.6279995441436768,\n",
              "  1.3881748914718628,\n",
              "  1.6182101964950562,\n",
              "  1.5120569467544556,\n",
              "  1.4794282913208008,\n",
              "  1.6001265048980713,\n",
              "  1.4113490581512451,\n",
              "  1.6856334209442139,\n",
              "  1.1998037099838257,\n",
              "  1.2688047885894775,\n",
              "  1.3340743780136108,\n",
              "  1.2177852392196655,\n",
              "  1.1220433712005615,\n",
              "  1.460120439529419,\n",
              "  1.5829753875732422,\n",
              "  1.1823755502700806,\n",
              "  1.2951900959014893,\n",
              "  1.319632887840271,\n",
              "  1.4559268951416016,\n",
              "  1.417270302772522,\n",
              "  1.261157751083374,\n",
              "  1.33846116065979,\n",
              "  1.7466261386871338,\n",
              "  1.366888165473938,\n",
              "  1.4882471561431885,\n",
              "  1.3318307399749756,\n",
              "  1.7554924488067627,\n",
              "  1.5401536226272583,\n",
              "  1.647952675819397,\n",
              "  1.2826087474822998,\n",
              "  1.1878448724746704,\n",
              "  1.6462843418121338,\n",
              "  1.4079174995422363,\n",
              "  1.447206735610962,\n",
              "  1.7175133228302002,\n",
              "  1.4140933752059937,\n",
              "  1.2834205627441406,\n",
              "  1.302491545677185,\n",
              "  1.4208564758300781,\n",
              "  1.2570571899414062,\n",
              "  1.342354416847229,\n",
              "  1.3863072395324707,\n",
              "  1.7859996557235718,\n",
              "  1.2572591304779053,\n",
              "  1.0825817584991455,\n",
              "  1.5196748971939087,\n",
              "  1.3215590715408325,\n",
              "  1.139374852180481,\n",
              "  1.1653711795806885,\n",
              "  1.1804583072662354,\n",
              "  1.3756355047225952,\n",
              "  1.016339659690857,\n",
              "  1.2876135110855103,\n",
              "  1.3790792226791382,\n",
              "  1.5303112268447876,\n",
              "  1.3391029834747314,\n",
              "  1.0277900695800781,\n",
              "  1.1804977655410767,\n",
              "  1.3324495553970337,\n",
              "  1.3911927938461304,\n",
              "  1.3426817655563354,\n",
              "  1.3660129308700562,\n",
              "  1.5310895442962646,\n",
              "  1.2219816446304321,\n",
              "  1.7522786855697632,\n",
              "  1.2220767736434937,\n",
              "  1.5137630701065063,\n",
              "  1.3677669763565063,\n",
              "  1.641731858253479,\n",
              "  1.5353878736495972,\n",
              "  1.4274741411209106,\n",
              "  1.3718528747558594,\n",
              "  1.5299317836761475,\n",
              "  1.163362741470337,\n",
              "  1.3929115533828735,\n",
              "  1.4702283143997192,\n",
              "  1.226339340209961,\n",
              "  1.0195261240005493,\n",
              "  1.471461534500122,\n",
              "  1.521833062171936,\n",
              "  1.681923270225525,\n",
              "  1.73923659324646,\n",
              "  1.5216848850250244,\n",
              "  1.444658637046814,\n",
              "  1.1972100734710693,\n",
              "  1.3039501905441284,\n",
              "  1.2119241952896118,\n",
              "  1.2633001804351807,\n",
              "  1.706709623336792,\n",
              "  1.0149800777435303,\n",
              "  1.4969313144683838,\n",
              "  1.3483200073242188,\n",
              "  1.2191311120986938,\n",
              "  1.526739478111267,\n",
              "  1.2275640964508057,\n",
              "  1.5442224740982056,\n",
              "  1.6884483098983765,\n",
              "  1.2689412832260132,\n",
              "  1.2431150674819946,\n",
              "  1.3466131687164307,\n",
              "  1.4180620908737183,\n",
              "  1.1883572340011597,\n",
              "  1.202162504196167,\n",
              "  1.4470053911209106,\n",
              "  1.478492021560669,\n",
              "  1.3009850978851318,\n",
              "  1.5783157348632812,\n",
              "  1.4910458326339722,\n",
              "  1.4461510181427002,\n",
              "  1.2045942544937134,\n",
              "  1.2647373676300049,\n",
              "  1.6483789682388306,\n",
              "  1.4857878684997559,\n",
              "  1.3881046772003174,\n",
              "  1.443967580795288,\n",
              "  1.5237406492233276,\n",
              "  1.5006792545318604,\n",
              "  1.2796388864517212,\n",
              "  1.2271891832351685,\n",
              "  1.0765368938446045,\n",
              "  1.2114198207855225,\n",
              "  1.290217638015747,\n",
              "  1.3455440998077393,\n",
              "  1.3575465679168701,\n",
              "  1.3846399784088135,\n",
              "  1.4016084671020508,\n",
              "  1.2651405334472656,\n",
              "  1.2337976694107056,\n",
              "  1.443699598312378,\n",
              "  1.2090600728988647,\n",
              "  1.530301809310913,\n",
              "  1.3959709405899048,\n",
              "  1.3526545763015747,\n",
              "  1.3362196683883667,\n",
              "  1.4077067375183105,\n",
              "  1.415908694267273,\n",
              "  1.0340681076049805,\n",
              "  1.3876750469207764,\n",
              "  1.3943969011306763,\n",
              "  1.5798265933990479,\n",
              "  1.9170434474945068,\n",
              "  1.5503063201904297,\n",
              "  1.1237081289291382,\n",
              "  1.335376501083374,\n",
              "  1.4299428462982178,\n",
              "  1.2707664966583252,\n",
              "  1.5039771795272827,\n",
              "  1.1829794645309448,\n",
              "  1.6309105157852173,\n",
              "  1.227236032485962,\n",
              "  1.5801118612289429,\n",
              "  1.1329352855682373,\n",
              "  1.3410505056381226,\n",
              "  1.54495370388031,\n",
              "  1.4536335468292236,\n",
              "  1.1564043760299683,\n",
              "  0.9116290807723999,\n",
              "  1.1970573663711548,\n",
              "  1.6780658960342407,\n",
              "  1.1541850566864014,\n",
              "  1.2733533382415771,\n",
              "  1.4918895959854126,\n",
              "  1.1991851329803467,\n",
              "  1.1608575582504272,\n",
              "  1.2894411087036133,\n",
              "  1.5666933059692383,\n",
              "  1.292164921760559,\n",
              "  1.3502612113952637,\n",
              "  1.3130173683166504,\n",
              "  1.308844804763794,\n",
              "  1.6112051010131836,\n",
              "  1.2731916904449463,\n",
              "  1.0575096607208252,\n",
              "  1.4115183353424072,\n",
              "  1.3057600259780884,\n",
              "  1.4620554447174072,\n",
              "  1.1725924015045166,\n",
              "  1.5509817600250244,\n",
              "  1.0888221263885498,\n",
              "  1.4698903560638428,\n",
              "  1.4049774408340454,\n",
              "  0.9023536443710327,\n",
              "  1.2562764883041382,\n",
              "  1.0307002067565918,\n",
              "  1.5356183052062988,\n",
              "  1.4735908508300781,\n",
              "  1.3692071437835693,\n",
              "  1.2493212223052979,\n",
              "  1.2344251871109009,\n",
              "  1.56601881980896,\n",
              "  1.496986746788025,\n",
              "  1.74433171749115,\n",
              "  1.3482073545455933,\n",
              "  1.5164469480514526,\n",
              "  1.0999324321746826,\n",
              "  1.2803558111190796,\n",
              "  1.5928471088409424,\n",
              "  1.159284234046936,\n",
              "  1.6658446788787842,\n",
              "  1.2958570718765259,\n",
              "  1.4553533792495728,\n",
              "  1.1784703731536865,\n",
              "  1.7274227142333984,\n",
              "  1.291644811630249,\n",
              "  1.8411210775375366,\n",
              "  1.4173305034637451,\n",
              "  1.6482009887695312,\n",
              "  1.312558650970459,\n",
              "  1.3054457902908325,\n",
              "  1.1462651491165161,\n",
              "  1.2229758501052856,\n",
              "  1.0855941772460938,\n",
              "  1.7236597537994385,\n",
              "  1.476269245147705],\n",
              " 'val_acc': [0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.05000000074505806,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.05000000074505806,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.05000000074505806,\n",
              "  0.05000000074505806,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.05000000074505806,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.05000000074505806,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.10000000149011612,\n",
              "  0.10000000149011612,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.10000000149011612,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.44999998807907104,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.05000000074505806,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.05000000074505806,\n",
              "  0.20000000298023224,\n",
              "  0.10000000149011612,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.10000000149011612,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.5,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.05000000074505806,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.10000000149011612,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.15000000596046448,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.10000000149011612,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.15000000596046448,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.550000011920929,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.15000000596046448,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.30000001192092896,\n",
              "  0.550000011920929,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.550000011920929,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.15000000596046448,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.6499999761581421,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.550000011920929,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.550000011920929,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.550000011920929,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.5,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.05000000074505806,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.20000000298023224,\n",
              "  0.44999998807907104,\n",
              "  0.6000000238418579,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.20000000298023224,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.05000000074505806,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.6000000238418579,\n",
              "  0.3499999940395355,\n",
              "  0.550000011920929,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.3499999940395355,\n",
              "  0.550000011920929,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.550000011920929,\n",
              "  0.15000000596046448,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.5,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.550000011920929,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.4000000059604645,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.550000011920929,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.15000000596046448,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.20000000298023224,\n",
              "  0.30000001192092896,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.05000000074505806,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.4000000059604645,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.44999998807907104,\n",
              "  0.550000011920929,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.4000000059604645,\n",
              "  0.44999998807907104,\n",
              "  0.3499999940395355,\n",
              "  0.5,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.20000000298023224,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.25,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.25,\n",
              "  0.44999998807907104,\n",
              "  0.44999998807907104,\n",
              "  0.5,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.10000000149011612,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.4000000059604645,\n",
              "  0.3499999940395355,\n",
              "  0.4000000059604645,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.3499999940395355,\n",
              "  0.30000001192092896,\n",
              "  0.30000001192092896,\n",
              "  0.44999998807907104,\n",
              "  0.15000000596046448,\n",
              "  0.4000000059604645,\n",
              "  0.25,\n",
              "  0.3499999940395355,\n",
              "  0.20000000298023224,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.30000001192092896,\n",
              "  0.10000000149011612,\n",
              "  0.25]}"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "79c46d8f-92a8-4c0e-9046-f573e3506844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "3d4b0db4-673f-4d67-cee5-41c4dae57ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_06e11f29-b8f6-4bf2-a46d-a3fd01f95102\", \"newdata_SEM1.h5\", 16615816)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}