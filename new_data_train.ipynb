{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+/dWjwXh6rlTFmrITjUWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/new_data_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "60e07b7b-d7ce-44d8-ab6a-5746d63e62b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - new data Remake.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "fbac4ef8-0add-4780-86da-f35a036996c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-500  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-500  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-500  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-500  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-500  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-500  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-500  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-500  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-500  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-500  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3327892a-f463-4aee-9219-81f76c61809c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-500</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-500</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3327892a-f463-4aee-9219-81f76c61809c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3327892a-f463-4aee-9219-81f76c61809c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3327892a-f463-4aee-9219-81f76c61809c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "ce691322-d6a4-41e3-8a3d-03e745d5398d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb80lEQVR4nO3df7AV5Z3n8fcn/mTAGUDMDVEimmASHFfUW8REJ3tnLX9AEtGplMF1lRgzZGZ1R6uwUpipim5cqzQjuhsrZRZLRzJD/DExBjKaicTxTOJkNYKLAiIRDa7cIIyK4CWOycXv/tHP1eZ4Lveee371bT6vqq7T/XT36e/p+9zv6fP0092KCMzMrFze1+kAzMys+ZzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzcC0TSJknbJI3NlX1ZUqWDYZk1Varnb0rqk7Rd0gOSpqR5d0r6XZo3MDwl6U9y07skRdUyH+r05yoaJ/fi2Q+4vNNBmLXY5yJiHDAZ2Arckpv3zYgYlxuOj4ifD0wDx6blxueW+X/t/gBF5+RePH8DXClpfPUMSZ+S9ISkHen1Ux2Iz6xpIuLfge8D0zsdS9k4uRfPSqACXJkvlDQReAD4FnAocBPwgKRD2x2gWbNI+gPgC8BjnY6lbJzci+nrwH+TdFiu7DPAcxHxdxHRHxF3Ac8Cn+tIhGaN+aGk14EdwOlkv1gHXCnp9dywpDMhjm5O7gUUEWuBfwQW5oo/CLxYteiLwOHtisusic6JiPHAwcBlwL9I+kCad2NEjM8N8zoX5ujl5F5cVwN/zrvJ+zfAkVXLfAjobWdQZs0UEbsj4gfAbuDUTsdTJk7uBRURG4F7gL9KRQ8Cx0j6z5L2l/QFspNQ/9ipGM0apcwcYAKwvtPxlImTe7F9AxgLEBGvAp8FFgCvAl8FPhsRr3QuPLMR+5GkPmAncB0wLyLWpXlfrerD7jo+AvLDOszMysdH7mZmJeTkbmZWQk7uZmYl5ORuZlZC+3c6AIBJkybF1KlTa87btWsXY8eOrTmvKBxj8zQS56pVq16JiMOGXrLzXOfbYzTE2bI6HxEdH0466aQYzCOPPDLovKJwjM3TSJzAymhCfQSmAI8AzwDrgMtT+URgBfBcep2QykV2z5+NwNPAiUNtw3W+PUZDnK2q826WMXuvfmBBREwHTgYulTSd7HYQD0fENOBh3r09xCxgWhrmA7e2P2SzPTm5m1WJiC0R8WQaf4PsysnDgTnAwE2slgDnpPE5wHfTwdRjwHhJk9scttkeCtHmblZUkqYCJwCPA10RsSXNehnoSuOHAy/lVtucyrbkypA0n+zInq6uLiqVSs1t9vX1DTqvKEZDjDA64mxVjIVP7mt6d/DFhQ90Ooy9WnBcv2NskqHi3HT9Z9oWi6RxwH3AFRGxU9I78yIiJNV1eXdELAYWA3R3d0dPT0/N5W5ZuoxFj+6qK9Z27heASqXCYPEXyWiIs1UxulnGrAZJB5Al9qWR3bUQYOtAc0t63ZbKe8lOwg44At+t0zpsxMld0kclrc4NOyVdIekaSb258tnNDNis1ZQdot8OrI+Im3KzlgMD9xafByzLlV+U7nB4MrAj13xj1hEjbpaJiA3ADABJ+5EdqdwPXAzcHBE3NiVCs/Y7BbgQWCNpdSr7GnA9cK+kS8gelHJemvcgMJusK+Rvyf4HzDqqWW3upwHPR8SL+XZJs9EoIh4l67tey2k1lg/g0pYGZVanZiX3ucBduenLJF1E9rDnBRGxvXqF4fYc6BqTnWQrMsfYPEPFWfSeD2ZF0XByl3QgcDZwVSq6FbgWiPS6CPhS9Xp19RxYU+xOPQuO63eMTTJUnJsu6GlfMGajWDN6y8wCnoyIrQARsTWy5yK+DdwGzGzCNszMrA7NSO7nk2uSqboy71xgbRO2YWZmdWjod7qkscDpwFdyxd+UNIOsWWZT1TwzM2uDhpJ7ROwCDq0qu7ChiMzMrGG+QtXMrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshIp/m0AzG9LUET4ft93PXrX28ZG7mVkJObmbmZWQk7uZWQk5uZuZlZCTu5lZCTm5m5mVkJO7mVkJObmbmZWQk7uZWQk1+oDsTcAbwG6gPyK6JU0E7gGmkj0g+7yI2N5YmGZmVo9mHLn/aUTMiIjuNL0QeDgipgEPp2kzM2ujVjTLzAGWpPElwDkt2IaZme1Fo8k9gIckrZI0P5V1RcSWNP4y0NXgNszMrE6N3hXy1IjolfR+YIWkZ/MzIyIkRa0V05fBfICuri4qlUrNDXSNgQXH9TcYZms5xuYZKs7B6omZ7amh5B4Rvel1m6T7gZnAVkmTI2KLpMnAtkHWXQwsBuju7o6enp6a27hl6TIWrSn2nYkXHNfvGJtkqDg3XdDTvmDMRrERN8tIGivpkIFx4AxgLbAcmJcWmwcsazRIMzOrTyOHcl3A/ZIG3ud7EfFPkp4A7pV0CfAicF7jYZqZWT1GnNwj4gXg+BrlrwKnNRKUmZk1pviNsGbWMiN5PN9IHs3Xru3Yu3z7ATOzEnJyN6tB0h2StklamyubKGmFpOfS64RULknfkrRR0tOSTuxc5GYZJ3ez2u4EzqoqG+zWGrOAaWmYD9zaphjNBuXkblZDRPwMeK2qeLBba8wBvhuZx4Dx6RoPs47xCVWz4Rvs1hqHAy/lltucyrbkykpzVXalUqGvr6+uq4VH8nmacTVyvXF2QqtidHI3G4G93VpjL+uU4qrsTRf0UKlUGCz+Wr44kt4yTbgaud44O6FVMbpZxmz4tg40t1TdWqMXmJJb7ohUZtYxxT08MCuegVtrXM+et9ZYDlwm6W7gE8COXPONjZD7xjfGyd2sBkl3AT3AJEmbgavJknqtW2s8CMwGNgK/BS5ue8BmVZzczWqIiPMHmfWeW2tERACXtjYis/q4zd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshEac3CVNkfSIpGckrZN0eSq/RlKvpNVpmN28cM3MbDgauf1AP7AgIp6UdAiwStKKNO/miLix8fDMzGwkRpzc013vtqTxNyStJ3tAgZmZdVhTbhwmaSpwAvA4cArZ7U8vAlaSHd1vr7FOKZ5KA46xmYaKs+hP1TErioaTu6RxwH3AFRGxU9KtwLVApNdFwJeq1yvLU2kgS0aOsTmGirMZT+cx2xc01FtG0gFkiX1pRPwAICK2RsTuiHgbuA2Y2XiYZmZWj0Z6ywi4HVgfETflyvNPfT8XWDvy8MzMbCQa+Z1+CnAhsEbS6lT2NeB8STPImmU2AV9pKEIzs2GqfjTfguP6h/Vw7jI+nq+R3jKPAqox68GRh2NmZs3gK1TNzErIyd3MrISK3zfOzApl6sIHht2WbZ3jI3czsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3Myshd4U0MxuB6lsdDEc7b3PgI3czsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3MyshJ3czsxJycjczKyEndzOzEmpZcpd0lqQNkjZKWtiq7ZgVheu8FUlLbj8gaT/g28DpwGbgCUnLI+KZVmzPrNNc5204at2yYKinWo30lgWturfMTGBjRLwAIOluYA7gim5l5To/io3kPjFFp4ho/ptKnwfOiogvp+kLgU9ExGW5ZeYD89PkR4ENg7zdJOCVpgfZXI6xeRqJ88iIOKyZwQyX63xhjYY4W1LnO3ZXyIhYDCweajlJKyOiuw0hjZhjbJ7REudIuM6332iIs1UxtuqEai8wJTd9RCozKyvXeSuUViX3J4Bpko6SdCAwF1jeom2ZFYHrvBVKS5plIqJf0mXAT4D9gDsiYt0I327In7EF4BibZ7TEuQfX+cIaDXG2JMaWnFA1M7PO8hWqZmYl5ORuZlZChU3uRbmUW9IUSY9IekbSOkmXp/JrJPVKWp2G2bl1rkpxb5B0Zhtj3SRpTYpnZSqbKGmFpOfS64RULknfSnE+LenENsT30dz+Wi1pp6QrirgvO6WT9V7SHZK2SVqbK6u7/kial5Z/TtK8Jsc42P9jYeKUdLCkX0p6KsX431P5UZIeT7Hck068I+mgNL0xzZ+ae6+R1/+IKNxAdkLqeeBo4EDgKWB6h2KZDJyYxg8BfgVMB64Brqyx/PQU70HAUelz7NemWDcBk6rKvgksTOMLgRvS+Gzgx4CAk4HHO/A3fhk4soj7skN1raP1Hvg0cCKwdqT1B5gIvJBeJ6TxCU2McbD/x8LEmbY1Lo0fADyetn0vMDeVfwf4yzT+X4HvpPG5wD1pvKH6X9Qj93cu5Y6I3wEDl3K3XURsiYgn0/gbwHrg8L2sMge4OyLeiohfAxvJPk+nzAGWpPElwDm58u9G5jFgvKTJbYzrNOD5iHhxL8sUbV+2WkfrfUT8DHitqrje+nMmsCIiXouI7cAK4KwmxjjY/2Nh4kzb6kuTB6QhgP8EfH+QGAdi/z5wmiTRYP0vanI/HHgpN72ZvSfUtkg/l04g+yYGuCz91Ltj4GcgnY09gIckrVJ2qTtAV0RsSeMvA11pvNP7eC5wV266aPuyE4r4eeutP237DFX/j4WKU9J+klYD28i+OJ4HXo+I/hrbeyeWNH8HcGijMRY1uReOpHHAfcAVEbETuBX4MDAD2AIs6mB4A06NiBOBWcClkj6dnxnZb72O931NbY1nA/+Qioq4L61KUeoP1Px/fEcR4oyI3RExg+xK5ZnAx9odQ1GTe6Eu5ZZ0AFlFWhoRPwCIiK3pD/g2cBtwuqSHaDB2SYdJelbSmHrjjIheSX3AOOB+skq1daC5Jb1uS4sPGmc6GXRsvduvwyzgyYjYmuKu3pcDPz0LVQ/aoIift9760/LPUOv/sYhxAkTE68AjwCfJmoQGLhzNb++dWNL8PwJebTTGoib3wlzKndq+bgfWR8RNkk6V9IvU0+M1Sf8K/BXwrxFxRopzbjoDfhQwDfhlHZtcCNwZEW/WGedYSYdExDhgK3AGsDbFM9ATYB6wLI0vBy5KvQlOBnbkftbeCHyjnu3X6XxyTTJVbf3nprgHYmxkX442han3OfXWn58AZ0iakJrXzkhlTVH9/1jEONMB2vg0PobsHv/ryZL85weJcSD2zwP/nH59NFb/m3F2uBUD2VnuX5G1Vf11B+M4lewn3tNp2A3cAPw9WRJ6AagAk3Pr/HWKewMwq45tHUR2688jRhDn0WRn1p8C1g3sM7K2u4eB54CfAhPj3TP6305xrgG6c+91MNmJtQ+0YH+OJTsq+aNc2d+lGJ5OFbrhfTlah07We7Iv3C3A78nady8ZYf35EtnJv43AxU2OMf//uDoNs4sUJ/AfgP+bYlwLfD2VH02WnDeSNUkelMoPTtMb0/yjc+814vrf8co8mgagm+ykSK15XwQeTeNfBfpyw+/JjsYh+8l1e/on6gX+B6l7E1lXtI1V71tJy/wivdePUkVeCuwkO9qbmls+gI+k8TFk7dcvkp2keRQYk+adTfYl8HraxsertrsCmNfpfe7Bg4eRDUVtlimqXwG7JS2RNCvXq2MPEfHNiBgXWRPJx4F/A+5Js+8E+oGPkJ3pPwP4cpp3HLUf4DAXuJDsTPmHgf8D/C1ZH931wNWDxHsjcBLwqbTsV4G3JR1DdpR2BXAY8CDwo4GLKpL1wPGD7gkzKzQn9zpEdlZ+4GfhbcC/SVouqavW8qm97YfA/4qIH6flZpOd4d8VEduAm8mSN8B44I0ab/W3EfF8ROwguyDj+Yj4aWTdpv6B7EuietvvI/vZeXlE9EZ2wvIXEfEW8AXggYhYERG/J/sSGEP2JTDgjRSPmY1CHXsS02gVEevJmmCQ9DGytvf/Se2TMbcDGyLihjR9JNkFDVuy80JA9gU70Jd1O9lVd9W25sbfrDE9rsY6k8ja8p6vMe+DZE01A5/pbUkvsWcf2kPImmzMbBTykXsDIuJZsmaWP66ep+y+IMeQnZQa8BLwFtktAsan4Q8jYqDb4dNpnWZ4Bfh3smacar8h+6IZiFVkXa7y3aw+TnZy1sxGISf3Okj6mKQFko5I01PIuvU9VrXcLLLukedGrktjZF2wHgIWSfpDSe+T9GFJ/zEt8kuyvrANXykXWZ/xO4CbJH0wXTH3SUkHkd3j4jOSTkt9hheQfen8IsV/MFlb/YpG4zCzznByr88bwCeAxyXtIkvqa8mSY94XyE5UrpfUl4bvpHkXkd0U6hmyZpjvk90MicjuJ3In8F+aFO+VZN2/niDr2ngD8L6I2JC2cQvZEf7ngM+l7ZOmKxHxmybFYWZt5icxFYykw4CfAydEnRcyNTGGx4FLImLtkAubWSE5uZuZlZCbZczMSsjJ3cyshJzczcxKqBAXMU2aNCmmTp1ac96uXbsYO3ZsewMqIO+HzN72w6pVq16JiMPaHJJZIRUiuU+dOpWVK1fWnFepVOjp6WlvQAXk/ZDZ236QtLdH9pntU9wsY2ZWQk7uZmYl5ORuZlZChWhzt6Gt6d3BFxc+UNc6m67/TIuiMbOi85G7mVkJDZncJX1U0urcsFPSFZKukdSbK5+dW+cqSRslbZB0Zms/gpmZVRuyWSbdQXAGgKT9yO75fT9wMXBzRNyYX17SdLInCx1L9lCIn0o6JiJ2Nzl2MzMbRL3NMqeRPeJtb/2J5wB3R8RbEfFrsid6zxxpgGZmVr96T6jOJXuw8oDLJF0ErAQWRMR2ske15R9esZk9H98GgKT5wHyArq4uKpVKzQ329fUNOm9f0jUGFhzXX9c6Zdxvrg9mwzPs5C7pQOBs4KpUdCtwLdnDoq8FFpE9kHlYImIxsBigu7s7Brvq0FdmZm5ZuoxFa+r7Lt50QU9rgukg1wez4amnWWYW8GREbAWIiK0RsTs9zu023m166SV7HueAI9jz2ZxmZtZi9ST388k1yUianJt3Ltnj5gCWA3MlHSTpKGAa2bNBzcysTYb1O1/SWOB04Cu54m9KmkHWLLNpYF5ErJN0L9kzQvuBS91TxsysvYaV3CNiF3BoVdmFe1n+OuC6xkIzM7OR8hWqZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYlNKzkLmmTpDWSVktamcomSloh6bn0OiGVS9K3JG2U9LSkE1v5AczM7L3qOXL/04iYERHdaXoh8HBETAMeTtMAs4BpaZgP3NqsYM3MbHgaaZaZAyxJ40uAc3Ll343MY8B4SZMb2I6ZmdVpuMk9gIckrZI0P5V1RcSWNP4y0JXGDwdeyq27OZWZmVmb7D/M5U6NiF5J7wdWSHo2PzMiQlLUs+H0JTEfoKuri0qlUnO5vr6+QeftS7rGwILj+utap4z7zfXBbHiGldwjoje9bpN0PzAT2CppckRsSc0u29LivcCU3OpHpLLq91wMLAbo7u6Onp6emtuuVCoMNm9fcsvSZSxaM9zv4symC3paE0wHuT6YDc+QzTKSxko6ZGAcOANYCywH5qXF5gHL0vhy4KLUa+ZkYEeu+cbMzNpgOIeCXcD9kgaW/15E/JOkJ4B7JV0CvAicl5Z/EJgNbAR+C1zc9KjNzGyvhkzuEfECcHyN8leB02qUB3BpU6IzM7MR8RWqZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCQyZ3SVMkPSLpGUnrJF2eyq+R1CtpdRpm59a5StJGSRskndnKD2BmZu+1/zCW6QcWRMSTkg4BVklakebdHBE35heWNB2YCxwLfBD4qaRjImJ3MwM3M7PBDXnkHhFbIuLJNP4GsB44fC+rzAHujoi3IuLXwEZgZjOCNTOz4RnOkfs7JE0FTgAeB04BLpN0EbCS7Oh+O1nifyy32mZqfBlImg/MB+jq6qJSqdTcZl9f36Dz9iVdY2DBcf11rVPG/eb6YDY8w07uksYB9wFXRMROSbcC1wKRXhcBXxru+0XEYmAxQHd3d/T09NRcrlKpMNi8fcktS5exaE1d38VsuqCnNcF0kOuD2fAMq7eMpAPIEvvSiPgBQERsjYjdEfE2cBvvNr30AlNyqx+RyszMrE2G01tGwO3A+oi4KVc+ObfYucDaNL4cmCvpIElHAdOAXzYvZDMzG8pwfuefAlwIrJG0OpV9DThf0gyyZplNwFcAImKdpHuBZ8h62lzqnjJmZu01ZHKPiEcB1Zj14F7WuQ64roG4zMysAb5C1cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshJzczcxKyMndzKyEnNzNzErIyd3MrISc3M3MSsjJ3cyshFqW3CWdJWmDpI2SFrZqO2Zm9l4tSe6S9gO+DcwCpgPnS5reim2Zmdl7terIfSawMSJeiIjfAXcDc1q0LTMzq7J/i973cOCl3PRm4BP5BSTNB+anyT5JGwZ5r0nAK02PcPSpez/ohhZF0ll72w9HtjMQsyJrVXIfUkQsBhYPtZyklRHR3YaQCs37IeP9YDY8rWqW6QWm5KaPSGVmZtYGrUruTwDTJB0l6UBgLrC8RdsyM7MqLWmWiYh+SZcBPwH2A+6IiHUjfLshm272Ed4PGe8Hs2FQRHQ6BjMzazJfoWpmVkJO7mZmJVSI5C7pcklrJa2TdEWN+T2SdkhanYavdyLOVpB0h6RtktbmyiZKWiHpufQ6YZB156VlnpM0r31RN1+D+2F3rm74xL0ZBUjukv4Y+HOyq1qPBz4r6SM1Fv15RMxIwzfaGmRr3QmcVVW2EHg4IqYBD6fpPUiaCFxNdnHYTODqwZLfKHEnI9gPyZu5unF2C2M0GzU6ntyBjwOPR8RvI6If+BfgzzocU9tExM+A16qK5wBL0vgS4Jwaq54JrIiI1yJiO7CC9ybHUaOB/WBmNRQhua8F/kTSoZL+AJjNnhdADfikpKck/VjSse0Nse26ImJLGn8Z6KqxTK1bPBze6sDabDj7AeBgSSslPSbJXwBmdPD2AwMiYr2kG4CHgF3AamB31WJPAkdGRJ+k2cAPgWntjbQzIiIk7fP9VYfYD0dGRK+ko4F/lrQmIp5vZ3xmRVOEI3ci4vaIOCkiPg1sB35VNX9nRPSl8QeBAyRN6kCo7bJV0mSA9LqtxjL7wi0ehrMfiIje9PoCUAFOaFeAZkVViOQu6f3p9UNk7e3fq5r/AUlK4zPJ4n613XG20XJgoPfLPGBZjWV+ApwhaUI6kXpGKiuTIfdD+vwHpfFJwCnAM22L0KygOt4sk9wn6VDg98ClEfG6pL8AiIjvAJ8H/lJSP/AmMDdKcmmtpLuAHmCSpM1kPWCuB+6VdAnwInBeWrYb+IuI+HJEvCbpWrL7+AB8IyKqT0iOGiPdD2Qn5P+3pLfJvvSvjwgnd9vn+fYDZmYlVIhmGTMzay4ndzOzEnJyNzMrISd3M7MScnI3MyshJ3czsxJycjczK6H/DxgwhIJdffxAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "2237394d-ff98-446c-bc52-63097487be68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-500','501-1000','1001-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "1ed26777-4bc2-4c5e-c1a3-7a6f8ad0e94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "a51c6bec-016e-4f9e-9ac6-413096868156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 665, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 665 (delta 151), reused 162 (delta 136), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (665/665), 12.11 MiB | 32.22 MiB/s, done.\n",
            "Resolving deltas: 100% (391/391), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-500')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '501-1000')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "train_3_dir = os.path.join(train_dir, '1001-3200')\n",
        "os.makedirs(train_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-500')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '501-1000')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "validation_3_dir = os.path.join(validation_dir, '1001-3200')\n",
        "os.makedirs(validation_3_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-500')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '501-1000')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n",
        "test_3_dir = os.path.join(test_dir, '1001-3200')\n",
        "os.makedirs(test_3_dir, exist_ok=True)\n",
        "     "
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(651,725)]\n",
        "train = df[df['No'].between(1,650)]\n",
        "test = df[df['No'].between(726,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-500' ]\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='501-1000' ]\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "T3_train = train[train['Class']=='1001-3200' ]\n",
        "T3_path_train = T3_train['path_Picture'].tolist()\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-500' ]\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='501-1000' ]\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "T3_val = val[val['Class']=='1001-3200']\n",
        "T3_path_val = T3_val['path_Picture'].tolist()\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-500' ]\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='501-1000' ]\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n",
        "T3_test = test[test['Class']=='1001-3200']\n",
        "T3_path_test = T3_test['path_Picture'].tolist()"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_train \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_test \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n",
        "fnames = T3_path_val \n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_3_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)))\n",
        "print('total training 3 images:', len(os.listdir(train_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)))\n",
        "print('total validation 3 images:', len(os.listdir(validation_3_dir)),'\\n')\n",
        "\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)))\n",
        "print('total test 3 images:', len(os.listdir(test_3_dir)),'\\n')"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "d37681e6-cab2-4485-eab3-0564abd10368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 211\n",
            "total training 2 images: 145\n",
            "total training 3 images: 325 \n",
            "\n",
            "total validation 1 images: 98\n",
            "total validation 2 images: 10\n",
            "total validation 3 images: 12 \n",
            "\n",
            "total test 1 images: 71\n",
            "total test 2 images: 6\n",
            "total test 3 images: 46 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 650  # จำนวนภาพ Train\n",
        "NUM_TEST = 75 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "727f6e8c-d7c2-4a24-fa35-ac6618b6d45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "a459a88a-cfd8-4118-cacf-ea086cef7b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "8eb35a4a-941e-4618-f186-143b1038ec0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,407\n",
            "Trainable params: 4,011,391\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "76f09167-4e5b-4e21-8840-6aa1badbb258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "31655d23-f701-4387-eca5-28496f45195c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 681 images belonging to 3 classes.\n",
            "Found 120 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450f2016-a5ab-4057-80bd-4483b388bdbe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "<ipython-input-24-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 23s 1s/step - loss: 3.6265 - acc: 0.2674 - val_loss: 1.3120 - val_acc: 0.3438\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 3s 283ms/step - loss: 3.2705 - acc: 0.2804 - val_loss: 1.1964 - val_acc: 0.3594\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 3.2383 - acc: 0.2707 - val_loss: 1.2837 - val_acc: 0.3594\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 3.0017 - acc: 0.2674 - val_loss: 1.1719 - val_acc: 0.3594\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 2.7583 - acc: 0.2853 - val_loss: 1.4065 - val_acc: 0.3125\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 2.5587 - acc: 0.2674 - val_loss: 1.1626 - val_acc: 0.3906\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 2.4192 - acc: 0.2674 - val_loss: 1.1706 - val_acc: 0.4219\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 2.2861 - acc: 0.2950 - val_loss: 1.2647 - val_acc: 0.3281\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 2.2804 - acc: 0.2447 - val_loss: 1.0833 - val_acc: 0.4219\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 2.1989 - acc: 0.2917 - val_loss: 1.1624 - val_acc: 0.3594\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 2.0946 - acc: 0.2771 - val_loss: 1.2453 - val_acc: 0.3281\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 2.0000 - acc: 0.2917 - val_loss: 1.2407 - val_acc: 0.3125\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.8093 - acc: 0.2901 - val_loss: 1.2584 - val_acc: 0.2969\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.6613 - acc: 0.3387 - val_loss: 1.3112 - val_acc: 0.2969\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.7615 - acc: 0.3128 - val_loss: 1.4729 - val_acc: 0.2031\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.6498 - acc: 0.3241 - val_loss: 1.3352 - val_acc: 0.2344\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.6859 - acc: 0.3193 - val_loss: 1.5112 - val_acc: 0.2344\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.7226 - acc: 0.3219 - val_loss: 1.4163 - val_acc: 0.2969\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.5911 - acc: 0.3517 - val_loss: 1.4848 - val_acc: 0.2188\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.6462 - acc: 0.3063 - val_loss: 1.4279 - val_acc: 0.2656\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5372 - acc: 0.3566 - val_loss: 1.5746 - val_acc: 0.1875\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5834 - acc: 0.3468 - val_loss: 1.4801 - val_acc: 0.2969\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.6482 - acc: 0.3422 - val_loss: 1.4472 - val_acc: 0.2656\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.5599 - acc: 0.3614 - val_loss: 1.4330 - val_acc: 0.3281\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4960 - acc: 0.3890 - val_loss: 1.5875 - val_acc: 0.2031\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.6447 - acc: 0.3728 - val_loss: 1.5645 - val_acc: 0.2344\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.6054 - acc: 0.3776 - val_loss: 1.4190 - val_acc: 0.2656\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.5356 - acc: 0.3760 - val_loss: 1.4164 - val_acc: 0.2969\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.4978 - acc: 0.3776 - val_loss: 1.4449 - val_acc: 0.3125\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5712 - acc: 0.3663 - val_loss: 1.3694 - val_acc: 0.3594\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5495 - acc: 0.3987 - val_loss: 1.4510 - val_acc: 0.2812\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.5544 - acc: 0.3598 - val_loss: 1.3880 - val_acc: 0.3438\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.5971 - acc: 0.3566 - val_loss: 1.4494 - val_acc: 0.3125\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5375 - acc: 0.3857 - val_loss: 1.3955 - val_acc: 0.2812\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.4877 - acc: 0.3841 - val_loss: 1.3822 - val_acc: 0.3125\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4786 - acc: 0.3890 - val_loss: 1.3404 - val_acc: 0.3594\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.5387 - acc: 0.3582 - val_loss: 1.3713 - val_acc: 0.3125\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.6065 - acc: 0.3371 - val_loss: 1.3220 - val_acc: 0.3438\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4820 - acc: 0.3890 - val_loss: 1.4765 - val_acc: 0.2656\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.4577 - acc: 0.3922 - val_loss: 1.3891 - val_acc: 0.2344\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.4697 - acc: 0.4019 - val_loss: 1.3630 - val_acc: 0.2500\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.5963 - acc: 0.3647 - val_loss: 1.3390 - val_acc: 0.3125\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.4961 - acc: 0.3857 - val_loss: 1.2360 - val_acc: 0.3750\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.5649 - acc: 0.3630 - val_loss: 1.3944 - val_acc: 0.2812\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.4855 - acc: 0.3922 - val_loss: 1.3229 - val_acc: 0.3594\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.5192 - acc: 0.3766 - val_loss: 1.3528 - val_acc: 0.2969\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.4418 - acc: 0.4198 - val_loss: 1.3796 - val_acc: 0.2656\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.5303 - acc: 0.3688 - val_loss: 1.3560 - val_acc: 0.2969\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.4410 - acc: 0.4133 - val_loss: 1.3900 - val_acc: 0.3125\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.5440 - acc: 0.3500 - val_loss: 1.2889 - val_acc: 0.3594\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.4501 - acc: 0.3809 - val_loss: 1.3130 - val_acc: 0.2969\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.5070 - acc: 0.3922 - val_loss: 1.3708 - val_acc: 0.3125\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.4708 - acc: 0.3971 - val_loss: 1.3918 - val_acc: 0.3125\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.4587 - acc: 0.3857 - val_loss: 1.2534 - val_acc: 0.3594\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3998 - acc: 0.4117 - val_loss: 1.2280 - val_acc: 0.3281\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.4032 - acc: 0.4109 - val_loss: 1.1947 - val_acc: 0.3906\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.5061 - acc: 0.4052 - val_loss: 1.2900 - val_acc: 0.3125\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.5024 - acc: 0.3890 - val_loss: 1.3226 - val_acc: 0.2812\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.4211 - acc: 0.4003 - val_loss: 1.2208 - val_acc: 0.3750\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3718 - acc: 0.4311 - val_loss: 1.1948 - val_acc: 0.3125\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.4735 - acc: 0.4149 - val_loss: 1.3402 - val_acc: 0.3125\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.4069 - acc: 0.3938 - val_loss: 1.2408 - val_acc: 0.3438\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.4579 - acc: 0.3859 - val_loss: 1.2776 - val_acc: 0.3125\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3436 - acc: 0.4311 - val_loss: 1.2453 - val_acc: 0.3281\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.4210 - acc: 0.3825 - val_loss: 1.2050 - val_acc: 0.3281\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4399 - acc: 0.4003 - val_loss: 1.2207 - val_acc: 0.3750\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.4342 - acc: 0.4068 - val_loss: 1.3812 - val_acc: 0.2344\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.4488 - acc: 0.4117 - val_loss: 1.1412 - val_acc: 0.3750\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.4447 - acc: 0.4149 - val_loss: 1.2572 - val_acc: 0.3438\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.4253 - acc: 0.4052 - val_loss: 1.3375 - val_acc: 0.2656\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.3976 - acc: 0.4198 - val_loss: 1.3362 - val_acc: 0.2656\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.4001 - acc: 0.4133 - val_loss: 1.2581 - val_acc: 0.3750\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3782 - acc: 0.4198 - val_loss: 1.2867 - val_acc: 0.2812\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.4255 - acc: 0.4117 - val_loss: 1.2351 - val_acc: 0.2969\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3179 - acc: 0.4500 - val_loss: 1.3490 - val_acc: 0.2344\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.4077 - acc: 0.4165 - val_loss: 1.2760 - val_acc: 0.2969\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.4694 - acc: 0.3874 - val_loss: 1.2608 - val_acc: 0.3125\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3770 - acc: 0.4003 - val_loss: 1.2396 - val_acc: 0.3281\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.3496 - acc: 0.4198 - val_loss: 1.3166 - val_acc: 0.2500\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.3303 - acc: 0.4425 - val_loss: 1.4125 - val_acc: 0.2188\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.4475 - acc: 0.3971 - val_loss: 1.3284 - val_acc: 0.2344\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.3237 - acc: 0.4489 - val_loss: 1.3010 - val_acc: 0.2656\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3901 - acc: 0.4100 - val_loss: 1.2404 - val_acc: 0.2656\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3960 - acc: 0.4198 - val_loss: 1.2059 - val_acc: 0.2812\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.3565 - acc: 0.4246 - val_loss: 1.1957 - val_acc: 0.2656\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.3244 - acc: 0.4198 - val_loss: 1.2553 - val_acc: 0.2656\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.3832 - acc: 0.4149 - val_loss: 1.3650 - val_acc: 0.2188\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.3214 - acc: 0.4327 - val_loss: 1.2044 - val_acc: 0.2969\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.3998 - acc: 0.4281 - val_loss: 1.2923 - val_acc: 0.1875\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3954 - acc: 0.4068 - val_loss: 1.3248 - val_acc: 0.2812\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.2888 - acc: 0.4538 - val_loss: 1.2731 - val_acc: 0.2969\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.3082 - acc: 0.4246 - val_loss: 1.3504 - val_acc: 0.2500\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.3839 - acc: 0.4165 - val_loss: 1.2598 - val_acc: 0.2344\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3029 - acc: 0.4506 - val_loss: 1.2458 - val_acc: 0.2656\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.3053 - acc: 0.4547 - val_loss: 1.2484 - val_acc: 0.2969\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3477 - acc: 0.4603 - val_loss: 1.1293 - val_acc: 0.3594\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3613 - acc: 0.4295 - val_loss: 1.2657 - val_acc: 0.2500\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.3265 - acc: 0.4295 - val_loss: 1.1998 - val_acc: 0.3281\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.2865 - acc: 0.4571 - val_loss: 1.1323 - val_acc: 0.3906\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.2918 - acc: 0.4246 - val_loss: 1.2666 - val_acc: 0.2188\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3291 - acc: 0.4506 - val_loss: 1.2452 - val_acc: 0.3281\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.3630 - acc: 0.4182 - val_loss: 1.1845 - val_acc: 0.3125\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2843 - acc: 0.4327 - val_loss: 1.2379 - val_acc: 0.3281\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.3524 - acc: 0.4214 - val_loss: 1.2839 - val_acc: 0.2656\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.3062 - acc: 0.4425 - val_loss: 1.3359 - val_acc: 0.2500\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.2904 - acc: 0.4652 - val_loss: 1.1943 - val_acc: 0.3125\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.3131 - acc: 0.4198 - val_loss: 1.1712 - val_acc: 0.3438\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.3138 - acc: 0.4230 - val_loss: 1.2115 - val_acc: 0.3125\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.3410 - acc: 0.4149 - val_loss: 1.2192 - val_acc: 0.3438\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1.2738 - acc: 0.4376 - val_loss: 1.2014 - val_acc: 0.2656\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.3237 - acc: 0.4457 - val_loss: 1.1415 - val_acc: 0.3438\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.3032 - acc: 0.4214 - val_loss: 1.3006 - val_acc: 0.2500\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2541 - acc: 0.4531 - val_loss: 1.3018 - val_acc: 0.2500\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.3375 - acc: 0.4408 - val_loss: 1.2656 - val_acc: 0.2656\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.3006 - acc: 0.4376 - val_loss: 1.3410 - val_acc: 0.2344\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2572 - acc: 0.4538 - val_loss: 1.1696 - val_acc: 0.3125\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.2276 - acc: 0.4719 - val_loss: 1.2877 - val_acc: 0.2812\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2739 - acc: 0.4344 - val_loss: 1.0948 - val_acc: 0.3594\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2663 - acc: 0.4522 - val_loss: 1.0823 - val_acc: 0.3750\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2109 - acc: 0.4635 - val_loss: 1.1923 - val_acc: 0.2500\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.2333 - acc: 0.4668 - val_loss: 1.2245 - val_acc: 0.2812\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2778 - acc: 0.4360 - val_loss: 1.1435 - val_acc: 0.3281\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.3187 - acc: 0.4425 - val_loss: 1.0957 - val_acc: 0.3281\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.2910 - acc: 0.4538 - val_loss: 1.2595 - val_acc: 0.2969\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2706 - acc: 0.4263 - val_loss: 1.1400 - val_acc: 0.2812\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2084 - acc: 0.4878 - val_loss: 1.3022 - val_acc: 0.1719\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2495 - acc: 0.4733 - val_loss: 1.1995 - val_acc: 0.2812\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2210 - acc: 0.4360 - val_loss: 1.1627 - val_acc: 0.2969\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2449 - acc: 0.4506 - val_loss: 1.2328 - val_acc: 0.2656\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 1.2303 - acc: 0.4587 - val_loss: 1.2264 - val_acc: 0.3125\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.2800 - acc: 0.4441 - val_loss: 1.1851 - val_acc: 0.2344\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.2493 - acc: 0.4489 - val_loss: 1.1589 - val_acc: 0.3438\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2237 - acc: 0.4668 - val_loss: 1.1961 - val_acc: 0.2969\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.2821 - acc: 0.4214 - val_loss: 1.1362 - val_acc: 0.3125\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2234 - acc: 0.4506 - val_loss: 1.2595 - val_acc: 0.2500\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2331 - acc: 0.4538 - val_loss: 1.2410 - val_acc: 0.2656\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2874 - acc: 0.4279 - val_loss: 1.1967 - val_acc: 0.3438\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2717 - acc: 0.4668 - val_loss: 1.1275 - val_acc: 0.3750\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.1810 - acc: 0.5057 - val_loss: 1.1462 - val_acc: 0.2969\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.2454 - acc: 0.4716 - val_loss: 1.3189 - val_acc: 0.2188\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2941 - acc: 0.4484 - val_loss: 1.1832 - val_acc: 0.2812\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2561 - acc: 0.4765 - val_loss: 1.2794 - val_acc: 0.2500\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2813 - acc: 0.4522 - val_loss: 1.1801 - val_acc: 0.2656\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1925 - acc: 0.4554 - val_loss: 1.1978 - val_acc: 0.2969\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1776 - acc: 0.4878 - val_loss: 1.1166 - val_acc: 0.3125\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2783 - acc: 0.4391 - val_loss: 1.1673 - val_acc: 0.3281\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.2922 - acc: 0.4473 - val_loss: 1.1515 - val_acc: 0.3438\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1785 - acc: 0.4522 - val_loss: 1.2126 - val_acc: 0.2812\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.1956 - acc: 0.4765 - val_loss: 1.1867 - val_acc: 0.3281\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2118 - acc: 0.4700 - val_loss: 1.2038 - val_acc: 0.2656\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2071 - acc: 0.4716 - val_loss: 1.3129 - val_acc: 0.1875\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2014 - acc: 0.4943 - val_loss: 1.3018 - val_acc: 0.2500\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2036 - acc: 0.4992 - val_loss: 1.2713 - val_acc: 0.2656\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 1.1799 - acc: 0.4781 - val_loss: 1.1497 - val_acc: 0.3125\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2302 - acc: 0.4700 - val_loss: 1.2154 - val_acc: 0.2969\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.2693 - acc: 0.4376 - val_loss: 1.1386 - val_acc: 0.3125\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1985 - acc: 0.4473 - val_loss: 1.2614 - val_acc: 0.2969\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 1.2008 - acc: 0.4668 - val_loss: 1.1891 - val_acc: 0.3125\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2226 - acc: 0.4733 - val_loss: 1.2540 - val_acc: 0.3281\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.2008 - acc: 0.4668 - val_loss: 1.2556 - val_acc: 0.2969\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.2074 - acc: 0.4733 - val_loss: 1.1224 - val_acc: 0.4375\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1992 - acc: 0.4797 - val_loss: 1.2995 - val_acc: 0.2656\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.2693 - acc: 0.4603 - val_loss: 1.2499 - val_acc: 0.3125\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1925 - acc: 0.4781 - val_loss: 1.2842 - val_acc: 0.2656\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.2398 - acc: 0.4625 - val_loss: 1.2507 - val_acc: 0.2656\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.2064 - acc: 0.4473 - val_loss: 1.2104 - val_acc: 0.3594\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.2055 - acc: 0.4992 - val_loss: 1.3039 - val_acc: 0.2969\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2158 - acc: 0.4749 - val_loss: 1.2663 - val_acc: 0.3125\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.1798 - acc: 0.4943 - val_loss: 1.3059 - val_acc: 0.2656\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1582 - acc: 0.4846 - val_loss: 1.2390 - val_acc: 0.2969\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1995 - acc: 0.5041 - val_loss: 1.3271 - val_acc: 0.2344\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2448 - acc: 0.4684 - val_loss: 1.1627 - val_acc: 0.2969\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2107 - acc: 0.4749 - val_loss: 1.2005 - val_acc: 0.3281\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 1.2157 - acc: 0.4846 - val_loss: 1.1582 - val_acc: 0.3750\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1268 - acc: 0.5316 - val_loss: 1.2529 - val_acc: 0.2500\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1884 - acc: 0.4830 - val_loss: 1.2382 - val_acc: 0.3125\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.2260 - acc: 0.4830 - val_loss: 1.2893 - val_acc: 0.2500\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 1.2722 - acc: 0.4635 - val_loss: 1.2771 - val_acc: 0.2969\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1565 - acc: 0.4895 - val_loss: 1.2155 - val_acc: 0.3125\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.2054 - acc: 0.4943 - val_loss: 1.2500 - val_acc: 0.2969\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1501 - acc: 0.5008 - val_loss: 1.2514 - val_acc: 0.2500\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1774 - acc: 0.4700 - val_loss: 1.2479 - val_acc: 0.2969\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.2471 - acc: 0.4506 - val_loss: 1.2008 - val_acc: 0.2969\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1793 - acc: 0.4911 - val_loss: 1.2582 - val_acc: 0.3438\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.1943 - acc: 0.4862 - val_loss: 1.2174 - val_acc: 0.3125\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1142 - acc: 0.5186 - val_loss: 1.2416 - val_acc: 0.2969\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1232 - acc: 0.5316 - val_loss: 1.2960 - val_acc: 0.2812\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.1377 - acc: 0.5122 - val_loss: 1.1486 - val_acc: 0.3281\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.2527 - acc: 0.4846 - val_loss: 1.2076 - val_acc: 0.3438\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2592 - acc: 0.4554 - val_loss: 1.2482 - val_acc: 0.2812\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.2774 - acc: 0.4360 - val_loss: 1.3505 - val_acc: 0.2812\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1824 - acc: 0.4911 - val_loss: 1.2525 - val_acc: 0.3750\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1745 - acc: 0.4684 - val_loss: 1.2011 - val_acc: 0.3281\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1532 - acc: 0.5203 - val_loss: 1.3102 - val_acc: 0.2500\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1954 - acc: 0.5041 - val_loss: 1.1765 - val_acc: 0.2969\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 1.1173 - acc: 0.5251 - val_loss: 1.1799 - val_acc: 0.3750\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1996 - acc: 0.4716 - val_loss: 1.2347 - val_acc: 0.3125\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1352 - acc: 0.4797 - val_loss: 1.2124 - val_acc: 0.2812\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1789 - acc: 0.4976 - val_loss: 1.2069 - val_acc: 0.3281\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1989 - acc: 0.4878 - val_loss: 1.2250 - val_acc: 0.3594\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.1473 - acc: 0.4959 - val_loss: 1.1985 - val_acc: 0.3438\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1958 - acc: 0.5057 - val_loss: 1.1916 - val_acc: 0.3125\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1450 - acc: 0.5138 - val_loss: 1.2710 - val_acc: 0.2656\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1358 - acc: 0.4830 - val_loss: 1.1552 - val_acc: 0.3281\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1442 - acc: 0.4812 - val_loss: 1.1987 - val_acc: 0.3125\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1247 - acc: 0.5024 - val_loss: 1.1492 - val_acc: 0.3438\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1964 - acc: 0.4878 - val_loss: 1.2402 - val_acc: 0.3438\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.2091 - acc: 0.4927 - val_loss: 1.1836 - val_acc: 0.3125\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1554 - acc: 0.5057 - val_loss: 1.2296 - val_acc: 0.3281\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0997 - acc: 0.5008 - val_loss: 1.2649 - val_acc: 0.3125\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1628 - acc: 0.4992 - val_loss: 1.2497 - val_acc: 0.2656\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1256 - acc: 0.5089 - val_loss: 1.2484 - val_acc: 0.2812\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1319 - acc: 0.5105 - val_loss: 1.1964 - val_acc: 0.2969\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1453 - acc: 0.4959 - val_loss: 1.1949 - val_acc: 0.2969\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0986 - acc: 0.5332 - val_loss: 1.2002 - val_acc: 0.3281\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1213 - acc: 0.5122 - val_loss: 1.1750 - val_acc: 0.2812\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.1741 - acc: 0.4992 - val_loss: 1.1669 - val_acc: 0.3281\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.1671 - acc: 0.4749 - val_loss: 1.1104 - val_acc: 0.3750\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1372 - acc: 0.5154 - val_loss: 1.1180 - val_acc: 0.2969\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1949 - acc: 0.4781 - val_loss: 1.2150 - val_acc: 0.3125\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1276 - acc: 0.4943 - val_loss: 1.1883 - val_acc: 0.3125\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0386 - acc: 0.5281 - val_loss: 1.1866 - val_acc: 0.3281\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.0932 - acc: 0.5316 - val_loss: 1.2133 - val_acc: 0.2656\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1015 - acc: 0.5219 - val_loss: 1.2051 - val_acc: 0.3438\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.1182 - acc: 0.5138 - val_loss: 1.1598 - val_acc: 0.3281\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1420 - acc: 0.5122 - val_loss: 1.2130 - val_acc: 0.3125\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1153 - acc: 0.5089 - val_loss: 1.2162 - val_acc: 0.3281\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.1441 - acc: 0.4927 - val_loss: 1.1673 - val_acc: 0.3438\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1193 - acc: 0.4922 - val_loss: 1.2463 - val_acc: 0.2812\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1365 - acc: 0.5024 - val_loss: 1.2562 - val_acc: 0.2812\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1925 - acc: 0.4878 - val_loss: 1.1627 - val_acc: 0.2969\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1618 - acc: 0.4830 - val_loss: 1.1349 - val_acc: 0.3438\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.2017 - acc: 0.5008 - val_loss: 1.2111 - val_acc: 0.2969\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1426 - acc: 0.5186 - val_loss: 1.2635 - val_acc: 0.3125\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 2s 192ms/step - loss: 1.0778 - acc: 0.5170 - val_loss: 1.1746 - val_acc: 0.3125\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0567 - acc: 0.5413 - val_loss: 1.2271 - val_acc: 0.3594\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1046 - acc: 0.5266 - val_loss: 1.2586 - val_acc: 0.2812\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.0937 - acc: 0.5008 - val_loss: 1.2252 - val_acc: 0.3438\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0694 - acc: 0.5300 - val_loss: 1.1516 - val_acc: 0.3281\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 1.0560 - acc: 0.5138 - val_loss: 1.2075 - val_acc: 0.2969\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1129 - acc: 0.5122 - val_loss: 1.1111 - val_acc: 0.3438\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1123 - acc: 0.5316 - val_loss: 1.0704 - val_acc: 0.3750\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1243 - acc: 0.5300 - val_loss: 1.1439 - val_acc: 0.3125\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 1.1445 - acc: 0.4976 - val_loss: 1.2090 - val_acc: 0.3281\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 1.1153 - acc: 0.5154 - val_loss: 1.3313 - val_acc: 0.2500\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1015 - acc: 0.5267 - val_loss: 1.3324 - val_acc: 0.2812\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0459 - acc: 0.5365 - val_loss: 1.3124 - val_acc: 0.2188\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.1685 - acc: 0.4814 - val_loss: 1.2499 - val_acc: 0.2969\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1228 - acc: 0.5186 - val_loss: 1.2433 - val_acc: 0.2969\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.0696 - acc: 0.5235 - val_loss: 1.1680 - val_acc: 0.3594\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1385 - acc: 0.5138 - val_loss: 1.2617 - val_acc: 0.2969\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0699 - acc: 0.5154 - val_loss: 1.2137 - val_acc: 0.2969\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1229 - acc: 0.4943 - val_loss: 1.3265 - val_acc: 0.2969\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0786 - acc: 0.5170 - val_loss: 1.2175 - val_acc: 0.2969\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0685 - acc: 0.5446 - val_loss: 1.2395 - val_acc: 0.2969\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0696 - acc: 0.5429 - val_loss: 1.2273 - val_acc: 0.2812\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.1231 - acc: 0.4878 - val_loss: 1.1821 - val_acc: 0.2812\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.1108 - acc: 0.4959 - val_loss: 1.1621 - val_acc: 0.3438\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1347 - acc: 0.5008 - val_loss: 1.2137 - val_acc: 0.2500\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1365 - acc: 0.5057 - val_loss: 1.2135 - val_acc: 0.2812\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.1524 - acc: 0.5186 - val_loss: 1.1984 - val_acc: 0.2969\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1229 - acc: 0.5031 - val_loss: 1.2088 - val_acc: 0.2500\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.1825 - acc: 0.4862 - val_loss: 1.2202 - val_acc: 0.3281\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0532 - acc: 0.5478 - val_loss: 1.3178 - val_acc: 0.2500\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0861 - acc: 0.5511 - val_loss: 1.1901 - val_acc: 0.2812\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0857 - acc: 0.5024 - val_loss: 1.1794 - val_acc: 0.3594\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.1280 - acc: 0.5063 - val_loss: 1.1758 - val_acc: 0.3125\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.1233 - acc: 0.5429 - val_loss: 1.1974 - val_acc: 0.2812\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.0497 - acc: 0.5316 - val_loss: 1.2594 - val_acc: 0.2969\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1615 - acc: 0.4765 - val_loss: 1.1996 - val_acc: 0.3281\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0456 - acc: 0.5203 - val_loss: 1.2973 - val_acc: 0.2812\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0353 - acc: 0.5344 - val_loss: 1.2353 - val_acc: 0.3125\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1048 - acc: 0.5235 - val_loss: 1.2110 - val_acc: 0.2812\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.1059 - acc: 0.5170 - val_loss: 1.1367 - val_acc: 0.3438\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.1005 - acc: 0.5284 - val_loss: 1.2157 - val_acc: 0.2969\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9935 - acc: 0.5689 - val_loss: 1.3105 - val_acc: 0.2188\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0435 - acc: 0.5328 - val_loss: 1.2695 - val_acc: 0.2500\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0668 - acc: 0.5478 - val_loss: 1.2342 - val_acc: 0.2656\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.1551 - acc: 0.5078 - val_loss: 1.3314 - val_acc: 0.2344\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.1012 - acc: 0.5203 - val_loss: 1.1945 - val_acc: 0.3281\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1342 - acc: 0.4943 - val_loss: 1.1957 - val_acc: 0.2500\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0963 - acc: 0.5332 - val_loss: 1.1430 - val_acc: 0.3438\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0640 - acc: 0.5235 - val_loss: 1.2291 - val_acc: 0.2344\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0562 - acc: 0.5348 - val_loss: 1.2468 - val_acc: 0.3438\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1025 - acc: 0.5284 - val_loss: 1.0631 - val_acc: 0.4062\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0497 - acc: 0.5494 - val_loss: 1.1965 - val_acc: 0.2812\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1342 - acc: 0.4959 - val_loss: 1.2893 - val_acc: 0.3125\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 1.1098 - acc: 0.5332 - val_loss: 1.2020 - val_acc: 0.2812\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0589 - acc: 0.5316 - val_loss: 1.3654 - val_acc: 0.2188\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0821 - acc: 0.5284 - val_loss: 1.1497 - val_acc: 0.3750\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0696 - acc: 0.5316 - val_loss: 1.2282 - val_acc: 0.3125\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.0663 - acc: 0.5381 - val_loss: 1.2472 - val_acc: 0.2500\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.1058 - acc: 0.5203 - val_loss: 1.3178 - val_acc: 0.2344\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.1300 - acc: 0.5089 - val_loss: 1.1397 - val_acc: 0.3594\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 1.0517 - acc: 0.5397 - val_loss: 1.1650 - val_acc: 0.2812\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0885 - acc: 0.5365 - val_loss: 1.1697 - val_acc: 0.3125\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0372 - acc: 0.5348 - val_loss: 1.1262 - val_acc: 0.2969\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.0505 - acc: 0.5429 - val_loss: 1.1961 - val_acc: 0.2969\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0906 - acc: 0.5251 - val_loss: 1.2003 - val_acc: 0.2656\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0826 - acc: 0.5138 - val_loss: 1.2290 - val_acc: 0.2812\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 1.0846 - acc: 0.5089 - val_loss: 1.3336 - val_acc: 0.2344\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0798 - acc: 0.5381 - val_loss: 1.2169 - val_acc: 0.2656\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0883 - acc: 0.5284 - val_loss: 1.2340 - val_acc: 0.2969\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1053 - acc: 0.5094 - val_loss: 1.2706 - val_acc: 0.3125\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.1166 - acc: 0.5559 - val_loss: 1.2928 - val_acc: 0.2969\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.0684 - acc: 0.5267 - val_loss: 1.2742 - val_acc: 0.3125\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.1194 - acc: 0.5122 - val_loss: 1.3089 - val_acc: 0.1562\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.1134 - acc: 0.5234 - val_loss: 1.2600 - val_acc: 0.2812\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.0049 - acc: 0.5446 - val_loss: 1.2911 - val_acc: 0.2812\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0251 - acc: 0.5348 - val_loss: 1.2015 - val_acc: 0.2969\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0342 - acc: 0.5284 - val_loss: 1.1901 - val_acc: 0.2969\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 1.0690 - acc: 0.5251 - val_loss: 1.2233 - val_acc: 0.2812\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.0406 - acc: 0.5105 - val_loss: 1.2944 - val_acc: 0.2031\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0700 - acc: 0.5297 - val_loss: 1.3623 - val_acc: 0.2031\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0234 - acc: 0.5640 - val_loss: 1.3060 - val_acc: 0.2031\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1089 - acc: 0.5154 - val_loss: 1.2158 - val_acc: 0.3281\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 1.0315 - acc: 0.5300 - val_loss: 1.2007 - val_acc: 0.2969\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0958 - acc: 0.5219 - val_loss: 1.2527 - val_acc: 0.2656\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 1.0848 - acc: 0.5073 - val_loss: 1.2568 - val_acc: 0.3125\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0048 - acc: 0.5494 - val_loss: 1.3363 - val_acc: 0.1719\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.1129 - acc: 0.5219 - val_loss: 1.2995 - val_acc: 0.3125\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0087 - acc: 0.5656 - val_loss: 1.2208 - val_acc: 0.2969\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 1.0095 - acc: 0.5478 - val_loss: 1.2252 - val_acc: 0.2500\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.9790 - acc: 0.5721 - val_loss: 1.1824 - val_acc: 0.3125\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0700 - acc: 0.5235 - val_loss: 1.2501 - val_acc: 0.3438\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0661 - acc: 0.5446 - val_loss: 1.1679 - val_acc: 0.2656\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0824 - acc: 0.5511 - val_loss: 1.1430 - val_acc: 0.3750\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0597 - acc: 0.5348 - val_loss: 1.2552 - val_acc: 0.2344\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.1408 - acc: 0.5154 - val_loss: 1.2449 - val_acc: 0.2969\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0199 - acc: 0.5478 - val_loss: 1.4032 - val_acc: 0.2188\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0049 - acc: 0.5705 - val_loss: 1.2290 - val_acc: 0.2344\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0200 - acc: 0.5446 - val_loss: 1.1527 - val_acc: 0.3125\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0770 - acc: 0.5348 - val_loss: 1.2048 - val_acc: 0.2969\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0500 - acc: 0.5559 - val_loss: 1.1265 - val_acc: 0.3438\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0417 - acc: 0.5721 - val_loss: 1.3489 - val_acc: 0.2656\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0702 - acc: 0.5057 - val_loss: 1.0923 - val_acc: 0.3281\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.0299 - acc: 0.5592 - val_loss: 1.2781 - val_acc: 0.2500\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 1.0786 - acc: 0.5316 - val_loss: 1.2142 - val_acc: 0.3438\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 0.9950 - acc: 0.5494 - val_loss: 1.3074 - val_acc: 0.2969\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0660 - acc: 0.5381 - val_loss: 1.2195 - val_acc: 0.2969\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9795 - acc: 0.5754 - val_loss: 1.1550 - val_acc: 0.2969\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 1.0587 - acc: 0.5154 - val_loss: 1.2414 - val_acc: 0.3438\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0976 - acc: 0.5284 - val_loss: 1.3308 - val_acc: 0.2812\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0148 - acc: 0.5478 - val_loss: 1.2105 - val_acc: 0.2969\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9896 - acc: 0.5462 - val_loss: 1.2614 - val_acc: 0.2812\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0313 - acc: 0.5559 - val_loss: 1.1897 - val_acc: 0.3438\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.0384 - acc: 0.5624 - val_loss: 1.2255 - val_acc: 0.3438\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0017 - acc: 0.5594 - val_loss: 1.2395 - val_acc: 0.3125\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.1000 - acc: 0.5186 - val_loss: 1.1710 - val_acc: 0.3750\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0465 - acc: 0.5478 - val_loss: 1.0835 - val_acc: 0.3281\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0648 - acc: 0.5267 - val_loss: 1.1203 - val_acc: 0.3594\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9739 - acc: 0.5851 - val_loss: 1.1912 - val_acc: 0.3281\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0786 - acc: 0.5186 - val_loss: 1.3358 - val_acc: 0.3125\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.0733 - acc: 0.5381 - val_loss: 1.2968 - val_acc: 0.3594\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0693 - acc: 0.5365 - val_loss: 1.1822 - val_acc: 0.3281\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0000 - acc: 0.5543 - val_loss: 1.2747 - val_acc: 0.2500\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9931 - acc: 0.5446 - val_loss: 1.2511 - val_acc: 0.2656\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0382 - acc: 0.5429 - val_loss: 1.1823 - val_acc: 0.3594\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.0024 - acc: 0.5397 - val_loss: 1.2604 - val_acc: 0.3438\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0268 - acc: 0.5721 - val_loss: 1.2199 - val_acc: 0.3281\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.0403 - acc: 0.5186 - val_loss: 1.3305 - val_acc: 0.2344\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0150 - acc: 0.5462 - val_loss: 1.2549 - val_acc: 0.2812\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0727 - acc: 0.5267 - val_loss: 1.1387 - val_acc: 0.3594\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0552 - acc: 0.5397 - val_loss: 1.3459 - val_acc: 0.1719\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 1.0187 - acc: 0.5527 - val_loss: 1.1709 - val_acc: 0.3438\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9862 - acc: 0.5754 - val_loss: 1.1464 - val_acc: 0.3594\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0319 - acc: 0.5188 - val_loss: 1.2950 - val_acc: 0.2344\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9475 - acc: 0.5891 - val_loss: 1.2747 - val_acc: 0.2188\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.0589 - acc: 0.5559 - val_loss: 1.1558 - val_acc: 0.3281\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0521 - acc: 0.5478 - val_loss: 1.1850 - val_acc: 0.3281\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9974 - acc: 0.5527 - val_loss: 1.3006 - val_acc: 0.2500\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 1.0283 - acc: 0.5531 - val_loss: 1.2521 - val_acc: 0.2969\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 1.0137 - acc: 0.5381 - val_loss: 1.2107 - val_acc: 0.2969\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0285 - acc: 0.5365 - val_loss: 1.3437 - val_acc: 0.2500\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9542 - acc: 0.5818 - val_loss: 1.1423 - val_acc: 0.2969\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0593 - acc: 0.5284 - val_loss: 1.1661 - val_acc: 0.3438\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0125 - acc: 0.5413 - val_loss: 1.2136 - val_acc: 0.2969\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1.0059 - acc: 0.5413 - val_loss: 1.2026 - val_acc: 0.2812\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0080 - acc: 0.5413 - val_loss: 1.2832 - val_acc: 0.2812\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9895 - acc: 0.5624 - val_loss: 1.2629 - val_acc: 0.2500\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9938 - acc: 0.5422 - val_loss: 1.2457 - val_acc: 0.3125\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9999 - acc: 0.5527 - val_loss: 1.2007 - val_acc: 0.3281\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0556 - acc: 0.5527 - val_loss: 1.2897 - val_acc: 0.2344\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0229 - acc: 0.5547 - val_loss: 1.0912 - val_acc: 0.3594\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9975 - acc: 0.5429 - val_loss: 1.2560 - val_acc: 0.3125\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9545 - acc: 0.5721 - val_loss: 1.1838 - val_acc: 0.3281\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9973 - acc: 0.5786 - val_loss: 1.2331 - val_acc: 0.3125\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 1.0597 - acc: 0.5511 - val_loss: 1.2706 - val_acc: 0.2969\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 1.0048 - acc: 0.5462 - val_loss: 1.2314 - val_acc: 0.2656\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 1.0232 - acc: 0.5575 - val_loss: 1.2245 - val_acc: 0.2656\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9880 - acc: 0.5413 - val_loss: 1.1238 - val_acc: 0.2969\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9818 - acc: 0.5802 - val_loss: 1.1993 - val_acc: 0.2500\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0079 - acc: 0.5413 - val_loss: 1.1959 - val_acc: 0.2500\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 1.0030 - acc: 0.5559 - val_loss: 1.2840 - val_acc: 0.2656\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0169 - acc: 0.5543 - val_loss: 1.2084 - val_acc: 0.2812\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9649 - acc: 0.5932 - val_loss: 1.1041 - val_acc: 0.3906\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9707 - acc: 0.5608 - val_loss: 1.2921 - val_acc: 0.2656\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9991 - acc: 0.5608 - val_loss: 1.3141 - val_acc: 0.2188\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0098 - acc: 0.5608 - val_loss: 1.2407 - val_acc: 0.3125\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0444 - acc: 0.5365 - val_loss: 1.2424 - val_acc: 0.2812\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0375 - acc: 0.5397 - val_loss: 1.2438 - val_acc: 0.2500\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.9828 - acc: 0.5640 - val_loss: 1.2872 - val_acc: 0.2656\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 1.0096 - acc: 0.5547 - val_loss: 1.2853 - val_acc: 0.2500\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9914 - acc: 0.5592 - val_loss: 1.1824 - val_acc: 0.2500\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9773 - acc: 0.5948 - val_loss: 1.1597 - val_acc: 0.3438\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9647 - acc: 0.5754 - val_loss: 1.2294 - val_acc: 0.2812\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 1.0555 - acc: 0.5122 - val_loss: 1.2037 - val_acc: 0.3281\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 1.0020 - acc: 0.5494 - val_loss: 1.1911 - val_acc: 0.2969\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9445 - acc: 0.5835 - val_loss: 1.3137 - val_acc: 0.2500\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0185 - acc: 0.5770 - val_loss: 1.2799 - val_acc: 0.2344\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0064 - acc: 0.5446 - val_loss: 1.4021 - val_acc: 0.1875\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 1.0169 - acc: 0.5575 - val_loss: 1.3013 - val_acc: 0.2344\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 1.0043 - acc: 0.5737 - val_loss: 1.1620 - val_acc: 0.3750\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 1.0465 - acc: 0.5511 - val_loss: 1.1759 - val_acc: 0.2656\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9626 - acc: 0.5781 - val_loss: 1.2982 - val_acc: 0.2031\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 1.0327 - acc: 0.5547 - val_loss: 1.1799 - val_acc: 0.3438\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 1.0193 - acc: 0.5446 - val_loss: 1.2288 - val_acc: 0.3281\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0289 - acc: 0.5543 - val_loss: 1.2880 - val_acc: 0.2812\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9913 - acc: 0.5592 - val_loss: 1.2892 - val_acc: 0.2656\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0356 - acc: 0.5543 - val_loss: 1.1067 - val_acc: 0.3281\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 0.9676 - acc: 0.5786 - val_loss: 1.2367 - val_acc: 0.3438\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9909 - acc: 0.5891 - val_loss: 1.2830 - val_acc: 0.3125\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9333 - acc: 0.5737 - val_loss: 1.2735 - val_acc: 0.2344\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9878 - acc: 0.5689 - val_loss: 1.1296 - val_acc: 0.2656\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0369 - acc: 0.5359 - val_loss: 1.1839 - val_acc: 0.2969\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9610 - acc: 0.5883 - val_loss: 1.2383 - val_acc: 0.2031\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9761 - acc: 0.5797 - val_loss: 1.1471 - val_acc: 0.3281\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9904 - acc: 0.5559 - val_loss: 1.2222 - val_acc: 0.2656\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 1.0538 - acc: 0.5344 - val_loss: 1.3684 - val_acc: 0.2344\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 1.0230 - acc: 0.5316 - val_loss: 1.2595 - val_acc: 0.2500\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9565 - acc: 0.5883 - val_loss: 1.2345 - val_acc: 0.2969\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9707 - acc: 0.5689 - val_loss: 1.2640 - val_acc: 0.2812\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9977 - acc: 0.5640 - val_loss: 1.1434 - val_acc: 0.3594\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9999 - acc: 0.5494 - val_loss: 1.2987 - val_acc: 0.2656\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9626 - acc: 0.5608 - val_loss: 1.1733 - val_acc: 0.2969\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0227 - acc: 0.5624 - val_loss: 1.2344 - val_acc: 0.2656\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9682 - acc: 0.5754 - val_loss: 1.2575 - val_acc: 0.2812\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 1.0188 - acc: 0.5689 - val_loss: 1.2773 - val_acc: 0.2656\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0085 - acc: 0.5867 - val_loss: 1.2620 - val_acc: 0.2812\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0101 - acc: 0.5624 - val_loss: 1.2142 - val_acc: 0.2812\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.9556 - acc: 0.5818 - val_loss: 1.2177 - val_acc: 0.2969\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9129 - acc: 0.5851 - val_loss: 1.2092 - val_acc: 0.2969\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9509 - acc: 0.5705 - val_loss: 1.2953 - val_acc: 0.2969\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 0.9769 - acc: 0.5608 - val_loss: 1.2586 - val_acc: 0.2812\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9828 - acc: 0.5721 - val_loss: 1.2686 - val_acc: 0.2812\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9528 - acc: 0.6062 - val_loss: 1.2149 - val_acc: 0.3125\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9579 - acc: 0.5818 - val_loss: 1.2891 - val_acc: 0.2656\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9961 - acc: 0.5689 - val_loss: 1.1835 - val_acc: 0.2812\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9545 - acc: 0.5818 - val_loss: 1.1590 - val_acc: 0.2969\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9788 - acc: 0.5656 - val_loss: 1.2578 - val_acc: 0.2812\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9764 - acc: 0.5721 - val_loss: 1.2384 - val_acc: 0.3594\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 1.0332 - acc: 0.5397 - val_loss: 1.2439 - val_acc: 0.2500\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0164 - acc: 0.5705 - val_loss: 1.2670 - val_acc: 0.2188\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9474 - acc: 0.5624 - val_loss: 1.1017 - val_acc: 0.3906\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9874 - acc: 0.5737 - val_loss: 1.2807 - val_acc: 0.2188\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9726 - acc: 0.5802 - val_loss: 1.2278 - val_acc: 0.2969\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9613 - acc: 0.5721 - val_loss: 1.2603 - val_acc: 0.2500\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9557 - acc: 0.5770 - val_loss: 1.1275 - val_acc: 0.3438\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9217 - acc: 0.5802 - val_loss: 1.2189 - val_acc: 0.3438\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9859 - acc: 0.5624 - val_loss: 1.2750 - val_acc: 0.2812\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 1.0894 - acc: 0.5397 - val_loss: 1.2065 - val_acc: 0.3906\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9610 - acc: 0.5948 - val_loss: 1.2674 - val_acc: 0.2656\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9919 - acc: 0.5656 - val_loss: 1.1369 - val_acc: 0.3438\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9977 - acc: 0.5608 - val_loss: 1.2336 - val_acc: 0.2656\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9886 - acc: 0.5559 - val_loss: 1.3380 - val_acc: 0.2344\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9582 - acc: 0.5770 - val_loss: 1.2268 - val_acc: 0.3125\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.9171 - acc: 0.6110 - val_loss: 1.1185 - val_acc: 0.3594\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 1.0087 - acc: 0.5429 - val_loss: 1.1835 - val_acc: 0.3281\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9833 - acc: 0.5543 - val_loss: 1.2540 - val_acc: 0.2812\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9686 - acc: 0.5835 - val_loss: 1.3403 - val_acc: 0.2812\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9961 - acc: 0.5867 - val_loss: 1.2398 - val_acc: 0.2656\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 1.0174 - acc: 0.5527 - val_loss: 1.2530 - val_acc: 0.2969\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9891 - acc: 0.5689 - val_loss: 1.3013 - val_acc: 0.2656\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9879 - acc: 0.5802 - val_loss: 1.1871 - val_acc: 0.3125\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9770 - acc: 0.5608 - val_loss: 1.2807 - val_acc: 0.3125\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.9664 - acc: 0.6062 - val_loss: 1.3744 - val_acc: 0.2812\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9384 - acc: 0.5997 - val_loss: 1.1692 - val_acc: 0.2969\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9411 - acc: 0.5689 - val_loss: 1.2289 - val_acc: 0.3125\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9993 - acc: 0.5640 - val_loss: 1.2672 - val_acc: 0.3594\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9402 - acc: 0.5721 - val_loss: 1.2309 - val_acc: 0.3125\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9938 - acc: 0.5900 - val_loss: 1.3721 - val_acc: 0.2188\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 1.0218 - acc: 0.5348 - val_loss: 1.2479 - val_acc: 0.3125\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9331 - acc: 0.5818 - val_loss: 1.2749 - val_acc: 0.3125\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9634 - acc: 0.5656 - val_loss: 1.2525 - val_acc: 0.3281\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9117 - acc: 0.6029 - val_loss: 1.2343 - val_acc: 0.3438\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9297 - acc: 0.5984 - val_loss: 1.3404 - val_acc: 0.3125\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9759 - acc: 0.5531 - val_loss: 1.2283 - val_acc: 0.3125\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9850 - acc: 0.5673 - val_loss: 1.3436 - val_acc: 0.2812\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.9385 - acc: 0.5867 - val_loss: 1.2079 - val_acc: 0.2656\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9265 - acc: 0.6159 - val_loss: 1.2948 - val_acc: 0.3125\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9296 - acc: 0.6045 - val_loss: 1.2289 - val_acc: 0.2812\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 1.0209 - acc: 0.5543 - val_loss: 1.2601 - val_acc: 0.2969\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9497 - acc: 0.5835 - val_loss: 1.1968 - val_acc: 0.3125\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9746 - acc: 0.5609 - val_loss: 1.1735 - val_acc: 0.3438\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9332 - acc: 0.6013 - val_loss: 1.2186 - val_acc: 0.3125\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9329 - acc: 0.5673 - val_loss: 1.1772 - val_acc: 0.2969\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9105 - acc: 0.5786 - val_loss: 1.1610 - val_acc: 0.2969\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9926 - acc: 0.5624 - val_loss: 1.2271 - val_acc: 0.3281\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9219 - acc: 0.5883 - val_loss: 1.2648 - val_acc: 0.2500\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9847 - acc: 0.5835 - val_loss: 1.0498 - val_acc: 0.3750\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.9206 - acc: 0.5997 - val_loss: 1.2704 - val_acc: 0.2969\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9585 - acc: 0.5916 - val_loss: 1.1612 - val_acc: 0.3281\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9448 - acc: 0.5932 - val_loss: 1.3241 - val_acc: 0.2344\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9560 - acc: 0.5953 - val_loss: 1.2802 - val_acc: 0.3125\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9339 - acc: 0.5883 - val_loss: 1.1779 - val_acc: 0.3594\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9634 - acc: 0.5754 - val_loss: 1.2802 - val_acc: 0.2656\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9782 - acc: 0.5797 - val_loss: 1.3231 - val_acc: 0.2656\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9684 - acc: 0.5689 - val_loss: 1.1640 - val_acc: 0.3281\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9610 - acc: 0.5721 - val_loss: 1.1922 - val_acc: 0.3594\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8987 - acc: 0.6159 - val_loss: 1.2421 - val_acc: 0.2188\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9622 - acc: 0.5802 - val_loss: 1.3349 - val_acc: 0.2812\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9671 - acc: 0.5813 - val_loss: 1.2634 - val_acc: 0.3281\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9280 - acc: 0.6062 - val_loss: 1.2414 - val_acc: 0.2969\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9173 - acc: 0.6045 - val_loss: 1.3306 - val_acc: 0.3125\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9607 - acc: 0.5932 - val_loss: 1.2441 - val_acc: 0.3281\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1.0102 - acc: 0.5527 - val_loss: 1.2349 - val_acc: 0.2188\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9985 - acc: 0.5413 - val_loss: 1.2690 - val_acc: 0.3281\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9643 - acc: 0.5802 - val_loss: 1.2110 - val_acc: 0.2656\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9674 - acc: 0.5689 - val_loss: 1.3141 - val_acc: 0.2812\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9302 - acc: 0.5802 - val_loss: 1.3063 - val_acc: 0.2500\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9783 - acc: 0.5813 - val_loss: 1.2116 - val_acc: 0.3125\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8351 - acc: 0.6321 - val_loss: 1.1907 - val_acc: 0.3594\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9211 - acc: 0.6045 - val_loss: 1.1914 - val_acc: 0.3125\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9648 - acc: 0.5689 - val_loss: 1.3929 - val_acc: 0.2344\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9083 - acc: 0.6029 - val_loss: 1.1738 - val_acc: 0.3125\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 1.0208 - acc: 0.5462 - val_loss: 1.1685 - val_acc: 0.3125\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9777 - acc: 0.5932 - val_loss: 1.1577 - val_acc: 0.3594\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9754 - acc: 0.5624 - val_loss: 1.1904 - val_acc: 0.3281\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9370 - acc: 0.5900 - val_loss: 1.2339 - val_acc: 0.3438\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9508 - acc: 0.5835 - val_loss: 1.2004 - val_acc: 0.3125\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9396 - acc: 0.5754 - val_loss: 1.2803 - val_acc: 0.3125\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9365 - acc: 0.5608 - val_loss: 1.3199 - val_acc: 0.2969\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9939 - acc: 0.5608 - val_loss: 1.2095 - val_acc: 0.3438\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9287 - acc: 0.5948 - val_loss: 1.1658 - val_acc: 0.3594\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 0.9642 - acc: 0.5818 - val_loss: 1.1475 - val_acc: 0.3750\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9343 - acc: 0.5818 - val_loss: 1.1597 - val_acc: 0.3906\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9524 - acc: 0.5721 - val_loss: 1.2024 - val_acc: 0.2812\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9486 - acc: 0.5953 - val_loss: 1.2098 - val_acc: 0.3594\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.9134 - acc: 0.6062 - val_loss: 1.2444 - val_acc: 0.3125\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9024 - acc: 0.6110 - val_loss: 1.3179 - val_acc: 0.2812\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9282 - acc: 0.5851 - val_loss: 1.2453 - val_acc: 0.3125\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9230 - acc: 0.5818 - val_loss: 1.2578 - val_acc: 0.3281\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9120 - acc: 0.6078 - val_loss: 1.1494 - val_acc: 0.3594\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9390 - acc: 0.5640 - val_loss: 1.1800 - val_acc: 0.3281\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9502 - acc: 0.5754 - val_loss: 1.2060 - val_acc: 0.3438\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8893 - acc: 0.6045 - val_loss: 1.1094 - val_acc: 0.3750\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9268 - acc: 0.5891 - val_loss: 1.2307 - val_acc: 0.2500\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9060 - acc: 0.6207 - val_loss: 1.2136 - val_acc: 0.2656\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9705 - acc: 0.5802 - val_loss: 1.2352 - val_acc: 0.3281\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9648 - acc: 0.5656 - val_loss: 1.2342 - val_acc: 0.3281\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9515 - acc: 0.5625 - val_loss: 1.1640 - val_acc: 0.3125\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.9198 - acc: 0.5786 - val_loss: 1.2174 - val_acc: 0.3281\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9830 - acc: 0.5737 - val_loss: 1.1420 - val_acc: 0.3594\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9370 - acc: 0.5802 - val_loss: 1.1186 - val_acc: 0.3281\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 0.9911 - acc: 0.5721 - val_loss: 1.2901 - val_acc: 0.2656\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9638 - acc: 0.5766 - val_loss: 1.3011 - val_acc: 0.2656\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8977 - acc: 0.6062 - val_loss: 1.3610 - val_acc: 0.2656\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9262 - acc: 0.5835 - val_loss: 1.1914 - val_acc: 0.3750\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8967 - acc: 0.5997 - val_loss: 1.1888 - val_acc: 0.3281\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9405 - acc: 0.5624 - val_loss: 1.1917 - val_acc: 0.3438\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9016 - acc: 0.5997 - val_loss: 1.2996 - val_acc: 0.2344\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9269 - acc: 0.5802 - val_loss: 1.2348 - val_acc: 0.2969\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9096 - acc: 0.5997 - val_loss: 1.2589 - val_acc: 0.2812\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9674 - acc: 0.5766 - val_loss: 1.2767 - val_acc: 0.2969\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9615 - acc: 0.5867 - val_loss: 1.3660 - val_acc: 0.3281\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9756 - acc: 0.5867 - val_loss: 1.2679 - val_acc: 0.2656\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9034 - acc: 0.6062 - val_loss: 1.2933 - val_acc: 0.2656\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9233 - acc: 0.5867 - val_loss: 1.2440 - val_acc: 0.3125\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9063 - acc: 0.6062 - val_loss: 1.2013 - val_acc: 0.3594\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9234 - acc: 0.5964 - val_loss: 1.2825 - val_acc: 0.2969\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9209 - acc: 0.6013 - val_loss: 1.2373 - val_acc: 0.2812\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9174 - acc: 0.6078 - val_loss: 1.3045 - val_acc: 0.2500\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9835 - acc: 0.5624 - val_loss: 1.1304 - val_acc: 0.3281\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9547 - acc: 0.5851 - val_loss: 1.1940 - val_acc: 0.3438\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9437 - acc: 0.5705 - val_loss: 1.2363 - val_acc: 0.3281\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9341 - acc: 0.5984 - val_loss: 1.2028 - val_acc: 0.3281\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9367 - acc: 0.5624 - val_loss: 1.3451 - val_acc: 0.2656\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9132 - acc: 0.6191 - val_loss: 1.1827 - val_acc: 0.3281\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9216 - acc: 0.5851 - val_loss: 1.2473 - val_acc: 0.2812\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.9624 - acc: 0.5916 - val_loss: 1.3310 - val_acc: 0.2500\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8983 - acc: 0.5964 - val_loss: 1.2347 - val_acc: 0.3281\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9163 - acc: 0.5984 - val_loss: 1.2464 - val_acc: 0.3125\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 0.9493 - acc: 0.5851 - val_loss: 1.2878 - val_acc: 0.3125\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9036 - acc: 0.5953 - val_loss: 1.2183 - val_acc: 0.2969\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9134 - acc: 0.5851 - val_loss: 1.2366 - val_acc: 0.2656\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9148 - acc: 0.5883 - val_loss: 1.3181 - val_acc: 0.2812\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9230 - acc: 0.6029 - val_loss: 1.1844 - val_acc: 0.3281\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 0.9633 - acc: 0.5770 - val_loss: 1.2463 - val_acc: 0.3750\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9492 - acc: 0.5867 - val_loss: 1.2439 - val_acc: 0.3125\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8788 - acc: 0.6143 - val_loss: 1.1525 - val_acc: 0.2812\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8632 - acc: 0.6126 - val_loss: 1.1843 - val_acc: 0.3906\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.9089 - acc: 0.6078 - val_loss: 1.2331 - val_acc: 0.2812\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9277 - acc: 0.5851 - val_loss: 1.1859 - val_acc: 0.3281\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9211 - acc: 0.6078 - val_loss: 1.2165 - val_acc: 0.3281\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8695 - acc: 0.6126 - val_loss: 1.2965 - val_acc: 0.2812\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8561 - acc: 0.6224 - val_loss: 1.3778 - val_acc: 0.2344\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8802 - acc: 0.6353 - val_loss: 1.2597 - val_acc: 0.2812\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9062 - acc: 0.6078 - val_loss: 1.2968 - val_acc: 0.2031\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8559 - acc: 0.6256 - val_loss: 1.2592 - val_acc: 0.3125\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8730 - acc: 0.6078 - val_loss: 1.0770 - val_acc: 0.3750\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9446 - acc: 0.5932 - val_loss: 1.1701 - val_acc: 0.4219\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9155 - acc: 0.5835 - val_loss: 1.2753 - val_acc: 0.3281\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9077 - acc: 0.5719 - val_loss: 1.2654 - val_acc: 0.2969\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8800 - acc: 0.6240 - val_loss: 1.1743 - val_acc: 0.3750\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.9338 - acc: 0.6110 - val_loss: 1.2432 - val_acc: 0.3281\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8930 - acc: 0.6224 - val_loss: 1.2417 - val_acc: 0.2500\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9169 - acc: 0.5705 - val_loss: 1.2351 - val_acc: 0.3281\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 2s 186ms/step - loss: 0.9382 - acc: 0.5900 - val_loss: 1.2247 - val_acc: 0.3281\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9054 - acc: 0.5964 - val_loss: 1.1530 - val_acc: 0.2969\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.9175 - acc: 0.5948 - val_loss: 1.2160 - val_acc: 0.3125\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9220 - acc: 0.5867 - val_loss: 1.2881 - val_acc: 0.3594\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9040 - acc: 0.6013 - val_loss: 1.2742 - val_acc: 0.3125\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.9147 - acc: 0.6062 - val_loss: 1.2757 - val_acc: 0.2969\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9819 - acc: 0.5575 - val_loss: 1.2263 - val_acc: 0.3281\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9212 - acc: 0.5981 - val_loss: 1.2103 - val_acc: 0.3594\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9311 - acc: 0.5818 - val_loss: 1.2885 - val_acc: 0.2969\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.9358 - acc: 0.5932 - val_loss: 1.1502 - val_acc: 0.3906\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9004 - acc: 0.5948 - val_loss: 1.2784 - val_acc: 0.2500\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9460 - acc: 0.5721 - val_loss: 1.0874 - val_acc: 0.3906\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9125 - acc: 0.5932 - val_loss: 1.2260 - val_acc: 0.3594\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9244 - acc: 0.6045 - val_loss: 1.1967 - val_acc: 0.3594\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9011 - acc: 0.6207 - val_loss: 1.1914 - val_acc: 0.3438\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8846 - acc: 0.6250 - val_loss: 1.2308 - val_acc: 0.2969\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9569 - acc: 0.5689 - val_loss: 1.2751 - val_acc: 0.2500\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9575 - acc: 0.5851 - val_loss: 1.2293 - val_acc: 0.3438\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9574 - acc: 0.5984 - val_loss: 1.2192 - val_acc: 0.3125\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9471 - acc: 0.5786 - val_loss: 1.2249 - val_acc: 0.3438\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8970 - acc: 0.5981 - val_loss: 1.2222 - val_acc: 0.3125\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9439 - acc: 0.5818 - val_loss: 1.2145 - val_acc: 0.3750\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9393 - acc: 0.5818 - val_loss: 1.2419 - val_acc: 0.2656\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9015 - acc: 0.6143 - val_loss: 1.2669 - val_acc: 0.2812\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9111 - acc: 0.5948 - val_loss: 1.1859 - val_acc: 0.3125\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9236 - acc: 0.5835 - val_loss: 1.1923 - val_acc: 0.3438\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9045 - acc: 0.6062 - val_loss: 1.2863 - val_acc: 0.2969\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.9071 - acc: 0.6110 - val_loss: 1.1321 - val_acc: 0.4219\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8855 - acc: 0.6207 - val_loss: 1.0919 - val_acc: 0.4688\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.9128 - acc: 0.5916 - val_loss: 1.2197 - val_acc: 0.3281\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 0.9275 - acc: 0.5786 - val_loss: 1.2287 - val_acc: 0.3594\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.9503 - acc: 0.5948 - val_loss: 1.2285 - val_acc: 0.3281\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8541 - acc: 0.6418 - val_loss: 1.2258 - val_acc: 0.2969\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9357 - acc: 0.6062 - val_loss: 1.2041 - val_acc: 0.3750\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8911 - acc: 0.6353 - val_loss: 1.1199 - val_acc: 0.3281\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9305 - acc: 0.5916 - val_loss: 1.2549 - val_acc: 0.2969\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9122 - acc: 0.5656 - val_loss: 1.1814 - val_acc: 0.2969\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8705 - acc: 0.6126 - val_loss: 1.0830 - val_acc: 0.3438\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9539 - acc: 0.5948 - val_loss: 1.2096 - val_acc: 0.2812\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8761 - acc: 0.6256 - val_loss: 1.2611 - val_acc: 0.2969\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8730 - acc: 0.6353 - val_loss: 1.2717 - val_acc: 0.3281\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8834 - acc: 0.6013 - val_loss: 1.3059 - val_acc: 0.2812\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.9365 - acc: 0.5851 - val_loss: 1.3428 - val_acc: 0.2812\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8944 - acc: 0.6159 - val_loss: 1.2239 - val_acc: 0.3906\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8889 - acc: 0.6013 - val_loss: 1.2296 - val_acc: 0.3438\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9017 - acc: 0.6031 - val_loss: 1.2401 - val_acc: 0.2969\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.9128 - acc: 0.5883 - val_loss: 1.1724 - val_acc: 0.2812\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9098 - acc: 0.6109 - val_loss: 1.1891 - val_acc: 0.3281\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 0.9365 - acc: 0.5818 - val_loss: 1.2921 - val_acc: 0.3438\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9087 - acc: 0.5719 - val_loss: 1.2111 - val_acc: 0.3438\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9058 - acc: 0.5900 - val_loss: 1.3478 - val_acc: 0.3125\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.9005 - acc: 0.6256 - val_loss: 1.2356 - val_acc: 0.2969\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 0.9016 - acc: 0.6013 - val_loss: 1.1660 - val_acc: 0.3750\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 3s 207ms/step - loss: 0.9317 - acc: 0.6078 - val_loss: 1.2784 - val_acc: 0.3125\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9040 - acc: 0.6207 - val_loss: 1.1606 - val_acc: 0.3594\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8487 - acc: 0.6321 - val_loss: 1.1435 - val_acc: 0.3594\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8758 - acc: 0.5948 - val_loss: 1.2077 - val_acc: 0.3750\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.9120 - acc: 0.6013 - val_loss: 1.2090 - val_acc: 0.2969\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9177 - acc: 0.6078 - val_loss: 1.2795 - val_acc: 0.2969\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8449 - acc: 0.6321 - val_loss: 1.2221 - val_acc: 0.3438\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9051 - acc: 0.6029 - val_loss: 1.2117 - val_acc: 0.3438\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8780 - acc: 0.6328 - val_loss: 1.3396 - val_acc: 0.2812\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8923 - acc: 0.6062 - val_loss: 1.1397 - val_acc: 0.3594\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.9023 - acc: 0.6013 - val_loss: 1.3163 - val_acc: 0.2500\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8929 - acc: 0.6175 - val_loss: 1.2686 - val_acc: 0.3438\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8806 - acc: 0.6110 - val_loss: 1.3579 - val_acc: 0.2344\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8590 - acc: 0.6256 - val_loss: 1.1461 - val_acc: 0.3906\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 2s 206ms/step - loss: 0.8694 - acc: 0.6159 - val_loss: 1.2746 - val_acc: 0.3281\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9054 - acc: 0.5932 - val_loss: 1.2442 - val_acc: 0.2656\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.9143 - acc: 0.5851 - val_loss: 1.2097 - val_acc: 0.3125\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8876 - acc: 0.6353 - val_loss: 1.2616 - val_acc: 0.3281\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8742 - acc: 0.6110 - val_loss: 1.3072 - val_acc: 0.2969\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8700 - acc: 0.6305 - val_loss: 1.2271 - val_acc: 0.3281\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8781 - acc: 0.6110 - val_loss: 1.2229 - val_acc: 0.3281\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.9473 - acc: 0.5948 - val_loss: 1.1631 - val_acc: 0.3594\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8767 - acc: 0.6159 - val_loss: 1.2479 - val_acc: 0.3125\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9317 - acc: 0.5948 - val_loss: 1.3057 - val_acc: 0.3125\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8999 - acc: 0.6240 - val_loss: 1.1766 - val_acc: 0.3594\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8445 - acc: 0.6434 - val_loss: 1.2986 - val_acc: 0.3125\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8589 - acc: 0.6402 - val_loss: 1.2751 - val_acc: 0.3281\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.9255 - acc: 0.6013 - val_loss: 1.3079 - val_acc: 0.2812\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9253 - acc: 0.6045 - val_loss: 1.2519 - val_acc: 0.3281\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8692 - acc: 0.6219 - val_loss: 1.1823 - val_acc: 0.3438\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9252 - acc: 0.5624 - val_loss: 1.3319 - val_acc: 0.3125\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8288 - acc: 0.6499 - val_loss: 1.2595 - val_acc: 0.3438\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8474 - acc: 0.6272 - val_loss: 1.1713 - val_acc: 0.3594\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8711 - acc: 0.6370 - val_loss: 1.1591 - val_acc: 0.3906\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.9230 - acc: 0.5770 - val_loss: 1.2104 - val_acc: 0.3438\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8974 - acc: 0.6078 - val_loss: 1.2806 - val_acc: 0.3281\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8881 - acc: 0.5916 - val_loss: 1.2406 - val_acc: 0.3281\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9289 - acc: 0.5867 - val_loss: 1.2313 - val_acc: 0.2969\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.9085 - acc: 0.6191 - val_loss: 1.2367 - val_acc: 0.3125\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8525 - acc: 0.6240 - val_loss: 1.2219 - val_acc: 0.3281\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8289 - acc: 0.6337 - val_loss: 1.1800 - val_acc: 0.3906\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8684 - acc: 0.6126 - val_loss: 1.1422 - val_acc: 0.4219\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8491 - acc: 0.6175 - val_loss: 1.1826 - val_acc: 0.3594\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9345 - acc: 0.6126 - val_loss: 1.2016 - val_acc: 0.3125\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8592 - acc: 0.6175 - val_loss: 1.2303 - val_acc: 0.3281\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.9024 - acc: 0.6000 - val_loss: 1.1983 - val_acc: 0.3438\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 2s 193ms/step - loss: 0.9347 - acc: 0.5770 - val_loss: 1.2854 - val_acc: 0.2812\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8810 - acc: 0.5981 - val_loss: 1.2930 - val_acc: 0.2969\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8569 - acc: 0.6175 - val_loss: 1.2077 - val_acc: 0.3438\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8789 - acc: 0.5997 - val_loss: 1.1553 - val_acc: 0.3594\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8735 - acc: 0.6467 - val_loss: 1.0986 - val_acc: 0.3906\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8826 - acc: 0.6078 - val_loss: 1.2931 - val_acc: 0.2656\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8995 - acc: 0.5932 - val_loss: 1.2689 - val_acc: 0.3438\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.8817 - acc: 0.5916 - val_loss: 1.1991 - val_acc: 0.3125\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8873 - acc: 0.6175 - val_loss: 1.1652 - val_acc: 0.3438\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9291 - acc: 0.5737 - val_loss: 1.1408 - val_acc: 0.3594\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8759 - acc: 0.6078 - val_loss: 1.2492 - val_acc: 0.3594\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8713 - acc: 0.6191 - val_loss: 1.3920 - val_acc: 0.2344\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9010 - acc: 0.5981 - val_loss: 1.3079 - val_acc: 0.3594\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8939 - acc: 0.6297 - val_loss: 1.3170 - val_acc: 0.2812\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8705 - acc: 0.6078 - val_loss: 1.3047 - val_acc: 0.2500\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8712 - acc: 0.6094 - val_loss: 1.1370 - val_acc: 0.4375\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.9307 - acc: 0.5754 - val_loss: 1.2040 - val_acc: 0.2969\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8907 - acc: 0.6045 - val_loss: 1.2128 - val_acc: 0.3594\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8326 - acc: 0.6110 - val_loss: 1.2696 - val_acc: 0.3281\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8500 - acc: 0.6078 - val_loss: 1.3268 - val_acc: 0.3281\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8391 - acc: 0.6288 - val_loss: 1.1781 - val_acc: 0.3281\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8486 - acc: 0.6434 - val_loss: 1.3402 - val_acc: 0.2812\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.9056 - acc: 0.5900 - val_loss: 1.2194 - val_acc: 0.3281\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8600 - acc: 0.6143 - val_loss: 1.1330 - val_acc: 0.3281\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8416 - acc: 0.6288 - val_loss: 1.1510 - val_acc: 0.3438\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8763 - acc: 0.6126 - val_loss: 1.1889 - val_acc: 0.3594\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8490 - acc: 0.6175 - val_loss: 1.2633 - val_acc: 0.2656\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8389 - acc: 0.6467 - val_loss: 1.2794 - val_acc: 0.2656\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8311 - acc: 0.6288 - val_loss: 1.3094 - val_acc: 0.3281\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8618 - acc: 0.6256 - val_loss: 1.1993 - val_acc: 0.3750\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8531 - acc: 0.6483 - val_loss: 1.2039 - val_acc: 0.3906\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8421 - acc: 0.6207 - val_loss: 1.1940 - val_acc: 0.3750\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8638 - acc: 0.6143 - val_loss: 1.2726 - val_acc: 0.3594\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9134 - acc: 0.5969 - val_loss: 1.1847 - val_acc: 0.3281\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8176 - acc: 0.6272 - val_loss: 1.2269 - val_acc: 0.3750\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8379 - acc: 0.6483 - val_loss: 1.2409 - val_acc: 0.3125\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8678 - acc: 0.6062 - val_loss: 1.1440 - val_acc: 0.3750\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8635 - acc: 0.6272 - val_loss: 1.2246 - val_acc: 0.3438\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.9068 - acc: 0.6224 - val_loss: 1.2723 - val_acc: 0.3438\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.9119 - acc: 0.6094 - val_loss: 1.1744 - val_acc: 0.4062\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8316 - acc: 0.6203 - val_loss: 1.2409 - val_acc: 0.3281\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8426 - acc: 0.6438 - val_loss: 1.1692 - val_acc: 0.3906\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8416 - acc: 0.6305 - val_loss: 1.2304 - val_acc: 0.3438\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8692 - acc: 0.5981 - val_loss: 1.2091 - val_acc: 0.3438\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8786 - acc: 0.6126 - val_loss: 1.2317 - val_acc: 0.2969\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8475 - acc: 0.6234 - val_loss: 1.1829 - val_acc: 0.3594\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8625 - acc: 0.6207 - val_loss: 1.1596 - val_acc: 0.3594\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8993 - acc: 0.5916 - val_loss: 1.2187 - val_acc: 0.3750\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8989 - acc: 0.6143 - val_loss: 1.2867 - val_acc: 0.3281\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8304 - acc: 0.6159 - val_loss: 1.1648 - val_acc: 0.3906\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8434 - acc: 0.6013 - val_loss: 1.1118 - val_acc: 0.3438\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8641 - acc: 0.6288 - val_loss: 1.1002 - val_acc: 0.3906\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.9315 - acc: 0.5859 - val_loss: 1.2964 - val_acc: 0.2812\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8301 - acc: 0.6207 - val_loss: 1.2516 - val_acc: 0.3438\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8993 - acc: 0.6045 - val_loss: 1.2600 - val_acc: 0.2969\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8670 - acc: 0.6207 - val_loss: 1.2127 - val_acc: 0.3125\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8868 - acc: 0.6305 - val_loss: 1.2382 - val_acc: 0.3125\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8844 - acc: 0.6094 - val_loss: 1.2649 - val_acc: 0.2969\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8693 - acc: 0.5932 - val_loss: 1.2630 - val_acc: 0.2969\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8332 - acc: 0.6175 - val_loss: 1.2299 - val_acc: 0.3594\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8806 - acc: 0.6159 - val_loss: 1.2502 - val_acc: 0.3125\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8990 - acc: 0.6321 - val_loss: 1.3417 - val_acc: 0.2656\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8556 - acc: 0.6126 - val_loss: 1.2282 - val_acc: 0.3125\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8771 - acc: 0.6078 - val_loss: 1.1921 - val_acc: 0.3281\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.9115 - acc: 0.5786 - val_loss: 1.1859 - val_acc: 0.3906\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.9217 - acc: 0.6110 - val_loss: 1.2213 - val_acc: 0.2812\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8549 - acc: 0.6240 - val_loss: 1.2634 - val_acc: 0.3125\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8541 - acc: 0.6062 - val_loss: 1.2526 - val_acc: 0.3594\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8265 - acc: 0.6451 - val_loss: 1.2099 - val_acc: 0.2812\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8771 - acc: 0.6013 - val_loss: 1.1501 - val_acc: 0.2969\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8878 - acc: 0.5981 - val_loss: 1.2062 - val_acc: 0.3281\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.8281 - acc: 0.6370 - val_loss: 1.2681 - val_acc: 0.3281\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8978 - acc: 0.5964 - val_loss: 1.1871 - val_acc: 0.3906\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8559 - acc: 0.6272 - val_loss: 1.1368 - val_acc: 0.3906\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8398 - acc: 0.6234 - val_loss: 1.2403 - val_acc: 0.3125\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.8701 - acc: 0.6094 - val_loss: 1.1182 - val_acc: 0.3906\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8641 - acc: 0.6207 - val_loss: 1.2040 - val_acc: 0.3438\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8636 - acc: 0.6256 - val_loss: 1.1535 - val_acc: 0.3750\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8741 - acc: 0.6159 - val_loss: 1.2654 - val_acc: 0.3281\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 0.8844 - acc: 0.6207 - val_loss: 1.1662 - val_acc: 0.3750\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8603 - acc: 0.6386 - val_loss: 1.0851 - val_acc: 0.4219\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8468 - acc: 0.6321 - val_loss: 1.2528 - val_acc: 0.2969\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8757 - acc: 0.6256 - val_loss: 1.2277 - val_acc: 0.3594\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8817 - acc: 0.6207 - val_loss: 1.2604 - val_acc: 0.2969\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8947 - acc: 0.6159 - val_loss: 1.2876 - val_acc: 0.3125\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8297 - acc: 0.6250 - val_loss: 1.2395 - val_acc: 0.3281\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8767 - acc: 0.6126 - val_loss: 1.1786 - val_acc: 0.3438\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8626 - acc: 0.6156 - val_loss: 1.2876 - val_acc: 0.2969\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8736 - acc: 0.6207 - val_loss: 1.2665 - val_acc: 0.2188\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8399 - acc: 0.6321 - val_loss: 1.3065 - val_acc: 0.2500\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8818 - acc: 0.6078 - val_loss: 1.2162 - val_acc: 0.3906\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8422 - acc: 0.6564 - val_loss: 1.1725 - val_acc: 0.3594\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8633 - acc: 0.6337 - val_loss: 1.1800 - val_acc: 0.3438\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8425 - acc: 0.6272 - val_loss: 1.1403 - val_acc: 0.3438\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.9096 - acc: 0.6110 - val_loss: 1.1432 - val_acc: 0.3594\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8626 - acc: 0.6191 - val_loss: 1.1825 - val_acc: 0.3594\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8057 - acc: 0.6191 - val_loss: 1.1329 - val_acc: 0.4688\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8499 - acc: 0.6207 - val_loss: 1.2646 - val_acc: 0.3281\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8426 - acc: 0.6094 - val_loss: 1.1142 - val_acc: 0.4688\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8782 - acc: 0.6175 - val_loss: 1.3187 - val_acc: 0.2969\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8818 - acc: 0.6272 - val_loss: 1.1987 - val_acc: 0.4062\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8420 - acc: 0.6305 - val_loss: 1.2604 - val_acc: 0.3438\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8341 - acc: 0.6402 - val_loss: 1.2455 - val_acc: 0.3281\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8653 - acc: 0.5981 - val_loss: 1.0891 - val_acc: 0.4531\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8523 - acc: 0.6321 - val_loss: 1.1671 - val_acc: 0.3906\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8240 - acc: 0.6532 - val_loss: 1.0973 - val_acc: 0.3594\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8553 - acc: 0.6288 - val_loss: 1.3053 - val_acc: 0.2969\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8850 - acc: 0.6045 - val_loss: 1.2164 - val_acc: 0.3281\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8665 - acc: 0.6110 - val_loss: 1.2101 - val_acc: 0.3281\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8721 - acc: 0.6483 - val_loss: 1.3223 - val_acc: 0.2812\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8294 - acc: 0.6613 - val_loss: 1.1842 - val_acc: 0.3750\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8739 - acc: 0.6110 - val_loss: 1.2272 - val_acc: 0.3750\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8855 - acc: 0.5906 - val_loss: 1.3064 - val_acc: 0.2812\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 0.8743 - acc: 0.6240 - val_loss: 1.2273 - val_acc: 0.3594\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.8064 - acc: 0.6532 - val_loss: 1.2726 - val_acc: 0.3438\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8567 - acc: 0.6013 - val_loss: 1.3478 - val_acc: 0.2656\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8464 - acc: 0.6240 - val_loss: 1.1906 - val_acc: 0.3438\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8757 - acc: 0.6094 - val_loss: 1.2648 - val_acc: 0.3125\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8482 - acc: 0.6094 - val_loss: 1.3293 - val_acc: 0.2656\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8617 - acc: 0.6207 - val_loss: 1.2587 - val_acc: 0.3281\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8265 - acc: 0.6094 - val_loss: 1.2629 - val_acc: 0.3438\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8563 - acc: 0.5932 - val_loss: 1.1597 - val_acc: 0.3906\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8454 - acc: 0.6469 - val_loss: 1.1495 - val_acc: 0.3125\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8702 - acc: 0.6224 - val_loss: 1.1891 - val_acc: 0.4062\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.7937 - acc: 0.6564 - val_loss: 1.1117 - val_acc: 0.3750\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8233 - acc: 0.6203 - val_loss: 1.2479 - val_acc: 0.2969\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8609 - acc: 0.6288 - val_loss: 1.1698 - val_acc: 0.3594\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8541 - acc: 0.6438 - val_loss: 1.2394 - val_acc: 0.3438\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8433 - acc: 0.6175 - val_loss: 1.1651 - val_acc: 0.3594\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8820 - acc: 0.6143 - val_loss: 1.1883 - val_acc: 0.3438\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8738 - acc: 0.5997 - val_loss: 1.1557 - val_acc: 0.3594\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8612 - acc: 0.6370 - val_loss: 1.1700 - val_acc: 0.3594\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8387 - acc: 0.6532 - val_loss: 1.3176 - val_acc: 0.2969\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.9182 - acc: 0.5851 - val_loss: 1.1227 - val_acc: 0.3750\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8435 - acc: 0.6272 - val_loss: 1.2389 - val_acc: 0.2969\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8069 - acc: 0.6453 - val_loss: 1.2512 - val_acc: 0.3281\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8515 - acc: 0.6029 - val_loss: 1.2643 - val_acc: 0.2969\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8774 - acc: 0.6337 - val_loss: 1.2694 - val_acc: 0.2656\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8332 - acc: 0.6143 - val_loss: 1.1384 - val_acc: 0.3281\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8118 - acc: 0.6515 - val_loss: 1.2895 - val_acc: 0.2812\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8405 - acc: 0.6305 - val_loss: 1.1619 - val_acc: 0.3750\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8203 - acc: 0.6305 - val_loss: 1.1619 - val_acc: 0.3125\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8273 - acc: 0.6370 - val_loss: 1.2230 - val_acc: 0.3438\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8682 - acc: 0.6159 - val_loss: 1.2991 - val_acc: 0.2812\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8722 - acc: 0.6110 - val_loss: 1.2530 - val_acc: 0.3281\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8595 - acc: 0.6386 - val_loss: 1.1924 - val_acc: 0.3438\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.7967 - acc: 0.6532 - val_loss: 1.2362 - val_acc: 0.3594\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 0.8583 - acc: 0.6191 - val_loss: 1.2596 - val_acc: 0.2969\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8532 - acc: 0.6126 - val_loss: 1.2227 - val_acc: 0.3594\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8638 - acc: 0.6240 - val_loss: 1.2955 - val_acc: 0.2812\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.8196 - acc: 0.6321 - val_loss: 1.2651 - val_acc: 0.3281\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8320 - acc: 0.6370 - val_loss: 1.1534 - val_acc: 0.3750\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8027 - acc: 0.6500 - val_loss: 1.1951 - val_acc: 0.3281\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8355 - acc: 0.6224 - val_loss: 1.2955 - val_acc: 0.2656\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8548 - acc: 0.6224 - val_loss: 1.2114 - val_acc: 0.3438\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8505 - acc: 0.6224 - val_loss: 1.2134 - val_acc: 0.3438\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8563 - acc: 0.6321 - val_loss: 1.2289 - val_acc: 0.3281\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8095 - acc: 0.6321 - val_loss: 1.2544 - val_acc: 0.3594\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8778 - acc: 0.6359 - val_loss: 1.3210 - val_acc: 0.2344\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8478 - acc: 0.6256 - val_loss: 1.2631 - val_acc: 0.2812\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8544 - acc: 0.6143 - val_loss: 1.3603 - val_acc: 0.2969\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8220 - acc: 0.6499 - val_loss: 1.2795 - val_acc: 0.2656\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8864 - acc: 0.6094 - val_loss: 1.2219 - val_acc: 0.3750\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8633 - acc: 0.6224 - val_loss: 1.2137 - val_acc: 0.3594\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.7835 - acc: 0.6580 - val_loss: 1.2860 - val_acc: 0.2969\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.8308 - acc: 0.6418 - val_loss: 1.2472 - val_acc: 0.3281\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 0.7945 - acc: 0.6677 - val_loss: 1.2249 - val_acc: 0.2812\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8677 - acc: 0.6143 - val_loss: 1.3218 - val_acc: 0.2812\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8836 - acc: 0.6240 - val_loss: 1.2181 - val_acc: 0.3906\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8489 - acc: 0.6143 - val_loss: 1.2998 - val_acc: 0.3125\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8555 - acc: 0.6191 - val_loss: 1.1683 - val_acc: 0.3750\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8514 - acc: 0.6240 - val_loss: 1.3249 - val_acc: 0.3125\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8667 - acc: 0.6159 - val_loss: 1.2946 - val_acc: 0.3594\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8859 - acc: 0.6370 - val_loss: 1.3246 - val_acc: 0.2812\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8356 - acc: 0.6143 - val_loss: 1.2072 - val_acc: 0.3125\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8186 - acc: 0.6321 - val_loss: 1.3056 - val_acc: 0.2656\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8162 - acc: 0.6467 - val_loss: 1.1987 - val_acc: 0.3906\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8510 - acc: 0.6288 - val_loss: 1.1841 - val_acc: 0.3750\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.8311 - acc: 0.6483 - val_loss: 1.2195 - val_acc: 0.3125\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.7682 - acc: 0.6613 - val_loss: 1.2223 - val_acc: 0.3125\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 0.8702 - acc: 0.6240 - val_loss: 1.2656 - val_acc: 0.3594\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.7941 - acc: 0.6272 - val_loss: 1.2841 - val_acc: 0.3594\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8226 - acc: 0.6451 - val_loss: 1.2152 - val_acc: 0.3438\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8282 - acc: 0.6402 - val_loss: 1.1301 - val_acc: 0.4062\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8628 - acc: 0.6288 - val_loss: 1.2928 - val_acc: 0.2812\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8126 - acc: 0.6694 - val_loss: 1.2119 - val_acc: 0.2969\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8370 - acc: 0.6256 - val_loss: 1.2662 - val_acc: 0.2500\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8495 - acc: 0.6386 - val_loss: 1.2471 - val_acc: 0.3281\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.8191 - acc: 0.6451 - val_loss: 1.1994 - val_acc: 0.2969\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8178 - acc: 0.6467 - val_loss: 1.2660 - val_acc: 0.3125\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.7786 - acc: 0.6548 - val_loss: 1.2642 - val_acc: 0.3281\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.8208 - acc: 0.6305 - val_loss: 1.2621 - val_acc: 0.3125\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8229 - acc: 0.6207 - val_loss: 1.3380 - val_acc: 0.2812\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8028 - acc: 0.6224 - val_loss: 1.1699 - val_acc: 0.3125\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8407 - acc: 0.6288 - val_loss: 1.1440 - val_acc: 0.4062\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8400 - acc: 0.5997 - val_loss: 1.1404 - val_acc: 0.3281\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8511 - acc: 0.5922 - val_loss: 1.1881 - val_acc: 0.3906\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8736 - acc: 0.6078 - val_loss: 1.3668 - val_acc: 0.2344\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 2s 188ms/step - loss: 0.8318 - acc: 0.6305 - val_loss: 1.2881 - val_acc: 0.2969\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8052 - acc: 0.6434 - val_loss: 1.1550 - val_acc: 0.3438\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8107 - acc: 0.6499 - val_loss: 1.1669 - val_acc: 0.4062\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8196 - acc: 0.6191 - val_loss: 1.2514 - val_acc: 0.3281\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8843 - acc: 0.6337 - val_loss: 1.1605 - val_acc: 0.3906\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8428 - acc: 0.6175 - val_loss: 1.1975 - val_acc: 0.3594\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8253 - acc: 0.6353 - val_loss: 1.1555 - val_acc: 0.3906\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.7915 - acc: 0.6596 - val_loss: 1.2077 - val_acc: 0.3750\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8114 - acc: 0.6451 - val_loss: 1.1191 - val_acc: 0.4062\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8538 - acc: 0.6370 - val_loss: 1.2171 - val_acc: 0.3594\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8447 - acc: 0.6337 - val_loss: 1.2390 - val_acc: 0.2656\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8420 - acc: 0.6143 - val_loss: 1.1893 - val_acc: 0.3750\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8320 - acc: 0.6288 - val_loss: 1.2616 - val_acc: 0.3438\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 2s 180ms/step - loss: 0.8366 - acc: 0.6272 - val_loss: 1.2012 - val_acc: 0.3750\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8674 - acc: 0.6256 - val_loss: 1.2500 - val_acc: 0.2969\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8291 - acc: 0.6483 - val_loss: 1.1368 - val_acc: 0.3281\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8475 - acc: 0.6288 - val_loss: 1.1723 - val_acc: 0.4219\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.8501 - acc: 0.6272 - val_loss: 1.1538 - val_acc: 0.3594\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8547 - acc: 0.6013 - val_loss: 1.2460 - val_acc: 0.3750\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8010 - acc: 0.6469 - val_loss: 1.1431 - val_acc: 0.3594\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.7631 - acc: 0.6596 - val_loss: 1.2917 - val_acc: 0.3125\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 2s 194ms/step - loss: 0.8533 - acc: 0.6434 - val_loss: 1.2173 - val_acc: 0.3438\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.7818 - acc: 0.6564 - val_loss: 1.1321 - val_acc: 0.3750\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.8280 - acc: 0.6272 - val_loss: 1.2599 - val_acc: 0.3281\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 0.8459 - acc: 0.6045 - val_loss: 1.3427 - val_acc: 0.2812\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.7923 - acc: 0.6613 - val_loss: 1.2349 - val_acc: 0.2812\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8331 - acc: 0.6305 - val_loss: 1.2329 - val_acc: 0.3594\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8295 - acc: 0.6548 - val_loss: 1.1893 - val_acc: 0.3438\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8032 - acc: 0.6467 - val_loss: 1.2011 - val_acc: 0.2969\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 2s 178ms/step - loss: 0.8960 - acc: 0.6321 - val_loss: 1.2163 - val_acc: 0.3125\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.8736 - acc: 0.6159 - val_loss: 1.2179 - val_acc: 0.2969\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8541 - acc: 0.6175 - val_loss: 1.3056 - val_acc: 0.3125\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8404 - acc: 0.6321 - val_loss: 1.2435 - val_acc: 0.2812\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8752 - acc: 0.6126 - val_loss: 1.0470 - val_acc: 0.3906\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8281 - acc: 0.6272 - val_loss: 1.2267 - val_acc: 0.2969\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8412 - acc: 0.6386 - val_loss: 1.0803 - val_acc: 0.3906\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.8204 - acc: 0.6143 - val_loss: 1.1741 - val_acc: 0.3438\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8552 - acc: 0.6207 - val_loss: 1.1753 - val_acc: 0.3594\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.7591 - acc: 0.6661 - val_loss: 1.1478 - val_acc: 0.3281\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8234 - acc: 0.6451 - val_loss: 1.1990 - val_acc: 0.3438\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 0.8024 - acc: 0.6467 - val_loss: 1.2133 - val_acc: 0.3125\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8729 - acc: 0.6175 - val_loss: 1.1571 - val_acc: 0.4062\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8037 - acc: 0.6483 - val_loss: 1.2284 - val_acc: 0.3125\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 2s 204ms/step - loss: 0.8551 - acc: 0.6143 - val_loss: 1.1783 - val_acc: 0.3750\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8249 - acc: 0.6596 - val_loss: 1.2061 - val_acc: 0.2969\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8283 - acc: 0.6029 - val_loss: 1.3300 - val_acc: 0.2500\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8544 - acc: 0.5948 - val_loss: 1.1857 - val_acc: 0.3125\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 0.7947 - acc: 0.6337 - val_loss: 1.2294 - val_acc: 0.2500\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8029 - acc: 0.6580 - val_loss: 1.2630 - val_acc: 0.3281\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.7902 - acc: 0.6499 - val_loss: 1.2169 - val_acc: 0.3125\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.8180 - acc: 0.6453 - val_loss: 1.2441 - val_acc: 0.3125\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8121 - acc: 0.6281 - val_loss: 1.2285 - val_acc: 0.3594\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.7786 - acc: 0.6677 - val_loss: 1.2212 - val_acc: 0.3281\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.8069 - acc: 0.6661 - val_loss: 1.2063 - val_acc: 0.3438\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.7684 - acc: 0.6677 - val_loss: 1.1403 - val_acc: 0.3438\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.7950 - acc: 0.6321 - val_loss: 1.2592 - val_acc: 0.2500\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 0.8288 - acc: 0.6483 - val_loss: 1.1676 - val_acc: 0.3750\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.8727 - acc: 0.6305 - val_loss: 1.1914 - val_acc: 0.3281\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.7969 - acc: 0.6469 - val_loss: 1.1976 - val_acc: 0.3281\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.9046 - acc: 0.6110 - val_loss: 1.2920 - val_acc: 0.3281\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8243 - acc: 0.6418 - val_loss: 1.2016 - val_acc: 0.3281\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8172 - acc: 0.6321 - val_loss: 1.1285 - val_acc: 0.3750\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.7967 - acc: 0.6564 - val_loss: 1.1188 - val_acc: 0.3281\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.7863 - acc: 0.6694 - val_loss: 1.1417 - val_acc: 0.4375\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.8201 - acc: 0.6337 - val_loss: 1.2443 - val_acc: 0.3125\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.7869 - acc: 0.6580 - val_loss: 1.2884 - val_acc: 0.2969\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 2s 187ms/step - loss: 0.8039 - acc: 0.6515 - val_loss: 1.1705 - val_acc: 0.3750\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.7648 - acc: 0.6645 - val_loss: 1.2002 - val_acc: 0.3281\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.7791 - acc: 0.6775 - val_loss: 1.2129 - val_acc: 0.2969\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.8210 - acc: 0.6402 - val_loss: 1.1183 - val_acc: 0.3438\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.8585 - acc: 0.6256 - val_loss: 1.1451 - val_acc: 0.3906\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.7849 - acc: 0.6515 - val_loss: 1.2143 - val_acc: 0.3438\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 2s 184ms/step - loss: 0.8240 - acc: 0.6224 - val_loss: 1.1995 - val_acc: 0.3906\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 2s 176ms/step - loss: 0.8371 - acc: 0.6515 - val_loss: 1.1772 - val_acc: 0.3438\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8281 - acc: 0.6370 - val_loss: 1.3033 - val_acc: 0.3125\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 2s 175ms/step - loss: 0.8224 - acc: 0.6467 - val_loss: 1.0476 - val_acc: 0.4219\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 2s 189ms/step - loss: 0.7991 - acc: 0.6580 - val_loss: 1.2358 - val_acc: 0.2812\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.7909 - acc: 0.6532 - val_loss: 1.2553 - val_acc: 0.3125\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8058 - acc: 0.6694 - val_loss: 1.1876 - val_acc: 0.3750\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.8554 - acc: 0.6467 - val_loss: 1.1384 - val_acc: 0.3281\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 2s 183ms/step - loss: 0.8783 - acc: 0.6094 - val_loss: 1.2110 - val_acc: 0.3438\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.8469 - acc: 0.6172 - val_loss: 1.3691 - val_acc: 0.2344\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 2s 174ms/step - loss: 0.7853 - acc: 0.6726 - val_loss: 1.2301 - val_acc: 0.3125\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.8483 - acc: 0.6175 - val_loss: 1.2272 - val_acc: 0.3125\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 2s 179ms/step - loss: 0.7804 - acc: 0.6594 - val_loss: 1.2263 - val_acc: 0.3125\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.8070 - acc: 0.6418 - val_loss: 1.2577 - val_acc: 0.2812\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 2s 182ms/step - loss: 0.7858 - acc: 0.6547 - val_loss: 1.1674 - val_acc: 0.3594\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.8202 - acc: 0.6297 - val_loss: 1.2310 - val_acc: 0.2656\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 2s 185ms/step - loss: 0.8190 - acc: 0.6353 - val_loss: 1.2049 - val_acc: 0.2969\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 2s 177ms/step - loss: 0.8354 - acc: 0.6515 - val_loss: 1.2823 - val_acc: 0.2500\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.7893 - acc: 0.6580 - val_loss: 1.2000 - val_acc: 0.3281\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.7748 - acc: 0.6596 - val_loss: 1.1096 - val_acc: 0.4062\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.7946 - acc: 0.6564 - val_loss: 1.3779 - val_acc: 0.2812\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 2s 181ms/step - loss: 0.8398 - acc: 0.6305 - val_loss: 1.1220 - val_acc: 0.3750\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.8422 - acc: 0.6175 - val_loss: 1.1790 - val_acc: 0.3750\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.7795 - acc: 0.6418 - val_loss: 1.2085 - val_acc: 0.2969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "57818ab2-0e0b-42c3-b872-0eac3fdf124b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [3.6264727115631104,\n",
              "  3.2705302238464355,\n",
              "  3.2383344173431396,\n",
              "  3.0017192363739014,\n",
              "  2.75832462310791,\n",
              "  2.558711528778076,\n",
              "  2.419227361679077,\n",
              "  2.2860567569732666,\n",
              "  2.280426263809204,\n",
              "  2.1989078521728516,\n",
              "  2.094616174697876,\n",
              "  1.9999561309814453,\n",
              "  1.8092952966690063,\n",
              "  1.661299228668213,\n",
              "  1.7614761590957642,\n",
              "  1.6498124599456787,\n",
              "  1.685860276222229,\n",
              "  1.7226215600967407,\n",
              "  1.5911409854888916,\n",
              "  1.6461796760559082,\n",
              "  1.537184715270996,\n",
              "  1.5834228992462158,\n",
              "  1.6482107639312744,\n",
              "  1.5599225759506226,\n",
              "  1.496017336845398,\n",
              "  1.644700050354004,\n",
              "  1.6054128408432007,\n",
              "  1.535597324371338,\n",
              "  1.4978013038635254,\n",
              "  1.5712027549743652,\n",
              "  1.549542784690857,\n",
              "  1.5544464588165283,\n",
              "  1.5970748662948608,\n",
              "  1.5374839305877686,\n",
              "  1.4876843690872192,\n",
              "  1.4786137342453003,\n",
              "  1.5387498140335083,\n",
              "  1.6065138578414917,\n",
              "  1.482018232345581,\n",
              "  1.4576730728149414,\n",
              "  1.4696571826934814,\n",
              "  1.596333384513855,\n",
              "  1.496059536933899,\n",
              "  1.5648517608642578,\n",
              "  1.4854570627212524,\n",
              "  1.5191763639450073,\n",
              "  1.4417626857757568,\n",
              "  1.5303399562835693,\n",
              "  1.4410008192062378,\n",
              "  1.5439932346343994,\n",
              "  1.4500830173492432,\n",
              "  1.50701105594635,\n",
              "  1.4708225727081299,\n",
              "  1.4586777687072754,\n",
              "  1.3997883796691895,\n",
              "  1.403205394744873,\n",
              "  1.5060882568359375,\n",
              "  1.5023913383483887,\n",
              "  1.4211091995239258,\n",
              "  1.371847152709961,\n",
              "  1.473467469215393,\n",
              "  1.4069056510925293,\n",
              "  1.4579485654830933,\n",
              "  1.3435614109039307,\n",
              "  1.4210349321365356,\n",
              "  1.4399008750915527,\n",
              "  1.4342008829116821,\n",
              "  1.4488331079483032,\n",
              "  1.4446688890457153,\n",
              "  1.4252666234970093,\n",
              "  1.3976095914840698,\n",
              "  1.4001119136810303,\n",
              "  1.3782001733779907,\n",
              "  1.4255045652389526,\n",
              "  1.3179365396499634,\n",
              "  1.4077303409576416,\n",
              "  1.4693827629089355,\n",
              "  1.376965045928955,\n",
              "  1.349629521369934,\n",
              "  1.3303406238555908,\n",
              "  1.4475071430206299,\n",
              "  1.323707938194275,\n",
              "  1.39009690284729,\n",
              "  1.3960456848144531,\n",
              "  1.3564687967300415,\n",
              "  1.3243533372879028,\n",
              "  1.3831766843795776,\n",
              "  1.321393609046936,\n",
              "  1.3998496532440186,\n",
              "  1.3953869342803955,\n",
              "  1.28878915309906,\n",
              "  1.3082324266433716,\n",
              "  1.3838576078414917,\n",
              "  1.3029242753982544,\n",
              "  1.3052504062652588,\n",
              "  1.3477007150650024,\n",
              "  1.3612802028656006,\n",
              "  1.3265312910079956,\n",
              "  1.2864816188812256,\n",
              "  1.29181706905365,\n",
              "  1.3290934562683105,\n",
              "  1.3630038499832153,\n",
              "  1.2843035459518433,\n",
              "  1.3524376153945923,\n",
              "  1.3062316179275513,\n",
              "  1.290422797203064,\n",
              "  1.3131177425384521,\n",
              "  1.313822627067566,\n",
              "  1.3410221338272095,\n",
              "  1.2737703323364258,\n",
              "  1.3236594200134277,\n",
              "  1.3031713962554932,\n",
              "  1.2540578842163086,\n",
              "  1.337525486946106,\n",
              "  1.3006209135055542,\n",
              "  1.2572436332702637,\n",
              "  1.2276157140731812,\n",
              "  1.2739331722259521,\n",
              "  1.266252875328064,\n",
              "  1.2109051942825317,\n",
              "  1.2333072423934937,\n",
              "  1.2777631282806396,\n",
              "  1.318737506866455,\n",
              "  1.2910200357437134,\n",
              "  1.270599365234375,\n",
              "  1.2084294557571411,\n",
              "  1.2494513988494873,\n",
              "  1.2210232019424438,\n",
              "  1.2449071407318115,\n",
              "  1.230299949645996,\n",
              "  1.2800278663635254,\n",
              "  1.2492811679840088,\n",
              "  1.2236847877502441,\n",
              "  1.2821170091629028,\n",
              "  1.2234418392181396,\n",
              "  1.2331112623214722,\n",
              "  1.2874139547348022,\n",
              "  1.2717435359954834,\n",
              "  1.180993914604187,\n",
              "  1.2453715801239014,\n",
              "  1.294058918952942,\n",
              "  1.2560901641845703,\n",
              "  1.2813161611557007,\n",
              "  1.1924747228622437,\n",
              "  1.1775996685028076,\n",
              "  1.2782659530639648,\n",
              "  1.2922075986862183,\n",
              "  1.178478717803955,\n",
              "  1.1955602169036865,\n",
              "  1.2118009328842163,\n",
              "  1.2071123123168945,\n",
              "  1.2014100551605225,\n",
              "  1.203596830368042,\n",
              "  1.179943323135376,\n",
              "  1.2302371263504028,\n",
              "  1.2692885398864746,\n",
              "  1.1984564065933228,\n",
              "  1.200762391090393,\n",
              "  1.222595453262329,\n",
              "  1.2008308172225952,\n",
              "  1.2074134349822998,\n",
              "  1.199163556098938,\n",
              "  1.2692933082580566,\n",
              "  1.1925485134124756,\n",
              "  1.2398107051849365,\n",
              "  1.206410527229309,\n",
              "  1.2054868936538696,\n",
              "  1.215774655342102,\n",
              "  1.1798287630081177,\n",
              "  1.1582019329071045,\n",
              "  1.1994988918304443,\n",
              "  1.2447773218154907,\n",
              "  1.2106987237930298,\n",
              "  1.2157368659973145,\n",
              "  1.1267513036727905,\n",
              "  1.1883584260940552,\n",
              "  1.2260247468948364,\n",
              "  1.2721679210662842,\n",
              "  1.1565227508544922,\n",
              "  1.2054450511932373,\n",
              "  1.1500662565231323,\n",
              "  1.177415370941162,\n",
              "  1.2471200227737427,\n",
              "  1.1792948246002197,\n",
              "  1.1942734718322754,\n",
              "  1.1141983270645142,\n",
              "  1.1231688261032104,\n",
              "  1.1376601457595825,\n",
              "  1.2527320384979248,\n",
              "  1.259189486503601,\n",
              "  1.277448296546936,\n",
              "  1.1823664903640747,\n",
              "  1.1745227575302124,\n",
              "  1.1532326936721802,\n",
              "  1.1954482793807983,\n",
              "  1.117263913154602,\n",
              "  1.1995717287063599,\n",
              "  1.1352094411849976,\n",
              "  1.1788833141326904,\n",
              "  1.1988813877105713,\n",
              "  1.1473486423492432,\n",
              "  1.1957752704620361,\n",
              "  1.1450399160385132,\n",
              "  1.1357855796813965,\n",
              "  1.1442064046859741,\n",
              "  1.1247235536575317,\n",
              "  1.1963859796524048,\n",
              "  1.2091292142868042,\n",
              "  1.1553963422775269,\n",
              "  1.0997449159622192,\n",
              "  1.16277277469635,\n",
              "  1.1256179809570312,\n",
              "  1.131869912147522,\n",
              "  1.145297646522522,\n",
              "  1.0985718965530396,\n",
              "  1.121252179145813,\n",
              "  1.174086570739746,\n",
              "  1.1671290397644043,\n",
              "  1.1372407674789429,\n",
              "  1.1948928833007812,\n",
              "  1.127563238143921,\n",
              "  1.0386388301849365,\n",
              "  1.0931893587112427,\n",
              "  1.1014779806137085,\n",
              "  1.1182317733764648,\n",
              "  1.1419695615768433,\n",
              "  1.115290641784668,\n",
              "  1.144102692604065,\n",
              "  1.1192845106124878,\n",
              "  1.136504888534546,\n",
              "  1.1924819946289062,\n",
              "  1.1617624759674072,\n",
              "  1.2016502618789673,\n",
              "  1.1426371335983276,\n",
              "  1.0778220891952515,\n",
              "  1.056693196296692,\n",
              "  1.1045573949813843,\n",
              "  1.0936740636825562,\n",
              "  1.0694085359573364,\n",
              "  1.0559618473052979,\n",
              "  1.112931251525879,\n",
              "  1.1123448610305786,\n",
              "  1.1242835521697998,\n",
              "  1.1444867849349976,\n",
              "  1.1152737140655518,\n",
              "  1.1014692783355713,\n",
              "  1.0458674430847168,\n",
              "  1.1685203313827515,\n",
              "  1.1227654218673706,\n",
              "  1.0695880651474,\n",
              "  1.1385172605514526,\n",
              "  1.0699223279953003,\n",
              "  1.122857689857483,\n",
              "  1.078617811203003,\n",
              "  1.0684655904769897,\n",
              "  1.0696496963500977,\n",
              "  1.1230766773223877,\n",
              "  1.1108325719833374,\n",
              "  1.1346607208251953,\n",
              "  1.1365299224853516,\n",
              "  1.1523864269256592,\n",
              "  1.1228638887405396,\n",
              "  1.1825437545776367,\n",
              "  1.0531729459762573,\n",
              "  1.0860646963119507,\n",
              "  1.085717797279358,\n",
              "  1.1279889345169067,\n",
              "  1.1233162879943848,\n",
              "  1.0496852397918701,\n",
              "  1.1614556312561035,\n",
              "  1.0456043481826782,\n",
              "  1.0352799892425537,\n",
              "  1.1048178672790527,\n",
              "  1.105949878692627,\n",
              "  1.100492238998413,\n",
              "  0.9935306906700134,\n",
              "  1.0435196161270142,\n",
              "  1.0668383836746216,\n",
              "  1.155091404914856,\n",
              "  1.1011676788330078,\n",
              "  1.1342136859893799,\n",
              "  1.0962969064712524,\n",
              "  1.0639770030975342,\n",
              "  1.0562227964401245,\n",
              "  1.102472186088562,\n",
              "  1.0496954917907715,\n",
              "  1.1341643333435059,\n",
              "  1.1098278760910034,\n",
              "  1.0588699579238892,\n",
              "  1.0820945501327515,\n",
              "  1.06955885887146,\n",
              "  1.066285252571106,\n",
              "  1.1058093309402466,\n",
              "  1.1299782991409302,\n",
              "  1.0517008304595947,\n",
              "  1.0884815454483032,\n",
              "  1.0372008085250854,\n",
              "  1.0505281686782837,\n",
              "  1.0906494855880737,\n",
              "  1.0826271772384644,\n",
              "  1.084552526473999,\n",
              "  1.0797573328018188,\n",
              "  1.0882540941238403,\n",
              "  1.105285406112671,\n",
              "  1.116632342338562,\n",
              "  1.068368911743164,\n",
              "  1.119402527809143,\n",
              "  1.1133885383605957,\n",
              "  1.0049020051956177,\n",
              "  1.025132179260254,\n",
              "  1.0341582298278809,\n",
              "  1.0689902305603027,\n",
              "  1.0405597686767578,\n",
              "  1.0699951648712158,\n",
              "  1.0233820676803589,\n",
              "  1.1089415550231934,\n",
              "  1.0314513444900513,\n",
              "  1.0958142280578613,\n",
              "  1.084820032119751,\n",
              "  1.0047516822814941,\n",
              "  1.11288583278656,\n",
              "  1.008702039718628,\n",
              "  1.0094634294509888,\n",
              "  0.9789613485336304,\n",
              "  1.0699682235717773,\n",
              "  1.066069483757019,\n",
              "  1.0823885202407837,\n",
              "  1.05968177318573,\n",
              "  1.1408179998397827,\n",
              "  1.0199023485183716,\n",
              "  1.0049418210983276,\n",
              "  1.0199849605560303,\n",
              "  1.07698655128479,\n",
              "  1.0499730110168457,\n",
              "  1.0417163372039795,\n",
              "  1.0702491998672485,\n",
              "  1.0298622846603394,\n",
              "  1.0786433219909668,\n",
              "  0.9949668049812317,\n",
              "  1.066009521484375,\n",
              "  0.9794789552688599,\n",
              "  1.0587129592895508,\n",
              "  1.0976309776306152,\n",
              "  1.0147677659988403,\n",
              "  0.9896459579467773,\n",
              "  1.0312625169754028,\n",
              "  1.0384312868118286,\n",
              "  1.0016981363296509,\n",
              "  1.0999746322631836,\n",
              "  1.0465457439422607,\n",
              "  1.064773678779602,\n",
              "  0.9738672375679016,\n",
              "  1.0786150693893433,\n",
              "  1.0732879638671875,\n",
              "  1.0692524909973145,\n",
              "  1.0000228881835938,\n",
              "  0.9931243062019348,\n",
              "  1.0382356643676758,\n",
              "  1.0024327039718628,\n",
              "  1.026811122894287,\n",
              "  1.040315866470337,\n",
              "  1.0150084495544434,\n",
              "  1.0726885795593262,\n",
              "  1.0551787614822388,\n",
              "  1.0187078714370728,\n",
              "  0.9862260222434998,\n",
              "  1.0319440364837646,\n",
              "  0.9474786520004272,\n",
              "  1.0589474439620972,\n",
              "  1.0520933866500854,\n",
              "  0.9974351525306702,\n",
              "  1.028311014175415,\n",
              "  1.0136756896972656,\n",
              "  1.0284923315048218,\n",
              "  0.9541813731193542,\n",
              "  1.059256672859192,\n",
              "  1.0124675035476685,\n",
              "  1.0059099197387695,\n",
              "  1.0079982280731201,\n",
              "  0.9895414113998413,\n",
              "  0.9937859773635864,\n",
              "  0.9999487400054932,\n",
              "  1.0555678606033325,\n",
              "  1.022929310798645,\n",
              "  0.997459352016449,\n",
              "  0.9544592499732971,\n",
              "  0.997252345085144,\n",
              "  1.0597056150436401,\n",
              "  1.0048261880874634,\n",
              "  1.0232203006744385,\n",
              "  0.9880435466766357,\n",
              "  0.9817708730697632,\n",
              "  1.0079470872879028,\n",
              "  1.0030409097671509,\n",
              "  1.0168745517730713,\n",
              "  0.9648764133453369,\n",
              "  0.9707404375076294,\n",
              "  0.9991284012794495,\n",
              "  1.009799838066101,\n",
              "  1.044420599937439,\n",
              "  1.0375112295150757,\n",
              "  0.9828429818153381,\n",
              "  1.0096185207366943,\n",
              "  0.9913823008537292,\n",
              "  0.9773469567298889,\n",
              "  0.9646607637405396,\n",
              "  1.0554841756820679,\n",
              "  1.0020250082015991,\n",
              "  0.9444965720176697,\n",
              "  1.0185376405715942,\n",
              "  1.0064207315444946,\n",
              "  1.0168914794921875,\n",
              "  1.0042611360549927,\n",
              "  1.0465219020843506,\n",
              "  0.9625598788261414,\n",
              "  1.0326924324035645,\n",
              "  1.019302248954773,\n",
              "  1.028860330581665,\n",
              "  0.9913036227226257,\n",
              "  1.0355826616287231,\n",
              "  0.9675583839416504,\n",
              "  0.9908748865127563,\n",
              "  0.9332748055458069,\n",
              "  0.9878349304199219,\n",
              "  1.0369001626968384,\n",
              "  0.9609788656234741,\n",
              "  0.976087749004364,\n",
              "  0.9904440641403198,\n",
              "  1.0538041591644287,\n",
              "  1.0230296850204468,\n",
              "  0.9564533233642578,\n",
              "  0.970719575881958,\n",
              "  0.9976658225059509,\n",
              "  0.9999493360519409,\n",
              "  0.9626289010047913,\n",
              "  1.022690773010254,\n",
              "  0.9682273864746094,\n",
              "  1.0187954902648926,\n",
              "  1.008535385131836,\n",
              "  1.0101090669631958,\n",
              "  0.9556253552436829,\n",
              "  0.9129377603530884,\n",
              "  0.950910210609436,\n",
              "  0.9768798351287842,\n",
              "  0.9828119277954102,\n",
              "  0.9527535438537598,\n",
              "  0.9578586220741272,\n",
              "  0.9961158037185669,\n",
              "  0.9545233845710754,\n",
              "  0.9788292646408081,\n",
              "  0.9764440655708313,\n",
              "  1.0332194566726685,\n",
              "  1.0164495706558228,\n",
              "  0.9473947882652283,\n",
              "  0.9873979687690735,\n",
              "  0.9725526571273804,\n",
              "  0.9612563252449036,\n",
              "  0.9557257890701294,\n",
              "  0.9216603636741638,\n",
              "  0.9859431385993958,\n",
              "  1.0894020795822144,\n",
              "  0.9610471129417419,\n",
              "  0.9919154644012451,\n",
              "  0.9976602792739868,\n",
              "  0.988594651222229,\n",
              "  0.9581880569458008,\n",
              "  0.9171209931373596,\n",
              "  1.0087484121322632,\n",
              "  0.9832606315612793,\n",
              "  0.9686402082443237,\n",
              "  0.9961299300193787,\n",
              "  1.01743483543396,\n",
              "  0.9890829920768738,\n",
              "  0.9878734946250916,\n",
              "  0.9769821166992188,\n",
              "  0.9664124846458435,\n",
              "  0.9383729100227356,\n",
              "  0.941112756729126,\n",
              "  0.9993232488632202,\n",
              "  0.9402062296867371,\n",
              "  0.9938138723373413,\n",
              "  1.0218452215194702,\n",
              "  0.933060348033905,\n",
              "  0.9634144902229309,\n",
              "  0.9117462038993835,\n",
              "  0.9297019839286804,\n",
              "  0.9758576154708862,\n",
              "  0.9849573373794556,\n",
              "  0.93848717212677,\n",
              "  0.9265193343162537,\n",
              "  0.9296249747276306,\n",
              "  1.020880937576294,\n",
              "  0.9497472047805786,\n",
              "  0.9746217727661133,\n",
              "  0.9332060813903809,\n",
              "  0.9328696131706238,\n",
              "  0.910529375076294,\n",
              "  0.9926307797431946,\n",
              "  0.9219292998313904,\n",
              "  0.9847224354743958,\n",
              "  0.9205559492111206,\n",
              "  0.9584785103797913,\n",
              "  0.9447770118713379,\n",
              "  0.9560295343399048,\n",
              "  0.9339319467544556,\n",
              "  0.9634366035461426,\n",
              "  0.9782260656356812,\n",
              "  0.9684382677078247,\n",
              "  0.9610123038291931,\n",
              "  0.8986640572547913,\n",
              "  0.9621699452400208,\n",
              "  0.9670903086662292,\n",
              "  0.9280062317848206,\n",
              "  0.9172824025154114,\n",
              "  0.960686445236206,\n",
              "  1.0101560354232788,\n",
              "  0.9984804391860962,\n",
              "  0.9642713665962219,\n",
              "  0.967400312423706,\n",
              "  0.9302358031272888,\n",
              "  0.9782506227493286,\n",
              "  0.8351114988327026,\n",
              "  0.9211245775222778,\n",
              "  0.9648200869560242,\n",
              "  0.9082526564598083,\n",
              "  1.0207512378692627,\n",
              "  0.977687656879425,\n",
              "  0.975365936756134,\n",
              "  0.9369715452194214,\n",
              "  0.9507570862770081,\n",
              "  0.9396354556083679,\n",
              "  0.936486542224884,\n",
              "  0.9939359426498413,\n",
              "  0.9287084937095642,\n",
              "  0.9642044901847839,\n",
              "  0.9343027472496033,\n",
              "  0.9524005055427551,\n",
              "  0.9486449956893921,\n",
              "  0.9134271144866943,\n",
              "  0.902416467666626,\n",
              "  0.9281955361366272,\n",
              "  0.9229516983032227,\n",
              "  0.9120370745658875,\n",
              "  0.9390493035316467,\n",
              "  0.9501908421516418,\n",
              "  0.8893056511878967,\n",
              "  0.9267786145210266,\n",
              "  0.9060073494911194,\n",
              "  0.9704516530036926,\n",
              "  0.9648106098175049,\n",
              "  0.9514533281326294,\n",
              "  0.9197768568992615,\n",
              "  0.9830271005630493,\n",
              "  0.9369709491729736,\n",
              "  0.9910974502563477,\n",
              "  0.9637505412101746,\n",
              "  0.8976958990097046,\n",
              "  0.9262455701828003,\n",
              "  0.8966517448425293,\n",
              "  0.9404815435409546,\n",
              "  0.9016091227531433,\n",
              "  0.9269220232963562,\n",
              "  0.9096156358718872,\n",
              "  0.9674175381660461,\n",
              "  0.9614758491516113,\n",
              "  0.9755706787109375,\n",
              "  0.9034311175346375,\n",
              "  0.9233146905899048,\n",
              "  0.9063190221786499,\n",
              "  0.9234287738800049,\n",
              "  0.9208638072013855,\n",
              "  0.9174115061759949,\n",
              "  0.9835002422332764,\n",
              "  0.9546846151351929,\n",
              "  0.943687379360199,\n",
              "  0.9341400861740112,\n",
              "  0.93669593334198,\n",
              "  0.9132069945335388,\n",
              "  0.9215704202651978,\n",
              "  0.9624475240707397,\n",
              "  0.8982753753662109,\n",
              "  0.9163286089897156,\n",
              "  0.9492987990379333,\n",
              "  0.9035884141921997,\n",
              "  0.9133982062339783,\n",
              "  0.9147700667381287,\n",
              "  0.9229740500450134,\n",
              "  0.9633194804191589,\n",
              "  0.9492002725601196,\n",
              "  0.8787610530853271,\n",
              "  0.8632031083106995,\n",
              "  0.9089422821998596,\n",
              "  0.9276619553565979,\n",
              "  0.9211496114730835,\n",
              "  0.8695175051689148,\n",
              "  0.8560842275619507,\n",
              "  0.88021320104599,\n",
              "  0.9062327146530151,\n",
              "  0.8558748364448547,\n",
              "  0.8729672431945801,\n",
              "  0.9446234107017517,\n",
              "  0.9154688715934753,\n",
              "  0.9077019691467285,\n",
              "  0.8800123929977417,\n",
              "  0.9337844848632812,\n",
              "  0.8930082321166992,\n",
              "  0.9168863296508789,\n",
              "  0.9382191896438599,\n",
              "  0.9053548574447632,\n",
              "  0.9175053238868713,\n",
              "  0.9220006465911865,\n",
              "  0.9040020704269409,\n",
              "  0.9147409796714783,\n",
              "  0.981866717338562,\n",
              "  0.9212172627449036,\n",
              "  0.931142270565033,\n",
              "  0.9357631206512451,\n",
              "  0.9004011154174805,\n",
              "  0.9459536075592041,\n",
              "  0.9125149846076965,\n",
              "  0.9244420528411865,\n",
              "  0.9010551571846008,\n",
              "  0.8846358060836792,\n",
              "  0.956916868686676,\n",
              "  0.9575091600418091,\n",
              "  0.9574406743049622,\n",
              "  0.9470875859260559,\n",
              "  0.8969950079917908,\n",
              "  0.9438905119895935,\n",
              "  0.9393059015274048,\n",
              "  0.9015206098556519,\n",
              "  0.9111043214797974,\n",
              "  0.9235972166061401,\n",
              "  0.9044862389564514,\n",
              "  0.9071285128593445,\n",
              "  0.8855401873588562,\n",
              "  0.9128146767616272,\n",
              "  0.9274795651435852,\n",
              "  0.9503051042556763,\n",
              "  0.8541321158409119,\n",
              "  0.9356594681739807,\n",
              "  0.8911275267601013,\n",
              "  0.9304938316345215,\n",
              "  0.9122429490089417,\n",
              "  0.8704796433448792,\n",
              "  0.9539313912391663,\n",
              "  0.8761193156242371,\n",
              "  0.8730000853538513,\n",
              "  0.883400022983551,\n",
              "  0.9364856481552124,\n",
              "  0.8943579196929932,\n",
              "  0.8888900876045227,\n",
              "  0.901652991771698,\n",
              "  0.912815272808075,\n",
              "  0.909781813621521,\n",
              "  0.9365271925926208,\n",
              "  0.9087409973144531,\n",
              "  0.9057598114013672,\n",
              "  0.9005335569381714,\n",
              "  0.901601254940033,\n",
              "  0.9316950440406799,\n",
              "  0.9039897918701172,\n",
              "  0.848663866519928,\n",
              "  0.8757773041725159,\n",
              "  0.912003755569458,\n",
              "  0.917698323726654,\n",
              "  0.8449255228042603,\n",
              "  0.9051403999328613,\n",
              "  0.8780235052108765,\n",
              "  0.8923338651657104,\n",
              "  0.902341365814209,\n",
              "  0.8929147720336914,\n",
              "  0.8805792331695557,\n",
              "  0.8589754700660706,\n",
              "  0.8693785071372986,\n",
              "  0.9054130911827087,\n",
              "  0.9143146276473999,\n",
              "  0.8875544667243958,\n",
              "  0.874159038066864,\n",
              "  0.8699806928634644,\n",
              "  0.8780791759490967,\n",
              "  0.9473401308059692,\n",
              "  0.8766602873802185,\n",
              "  0.9317094683647156,\n",
              "  0.8998635411262512,\n",
              "  0.8445115089416504,\n",
              "  0.8589444756507874,\n",
              "  0.9254816174507141,\n",
              "  0.9252820014953613,\n",
              "  0.8691747784614563,\n",
              "  0.9251656532287598,\n",
              "  0.8287774324417114,\n",
              "  0.847390353679657,\n",
              "  0.8710870146751404,\n",
              "  0.9230339527130127,\n",
              "  0.8974428176879883,\n",
              "  0.8880712389945984,\n",
              "  0.9289361834526062,\n",
              "  0.9085360169410706,\n",
              "  0.8525152206420898,\n",
              "  0.8288710713386536,\n",
              "  0.8684461116790771,\n",
              "  0.8491064310073853,\n",
              "  0.9344744682312012,\n",
              "  0.8592057228088379,\n",
              "  0.9024332761764526,\n",
              "  0.9347025752067566,\n",
              "  0.880988359451294,\n",
              "  0.8568589091300964,\n",
              "  0.8789410591125488,\n",
              "  0.8734742999076843,\n",
              "  0.8825960755348206,\n",
              "  0.8995175361633301,\n",
              "  0.881748378276825,\n",
              "  0.8872575759887695,\n",
              "  0.9290973544120789,\n",
              "  0.8759444952011108,\n",
              "  0.8713152408599854,\n",
              "  0.9009979367256165,\n",
              "  0.8939154744148254,\n",
              "  0.8704966306686401,\n",
              "  0.871238648891449,\n",
              "  0.9307156205177307,\n",
              "  0.890733540058136,\n",
              "  0.8326324224472046,\n",
              "  0.8499996066093445,\n",
              "  0.8390635251998901,\n",
              "  0.84856116771698,\n",
              "  0.9055842757225037,\n",
              "  0.8600408434867859,\n",
              "  0.8416362404823303,\n",
              "  0.8763453364372253,\n",
              "  0.8489778637886047,\n",
              "  0.8389350771903992,\n",
              "  0.8311339020729065,\n",
              "  0.8618380427360535,\n",
              "  0.8531383275985718,\n",
              "  0.8421000838279724,\n",
              "  0.8637855648994446,\n",
              "  0.9133998155593872,\n",
              "  0.8176000714302063,\n",
              "  0.8379497528076172,\n",
              "  0.8677785396575928,\n",
              "  0.8634684085845947,\n",
              "  0.9067674875259399,\n",
              "  0.9119192361831665,\n",
              "  0.8315523862838745,\n",
              "  0.8426286578178406,\n",
              "  0.8415685892105103,\n",
              "  0.8692196607589722,\n",
              "  0.8786468505859375,\n",
              "  0.8475079536437988,\n",
              "  0.8625351786613464,\n",
              "  0.8993203639984131,\n",
              "  0.8989392518997192,\n",
              "  0.8303831815719604,\n",
              "  0.8433863520622253,\n",
              "  0.864064633846283,\n",
              "  0.9314578175544739,\n",
              "  0.8301392793655396,\n",
              "  0.8992738127708435,\n",
              "  0.8670402765274048,\n",
              "  0.8867509961128235,\n",
              "  0.8844082355499268,\n",
              "  0.869347870349884,\n",
              "  0.8332456350326538,\n",
              "  0.8805684447288513,\n",
              "  0.8989733457565308,\n",
              "  0.8556022047996521,\n",
              "  0.8770971298217773,\n",
              "  0.9114568829536438,\n",
              "  0.9216749668121338,\n",
              "  0.8548625707626343,\n",
              "  0.8541118502616882,\n",
              "  0.826493501663208,\n",
              "  0.8770756125450134,\n",
              "  0.887805700302124,\n",
              "  0.8280667066574097,\n",
              "  0.8977981805801392,\n",
              "  0.8559198379516602,\n",
              "  0.8397771716117859,\n",
              "  0.8701472878456116,\n",
              "  0.8641405701637268,\n",
              "  0.8635914921760559,\n",
              "  0.874101459980011,\n",
              "  0.8843778371810913,\n",
              "  0.8603143692016602,\n",
              "  0.8467860221862793,\n",
              "  0.8757268786430359,\n",
              "  0.8817363381385803,\n",
              "  0.8947359323501587,\n",
              "  0.829727828502655,\n",
              "  0.8767059445381165,\n",
              "  0.8625901341438293,\n",
              "  0.8736323118209839,\n",
              "  0.839922308921814,\n",
              "  0.8818375468254089,\n",
              "  0.8421908617019653,\n",
              "  0.8632623553276062,\n",
              "  0.842536985874176,\n",
              "  0.9095838665962219,\n",
              "  0.8625975251197815,\n",
              "  0.8057361245155334,\n",
              "  0.8498553037643433,\n",
              "  0.8425856828689575,\n",
              "  0.8782355785369873,\n",
              "  0.8818446397781372,\n",
              "  0.8420441746711731,\n",
              "  0.8341304659843445,\n",
              "  0.865287184715271,\n",
              "  0.8523426055908203,\n",
              "  0.8240206241607666,\n",
              "  0.8552936315536499,\n",
              "  0.8850343227386475,\n",
              "  0.8664612174034119,\n",
              "  0.8720733523368835,\n",
              "  0.8293548226356506,\n",
              "  0.8738535642623901,\n",
              "  0.8854911923408508,\n",
              "  0.8742960691452026,\n",
              "  0.8063932657241821,\n",
              "  0.8566718697547913,\n",
              "  0.846368670463562,\n",
              "  0.8757456541061401,\n",
              "  0.8482220768928528,\n",
              "  0.8616758584976196,\n",
              "  0.8265376687049866,\n",
              "  0.8563470840454102,\n",
              "  0.8454028964042664,\n",
              "  0.870188295841217,\n",
              "  0.7936616539955139,\n",
              "  0.8233150243759155,\n",
              "  0.8609403371810913,\n",
              "  0.8541274070739746,\n",
              "  0.843317985534668,\n",
              "  0.8819710612297058,\n",
              "  0.8738163709640503,\n",
              "  0.8612234592437744,\n",
              "  0.8387335538864136,\n",
              "  0.9182162284851074,\n",
              "  0.8435177206993103,\n",
              "  0.8068849444389343,\n",
              "  0.8514617681503296,\n",
              "  0.877357006072998,\n",
              "  0.8331876397132874,\n",
              "  0.811802864074707,\n",
              "  0.8405342102050781,\n",
              "  0.8202763199806213,\n",
              "  0.8272703886032104,\n",
              "  0.868150532245636,\n",
              "  0.8721727728843689,\n",
              "  0.8595177531242371,\n",
              "  0.7966911792755127,\n",
              "  0.8582503795623779,\n",
              "  0.8531739711761475,\n",
              "  0.8638023734092712,\n",
              "  0.8196205496788025,\n",
              "  0.8320096135139465,\n",
              "  0.8027364611625671,\n",
              "  0.8354597091674805,\n",
              "  0.8548473119735718,\n",
              "  0.8504574298858643,\n",
              "  0.8563032746315002,\n",
              "  0.8094667196273804,\n",
              "  0.8777812719345093,\n",
              "  0.8478466868400574,\n",
              "  0.854361891746521,\n",
              "  0.8219530582427979,\n",
              "  0.8863672018051147,\n",
              "  0.8633070588111877,\n",
              "  0.7835001945495605,\n",
              "  0.8308162093162537,\n",
              "  0.7945317029953003,\n",
              "  0.8676716089248657,\n",
              "  0.8835808038711548,\n",
              "  0.8489363789558411,\n",
              "  0.8554666638374329,\n",
              "  0.851352870464325,\n",
              "  0.8666776418685913,\n",
              "  0.8859167695045471,\n",
              "  0.8356232047080994,\n",
              "  0.8185828328132629,\n",
              "  0.816233217716217,\n",
              "  0.8509634137153625,\n",
              "  0.831119179725647,\n",
              "  0.7681624889373779,\n",
              "  0.8702200651168823,\n",
              "  0.7941194176673889,\n",
              "  0.8225806951522827,\n",
              "  0.8282085657119751,\n",
              "  0.8627588152885437,\n",
              "  0.8126353025436401,\n",
              "  0.8369736075401306,\n",
              "  0.8495485186576843,\n",
              "  0.8190849423408508,\n",
              "  0.8177863955497742,\n",
              "  0.7786419987678528,\n",
              "  0.8208073973655701,\n",
              "  0.8229002952575684,\n",
              "  0.8028309941291809,\n",
              "  0.8406562209129333,\n",
              "  0.840041995048523,\n",
              "  0.851092517375946,\n",
              "  0.8736392259597778,\n",
              "  0.8317587375640869,\n",
              "  0.805168867111206,\n",
              "  0.8106661438941956,\n",
              "  0.8195798397064209,\n",
              "  0.8843061327934265,\n",
              "  0.8428478240966797,\n",
              "  0.8252865076065063,\n",
              "  0.7915288805961609,\n",
              "  0.8114028573036194,\n",
              "  0.8538042902946472,\n",
              "  0.8447023034095764,\n",
              "  0.8419755101203918,\n",
              "  0.831993579864502,\n",
              "  0.836625337600708,\n",
              "  0.8673759698867798,\n",
              "  0.8291204571723938,\n",
              "  0.8475309014320374,\n",
              "  0.8500614762306213,\n",
              "  0.8546799421310425,\n",
              "  0.8010460138320923,\n",
              "  0.7630776166915894,\n",
              "  0.8533145189285278,\n",
              "  0.7818371057510376,\n",
              "  0.827989935874939,\n",
              "  0.8458662629127502,\n",
              "  0.792289137840271,\n",
              "  0.8331111669540405,\n",
              "  0.8294644355773926,\n",
              "  0.8031518459320068,\n",
              "  0.8959572911262512,\n",
              "  0.8736492991447449,\n",
              "  0.8540907502174377,\n",
              "  0.8403589129447937,\n",
              "  0.8752346634864807,\n",
              "  0.8280568718910217,\n",
              "  0.8411668539047241,\n",
              "  0.820426881313324,\n",
              "  0.8551715016365051,\n",
              "  0.7590583562850952,\n",
              "  0.8234498500823975,\n",
              "  0.8023566603660583,\n",
              "  0.8728747367858887,\n",
              "  0.803736686706543,\n",
              "  0.8551334738731384,\n",
              "  0.8249183893203735,\n",
              "  0.8283488750457764,\n",
              "  0.8544409275054932,\n",
              "  0.79466712474823,\n",
              "  0.8028566241264343,\n",
              "  0.790185809135437,\n",
              "  0.817979633808136,\n",
              "  0.8120595216751099,\n",
              "  0.7786291241645813,\n",
              "  0.8069403767585754,\n",
              "  0.7683517336845398,\n",
              "  0.7949921488761902,\n",
              "  0.8288416862487793,\n",
              "  0.8727049231529236,\n",
              "  0.7969030141830444,\n",
              "  0.9046359658241272,\n",
              "  0.8243306875228882,\n",
              "  0.8171792030334473,\n",
              "  0.7967476844787598,\n",
              "  0.7863477468490601,\n",
              "  0.8201099634170532,\n",
              "  0.7869429588317871,\n",
              "  0.8039103150367737,\n",
              "  0.7648155689239502,\n",
              "  0.7791287899017334,\n",
              "  0.821036696434021,\n",
              "  0.8584756851196289,\n",
              "  0.7849329113960266,\n",
              "  0.8240453600883484,\n",
              "  0.8371075391769409,\n",
              "  0.8281382322311401,\n",
              "  0.8224261999130249,\n",
              "  0.7990758419036865,\n",
              "  0.7908508777618408,\n",
              "  0.8057510852813721,\n",
              "  0.8553999066352844,\n",
              "  0.8782774209976196,\n",
              "  0.8468915820121765,\n",
              "  0.7853256464004517,\n",
              "  0.8482964634895325,\n",
              "  0.7803621292114258,\n",
              "  0.8069797158241272,\n",
              "  0.7858033180236816,\n",
              "  0.8201878666877747,\n",
              "  0.819025993347168,\n",
              "  0.8354257941246033,\n",
              "  0.7893080115318298,\n",
              "  0.774827241897583,\n",
              "  0.794553816318512,\n",
              "  0.8397638201713562,\n",
              "  0.8422091603279114,\n",
              "  0.7795313596725464],\n",
              " 'acc': [0.26742300391197205,\n",
              "  0.2803889811038971,\n",
              "  0.2706645131111145,\n",
              "  0.26742300391197205,\n",
              "  0.2852512300014496,\n",
              "  0.26742300391197205,\n",
              "  0.26742300391197205,\n",
              "  0.2949756979942322,\n",
              "  0.2447325736284256,\n",
              "  0.2917341887950897,\n",
              "  0.277147501707077,\n",
              "  0.2917341887950897,\n",
              "  0.2901134490966797,\n",
              "  0.33873581886291504,\n",
              "  0.31280389428138733,\n",
              "  0.32414910197257996,\n",
              "  0.31928688287734985,\n",
              "  0.3218750059604645,\n",
              "  0.3517017960548401,\n",
              "  0.3063209056854248,\n",
              "  0.3565640151500702,\n",
              "  0.3468395471572876,\n",
              "  0.3421874940395355,\n",
              "  0.3614262640476227,\n",
              "  0.3889789283275604,\n",
              "  0.3727714717388153,\n",
              "  0.3776337206363678,\n",
              "  0.3760129511356354,\n",
              "  0.3776337206363678,\n",
              "  0.3662884831428528,\n",
              "  0.398703396320343,\n",
              "  0.35980552434921265,\n",
              "  0.3565640151500702,\n",
              "  0.38573744893074036,\n",
              "  0.38411667943000793,\n",
              "  0.3889789283275604,\n",
              "  0.3581847548484802,\n",
              "  0.337115079164505,\n",
              "  0.3889789283275604,\n",
              "  0.3922204077243805,\n",
              "  0.4019449055194855,\n",
              "  0.36466774344444275,\n",
              "  0.38573744893074036,\n",
              "  0.3630470037460327,\n",
              "  0.3922204077243805,\n",
              "  0.3765625059604645,\n",
              "  0.4197731018066406,\n",
              "  0.3687500059604645,\n",
              "  0.4132901132106781,\n",
              "  0.3499999940395355,\n",
              "  0.38087520003318787,\n",
              "  0.3922204077243805,\n",
              "  0.397082656621933,\n",
              "  0.38573744893074036,\n",
              "  0.41166937351226807,\n",
              "  0.41093748807907104,\n",
              "  0.40518638491630554,\n",
              "  0.3889789283275604,\n",
              "  0.40032413601875305,\n",
              "  0.43111830949783325,\n",
              "  0.41491085290908813,\n",
              "  0.3938411772251129,\n",
              "  0.38593751192092896,\n",
              "  0.43111830949783325,\n",
              "  0.3824959397315979,\n",
              "  0.40032413601875305,\n",
              "  0.4068071246147156,\n",
              "  0.41166937351226807,\n",
              "  0.41491085290908813,\n",
              "  0.40518638491630554,\n",
              "  0.4197731018066406,\n",
              "  0.4132901132106781,\n",
              "  0.4197731018066406,\n",
              "  0.41166937351226807,\n",
              "  0.44999998807907104,\n",
              "  0.41653159260749817,\n",
              "  0.3873581886291504,\n",
              "  0.40032413601875305,\n",
              "  0.4197731018066406,\n",
              "  0.44246354699134827,\n",
              "  0.397082656621933,\n",
              "  0.4489465057849884,\n",
              "  0.41004863381385803,\n",
              "  0.4197731018066406,\n",
              "  0.4246353209018707,\n",
              "  0.4197731018066406,\n",
              "  0.41491085290908813,\n",
              "  0.4327390491962433,\n",
              "  0.4281249940395355,\n",
              "  0.4068071246147156,\n",
              "  0.4538087546825409,\n",
              "  0.4246353209018707,\n",
              "  0.41653159260749817,\n",
              "  0.4505672752857208,\n",
              "  0.4546875059604645,\n",
              "  0.4602917432785034,\n",
              "  0.4294975697994232,\n",
              "  0.4294975697994232,\n",
              "  0.45705023407936096,\n",
              "  0.4246353209018707,\n",
              "  0.4505672752857208,\n",
              "  0.4181523621082306,\n",
              "  0.4327390491962433,\n",
              "  0.42139384150505066,\n",
              "  0.44246354699134827,\n",
              "  0.4651539623737335,\n",
              "  0.4197731018066406,\n",
              "  0.4230145812034607,\n",
              "  0.41491085290908813,\n",
              "  0.4376012980937958,\n",
              "  0.44570502638816833,\n",
              "  0.42139384150505066,\n",
              "  0.453125,\n",
              "  0.44084277749061584,\n",
              "  0.4376012980937958,\n",
              "  0.4538087546825409,\n",
              "  0.47187501192092896,\n",
              "  0.43437498807907104,\n",
              "  0.45218801498413086,\n",
              "  0.4635332226753235,\n",
              "  0.46677470207214355,\n",
              "  0.43598055839538574,\n",
              "  0.44246354699134827,\n",
              "  0.4538087546825409,\n",
              "  0.42625609040260315,\n",
              "  0.48784440755844116,\n",
              "  0.4732576906681061,\n",
              "  0.43598055839538574,\n",
              "  0.4505672752857208,\n",
              "  0.458670973777771,\n",
              "  0.4440842866897583,\n",
              "  0.4489465057849884,\n",
              "  0.46677470207214355,\n",
              "  0.42139384150505066,\n",
              "  0.4505672752857208,\n",
              "  0.4538087546825409,\n",
              "  0.4278768301010132,\n",
              "  0.46677470207214355,\n",
              "  0.5056726336479187,\n",
              "  0.47163695096969604,\n",
              "  0.44843751192092896,\n",
              "  0.47649919986724854,\n",
              "  0.45218801498413086,\n",
              "  0.4554294943809509,\n",
              "  0.48784440755844116,\n",
              "  0.4390625059604645,\n",
              "  0.44732576608657837,\n",
              "  0.45218801498413086,\n",
              "  0.47649919986724854,\n",
              "  0.470016211271286,\n",
              "  0.47163695096969604,\n",
              "  0.4943273961544037,\n",
              "  0.4991896152496338,\n",
              "  0.47811993956565857,\n",
              "  0.470016211271286,\n",
              "  0.4376012980937958,\n",
              "  0.44732576608657837,\n",
              "  0.46677470207214355,\n",
              "  0.4732576906681061,\n",
              "  0.46677470207214355,\n",
              "  0.4732576906681061,\n",
              "  0.4797406792640686,\n",
              "  0.4602917432785034,\n",
              "  0.47811993956565857,\n",
              "  0.4625000059604645,\n",
              "  0.44732576608657837,\n",
              "  0.4991896152496338,\n",
              "  0.4748784303665161,\n",
              "  0.4943273961544037,\n",
              "  0.4846029281616211,\n",
              "  0.5040518641471863,\n",
              "  0.468395471572876,\n",
              "  0.4748784303665161,\n",
              "  0.4846029281616211,\n",
              "  0.531604528427124,\n",
              "  0.48298215866088867,\n",
              "  0.48298215866088867,\n",
              "  0.4635332226753235,\n",
              "  0.4894651472568512,\n",
              "  0.4943273961544037,\n",
              "  0.5008103847503662,\n",
              "  0.470016211271286,\n",
              "  0.4505672752857208,\n",
              "  0.49108588695526123,\n",
              "  0.48622366786003113,\n",
              "  0.518638551235199,\n",
              "  0.531604528427124,\n",
              "  0.5121555924415588,\n",
              "  0.4846029281616211,\n",
              "  0.4554294943809509,\n",
              "  0.43598055839538574,\n",
              "  0.49108588695526123,\n",
              "  0.468395471572876,\n",
              "  0.5202593207359314,\n",
              "  0.5040518641471863,\n",
              "  0.5251215696334839,\n",
              "  0.47163695096969604,\n",
              "  0.4797406792640686,\n",
              "  0.49756887555122375,\n",
              "  0.48784440755844116,\n",
              "  0.4959481358528137,\n",
              "  0.5056726336479187,\n",
              "  0.5137763619422913,\n",
              "  0.48298215866088867,\n",
              "  0.48124998807907104,\n",
              "  0.5024310946464539,\n",
              "  0.48784440755844116,\n",
              "  0.49270665645599365,\n",
              "  0.5056726336479187,\n",
              "  0.5008103847503662,\n",
              "  0.4991896152496338,\n",
              "  0.5089141130447388,\n",
              "  0.5105348229408264,\n",
              "  0.4959481358528137,\n",
              "  0.5332252979278564,\n",
              "  0.5121555924415588,\n",
              "  0.4991896152496338,\n",
              "  0.4748784303665161,\n",
              "  0.5153970718383789,\n",
              "  0.47811993956565857,\n",
              "  0.4943273961544037,\n",
              "  0.528124988079071,\n",
              "  0.531604528427124,\n",
              "  0.5218800902366638,\n",
              "  0.5137763619422913,\n",
              "  0.5121555924415588,\n",
              "  0.5089141130447388,\n",
              "  0.49270665645599365,\n",
              "  0.4921875,\n",
              "  0.5024310946464539,\n",
              "  0.48784440755844116,\n",
              "  0.48298215866088867,\n",
              "  0.5008103847503662,\n",
              "  0.518638551235199,\n",
              "  0.5170178413391113,\n",
              "  0.541329026222229,\n",
              "  0.526562511920929,\n",
              "  0.5008103847503662,\n",
              "  0.5299838185310364,\n",
              "  0.5137763619422913,\n",
              "  0.5121555924415588,\n",
              "  0.531604528427124,\n",
              "  0.5299838185310364,\n",
              "  0.49756887555122375,\n",
              "  0.5153970718383789,\n",
              "  0.5267422795295715,\n",
              "  0.5364667773246765,\n",
              "  0.48136141896247864,\n",
              "  0.518638551235199,\n",
              "  0.5235008001327515,\n",
              "  0.5137763619422913,\n",
              "  0.5153970718383789,\n",
              "  0.4943273961544037,\n",
              "  0.5170178413391113,\n",
              "  0.5445705056190491,\n",
              "  0.5429497361183167,\n",
              "  0.48784440755844116,\n",
              "  0.4959481358528137,\n",
              "  0.5008103847503662,\n",
              "  0.5056726336479187,\n",
              "  0.518638551235199,\n",
              "  0.503125011920929,\n",
              "  0.48622366786003113,\n",
              "  0.5478119850158691,\n",
              "  0.5510534644126892,\n",
              "  0.5024310946464539,\n",
              "  0.5062500238418579,\n",
              "  0.5429497361183167,\n",
              "  0.531604528427124,\n",
              "  0.47649919986724854,\n",
              "  0.5202593207359314,\n",
              "  0.534375011920929,\n",
              "  0.5235008001327515,\n",
              "  0.5170178413391113,\n",
              "  0.528363049030304,\n",
              "  0.5688816905021667,\n",
              "  0.5328124761581421,\n",
              "  0.5478119850158691,\n",
              "  0.5078125,\n",
              "  0.520312488079071,\n",
              "  0.4943273961544037,\n",
              "  0.5332252979278564,\n",
              "  0.5235008001327515,\n",
              "  0.5348460078239441,\n",
              "  0.528363049030304,\n",
              "  0.5494327545166016,\n",
              "  0.4959481358528137,\n",
              "  0.5332252979278564,\n",
              "  0.531604528427124,\n",
              "  0.528363049030304,\n",
              "  0.531604528427124,\n",
              "  0.5380875468254089,\n",
              "  0.5202593207359314,\n",
              "  0.5089141130447388,\n",
              "  0.5397082567214966,\n",
              "  0.5364667773246765,\n",
              "  0.5348460078239441,\n",
              "  0.5429497361183167,\n",
              "  0.5251215696334839,\n",
              "  0.5137763619422913,\n",
              "  0.5089141130447388,\n",
              "  0.5380875468254089,\n",
              "  0.528363049030304,\n",
              "  0.5093749761581421,\n",
              "  0.5559157133102417,\n",
              "  0.5267422795295715,\n",
              "  0.5121555924415588,\n",
              "  0.5234375,\n",
              "  0.5445705056190491,\n",
              "  0.5348460078239441,\n",
              "  0.528363049030304,\n",
              "  0.5251215696334839,\n",
              "  0.5105348229408264,\n",
              "  0.5296875238418579,\n",
              "  0.5640194416046143,\n",
              "  0.5153970718383789,\n",
              "  0.5299838185310364,\n",
              "  0.5218800902366638,\n",
              "  0.5072933435440063,\n",
              "  0.5494327545166016,\n",
              "  0.5218800902366638,\n",
              "  0.5656402111053467,\n",
              "  0.5478119850158691,\n",
              "  0.5721231698989868,\n",
              "  0.5235008001327515,\n",
              "  0.5445705056190491,\n",
              "  0.5510534644126892,\n",
              "  0.5348460078239441,\n",
              "  0.5153970718383789,\n",
              "  0.5478119850158691,\n",
              "  0.5705024600028992,\n",
              "  0.5445705056190491,\n",
              "  0.5348460078239441,\n",
              "  0.5559157133102417,\n",
              "  0.5721231698989868,\n",
              "  0.5056726336479187,\n",
              "  0.5591571927070618,\n",
              "  0.531604528427124,\n",
              "  0.5494327545166016,\n",
              "  0.5380875468254089,\n",
              "  0.5753646492958069,\n",
              "  0.5153970718383789,\n",
              "  0.528363049030304,\n",
              "  0.5478119850158691,\n",
              "  0.5461912751197815,\n",
              "  0.5559157133102417,\n",
              "  0.5623987317085266,\n",
              "  0.559374988079071,\n",
              "  0.518638551235199,\n",
              "  0.5478119850158691,\n",
              "  0.5267422795295715,\n",
              "  0.5850891470909119,\n",
              "  0.518638551235199,\n",
              "  0.5380875468254089,\n",
              "  0.5364667773246765,\n",
              "  0.554295003414154,\n",
              "  0.5445705056190491,\n",
              "  0.5429497361183167,\n",
              "  0.5397082567214966,\n",
              "  0.5721231698989868,\n",
              "  0.518638551235199,\n",
              "  0.5461912751197815,\n",
              "  0.5267422795295715,\n",
              "  0.5397082567214966,\n",
              "  0.5526742339134216,\n",
              "  0.5753646492958069,\n",
              "  0.518750011920929,\n",
              "  0.589062511920929,\n",
              "  0.5559157133102417,\n",
              "  0.5478119850158691,\n",
              "  0.5526742339134216,\n",
              "  0.5531250238418579,\n",
              "  0.5380875468254089,\n",
              "  0.5364667773246765,\n",
              "  0.5818476676940918,\n",
              "  0.528363049030304,\n",
              "  0.541329026222229,\n",
              "  0.541329026222229,\n",
              "  0.541329026222229,\n",
              "  0.5623987317085266,\n",
              "  0.542187511920929,\n",
              "  0.5526742339134216,\n",
              "  0.5526742339134216,\n",
              "  0.5546875,\n",
              "  0.5429497361183167,\n",
              "  0.5721231698989868,\n",
              "  0.5786061882972717,\n",
              "  0.5510534644126892,\n",
              "  0.5461912751197815,\n",
              "  0.5575364828109741,\n",
              "  0.541329026222229,\n",
              "  0.5802268981933594,\n",
              "  0.541329026222229,\n",
              "  0.5559157133102417,\n",
              "  0.554295003414154,\n",
              "  0.5931928753852844,\n",
              "  0.5607779622077942,\n",
              "  0.5607779622077942,\n",
              "  0.5607779622077942,\n",
              "  0.5364667773246765,\n",
              "  0.5397082567214966,\n",
              "  0.5640194416046143,\n",
              "  0.5546875,\n",
              "  0.5591571927070618,\n",
              "  0.5948135852813721,\n",
              "  0.5753646492958069,\n",
              "  0.5121555924415588,\n",
              "  0.5494327545166016,\n",
              "  0.5834683775901794,\n",
              "  0.5769854187965393,\n",
              "  0.5445705056190491,\n",
              "  0.5575364828109741,\n",
              "  0.5737439393997192,\n",
              "  0.5510534644126892,\n",
              "  0.578125,\n",
              "  0.5546875,\n",
              "  0.5445705056190491,\n",
              "  0.554295003414154,\n",
              "  0.5591571927070618,\n",
              "  0.554295003414154,\n",
              "  0.5786061882972717,\n",
              "  0.589062511920929,\n",
              "  0.5737439393997192,\n",
              "  0.5688816905021667,\n",
              "  0.535937488079071,\n",
              "  0.5883306264877319,\n",
              "  0.5796874761581421,\n",
              "  0.5559157133102417,\n",
              "  0.534375011920929,\n",
              "  0.531604528427124,\n",
              "  0.5883306264877319,\n",
              "  0.5688816905021667,\n",
              "  0.5640194416046143,\n",
              "  0.5494327545166016,\n",
              "  0.5607779622077942,\n",
              "  0.5623987317085266,\n",
              "  0.5753646492958069,\n",
              "  0.5688816905021667,\n",
              "  0.5867098569869995,\n",
              "  0.5623987317085266,\n",
              "  0.5818476676940918,\n",
              "  0.5850891470909119,\n",
              "  0.5705024600028992,\n",
              "  0.5607779622077942,\n",
              "  0.5721231698989868,\n",
              "  0.6061588525772095,\n",
              "  0.5818476676940918,\n",
              "  0.5688816905021667,\n",
              "  0.5818476676940918,\n",
              "  0.565625011920929,\n",
              "  0.5721231698989868,\n",
              "  0.5397082567214966,\n",
              "  0.5705024600028992,\n",
              "  0.5623987317085266,\n",
              "  0.5737439393997192,\n",
              "  0.5802268981933594,\n",
              "  0.5721231698989868,\n",
              "  0.5769854187965393,\n",
              "  0.5802268981933594,\n",
              "  0.5623987317085266,\n",
              "  0.5397082567214966,\n",
              "  0.5948135852813721,\n",
              "  0.5656402111053467,\n",
              "  0.5607779622077942,\n",
              "  0.5559157133102417,\n",
              "  0.5769854187965393,\n",
              "  0.6110210418701172,\n",
              "  0.5429497361183167,\n",
              "  0.554295003414154,\n",
              "  0.5834683775901794,\n",
              "  0.5867098569869995,\n",
              "  0.5526742339134216,\n",
              "  0.5688816905021667,\n",
              "  0.5802268981933594,\n",
              "  0.5607779622077942,\n",
              "  0.6061588525772095,\n",
              "  0.5996758341789246,\n",
              "  0.5688816905021667,\n",
              "  0.5640194416046143,\n",
              "  0.5721231698989868,\n",
              "  0.5899513959884644,\n",
              "  0.5348460078239441,\n",
              "  0.5818476676940918,\n",
              "  0.5656402111053467,\n",
              "  0.6029173135757446,\n",
              "  0.598437488079071,\n",
              "  0.5531250238418579,\n",
              "  0.5672609210014343,\n",
              "  0.5867098569869995,\n",
              "  0.6158832907676697,\n",
              "  0.604538083076477,\n",
              "  0.554295003414154,\n",
              "  0.5834683775901794,\n",
              "  0.5609375238418579,\n",
              "  0.601296603679657,\n",
              "  0.5672609210014343,\n",
              "  0.5786061882972717,\n",
              "  0.5623987317085266,\n",
              "  0.5883306264877319,\n",
              "  0.5834683775901794,\n",
              "  0.5996758341789246,\n",
              "  0.591572105884552,\n",
              "  0.5931928753852844,\n",
              "  0.5953124761581421,\n",
              "  0.5883306264877319,\n",
              "  0.5753646492958069,\n",
              "  0.5796874761581421,\n",
              "  0.5688816905021667,\n",
              "  0.5721231698989868,\n",
              "  0.6158832907676697,\n",
              "  0.5802268981933594,\n",
              "  0.581250011920929,\n",
              "  0.6061588525772095,\n",
              "  0.604538083076477,\n",
              "  0.5931928753852844,\n",
              "  0.5526742339134216,\n",
              "  0.541329026222229,\n",
              "  0.5802268981933594,\n",
              "  0.5688816905021667,\n",
              "  0.5802268981933594,\n",
              "  0.581250011920929,\n",
              "  0.6320907473564148,\n",
              "  0.604538083076477,\n",
              "  0.5688816905021667,\n",
              "  0.6029173135757446,\n",
              "  0.5461912751197815,\n",
              "  0.5931928753852844,\n",
              "  0.5623987317085266,\n",
              "  0.5899513959884644,\n",
              "  0.5834683775901794,\n",
              "  0.5753646492958069,\n",
              "  0.5607779622077942,\n",
              "  0.5607779622077942,\n",
              "  0.5948135852813721,\n",
              "  0.5818476676940918,\n",
              "  0.5818476676940918,\n",
              "  0.5721231698989868,\n",
              "  0.5953124761581421,\n",
              "  0.6061588525772095,\n",
              "  0.6110210418701172,\n",
              "  0.5850891470909119,\n",
              "  0.5818476676940918,\n",
              "  0.6077795624732971,\n",
              "  0.5640194416046143,\n",
              "  0.5753646492958069,\n",
              "  0.604538083076477,\n",
              "  0.589062511920929,\n",
              "  0.6207455396652222,\n",
              "  0.5802268981933594,\n",
              "  0.5656402111053467,\n",
              "  0.5625,\n",
              "  0.5786061882972717,\n",
              "  0.5737439393997192,\n",
              "  0.5802268981933594,\n",
              "  0.5721231698989868,\n",
              "  0.5765625238418579,\n",
              "  0.6061588525772095,\n",
              "  0.5834683775901794,\n",
              "  0.5996758341789246,\n",
              "  0.5623987317085266,\n",
              "  0.5996758341789246,\n",
              "  0.5802268981933594,\n",
              "  0.5996758341789246,\n",
              "  0.5765625238418579,\n",
              "  0.5867098569869995,\n",
              "  0.5867098569869995,\n",
              "  0.6061588525772095,\n",
              "  0.5867098569869995,\n",
              "  0.6061588525772095,\n",
              "  0.5964343547821045,\n",
              "  0.601296603679657,\n",
              "  0.6077795624732971,\n",
              "  0.5623987317085266,\n",
              "  0.5850891470909119,\n",
              "  0.5705024600028992,\n",
              "  0.598437488079071,\n",
              "  0.5623987317085266,\n",
              "  0.6191247701644897,\n",
              "  0.5850891470909119,\n",
              "  0.591572105884552,\n",
              "  0.5964343547821045,\n",
              "  0.598437488079071,\n",
              "  0.5850891470909119,\n",
              "  0.5953124761581421,\n",
              "  0.5850891470909119,\n",
              "  0.5883306264877319,\n",
              "  0.6029173135757446,\n",
              "  0.5769854187965393,\n",
              "  0.5867098569869995,\n",
              "  0.614262580871582,\n",
              "  0.6126418113708496,\n",
              "  0.6077795624732971,\n",
              "  0.5850891470909119,\n",
              "  0.6077795624732971,\n",
              "  0.6126418113708496,\n",
              "  0.6223663091659546,\n",
              "  0.6353322267532349,\n",
              "  0.6077795624732971,\n",
              "  0.6256077885627747,\n",
              "  0.6077795624732971,\n",
              "  0.5931928753852844,\n",
              "  0.5834683775901794,\n",
              "  0.5718749761581421,\n",
              "  0.6239870190620422,\n",
              "  0.6110210418701172,\n",
              "  0.6223663091659546,\n",
              "  0.5705024600028992,\n",
              "  0.5899513959884644,\n",
              "  0.5964343547821045,\n",
              "  0.5948135852813721,\n",
              "  0.5867098569869995,\n",
              "  0.601296603679657,\n",
              "  0.6061588525772095,\n",
              "  0.5575364828109741,\n",
              "  0.5980551242828369,\n",
              "  0.5818476676940918,\n",
              "  0.5931928753852844,\n",
              "  0.5948135852813721,\n",
              "  0.5721231698989868,\n",
              "  0.5931928753852844,\n",
              "  0.604538083076477,\n",
              "  0.6207455396652222,\n",
              "  0.625,\n",
              "  0.5688816905021667,\n",
              "  0.5850891470909119,\n",
              "  0.598437488079071,\n",
              "  0.5786061882972717,\n",
              "  0.5980551242828369,\n",
              "  0.5818476676940918,\n",
              "  0.5818476676940918,\n",
              "  0.614262580871582,\n",
              "  0.5948135852813721,\n",
              "  0.5834683775901794,\n",
              "  0.6061588525772095,\n",
              "  0.6110210418701172,\n",
              "  0.6207455396652222,\n",
              "  0.591572105884552,\n",
              "  0.5786061882972717,\n",
              "  0.5948135852813721,\n",
              "  0.6418152451515198,\n",
              "  0.6061588525772095,\n",
              "  0.6353322267532349,\n",
              "  0.591572105884552,\n",
              "  0.5656402111053467,\n",
              "  0.6126418113708496,\n",
              "  0.5948135852813721,\n",
              "  0.6256077885627747,\n",
              "  0.6353322267532349,\n",
              "  0.601296603679657,\n",
              "  0.5850891470909119,\n",
              "  0.6158832907676697,\n",
              "  0.601296603679657,\n",
              "  0.6031249761581421,\n",
              "  0.5883306264877319,\n",
              "  0.6109374761581421,\n",
              "  0.5818476676940918,\n",
              "  0.5718749761581421,\n",
              "  0.5899513959884644,\n",
              "  0.6256077885627747,\n",
              "  0.601296603679657,\n",
              "  0.6077795624732971,\n",
              "  0.6207455396652222,\n",
              "  0.6320907473564148,\n",
              "  0.5948135852813721,\n",
              "  0.601296603679657,\n",
              "  0.6077795624732971,\n",
              "  0.6320907473564148,\n",
              "  0.6029173135757446,\n",
              "  0.6328125,\n",
              "  0.6061588525772095,\n",
              "  0.601296603679657,\n",
              "  0.6175040602684021,\n",
              "  0.6110210418701172,\n",
              "  0.6256077885627747,\n",
              "  0.6158832907676697,\n",
              "  0.5931928753852844,\n",
              "  0.5850891470909119,\n",
              "  0.6353322267532349,\n",
              "  0.6110210418701172,\n",
              "  0.6304700374603271,\n",
              "  0.6110210418701172,\n",
              "  0.5948135852813721,\n",
              "  0.6158832907676697,\n",
              "  0.5948135852813721,\n",
              "  0.6239870190620422,\n",
              "  0.6434359550476074,\n",
              "  0.6401944756507874,\n",
              "  0.601296603679657,\n",
              "  0.604538083076477,\n",
              "  0.621874988079071,\n",
              "  0.5623987317085266,\n",
              "  0.6499189734458923,\n",
              "  0.6272284984588623,\n",
              "  0.6369529962539673,\n",
              "  0.5769854187965393,\n",
              "  0.6077795624732971,\n",
              "  0.591572105884552,\n",
              "  0.5867098569869995,\n",
              "  0.6191247701644897,\n",
              "  0.6239870190620422,\n",
              "  0.6337115168571472,\n",
              "  0.6126418113708496,\n",
              "  0.6175040602684021,\n",
              "  0.6126418113708496,\n",
              "  0.6175040602684021,\n",
              "  0.6000000238418579,\n",
              "  0.5769854187965393,\n",
              "  0.5980551242828369,\n",
              "  0.6175040602684021,\n",
              "  0.5996758341789246,\n",
              "  0.6466774940490723,\n",
              "  0.6077795624732971,\n",
              "  0.5931928753852844,\n",
              "  0.591572105884552,\n",
              "  0.6175040602684021,\n",
              "  0.5737439393997192,\n",
              "  0.6077795624732971,\n",
              "  0.6191247701644897,\n",
              "  0.5980551242828369,\n",
              "  0.629687488079071,\n",
              "  0.6077795624732971,\n",
              "  0.6094003319740295,\n",
              "  0.5753646492958069,\n",
              "  0.604538083076477,\n",
              "  0.6110210418701172,\n",
              "  0.6077795624732971,\n",
              "  0.6288492679595947,\n",
              "  0.6434359550476074,\n",
              "  0.5899513959884644,\n",
              "  0.614262580871582,\n",
              "  0.6288492679595947,\n",
              "  0.6126418113708496,\n",
              "  0.6175040602684021,\n",
              "  0.6466774940490723,\n",
              "  0.6288492679595947,\n",
              "  0.6256077885627747,\n",
              "  0.6482982039451599,\n",
              "  0.6207455396652222,\n",
              "  0.614262580871582,\n",
              "  0.596875011920929,\n",
              "  0.6272284984588623,\n",
              "  0.6482982039451599,\n",
              "  0.6061588525772095,\n",
              "  0.6272284984588623,\n",
              "  0.6223663091659546,\n",
              "  0.6094003319740295,\n",
              "  0.620312511920929,\n",
              "  0.643750011920929,\n",
              "  0.6304700374603271,\n",
              "  0.5980551242828369,\n",
              "  0.6126418113708496,\n",
              "  0.6234375238418579,\n",
              "  0.6207455396652222,\n",
              "  0.591572105884552,\n",
              "  0.614262580871582,\n",
              "  0.6158832907676697,\n",
              "  0.601296603679657,\n",
              "  0.6288492679595947,\n",
              "  0.5859375,\n",
              "  0.6207455396652222,\n",
              "  0.604538083076477,\n",
              "  0.6207455396652222,\n",
              "  0.6304700374603271,\n",
              "  0.6094003319740295,\n",
              "  0.5931928753852844,\n",
              "  0.6175040602684021,\n",
              "  0.6158832907676697,\n",
              "  0.6320907473564148,\n",
              "  0.6126418113708496,\n",
              "  0.6077795624732971,\n",
              "  0.5786061882972717,\n",
              "  0.6110210418701172,\n",
              "  0.6239870190620422,\n",
              "  0.6061588525772095,\n",
              "  0.6450567245483398,\n",
              "  0.601296603679657,\n",
              "  0.5980551242828369,\n",
              "  0.6369529962539673,\n",
              "  0.5964343547821045,\n",
              "  0.6272284984588623,\n",
              "  0.6234375238418579,\n",
              "  0.6094003319740295,\n",
              "  0.6207455396652222,\n",
              "  0.6256077885627747,\n",
              "  0.6158832907676697,\n",
              "  0.6207455396652222,\n",
              "  0.6385737657546997,\n",
              "  0.6320907473564148,\n",
              "  0.6256077885627747,\n",
              "  0.6207455396652222,\n",
              "  0.6158832907676697,\n",
              "  0.625,\n",
              "  0.6126418113708496,\n",
              "  0.6156250238418579,\n",
              "  0.6207455396652222,\n",
              "  0.6320907473564148,\n",
              "  0.6077795624732971,\n",
              "  0.6564019322395325,\n",
              "  0.6337115168571472,\n",
              "  0.6272284984588623,\n",
              "  0.6110210418701172,\n",
              "  0.6191247701644897,\n",
              "  0.6191247701644897,\n",
              "  0.6207455396652222,\n",
              "  0.6094003319740295,\n",
              "  0.6175040602684021,\n",
              "  0.6272284984588623,\n",
              "  0.6304700374603271,\n",
              "  0.6401944756507874,\n",
              "  0.5980551242828369,\n",
              "  0.6320907473564148,\n",
              "  0.6531604528427124,\n",
              "  0.6288492679595947,\n",
              "  0.604538083076477,\n",
              "  0.6110210418701172,\n",
              "  0.6482982039451599,\n",
              "  0.661264181137085,\n",
              "  0.6110210418701172,\n",
              "  0.590624988079071,\n",
              "  0.6239870190620422,\n",
              "  0.6531604528427124,\n",
              "  0.601296603679657,\n",
              "  0.6239870190620422,\n",
              "  0.6094003319740295,\n",
              "  0.6094003319740295,\n",
              "  0.6207455396652222,\n",
              "  0.6094003319740295,\n",
              "  0.5931928753852844,\n",
              "  0.6468750238418579,\n",
              "  0.6223663091659546,\n",
              "  0.6564019322395325,\n",
              "  0.620312511920929,\n",
              "  0.6288492679595947,\n",
              "  0.643750011920929,\n",
              "  0.6175040602684021,\n",
              "  0.614262580871582,\n",
              "  0.5996758341789246,\n",
              "  0.6369529962539673,\n",
              "  0.6531604528427124,\n",
              "  0.5850891470909119,\n",
              "  0.6272284984588623,\n",
              "  0.645312488079071,\n",
              "  0.6029173135757446,\n",
              "  0.6337115168571472,\n",
              "  0.614262580871582,\n",
              "  0.65153968334198,\n",
              "  0.6304700374603271,\n",
              "  0.6304700374603271,\n",
              "  0.6369529962539673,\n",
              "  0.6158832907676697,\n",
              "  0.6110210418701172,\n",
              "  0.6385737657546997,\n",
              "  0.6531604528427124,\n",
              "  0.6191247701644897,\n",
              "  0.6126418113708496,\n",
              "  0.6239870190620422,\n",
              "  0.6320907473564148,\n",
              "  0.6369529962539673,\n",
              "  0.6499999761581421,\n",
              "  0.6223663091659546,\n",
              "  0.6223663091659546,\n",
              "  0.6223663091659546,\n",
              "  0.6320907473564148,\n",
              "  0.6320907473564148,\n",
              "  0.635937511920929,\n",
              "  0.6256077885627747,\n",
              "  0.614262580871582,\n",
              "  0.6499189734458923,\n",
              "  0.6094003319740295,\n",
              "  0.6223663091659546,\n",
              "  0.6580227017402649,\n",
              "  0.6418152451515198,\n",
              "  0.6677471399307251,\n",
              "  0.614262580871582,\n",
              "  0.6239870190620422,\n",
              "  0.614262580871582,\n",
              "  0.6191247701644897,\n",
              "  0.6239870190620422,\n",
              "  0.6158832907676697,\n",
              "  0.6369529962539673,\n",
              "  0.614262580871582,\n",
              "  0.6320907473564148,\n",
              "  0.6466774940490723,\n",
              "  0.6288492679595947,\n",
              "  0.6482982039451599,\n",
              "  0.661264181137085,\n",
              "  0.6239870190620422,\n",
              "  0.6272284984588623,\n",
              "  0.6450567245483398,\n",
              "  0.6401944756507874,\n",
              "  0.6288492679595947,\n",
              "  0.6693679094314575,\n",
              "  0.6256077885627747,\n",
              "  0.6385737657546997,\n",
              "  0.6450567245483398,\n",
              "  0.6466774940490723,\n",
              "  0.6547812223434448,\n",
              "  0.6304700374603271,\n",
              "  0.6207455396652222,\n",
              "  0.6223663091659546,\n",
              "  0.6288492679595947,\n",
              "  0.5996758341789246,\n",
              "  0.5921875238418579,\n",
              "  0.6077795624732971,\n",
              "  0.6304700374603271,\n",
              "  0.6434359550476074,\n",
              "  0.6499189734458923,\n",
              "  0.6191247701644897,\n",
              "  0.6337115168571472,\n",
              "  0.6175040602684021,\n",
              "  0.6353322267532349,\n",
              "  0.6596434116363525,\n",
              "  0.6450567245483398,\n",
              "  0.6369529962539673,\n",
              "  0.6337115168571472,\n",
              "  0.614262580871582,\n",
              "  0.6288492679595947,\n",
              "  0.6272284984588623,\n",
              "  0.6256077885627747,\n",
              "  0.6482982039451599,\n",
              "  0.6288492679595947,\n",
              "  0.6272284984588623,\n",
              "  0.601296603679657,\n",
              "  0.6468750238418579,\n",
              "  0.6596434116363525,\n",
              "  0.6434359550476074,\n",
              "  0.6564019322395325,\n",
              "  0.6272284984588623,\n",
              "  0.604538083076477,\n",
              "  0.661264181137085,\n",
              "  0.6304700374603271,\n",
              "  0.6547812223434448,\n",
              "  0.6466774940490723,\n",
              "  0.6320907473564148,\n",
              "  0.6158832907676697,\n",
              "  0.6175040602684021,\n",
              "  0.6320907473564148,\n",
              "  0.6126418113708496,\n",
              "  0.6272284984588623,\n",
              "  0.6385737657546997,\n",
              "  0.614262580871582,\n",
              "  0.6207455396652222,\n",
              "  0.6661264300346375,\n",
              "  0.6450567245483398,\n",
              "  0.6466774940490723,\n",
              "  0.6175040602684021,\n",
              "  0.6482982039451599,\n",
              "  0.614262580871582,\n",
              "  0.6596434116363525,\n",
              "  0.6029173135757446,\n",
              "  0.5948135852813721,\n",
              "  0.6337115168571472,\n",
              "  0.6580227017402649,\n",
              "  0.6499189734458923,\n",
              "  0.645312488079071,\n",
              "  0.628125011920929,\n",
              "  0.6677471399307251,\n",
              "  0.6661264300346375,\n",
              "  0.6677471399307251,\n",
              "  0.6320907473564148,\n",
              "  0.6482982039451599,\n",
              "  0.6304700374603271,\n",
              "  0.6468750238418579,\n",
              "  0.6110210418701172,\n",
              "  0.6418152451515198,\n",
              "  0.6320907473564148,\n",
              "  0.6564019322395325,\n",
              "  0.6693679094314575,\n",
              "  0.6337115168571472,\n",
              "  0.6580227017402649,\n",
              "  0.65153968334198,\n",
              "  0.664505660533905,\n",
              "  0.6774716377258301,\n",
              "  0.6401944756507874,\n",
              "  0.6256077885627747,\n",
              "  0.65153968334198,\n",
              "  0.6223663091659546,\n",
              "  0.65153968334198,\n",
              "  0.6369529962539673,\n",
              "  0.6466774940490723,\n",
              "  0.6580227017402649,\n",
              "  0.6531604528427124,\n",
              "  0.6693679094314575,\n",
              "  0.6466774940490723,\n",
              "  0.609375,\n",
              "  0.6171875,\n",
              "  0.6726093888282776,\n",
              "  0.6175040602684021,\n",
              "  0.659375011920929,\n",
              "  0.6418152451515198,\n",
              "  0.6546875238418579,\n",
              "  0.629687488079071,\n",
              "  0.6353322267532349,\n",
              "  0.65153968334198,\n",
              "  0.6580227017402649,\n",
              "  0.6596434116363525,\n",
              "  0.6564019322395325,\n",
              "  0.6304700374603271,\n",
              "  0.6175040602684021,\n",
              "  0.6418152451515198],\n",
              " 'val_loss': [1.3119542598724365,\n",
              "  1.1963841915130615,\n",
              "  1.2837214469909668,\n",
              "  1.1718544960021973,\n",
              "  1.4065041542053223,\n",
              "  1.162570595741272,\n",
              "  1.17058527469635,\n",
              "  1.2647439241409302,\n",
              "  1.0833255052566528,\n",
              "  1.1623549461364746,\n",
              "  1.245307445526123,\n",
              "  1.2406995296478271,\n",
              "  1.2583980560302734,\n",
              "  1.31122887134552,\n",
              "  1.472935438156128,\n",
              "  1.3351805210113525,\n",
              "  1.5112215280532837,\n",
              "  1.416257381439209,\n",
              "  1.4848008155822754,\n",
              "  1.427899956703186,\n",
              "  1.5745570659637451,\n",
              "  1.4800928831100464,\n",
              "  1.4472322463989258,\n",
              "  1.4330315589904785,\n",
              "  1.5875153541564941,\n",
              "  1.5645239353179932,\n",
              "  1.4190281629562378,\n",
              "  1.4163881540298462,\n",
              "  1.4448859691619873,\n",
              "  1.3694345951080322,\n",
              "  1.450985312461853,\n",
              "  1.3880425691604614,\n",
              "  1.449399709701538,\n",
              "  1.3954625129699707,\n",
              "  1.382169485092163,\n",
              "  1.340423822402954,\n",
              "  1.3712618350982666,\n",
              "  1.3219974040985107,\n",
              "  1.4765074253082275,\n",
              "  1.38907790184021,\n",
              "  1.363006830215454,\n",
              "  1.3389785289764404,\n",
              "  1.2359516620635986,\n",
              "  1.3944398164749146,\n",
              "  1.3228704929351807,\n",
              "  1.352848768234253,\n",
              "  1.3796428442001343,\n",
              "  1.3560078144073486,\n",
              "  1.3899860382080078,\n",
              "  1.2888829708099365,\n",
              "  1.3129615783691406,\n",
              "  1.3707597255706787,\n",
              "  1.3917518854141235,\n",
              "  1.2533882856369019,\n",
              "  1.227961778640747,\n",
              "  1.1947065591812134,\n",
              "  1.2900090217590332,\n",
              "  1.3226138353347778,\n",
              "  1.2208306789398193,\n",
              "  1.1947824954986572,\n",
              "  1.3401846885681152,\n",
              "  1.2408465147018433,\n",
              "  1.2775771617889404,\n",
              "  1.245314598083496,\n",
              "  1.2049965858459473,\n",
              "  1.220657467842102,\n",
              "  1.3812254667282104,\n",
              "  1.1411641836166382,\n",
              "  1.257188320159912,\n",
              "  1.337517499923706,\n",
              "  1.3362274169921875,\n",
              "  1.2580597400665283,\n",
              "  1.286687970161438,\n",
              "  1.2350571155548096,\n",
              "  1.3490407466888428,\n",
              "  1.2759549617767334,\n",
              "  1.260758876800537,\n",
              "  1.2395776510238647,\n",
              "  1.3165946006774902,\n",
              "  1.4124834537506104,\n",
              "  1.3284428119659424,\n",
              "  1.300952434539795,\n",
              "  1.2404087781906128,\n",
              "  1.2058637142181396,\n",
              "  1.1956814527511597,\n",
              "  1.2553119659423828,\n",
              "  1.3649792671203613,\n",
              "  1.204417109489441,\n",
              "  1.2923455238342285,\n",
              "  1.3248095512390137,\n",
              "  1.2730954885482788,\n",
              "  1.3503868579864502,\n",
              "  1.2598211765289307,\n",
              "  1.245814323425293,\n",
              "  1.248427152633667,\n",
              "  1.1293275356292725,\n",
              "  1.2657331228256226,\n",
              "  1.1997743844985962,\n",
              "  1.1323301792144775,\n",
              "  1.2666046619415283,\n",
              "  1.2451781034469604,\n",
              "  1.1844500303268433,\n",
              "  1.2379218339920044,\n",
              "  1.2839252948760986,\n",
              "  1.3358685970306396,\n",
              "  1.1942933797836304,\n",
              "  1.1711699962615967,\n",
              "  1.2114856243133545,\n",
              "  1.2192243337631226,\n",
              "  1.2013705968856812,\n",
              "  1.1415331363677979,\n",
              "  1.3006294965744019,\n",
              "  1.3017933368682861,\n",
              "  1.2656042575836182,\n",
              "  1.3410208225250244,\n",
              "  1.1696416139602661,\n",
              "  1.2876646518707275,\n",
              "  1.0948107242584229,\n",
              "  1.0823400020599365,\n",
              "  1.1923205852508545,\n",
              "  1.2245378494262695,\n",
              "  1.1434730291366577,\n",
              "  1.0957226753234863,\n",
              "  1.2594890594482422,\n",
              "  1.1399552822113037,\n",
              "  1.3022443056106567,\n",
              "  1.1994547843933105,\n",
              "  1.1626943349838257,\n",
              "  1.232804298400879,\n",
              "  1.2264044284820557,\n",
              "  1.1850502490997314,\n",
              "  1.158907413482666,\n",
              "  1.1961125135421753,\n",
              "  1.1362411975860596,\n",
              "  1.259450912475586,\n",
              "  1.241014003753662,\n",
              "  1.196732521057129,\n",
              "  1.1275193691253662,\n",
              "  1.1461851596832275,\n",
              "  1.318914771080017,\n",
              "  1.1831915378570557,\n",
              "  1.2793850898742676,\n",
              "  1.1800751686096191,\n",
              "  1.197777509689331,\n",
              "  1.1166043281555176,\n",
              "  1.1673122644424438,\n",
              "  1.1515374183654785,\n",
              "  1.2125976085662842,\n",
              "  1.186671257019043,\n",
              "  1.203791618347168,\n",
              "  1.312902808189392,\n",
              "  1.301809310913086,\n",
              "  1.2712690830230713,\n",
              "  1.1496515274047852,\n",
              "  1.2154449224472046,\n",
              "  1.1385688781738281,\n",
              "  1.2614355087280273,\n",
              "  1.1890943050384521,\n",
              "  1.2539880275726318,\n",
              "  1.2556097507476807,\n",
              "  1.1223962306976318,\n",
              "  1.299536943435669,\n",
              "  1.2498786449432373,\n",
              "  1.2842328548431396,\n",
              "  1.2507259845733643,\n",
              "  1.2104403972625732,\n",
              "  1.3038809299468994,\n",
              "  1.2663167715072632,\n",
              "  1.305917739868164,\n",
              "  1.2389953136444092,\n",
              "  1.3270649909973145,\n",
              "  1.1626570224761963,\n",
              "  1.2005391120910645,\n",
              "  1.1581696271896362,\n",
              "  1.2529025077819824,\n",
              "  1.2381770610809326,\n",
              "  1.289273977279663,\n",
              "  1.2770781517028809,\n",
              "  1.215481162071228,\n",
              "  1.2500104904174805,\n",
              "  1.2514474391937256,\n",
              "  1.2479159832000732,\n",
              "  1.2008386850357056,\n",
              "  1.2581958770751953,\n",
              "  1.2174482345581055,\n",
              "  1.2415679693222046,\n",
              "  1.2959625720977783,\n",
              "  1.1486424207687378,\n",
              "  1.207590937614441,\n",
              "  1.248225450515747,\n",
              "  1.3505014181137085,\n",
              "  1.252495527267456,\n",
              "  1.2011446952819824,\n",
              "  1.3102338314056396,\n",
              "  1.176472544670105,\n",
              "  1.1798871755599976,\n",
              "  1.2346999645233154,\n",
              "  1.2124371528625488,\n",
              "  1.2069329023361206,\n",
              "  1.2250428199768066,\n",
              "  1.1984912157058716,\n",
              "  1.191624402999878,\n",
              "  1.2709894180297852,\n",
              "  1.1552361249923706,\n",
              "  1.1986600160598755,\n",
              "  1.149226427078247,\n",
              "  1.24021577835083,\n",
              "  1.1835997104644775,\n",
              "  1.2295812368392944,\n",
              "  1.2648812532424927,\n",
              "  1.2497177124023438,\n",
              "  1.2483549118041992,\n",
              "  1.196446180343628,\n",
              "  1.1948597431182861,\n",
              "  1.200166940689087,\n",
              "  1.1750072240829468,\n",
              "  1.166855812072754,\n",
              "  1.110429048538208,\n",
              "  1.1179841756820679,\n",
              "  1.2149970531463623,\n",
              "  1.1883018016815186,\n",
              "  1.186582326889038,\n",
              "  1.213337779045105,\n",
              "  1.2050509452819824,\n",
              "  1.159755825996399,\n",
              "  1.2130311727523804,\n",
              "  1.2161552906036377,\n",
              "  1.1673352718353271,\n",
              "  1.246286392211914,\n",
              "  1.256204605102539,\n",
              "  1.162747859954834,\n",
              "  1.1349012851715088,\n",
              "  1.2110716104507446,\n",
              "  1.2634860277175903,\n",
              "  1.1745985746383667,\n",
              "  1.2271491289138794,\n",
              "  1.2585525512695312,\n",
              "  1.2251909971237183,\n",
              "  1.1516146659851074,\n",
              "  1.2074882984161377,\n",
              "  1.1110904216766357,\n",
              "  1.0703681707382202,\n",
              "  1.143871784210205,\n",
              "  1.2089534997940063,\n",
              "  1.3313353061676025,\n",
              "  1.3324391841888428,\n",
              "  1.312396764755249,\n",
              "  1.2499489784240723,\n",
              "  1.2432873249053955,\n",
              "  1.1679952144622803,\n",
              "  1.2617135047912598,\n",
              "  1.2137222290039062,\n",
              "  1.326481580734253,\n",
              "  1.2175488471984863,\n",
              "  1.2394814491271973,\n",
              "  1.2272940874099731,\n",
              "  1.1821422576904297,\n",
              "  1.1620500087738037,\n",
              "  1.2136938571929932,\n",
              "  1.2135188579559326,\n",
              "  1.1983671188354492,\n",
              "  1.2087812423706055,\n",
              "  1.220220685005188,\n",
              "  1.317824125289917,\n",
              "  1.190122127532959,\n",
              "  1.1793807744979858,\n",
              "  1.1758216619491577,\n",
              "  1.1973741054534912,\n",
              "  1.2594316005706787,\n",
              "  1.1995632648468018,\n",
              "  1.2973469495773315,\n",
              "  1.2353029251098633,\n",
              "  1.2110443115234375,\n",
              "  1.1367225646972656,\n",
              "  1.2157042026519775,\n",
              "  1.3104820251464844,\n",
              "  1.2694544792175293,\n",
              "  1.234222173690796,\n",
              "  1.3314299583435059,\n",
              "  1.1945078372955322,\n",
              "  1.1957433223724365,\n",
              "  1.142961025238037,\n",
              "  1.2291104793548584,\n",
              "  1.2468069791793823,\n",
              "  1.063078761100769,\n",
              "  1.1964858770370483,\n",
              "  1.2893061637878418,\n",
              "  1.2020453214645386,\n",
              "  1.3653571605682373,\n",
              "  1.1496920585632324,\n",
              "  1.2282047271728516,\n",
              "  1.247223138809204,\n",
              "  1.3178260326385498,\n",
              "  1.1396594047546387,\n",
              "  1.1650197505950928,\n",
              "  1.1697041988372803,\n",
              "  1.126232385635376,\n",
              "  1.1960920095443726,\n",
              "  1.2003345489501953,\n",
              "  1.2289748191833496,\n",
              "  1.3336341381072998,\n",
              "  1.2169444561004639,\n",
              "  1.233954668045044,\n",
              "  1.270561695098877,\n",
              "  1.2927882671356201,\n",
              "  1.2741775512695312,\n",
              "  1.308868169784546,\n",
              "  1.259972095489502,\n",
              "  1.2911152839660645,\n",
              "  1.2014787197113037,\n",
              "  1.1900789737701416,\n",
              "  1.2233211994171143,\n",
              "  1.2944393157958984,\n",
              "  1.3623119592666626,\n",
              "  1.3060369491577148,\n",
              "  1.2158377170562744,\n",
              "  1.200748085975647,\n",
              "  1.2526562213897705,\n",
              "  1.256789207458496,\n",
              "  1.336305856704712,\n",
              "  1.299511194229126,\n",
              "  1.220820426940918,\n",
              "  1.225192666053772,\n",
              "  1.182432770729065,\n",
              "  1.250077724456787,\n",
              "  1.1678745746612549,\n",
              "  1.1429827213287354,\n",
              "  1.255244255065918,\n",
              "  1.2449196577072144,\n",
              "  1.4031693935394287,\n",
              "  1.2289981842041016,\n",
              "  1.1526553630828857,\n",
              "  1.2047762870788574,\n",
              "  1.1265422105789185,\n",
              "  1.3489190340042114,\n",
              "  1.0922973155975342,\n",
              "  1.278121829032898,\n",
              "  1.2142221927642822,\n",
              "  1.3073906898498535,\n",
              "  1.2195353507995605,\n",
              "  1.1549932956695557,\n",
              "  1.2413872480392456,\n",
              "  1.3308379650115967,\n",
              "  1.2104637622833252,\n",
              "  1.2613651752471924,\n",
              "  1.18965482711792,\n",
              "  1.2255003452301025,\n",
              "  1.2395365238189697,\n",
              "  1.1710295677185059,\n",
              "  1.083452582359314,\n",
              "  1.120300531387329,\n",
              "  1.1911758184432983,\n",
              "  1.3357927799224854,\n",
              "  1.296832799911499,\n",
              "  1.1821645498275757,\n",
              "  1.2747020721435547,\n",
              "  1.2511413097381592,\n",
              "  1.182266116142273,\n",
              "  1.2603888511657715,\n",
              "  1.2198646068572998,\n",
              "  1.3304634094238281,\n",
              "  1.2548778057098389,\n",
              "  1.1387410163879395,\n",
              "  1.3458715677261353,\n",
              "  1.1708648204803467,\n",
              "  1.146357774734497,\n",
              "  1.2949929237365723,\n",
              "  1.27469801902771,\n",
              "  1.1557550430297852,\n",
              "  1.1849640607833862,\n",
              "  1.3006155490875244,\n",
              "  1.2520699501037598,\n",
              "  1.210702896118164,\n",
              "  1.343738317489624,\n",
              "  1.1423237323760986,\n",
              "  1.1660737991333008,\n",
              "  1.2135868072509766,\n",
              "  1.2026162147521973,\n",
              "  1.2832415103912354,\n",
              "  1.2629015445709229,\n",
              "  1.2457143068313599,\n",
              "  1.2007191181182861,\n",
              "  1.2896702289581299,\n",
              "  1.091217041015625,\n",
              "  1.2560112476348877,\n",
              "  1.1837552785873413,\n",
              "  1.2331068515777588,\n",
              "  1.2705674171447754,\n",
              "  1.2313549518585205,\n",
              "  1.2244935035705566,\n",
              "  1.123750925064087,\n",
              "  1.1992989778518677,\n",
              "  1.1959049701690674,\n",
              "  1.2840019464492798,\n",
              "  1.208430290222168,\n",
              "  1.1040743589401245,\n",
              "  1.292116403579712,\n",
              "  1.3141098022460938,\n",
              "  1.2406938076019287,\n",
              "  1.2423624992370605,\n",
              "  1.2437667846679688,\n",
              "  1.2872343063354492,\n",
              "  1.2853059768676758,\n",
              "  1.1824278831481934,\n",
              "  1.1597390174865723,\n",
              "  1.2293736934661865,\n",
              "  1.2036691904067993,\n",
              "  1.1910640001296997,\n",
              "  1.3136533498764038,\n",
              "  1.279897928237915,\n",
              "  1.4020832777023315,\n",
              "  1.301314353942871,\n",
              "  1.1620410680770874,\n",
              "  1.1759384870529175,\n",
              "  1.2981939315795898,\n",
              "  1.1798938512802124,\n",
              "  1.2288258075714111,\n",
              "  1.288047194480896,\n",
              "  1.2892206907272339,\n",
              "  1.1066803932189941,\n",
              "  1.2366820573806763,\n",
              "  1.2830226421356201,\n",
              "  1.2734822034835815,\n",
              "  1.1295666694641113,\n",
              "  1.183913230895996,\n",
              "  1.2382900714874268,\n",
              "  1.1471049785614014,\n",
              "  1.2222232818603516,\n",
              "  1.3684273958206177,\n",
              "  1.2595057487487793,\n",
              "  1.2345468997955322,\n",
              "  1.264036774635315,\n",
              "  1.143425703048706,\n",
              "  1.2986963987350464,\n",
              "  1.1733372211456299,\n",
              "  1.234381079673767,\n",
              "  1.25746750831604,\n",
              "  1.2772514820098877,\n",
              "  1.2620398998260498,\n",
              "  1.2141869068145752,\n",
              "  1.2176790237426758,\n",
              "  1.2092411518096924,\n",
              "  1.295272707939148,\n",
              "  1.2585840225219727,\n",
              "  1.268596887588501,\n",
              "  1.2148874998092651,\n",
              "  1.28912353515625,\n",
              "  1.1835342645645142,\n",
              "  1.1590287685394287,\n",
              "  1.2577722072601318,\n",
              "  1.2383930683135986,\n",
              "  1.2439095973968506,\n",
              "  1.266963005065918,\n",
              "  1.1017115116119385,\n",
              "  1.2806854248046875,\n",
              "  1.2277848720550537,\n",
              "  1.260305404663086,\n",
              "  1.1274921894073486,\n",
              "  1.218871831893921,\n",
              "  1.2750431299209595,\n",
              "  1.206481695175171,\n",
              "  1.2674200534820557,\n",
              "  1.136875867843628,\n",
              "  1.2336320877075195,\n",
              "  1.3379895687103271,\n",
              "  1.226832628250122,\n",
              "  1.1185418367385864,\n",
              "  1.1834661960601807,\n",
              "  1.2540125846862793,\n",
              "  1.340329647064209,\n",
              "  1.2397786378860474,\n",
              "  1.252967119216919,\n",
              "  1.301283359527588,\n",
              "  1.1871037483215332,\n",
              "  1.2806769609451294,\n",
              "  1.3744072914123535,\n",
              "  1.1691721677780151,\n",
              "  1.2288703918457031,\n",
              "  1.2671867609024048,\n",
              "  1.23091459274292,\n",
              "  1.3721258640289307,\n",
              "  1.2478556632995605,\n",
              "  1.274890422821045,\n",
              "  1.2524842023849487,\n",
              "  1.234252691268921,\n",
              "  1.3403818607330322,\n",
              "  1.228309154510498,\n",
              "  1.3436393737792969,\n",
              "  1.2079211473464966,\n",
              "  1.2947828769683838,\n",
              "  1.2288932800292969,\n",
              "  1.2601003646850586,\n",
              "  1.1968417167663574,\n",
              "  1.1734983921051025,\n",
              "  1.2185689210891724,\n",
              "  1.1772490739822388,\n",
              "  1.1610040664672852,\n",
              "  1.2271406650543213,\n",
              "  1.2648096084594727,\n",
              "  1.049821138381958,\n",
              "  1.2703654766082764,\n",
              "  1.161160945892334,\n",
              "  1.3240721225738525,\n",
              "  1.2802226543426514,\n",
              "  1.177918791770935,\n",
              "  1.2802276611328125,\n",
              "  1.3231027126312256,\n",
              "  1.1639931201934814,\n",
              "  1.1922277212142944,\n",
              "  1.2420634031295776,\n",
              "  1.3349127769470215,\n",
              "  1.263399362564087,\n",
              "  1.2413609027862549,\n",
              "  1.3306151628494263,\n",
              "  1.2440855503082275,\n",
              "  1.2349042892456055,\n",
              "  1.2689908742904663,\n",
              "  1.2109971046447754,\n",
              "  1.3141281604766846,\n",
              "  1.3062894344329834,\n",
              "  1.2116477489471436,\n",
              "  1.1907439231872559,\n",
              "  1.1913797855377197,\n",
              "  1.3928897380828857,\n",
              "  1.1738107204437256,\n",
              "  1.1684552431106567,\n",
              "  1.1577179431915283,\n",
              "  1.1903884410858154,\n",
              "  1.2338590621948242,\n",
              "  1.200406789779663,\n",
              "  1.2803120613098145,\n",
              "  1.3198838233947754,\n",
              "  1.2095187902450562,\n",
              "  1.1658470630645752,\n",
              "  1.1475272178649902,\n",
              "  1.1597130298614502,\n",
              "  1.2023940086364746,\n",
              "  1.2098028659820557,\n",
              "  1.2444171905517578,\n",
              "  1.3179404735565186,\n",
              "  1.2453198432922363,\n",
              "  1.2578424215316772,\n",
              "  1.1494187116622925,\n",
              "  1.1799918413162231,\n",
              "  1.2059643268585205,\n",
              "  1.1094199419021606,\n",
              "  1.2307394742965698,\n",
              "  1.2136247158050537,\n",
              "  1.2352347373962402,\n",
              "  1.2342480421066284,\n",
              "  1.1639554500579834,\n",
              "  1.2174192667007446,\n",
              "  1.1420049667358398,\n",
              "  1.1186364889144897,\n",
              "  1.290062427520752,\n",
              "  1.3010945320129395,\n",
              "  1.3610206842422485,\n",
              "  1.1914037466049194,\n",
              "  1.188805103302002,\n",
              "  1.1917014122009277,\n",
              "  1.2996389865875244,\n",
              "  1.2347838878631592,\n",
              "  1.2588531970977783,\n",
              "  1.2767091989517212,\n",
              "  1.3660027980804443,\n",
              "  1.2678965330123901,\n",
              "  1.2933297157287598,\n",
              "  1.2440332174301147,\n",
              "  1.201265573501587,\n",
              "  1.2825405597686768,\n",
              "  1.2372814416885376,\n",
              "  1.3045097589492798,\n",
              "  1.1304224729537964,\n",
              "  1.1940460205078125,\n",
              "  1.2363345623016357,\n",
              "  1.2027887105941772,\n",
              "  1.345080852508545,\n",
              "  1.1826670169830322,\n",
              "  1.2473031282424927,\n",
              "  1.3309588432312012,\n",
              "  1.2346959114074707,\n",
              "  1.2464425563812256,\n",
              "  1.2878257036209106,\n",
              "  1.2183079719543457,\n",
              "  1.2366368770599365,\n",
              "  1.3180580139160156,\n",
              "  1.184436559677124,\n",
              "  1.2463325262069702,\n",
              "  1.2439463138580322,\n",
              "  1.1524944305419922,\n",
              "  1.1843087673187256,\n",
              "  1.2331492900848389,\n",
              "  1.185935378074646,\n",
              "  1.216524600982666,\n",
              "  1.2965339422225952,\n",
              "  1.377772331237793,\n",
              "  1.2596557140350342,\n",
              "  1.2968077659606934,\n",
              "  1.2592194080352783,\n",
              "  1.0770461559295654,\n",
              "  1.17006254196167,\n",
              "  1.2753207683563232,\n",
              "  1.26543128490448,\n",
              "  1.174317717552185,\n",
              "  1.2431588172912598,\n",
              "  1.2416999340057373,\n",
              "  1.2351248264312744,\n",
              "  1.2247470617294312,\n",
              "  1.15298593044281,\n",
              "  1.2160353660583496,\n",
              "  1.2881466150283813,\n",
              "  1.2741788625717163,\n",
              "  1.275692343711853,\n",
              "  1.226339340209961,\n",
              "  1.210289478302002,\n",
              "  1.2885291576385498,\n",
              "  1.1501712799072266,\n",
              "  1.2784291505813599,\n",
              "  1.0873684883117676,\n",
              "  1.2260363101959229,\n",
              "  1.1967177391052246,\n",
              "  1.1913564205169678,\n",
              "  1.2308152914047241,\n",
              "  1.2751410007476807,\n",
              "  1.2292695045471191,\n",
              "  1.2191638946533203,\n",
              "  1.224891185760498,\n",
              "  1.2221661806106567,\n",
              "  1.2144650220870972,\n",
              "  1.241924524307251,\n",
              "  1.2668876647949219,\n",
              "  1.185863971710205,\n",
              "  1.1922821998596191,\n",
              "  1.286299467086792,\n",
              "  1.1321138143539429,\n",
              "  1.0918803215026855,\n",
              "  1.219719409942627,\n",
              "  1.2287489175796509,\n",
              "  1.228527307510376,\n",
              "  1.2257509231567383,\n",
              "  1.2040973901748657,\n",
              "  1.1199305057525635,\n",
              "  1.2548565864562988,\n",
              "  1.18141508102417,\n",
              "  1.0830237865447998,\n",
              "  1.20955491065979,\n",
              "  1.261108636856079,\n",
              "  1.2716729640960693,\n",
              "  1.305948257446289,\n",
              "  1.3427867889404297,\n",
              "  1.2238845825195312,\n",
              "  1.229585886001587,\n",
              "  1.2400672435760498,\n",
              "  1.1723763942718506,\n",
              "  1.1891119480133057,\n",
              "  1.2920973300933838,\n",
              "  1.2110693454742432,\n",
              "  1.3477826118469238,\n",
              "  1.2355523109436035,\n",
              "  1.1660383939743042,\n",
              "  1.2783536911010742,\n",
              "  1.1606215238571167,\n",
              "  1.1434733867645264,\n",
              "  1.2077064514160156,\n",
              "  1.2089647054672241,\n",
              "  1.2795495986938477,\n",
              "  1.2221174240112305,\n",
              "  1.2116632461547852,\n",
              "  1.339615821838379,\n",
              "  1.1397106647491455,\n",
              "  1.3162505626678467,\n",
              "  1.2686457633972168,\n",
              "  1.3579390048980713,\n",
              "  1.1461436748504639,\n",
              "  1.2746466398239136,\n",
              "  1.244225025177002,\n",
              "  1.2096645832061768,\n",
              "  1.2616053819656372,\n",
              "  1.3071906566619873,\n",
              "  1.2270963191986084,\n",
              "  1.2228548526763916,\n",
              "  1.1630678176879883,\n",
              "  1.2479079961776733,\n",
              "  1.3056718111038208,\n",
              "  1.1765543222427368,\n",
              "  1.2985830307006836,\n",
              "  1.2750608921051025,\n",
              "  1.3079121112823486,\n",
              "  1.2518681287765503,\n",
              "  1.1822996139526367,\n",
              "  1.3319038152694702,\n",
              "  1.2595075368881226,\n",
              "  1.171278476715088,\n",
              "  1.1590659618377686,\n",
              "  1.2103596925735474,\n",
              "  1.2806367874145508,\n",
              "  1.2406442165374756,\n",
              "  1.231293797492981,\n",
              "  1.236720323562622,\n",
              "  1.221858024597168,\n",
              "  1.1800410747528076,\n",
              "  1.1422337293624878,\n",
              "  1.1825798749923706,\n",
              "  1.2015511989593506,\n",
              "  1.2302687168121338,\n",
              "  1.1983082294464111,\n",
              "  1.2853612899780273,\n",
              "  1.2930357456207275,\n",
              "  1.2077131271362305,\n",
              "  1.1552622318267822,\n",
              "  1.0985867977142334,\n",
              "  1.2931278944015503,\n",
              "  1.2689224481582642,\n",
              "  1.199094533920288,\n",
              "  1.165170669555664,\n",
              "  1.1407638788223267,\n",
              "  1.2491910457611084,\n",
              "  1.3920326232910156,\n",
              "  1.3078510761260986,\n",
              "  1.3170031309127808,\n",
              "  1.304673671722412,\n",
              "  1.1370189189910889,\n",
              "  1.2040388584136963,\n",
              "  1.212838888168335,\n",
              "  1.269578456878662,\n",
              "  1.3268409967422485,\n",
              "  1.1781251430511475,\n",
              "  1.3402092456817627,\n",
              "  1.2194294929504395,\n",
              "  1.132995367050171,\n",
              "  1.1510398387908936,\n",
              "  1.1889344453811646,\n",
              "  1.2633209228515625,\n",
              "  1.2794430255889893,\n",
              "  1.3094260692596436,\n",
              "  1.1992539167404175,\n",
              "  1.2039295434951782,\n",
              "  1.1939871311187744,\n",
              "  1.2726428508758545,\n",
              "  1.1847467422485352,\n",
              "  1.2269396781921387,\n",
              "  1.2409433126449585,\n",
              "  1.1439675092697144,\n",
              "  1.2245666980743408,\n",
              "  1.2723414897918701,\n",
              "  1.1744437217712402,\n",
              "  1.2409470081329346,\n",
              "  1.1691888570785522,\n",
              "  1.230443000793457,\n",
              "  1.2091403007507324,\n",
              "  1.2316702604293823,\n",
              "  1.1829102039337158,\n",
              "  1.1596009731292725,\n",
              "  1.2187222242355347,\n",
              "  1.2866706848144531,\n",
              "  1.1648001670837402,\n",
              "  1.1118407249450684,\n",
              "  1.100204348564148,\n",
              "  1.2963666915893555,\n",
              "  1.251600980758667,\n",
              "  1.2599880695343018,\n",
              "  1.21270751953125,\n",
              "  1.2381832599639893,\n",
              "  1.26485276222229,\n",
              "  1.2630484104156494,\n",
              "  1.2298583984375,\n",
              "  1.2502260208129883,\n",
              "  1.341712236404419,\n",
              "  1.2282195091247559,\n",
              "  1.19206702709198,\n",
              "  1.1859389543533325,\n",
              "  1.221327543258667,\n",
              "  1.2633986473083496,\n",
              "  1.252607822418213,\n",
              "  1.2098654508590698,\n",
              "  1.1501212120056152,\n",
              "  1.2061830759048462,\n",
              "  1.2680659294128418,\n",
              "  1.1870741844177246,\n",
              "  1.1368314027786255,\n",
              "  1.2403359413146973,\n",
              "  1.118186593055725,\n",
              "  1.2039591073989868,\n",
              "  1.153545618057251,\n",
              "  1.26542067527771,\n",
              "  1.166170358657837,\n",
              "  1.0850961208343506,\n",
              "  1.2527716159820557,\n",
              "  1.2276980876922607,\n",
              "  1.2603919506072998,\n",
              "  1.2875665426254272,\n",
              "  1.2395058870315552,\n",
              "  1.1786445379257202,\n",
              "  1.2876427173614502,\n",
              "  1.2664960622787476,\n",
              "  1.306518316268921,\n",
              "  1.2162243127822876,\n",
              "  1.1725150346755981,\n",
              "  1.1799829006195068,\n",
              "  1.1403292417526245,\n",
              "  1.1431914567947388,\n",
              "  1.1824865341186523,\n",
              "  1.1328872442245483,\n",
              "  1.2645800113677979,\n",
              "  1.1142375469207764,\n",
              "  1.3187003135681152,\n",
              "  1.1987310647964478,\n",
              "  1.260411262512207,\n",
              "  1.2455424070358276,\n",
              "  1.089103102684021,\n",
              "  1.1670563220977783,\n",
              "  1.097316026687622,\n",
              "  1.3053319454193115,\n",
              "  1.2163941860198975,\n",
              "  1.210123062133789,\n",
              "  1.322258472442627,\n",
              "  1.1841801404953003,\n",
              "  1.2272412776947021,\n",
              "  1.3063623905181885,\n",
              "  1.2272651195526123,\n",
              "  1.2725518941879272,\n",
              "  1.3477656841278076,\n",
              "  1.1905639171600342,\n",
              "  1.2648229598999023,\n",
              "  1.3292816877365112,\n",
              "  1.2587379217147827,\n",
              "  1.2628917694091797,\n",
              "  1.1597325801849365,\n",
              "  1.149458885192871,\n",
              "  1.1891310214996338,\n",
              "  1.1116633415222168,\n",
              "  1.2478559017181396,\n",
              "  1.1698001623153687,\n",
              "  1.239448070526123,\n",
              "  1.1651113033294678,\n",
              "  1.1883103847503662,\n",
              "  1.1557183265686035,\n",
              "  1.1700172424316406,\n",
              "  1.3176088333129883,\n",
              "  1.122705101966858,\n",
              "  1.2389092445373535,\n",
              "  1.251211166381836,\n",
              "  1.2643260955810547,\n",
              "  1.2694025039672852,\n",
              "  1.1383556127548218,\n",
              "  1.2895050048828125,\n",
              "  1.161928415298462,\n",
              "  1.1618971824645996,\n",
              "  1.2229886054992676,\n",
              "  1.2990593910217285,\n",
              "  1.2529757022857666,\n",
              "  1.1923543214797974,\n",
              "  1.2361887693405151,\n",
              "  1.2596142292022705,\n",
              "  1.2226500511169434,\n",
              "  1.2954634428024292,\n",
              "  1.265114426612854,\n",
              "  1.153378963470459,\n",
              "  1.1950690746307373,\n",
              "  1.2955158948898315,\n",
              "  1.21136474609375,\n",
              "  1.2133516073226929,\n",
              "  1.2288875579833984,\n",
              "  1.2544420957565308,\n",
              "  1.3210008144378662,\n",
              "  1.2631347179412842,\n",
              "  1.360312819480896,\n",
              "  1.2795485258102417,\n",
              "  1.2218544483184814,\n",
              "  1.21372652053833,\n",
              "  1.285996675491333,\n",
              "  1.247240424156189,\n",
              "  1.2249257564544678,\n",
              "  1.3218013048171997,\n",
              "  1.2181377410888672,\n",
              "  1.2998175621032715,\n",
              "  1.168300986289978,\n",
              "  1.3249475955963135,\n",
              "  1.2945518493652344,\n",
              "  1.3245726823806763,\n",
              "  1.2071611881256104,\n",
              "  1.3055634498596191,\n",
              "  1.1986654996871948,\n",
              "  1.1840639114379883,\n",
              "  1.2195122241973877,\n",
              "  1.222274661064148,\n",
              "  1.265602469444275,\n",
              "  1.284084677696228,\n",
              "  1.2151641845703125,\n",
              "  1.1301121711730957,\n",
              "  1.2927719354629517,\n",
              "  1.2118712663650513,\n",
              "  1.2662098407745361,\n",
              "  1.2471017837524414,\n",
              "  1.1993608474731445,\n",
              "  1.2659945487976074,\n",
              "  1.26421058177948,\n",
              "  1.262089490890503,\n",
              "  1.3379645347595215,\n",
              "  1.1698894500732422,\n",
              "  1.1439805030822754,\n",
              "  1.1403888463974,\n",
              "  1.1881464719772339,\n",
              "  1.3668229579925537,\n",
              "  1.288097858428955,\n",
              "  1.1549855470657349,\n",
              "  1.1669363975524902,\n",
              "  1.2513508796691895,\n",
              "  1.1605095863342285,\n",
              "  1.1974644660949707,\n",
              "  1.1554847955703735,\n",
              "  1.2077137231826782,\n",
              "  1.119088888168335,\n",
              "  1.2170771360397339,\n",
              "  1.2389662265777588,\n",
              "  1.1892811059951782,\n",
              "  1.2615993022918701,\n",
              "  1.2012088298797607,\n",
              "  1.2499563694000244,\n",
              "  1.1368305683135986,\n",
              "  1.172271728515625,\n",
              "  1.1538140773773193,\n",
              "  1.2459733486175537,\n",
              "  1.1431312561035156,\n",
              "  1.2917399406433105,\n",
              "  1.2172874212265015,\n",
              "  1.1320513486862183,\n",
              "  1.259921908378601,\n",
              "  1.3426542282104492,\n",
              "  1.2348580360412598,\n",
              "  1.2328765392303467,\n",
              "  1.1892647743225098,\n",
              "  1.2011440992355347,\n",
              "  1.2163456678390503,\n",
              "  1.2178528308868408,\n",
              "  1.3055698871612549,\n",
              "  1.2435334920883179,\n",
              "  1.0469633340835571,\n",
              "  1.2266608476638794,\n",
              "  1.0802810192108154,\n",
              "  1.1741210222244263,\n",
              "  1.175263524055481,\n",
              "  1.1477506160736084,\n",
              "  1.1990244388580322,\n",
              "  1.2132656574249268,\n",
              "  1.157139778137207,\n",
              "  1.2283930778503418,\n",
              "  1.178312063217163,\n",
              "  1.2060999870300293,\n",
              "  1.3299715518951416,\n",
              "  1.1857047080993652,\n",
              "  1.2293598651885986,\n",
              "  1.2629923820495605,\n",
              "  1.2169032096862793,\n",
              "  1.2441024780273438,\n",
              "  1.2285048961639404,\n",
              "  1.2212367057800293,\n",
              "  1.2063297033309937,\n",
              "  1.1403312683105469,\n",
              "  1.259171485900879,\n",
              "  1.1676164865493774,\n",
              "  1.1914095878601074,\n",
              "  1.197618007659912,\n",
              "  1.2919695377349854,\n",
              "  1.2015825510025024,\n",
              "  1.1284894943237305,\n",
              "  1.118847131729126,\n",
              "  1.1416805982589722,\n",
              "  1.2443137168884277,\n",
              "  1.2884314060211182,\n",
              "  1.1705416440963745,\n",
              "  1.200153112411499,\n",
              "  1.2129303216934204,\n",
              "  1.1182595491409302,\n",
              "  1.1451245546340942,\n",
              "  1.2143281698226929,\n",
              "  1.1994956731796265,\n",
              "  1.1771544218063354,\n",
              "  1.3033123016357422,\n",
              "  1.0476171970367432,\n",
              "  1.235842227935791,\n",
              "  1.2552562952041626,\n",
              "  1.1875982284545898,\n",
              "  1.1384402513504028,\n",
              "  1.2109720706939697,\n",
              "  1.3691184520721436,\n",
              "  1.2301280498504639,\n",
              "  1.2272206544876099,\n",
              "  1.2262732982635498,\n",
              "  1.2576886415481567,\n",
              "  1.1673533916473389,\n",
              "  1.2309949398040771,\n",
              "  1.2048890590667725,\n",
              "  1.282291054725647,\n",
              "  1.2000073194503784,\n",
              "  1.1096022129058838,\n",
              "  1.3779411315917969,\n",
              "  1.1219749450683594,\n",
              "  1.179014801979065,\n",
              "  1.2084654569625854],\n",
              " 'val_acc': [0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.328125,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.203125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.234375,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.1875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.21875,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.171875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.1875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.4375,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.21875,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.15625,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.203125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.171875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.171875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.234375,\n",
              "  0.21875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.234375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.25,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.234375,\n",
              "  0.1875,\n",
              "  0.234375,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.203125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.203125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.25,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.21875,\n",
              "  0.390625,\n",
              "  0.21875,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.21875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.21875,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.21875,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.25,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.203125,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.25,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.265625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.421875,\n",
              "  0.46875,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.25,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.390625,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.234375,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.25,\n",
              "  0.4375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.421875,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.21875,\n",
              "  0.25,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.46875,\n",
              "  0.328125,\n",
              "  0.46875,\n",
              "  0.296875,\n",
              "  0.40625,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.453125,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.40625,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.265625,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.328125,\n",
              "  0.359375,\n",
              "  0.234375,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.265625,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.234375,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.40625,\n",
              "  0.328125,\n",
              "  0.390625,\n",
              "  0.359375,\n",
              "  0.390625,\n",
              "  0.375,\n",
              "  0.40625,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.375,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.328125,\n",
              "  0.421875,\n",
              "  0.359375,\n",
              "  0.375,\n",
              "  0.359375,\n",
              "  0.3125,\n",
              "  0.34375,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.28125,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.34375,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.390625,\n",
              "  0.296875,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.40625,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.3125,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.359375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.34375,\n",
              "  0.25,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.328125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.4375,\n",
              "  0.3125,\n",
              "  0.296875,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.296875,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.390625,\n",
              "  0.34375,\n",
              "  0.3125,\n",
              "  0.421875,\n",
              "  0.28125,\n",
              "  0.3125,\n",
              "  0.375,\n",
              "  0.328125,\n",
              "  0.34375,\n",
              "  0.234375,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.3125,\n",
              "  0.28125,\n",
              "  0.359375,\n",
              "  0.265625,\n",
              "  0.296875,\n",
              "  0.25,\n",
              "  0.328125,\n",
              "  0.40625,\n",
              "  0.28125,\n",
              "  0.375,\n",
              "  0.375,\n",
              "  0.296875]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "5267e26a-0fc9-46f0-fc82-5666bf8d1f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/newdata_SEM1.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/newdata_SEM1.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "5bba50ed-2230-4378-ae32-91cca8768b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_67cdd1d3-ea9e-41b9-94f9-eb944cf17971\", \"newdata_SEM1.h5\", 16615536)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}