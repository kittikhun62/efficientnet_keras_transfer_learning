{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOrjy1ioM1nrK0h2hng9g40",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/2Class_datanew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "08e5bc3b-9406-4281-c6ef-8259baaf7f12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class เพิ่ม 4 paper.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "2cd6fa84-bbe8-4889-85bf-69a566ba5184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "825  826  1-s2.0-S2095268622000210-main   \n",
              "826  827  1-s2.0-S2095268622000210-main   \n",
              "827  828  1-s2.0-S2095268622000210-main   \n",
              "828  829  1-s2.0-S2095268622000210-main   \n",
              "829  830  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "825  Integration of preparation of K, Na-embedded a...   \n",
              "826  Integration of preparation of K, Na-embedded a...   \n",
              "827  Integration of preparation of K, Na-embedded a...   \n",
              "828  Integration of preparation of K, Na-embedded a...   \n",
              "829  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "825  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "826  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "827  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "828  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "829  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "825  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "826  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "827  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "828  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "829  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "825          10         0  \n",
              "826          10         0  \n",
              "827          10         0  \n",
              "828          10         0  \n",
              "829          10         0  \n",
              "\n",
              "[830 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5db88b22-d300-41cc-9e51-9e55eabee446\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>826</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>827</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>828</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>829</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>830</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>830 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5db88b22-d300-41cc-9e51-9e55eabee446')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5db88b22-d300-41cc-9e51-9e55eabee446 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5db88b22-d300-41cc-9e51-9e55eabee446');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "876128c6-24e2-408d-8a15-d6354b180be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhT0lEQVR4nO3de7hcVZnn8e9P7gYkgeAxQiSgoAYZEfIgCO1EMyIEJdDtKMhAuDihp2EaxjB21KeV1nEGkEs3jIMPCE20kYsIErkoETkqjxJI6EASwiXBIMSQyC2QqEjCO3+sVVAUdc6pU9d9dn6f59nP2bX2rtpv7bPqrV1rr722IgIzMyufN/U6ADMz6wwneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykn+AKRtELSGkmjqso+J6m/h2GZtVWu53+StE7Sc5JukTQ+L7tS0l/yssp0v6S/qnq8XlLUrPOOXr+vInKCL57NgNN7HYRZh30yIrYFxgGrgYurlp0bEdtWTe+PiF9VHgN75fVGV63zu26/gZHACb54vgmcKWl07QJJH5J0r6S1+e+Huh+eWftExJ+B64GJvY6ljJzgi2c+0A+cWV0oaQfgFuAiYEfgAuAWSTt2O0CzdpH0ZuAzwN29jqWMnOCL6SvAf5e0U1XZ4cCjEfG9iNgQEVcDDwGf7EmEZq35kaTngbXAx0i/XCvOlPR81TS7JxGWgBN8AUXEYuBmYFZV8duBx2tWfRzYuVtxmbXRkRExGtgaOA34haS35WXnRcToqml6z6Ic4Zzgi+urwH/ltQT+e2DXmnXeAazsZlBm7RQRGyPiBmAjcHCv4ykbJ/iCiohlwLXA3+eiW4E9JX1W0uaSPkM6MXVzr2I0a5WSacAYYGmv4ykbJ/hi+xowCiAingE+AcwEngG+AHwiIp7uXXhmTfuxpHXAC8A3gOkRsSQv+0JNH3fX8SbJN/wwMysnH8GbmZWUE7yZWUk5wZuZlZQTvJlZSW3e6wAAxo4dGxMmTKi7bP369YwaNarusiIocnxFjg3aH9+CBQuejoidhl6z90ZynYeRESOMjDhbiXHIOh8RPZ/222+/GMidd9454LIiKHJ8RY4tov3xAfOjAPW5kWkk1/mIkRFjxMiIs5UYh6rzbqIxMyspJ3gzs5JygjczK6lCnGQdzKKVazlh1i29DmNAM/feUNj4ihwbNBffirMP71A0xdFMnd8U9osNn4/gzcxKygnezKyknODNzErKCd7MrKSaPskq6d2kG1JU7E66l+ho0p2I/pDLvxQRtza7HTMza07TCT4iHgb2AZC0GenWcTcCJwIXRsR57QjQzMya064mminA8oiovSm02YgjabykOyU9KGmJpNNz+Q6S5kp6NP8dk8sl6SJJyyQ9IGnf3r4Ds6RdCf5o4Oqqx6flin5F5UNgNoJsAGZGxETgAOBUSROBWcAdEbEHcEd+DHAYsEeeZgCXdD9kszdq+UInSVsCRwBfzEWXAF8HIv89HzipzvNmkD4M9PX10d/fX/f1+7ZJF8QUVZHjK3Js0Fx8A9WTdoqIVcCqPP+ipKXAzsA0YHJebTbQD/xDLv9uHvzpbkmjJY3Lr2PWM+24kvUw4L6IWA1Q+Qsg6TLg5npPiohLgUsBJk2aFJMnT6774hdfdRPnLyruBbcz995Q2PiKHBs0F9+KYyd3JpgBSJoAfACYB/RVJe2ngL48vzPwRNXTnsxlr0vwnTyo6cYXX7V169Z1fZvNGAlxdjLGdnz6j6GqeabmyOUoYHEbtmHWdZK2BX4InBERL0h6dVlEhKRh3bG+kwc13f7i6+/vZ6D4i2QkxNnJGFtK8JJGAR8DTqkqPlfSPqQmmhU1y8xGBElbkJL7VRFxQy5eXTmAkTQOWJPLVwLjq56+Sy4z66mWEnxErAd2rCk7rqWIzHpM6VD9cmBpRFxQtWgOMB04O/+9qar8NEnXAB8E1rr93YqguA20Zr1zEHAcsEjSwlz2JVJiv07SycDjwKfzsluBqcAy4I+ka0HMes4J3qxGRNwFaIDFU+qsH8CpHQ3KrAkei8bMrKSc4M3MSsoJ3syspJzgzcxKygnezKyknODNzErKCd7MrKSc4M3MSsoJ3syspJzgzcxKygnezKyknODNzErKg42ZlcCEWbc09bwVZx/e5kisSHwEb2ZWUk7wZmYl5QRvZlZSrd6TdQXwIrAR2BARkyTtAFwLTCDdk/XTEfFca2GamdlwteMI/iMRsU9ETMqPZwF3RMQewB35sZmZdVknmmimAbPz/GzgyA5sw8zMhtBqgg/gdkkLJM3IZX1Vd5R/CuhrcRtmZtaEVvvBHxwRKyW9FZgr6aHqhRERkqLeE/MXwgyAvr4++vv7626gbxuYufeGFsPsnCLHV+TYoLn4BqonZvZGLSX4iFiZ/66RdCOwP7Ba0riIWCVpHLBmgOdeClwKMGnSpJg8eXLdbVx81U2cv6i412PN3HtDYeMrcmzQXHwrjp3cmWDMSqjpJhpJoyRtV5kHDgEWA3OA6Xm16cBNrQZpZmbD18rhXR9wo6TK63w/In4i6V7gOkknA48Dn249TDMzG66mE3xEPAa8v075M8CUVoIyM7PW+UpWM7OScoI3MyspJ3gzs5Iqbh86M+u4ZsaR9xjyI4cTvJl1nL9IesNNNGZmJeUEb1aHpCskrZG0uKpsB0lzJT2a/47J5ZJ0kaRlkh6QtG/vIjd7jRO8WX1XAofWlA00FPZhwB55mgFc0qUYzQblBG9WR0T8Eni2pnigobCnAd+N5G5gdB6HyaynfJLVrHEDDYW9M/BE1XpP5rJVVWWlGUG1v7+fdevWDWtkz2beTztGDh1unL3QyRid4M2aMNhQ2IM8pxQjqK44djL9/f0MFH89JzTTi6YNI4cON85e6GSMxa1FZsUz0FDYK4HxVevtksusBe5a2Tq3wZs1bqChsOcAx+feNAcAa6uacsx6xkfwZnVIuhqYDIyV9CTwVeBs6g+FfSswFVgG/BE4sesBm9XhBG9WR0QcM8CiNwyFHREBnNrZiMyGz000ZmYl5QRvZlZSTvBmZiXlBG9mVlJNJ3hJ4yXdKelBSUsknZ7Lz5K0UtLCPE1tX7hmZtaoVnrRbABmRsR9krYDFkiam5ddGBHntR6emZk1q+kEny/kWJXnX5S0lDT+hpmZFUBb+sFLmgB8AJgHHAScJul4YD7pKP+5Os8pxcBLRY6vyLFBc/EVfeAosyJpOcFL2hb4IXBGRLwg6RLg60Dkv+cDJ9U+rywDL83ce0Nh4ytybNBcfO0YgMpsU9FSLxpJW5CS+1URcQNARKyOiI0R8QpwGbB/62GamdlwtdKLRsDlwNKIuKCqvPpGB0cBi2ufa2ZmndfK7/eDgOOARZIW5rIvAcdI2ofURLMCOKWFbZiZWZNa6UVzF6A6i25tPhwzM2uX4p6BMzMbptqbhMzce0NDd5Mq641CPFSBmVlJOcGbmZWUE7yZWUm5Dd7MhmXCrFsabtu23vIRvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUm5F42ZWRNqr5ptRLevmPURvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUk5wZuZlZQTvJlZSXUswUs6VNLDkpZJmtWp7ZgVheu8FU1HLnSStBnwLeBjwJPAvZLmRMSDndieWa+5zlsj6l0cNdTQy61cHNWpK1n3B5ZFxGMAkq4BpgGu7FZWrvMjWDNXpY4Eioj2v6j0KeDQiPhcfnwc8MGIOK1qnRnAjPzw3cDDA7zcWODptgfZPkWOr8ixQfvj2zUidmrj6zVsE6vzMDJihJERZysxDlrnezYWTURcClw61HqS5kfEpC6E1JQix1fk2KD48bVbWeo8jIwYYWTE2ckYO3WSdSUwvurxLrnMrKxc561wOpXg7wX2kLSbpC2Bo4E5HdqWWRG4zlvhdKSJJiI2SDoN+CmwGXBFRCxp8uWG/EnbY0WOr8ixQfHja9gmVudhZMQIIyPOjsXYkZOsZmbWe76S1cyspJzgzcxKqrAJvgiXfUsaL+lOSQ9KWiLp9Fx+lqSVkhbmaWrVc76YY35Y0se7EOMKSYtyHPNz2Q6S5kp6NP8dk8sl6aIc3wOS9u1wbO+u2kcLJb0g6Ywi7b+i6WW9l3SFpDWSFleVDbsuSZqe139U0vQ2xzjQZ7IwcUraWtI9ku7PMf5TLt9N0rwcy7X5ZDyStsqPl+XlE6peq7XPQ0QUbiKdpFoO7A5sCdwPTOxBHOOAffP8dsAjwETgLODMOutPzLFuBeyW38NmHY5xBTC2puxcYFaenwWck+enArcBAg4A5nX5f/oUsGuR9l+Rpl7Xe+DDwL7A4mbrErAD8Fj+OybPj2ljjAN9JgsTZ97Wtnl+C2Be3vZ1wNG5/NvAf8vzfwd8O88fDVyb51v+PBT1CP7Vy74j4i9A5bLvroqIVRFxX55/EVgK7DzIU6YB10TESxHxW2AZ6b102zRgdp6fDRxZVf7dSO4GRksa16WYpgDLI+LxQdYpyv7rlZ7W+4j4JfBsTfFw69LHgbkR8WxEPAfMBQ5tY4wDfSYLE2fe1rr8cIs8BfBR4PoBYqzEfj0wRZJow+ehqAl+Z+CJqsdPMnhi7bj8s+kDpG9jgNPyT74rKj8H6U3cAdwuaYHSpfAAfRGxKs8/BfT1ML6Ko4Grqx4XZf8VSRHf/3DrUtfeQ81nslBxStpM0kJgDenLYznwfERsqLO9V2PJy9cCO7YjxqIm+EKRtC3wQ+CMiHgBuAR4J7APsAo4v3fRcXBE7AscBpwq6cPVCyP91utpX9jc1ngE8INcVKT9Zw0qQl2qqPOZfFUR4oyIjRGxD+mK5v2B9/QijqIm+MJc9i1pC1JFuioibgCIiNX5H/gKcBmwv6RjST/7Wopb0k6SHpK0TSPrR8TK/HcNcCPpaOHZStNL/rsmr97Qfs0niPYaTtxDOAy4LyJW51jfsP+GE1+JFfH9rx5mXer4e6j3mSxinAAR8TxwJ3AgqXmocnFp9fZejSUv3x54ph0xFjXBF+Ky79wOdjmwNCIuyGUHS7pX0lpJz5L+eb+PiKtIbWpH57PiuwF7APcMc7OzgCsj4k8NxDdK0naVeeAQUrvdD4FKr4DpwE15fg5wfO5ZcACwtupnbbXzgK8NM+7BHENV80xNu/9RQKXXxhxa338jWSHqfY05DK8u/RT4tKTrctPbIbmsLep9JluI8xBJY9odZz5IG53ntyHdI2ApKVd8aoAYK7F/Cvh5/hXS+uehHWeNOzGRzn4/Qmq7+nKPYjiY9FPvAWBh/rsO+BWwKE+/AT5a9Zwv55gfBg4b5va2Ig0bukuD6+9OOst+P7Cksp9I7Xd3AI8CPwN2iNfO7n8rx7cImDTA625NOtn2tjbsw1Gko5Htq8q+l7f/QK7E49qx/8ow9bLek76EVwEvk9p7Tx6iLt0O/BnYmOvtbfkzcxPwIumk4IltjrH2M7kw77MdgV/kz+dGUtv1Z6vq/ArgBeAP+fkTgJNyjG2NE/gPwL/nGBcDX8nlu5MS9DJSc+VWuXzr/HhZXr571Wu19HnoeYUeSRMwiXSipN6yE4C78vwXckWrTC+Tjsoh/fy6PH+QVgL/i9z1idRNbVnN6/bndX6dX+vHuTJflSvsvcCEqvUDeFee34bUvv046cTNXcA2edkRpC+F5/M23luz3bnA9F7vc0/FnIDPk5pB/pr0Jb4F8Engm6RusP/Wg5iuBq4Fts1fBGuBvfKyPlJ3xAMrCb7X+7AbU1GbaIrqEWCjpNmSDqvq/fE6EXFuRGwbEdsC7yUdNVybF18JbADeReoBcAjwubxsb+rfBOJo4DjSGfR3kn41/CupD+9S4KsDxHsesB/wobzuF4BXJO1J+jCcAewE3Ar8uHLhRbYUeP9AO8I2XZK2JzXhnRoRN0TE+oh4OSJ+HBH/s876P5D0VG7W/GX1+R1JU5UuWnpR6eK3M3P5WEk3S3pe0rOSfiVpwHyVmyj/BvjHiFgXEXeRfh0eB6+e9/l/pAOiTYYT/DBEOltf+Yl4GfAHSXMk9dVbP7e//Qj4l4i4La83lXTmf32kE6MXkhI4wGjST9ta/xoRyyNiLeln8PKI+FmkLlU/IH1R1G77TaSfoKdHxMpIJzV/HREvAZ8BbomIuRHxMumLYBvSF0HFizkes1oHkpoVbmxw/dtI7cdvBe4j/fqsuBw4JSK2A94H/DyXzyQ1E+1EOvr+EoP3jNkT2BARj1SV3Q+0s7PAiNOzOzqNVBGxlNQcg6T3AP8G/DP1T9BcDjwcEefkx7uSfsquSueKgPQlW+nr+hzp6rxaq6vm/1Tn8bZ1njOW9CFcXmfZ20nNNpX39IqkJ3h9H9vtSM03ZrV2BJ6O1/p0DyoirqjMSzoLeE7S9vmA5WVgoqT7I11w9Fxe9WXSVau7RsQy0nmvwWxLarKstpb6n6dNho/gWxARD5GaXN5Xu0xpHJE9SSeqKp4AXiINLTA6T2+JiMpRxgP5Oe3wNOkE2DvrLPs96cumEqtI3bGqu2C9l3QEZFbrGWBsVZe/AeULfs6WtFzSC6STnZAOQCA1q0wFHpf0C0kH5vJvkk463i7pMQ09Ls864C01ZW+h/i/iTYYT/DBIeo+kmZJ2yY/Hk7oA3l2z3mHA3wNHRVV3x0jds24Hzpf0FklvkvROSf8xr3IPqa9sy1fURepjfgVwgaS35w/agZK2Io2JcbikKblP8UzSF8+vc/xbk9ru57Yah5XSb0j15cgG1v0sqevufyJ1MJiQywUQEfdGxDRS882PSHWTiHgxImZGxO6kDgGflzRlkO08AmwuaY+qsveTOhJsspzgh+dF4IPAPEnrSYl9MSlBVvsMqe1wqaR1efp2XnY8aSCpB0k/R68n/RQl0vgjVwL/pU3xnknqjngvqdvjOcCbIuLhvI2LSUf6nwQ+mbdPftwfEb9vUxxWIrlp5SvAtyQdKenNkrbIHQ/OrVl9O9KXwTPAm4H/XVkgaUtJx+bmmpdJTSyv5GWfkPSu/OtyLanr4yuDxLQeuAH4Wr4+5CDSF8v3qra3NakrMsBW+XG59bobj6fXT6QvhofI3Rl7FMM84H293heeij0BxwLzgfWk8V9uIZ2oP4vcTZLUNl7pF/846QAnSL3ItgR+QjrQqXT5PTg/73+QmnPWk062/mMD8exA+hWwHvgd8Nma5VE79XofdnryLfvMzErKTTRmZiXlbpJmNiJIegfp3FU9EyPid92MZyRwE42ZWUkV4gh+7NixMWHChLrL1q9fz6hRo7obUAF5PySD7YcFCxY8HRE7dTmkprjOD837IWmlzhciwU+YMIH58+fXXdbf38/kyZO7G1ABeT8kg+0HSYPdDrBQXOeH5v2QtFLnfZLVzKyknODNzErKCd7MrKQK0QZvQ1u0ci0nzLplWM9ZcfbhHYrGOsn/a2sXH8GbmZWUE7yZWUk5wZsNIA+x/O+Sbs6Pd5M0T9IySddWbnGY73p/bS6fJ2lCTwM3y5zgzQZ2OunetBXnABdGxLtIIyBWbuZyMvBcLr8wr2fWc0MmeEnvlrSwanpB0hmSzso3ya2UT616zhfz0czDkj7e2bdg1n75pi6HA9/JjwV8lDR+P8BsXrvhxbT8mLx8iqruyWjWK0P2ool0c4h9IP1kJd3W7UbgRNLRzHnV60uaSLqJ9F6ke3/+TNKeEbGxvaGbddQ/A1/gtXt67gg8H6/dh/RJXruH7c7k++pGxAZJa/P6T1e/oKQZwAyAvr4++vv76264bxuYuXdDtzt91UCvNZKtW7eudO9r0cq1w37Obttv1vR+GG43ySnA8oh4fJADlGnANRHxEvBbScuA/Um3+TIrPEmfANZExAJJk9v1uhFxKXApwKRJk2Kgy88vvuomzl80vI/mimPrv9ZIVsahCobb/RXgykNHNb0fhpvgjwaurnp8mqTjSXd1mRnprug78/p7lFYf6byq0aOZMn6LN8NHdUmX6sNBwBG52XFr0s2b/4V0v9zN81H8Lrx2k/KVpJuWP5lvRL096RZ1Zj3VcILPPQaOAL6Yiy4Bvk669dXXgfOBkxp9vUaPZsr4Ld4MH9Ul3agPEfFFcj3PR/BnRsSxkn4AfAq4BphOuhUdwJz8+Dd5+c/D43BbAQynF81hwH0RsRogIlZHxMaIeAW4jNQMA68dzVRUH+mYjWT/AHw+NzvuCFyeyy8Hdszlnwdm9Sg+s9cZziHhMVQ1z0gaFxGr8sOjgMV5fg7wfUkXkE6y7gHc04ZYzbouIvqB/jz/GK8dyFSv82fgP3c1MLMGNJTgJY0CPgacUlV8rqR9SE00KyrLImKJpOtIt9baAJzqHjRmZt3XUIKPiPWkn6TVZccNsv43gG+0FpqZmbXCV7KamZWUE7yZWUk5wZuZlZQTvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUk5wZuZlZQTvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUk5wZuZlZQTvJlZSTnBm5mVlBO8mVlJNZTgJa2QtEjSQknzc9kOkuZKejT/HZPLJekiScskPSBp306+ATMzq284R/AfiYh9ImJSfjwLuCMi9gDuyI8BDgP2yNMM4JJ2BWtmZo1rpYlmGjA7z88Gjqwq/24kdwOjJY1rYTtmZtaERhN8ALdLWiBpRi7ri4hVef4poC/P7ww8UfXcJ3OZmZl10eYNrndwRKyU9FZgrqSHqhdGREiK4Ww4f1HMAOjr66O/v7/ueuvWrRtw2aakbxuYufeGYT2njPvN9cGscQ0l+IhYmf+ukXQjsD+wWtK4iFiVm2DW5NVXAuOrnr5LLqt9zUuBSwEmTZoUkydPrrvt/v5+Blq2Kbn4qps4f1Gj38fJimMndyaYHnJ9MGvckE00kkZJ2q4yDxwCLAbmANPzatOBm/L8HOD43JvmAGBtVVOOmZl1SSOHhH3AjZIq638/In4i6V7gOkknA48Dn87r3wpMBZYBfwRObHvUZmY2pCETfEQ8Bry/TvkzwJQ65QGc2pbozMysab6S1cyspJzgzcxKygnezKyknODNzErKCd6shqTxku6U9KCkJZJOz+UeYM9GFCd4szfaAMyMiInAAcCpkibiAfZshHGCN6sREasi4r48/yKwlDSekgfYsxFleNe+m21iJE0APgDMY/gD7L3uCu5Gx1/yuENJGccdGu7/FVrbD07wZgOQtC3wQ+CMiHghX80NNDfAXqPjL3ncoaSM4w6dMOuWYT/nykNHNb0f3ERjVoekLUjJ/aqIuCEXr640vTQzwJ5ZtznBm9VQOlS/HFgaERdULfIAezaiuInG7I0OAo4DFklamMu+BJyNB9izEcQJ3qxGRNwFaIDFHmDPRgw30ZiZlZQTvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUkNmeAHGTr1LEkrJS3M09Sq53wxD536sKSPd/INmJlZfY30g68MnXqfpO2ABZLm5mUXRsR51SvnYVWPBvYC3g78TNKeEbGxnYGbmdnghjyCH2To1IFMA66JiJci4rekq/v2b0ewZmbWuGFdyVozdOpBwGmSjgfmk47ynyMl/7urnlYZOrX2tRoaOrWMQ4Y2w0PIJq4PZo1rOMHXGTr1EuDrQOS/5wMnNfp6jQ6dWsYhQ5vhIWQT1wezxjXUi6be0KkRsToiNkbEK8BlvNYM46FTzcwKoJFeNHWHTq25JdlRwOI8Pwc4WtJWknYj3afynvaFbGZmjWjkN/9AQ6ceI2kfUhPNCuAUgIhYIuk64EFSD5xT3YPGzKz7hkzwgwydeusgz/kG8I0W4jIzsxb5SlYzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyupjiV4SYdKeljSMkmzOrUds6Jwnbei6UiCl7QZ8C3gMGAi6QbdEzuxLbMicJ23IurUEfz+wLKIeCwi/gJcA0zr0LbMisB13gpn8w697s7AE1WPnwQ+WL2CpBnAjPxwnaSHB3itscDTbY9w5Bn2ftA5HYqktwbbD7t2M5AaPa3zm+D/epPxkXOar/OdSvBDiohLgUuHWk/S/IiY1IWQCs37IRnJ+8F1fni8H5JW9kOnmmhWAuOrHu+Sy8zKynXeCqdTCf5eYA9Ju0naEjgamNOhbZkVgeu8FU5HmmgiYoOk04CfApsBV0TEkiZfbsiftJsI74ekkPvBdb4jvB+SpveDIqKdgZiZWUH4SlYzs5JygjczK6lCJHhJp0taLGmJpDPqLJ8saa2khXn6Sg/C7AhJV0haI2lxVdkOkuZKejT/HTPAc6fndR6VNL17Ubdfi/thY1XdGDEnNoca2kDSVpKuzcvnSZrQgzA7roH9cIKkP1T9jz/Xizg7rd5noGa5JF2U99MDkvYd8kUjoqcT8D5gMfBm0knfnwHvqllnMnBzr2Pt0Pv/MLAvsLiq7FxgVp6fBZxT53k7AI/lv2Py/Jhev59u74e8bF2v42/i/W4GLAd2B7YE7gcm1qzzd8C38/zRwLW9jrtH++EE4P/2OtYu7Is3fAZqlk8FbgMEHADMG+o1i3AE/15SoH+MiA3AL4C/7nFMXRMRvwSerSmeBszO87OBI+s89ePA3Ih4NiKeA+YCh3Yqzk5rYT+MVI0MbVD9/q8HpkhSF2PsBg/xkA3wGag2DfhuJHcDoyWNG+w1i5DgFwN/JWlHSW8mfUuNr7PegZLul3SbpL26G2LX9UXEqjz/FNBXZ516l8bv3OnAuqyR/QCwtaT5ku6WdGR3QmtZI/+/V9fJBz9rgR27El33NFqP/yY3S1wvqV5+2BQM+zPfs6EKKiJiqaRzgNuB9cBCYGPNavcBu0bEOklTgR8Be3Qzzl6JiJC0yfdlHWI/7BoRKyXtDvxc0qKIWN7N+KyjfgxcHREvSTqF9Kvmoz2OaUQowhE8EXF5ROwXER8GngMeqVn+QkSsy/O3AltIGtuDULtldeWnV/67ps46m8Kl8Y3sByJiZf77GNAPfKBbAbagkf/fq+tI2hzYHnimK9F1z5D7ISKeiYiX8sPvAPt1KbaiGfZnvhAJXtJb8993kNrfv1+z/G2VtkdJ+5PiLltFrzYHqPSKmQ7cVGednwKHSBqTe5ccksvKZMj9kN//Vnl+LHAQ8GDXImxeI0MbVL//TwE/j3y2rUSG3A817cxHAEu7GF+RzAGOz71pDgDWVjVh1tfrM8e5vv6K9KG8H5iSy/4W+Ns8fxqwJC+/G/hQr2Nu43u/GlgFvExqUzuZ1M56B/AoqVfRDnndScB3qp57ErAsTyf2+r30Yj8AHwIW5bqxCDi51+9lGO95KunX6nLgy7nsa8AReX5r4Af5/3sPsHuvY+7Rfvg/VZ//O4H39DrmDu2Hep+B6jwo0k1llue6Pmmo1/RQBWZmJVWIJhozM2s/J3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3Myup/w/ruxBZvn2tkgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "9b4ad114-3b89-4516-ef19-e31a1c89de51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "c9d7e153-718e-4002-86da-042e55cb7b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "f860a9a7-5c23-45c6-b3ca-1ebda4e43346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 1067, done.\u001b[K\n",
            "remote: Counting objects: 100% (230/230), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 1067 (delta 114), reused 161 (delta 79), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (1067/1067), 13.93 MiB | 10.45 MiB/s, done.\n",
            "Resolving deltas: 100% (611/611), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-800')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '801-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-800')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '801-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-800')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '801-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(629,729)]\n",
        "train = df[df['No'].between(1,628)]\n",
        "test = df[df['No'].between(730,830)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-800']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='801-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-800']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='801-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-800']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='801-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "679bfecb-00db-4e84-d08c-6e76ab833391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 331\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 628  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "ddd0a60a-2da3-47c8-b4b0-0fb74dc94ff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "5ea03460-7018-453a-bb0d-434ad0bd38db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "1f3590fa-513e-4458-cc20-2c68694b9591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "b85df943-ad4c-4813-a0db-bf564cb25d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "7630e91e-d368-42b2-88bc-6bd42c0621f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 628 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=6e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7766fa66-a72d-4477-f2e0-7020eaf10dd9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-4a9c57c61c19>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "39/39 [==============================] - 18s 115ms/step - loss: 1.6602 - acc: 0.5212 - val_loss: 0.8831 - val_acc: 0.5833\n",
            "Epoch 2/1000\n",
            "39/39 [==============================] - 3s 76ms/step - loss: 1.2081 - acc: 0.4951 - val_loss: 0.6843 - val_acc: 0.5833\n",
            "Epoch 3/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1.0859 - acc: 0.5507 - val_loss: 0.6872 - val_acc: 0.5938\n",
            "Epoch 4/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1.2017 - acc: 0.5359 - val_loss: 0.7100 - val_acc: 0.5625\n",
            "Epoch 5/1000\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.0694 - acc: 0.5261 - val_loss: 0.6969 - val_acc: 0.5625\n",
            "Epoch 6/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1.1308 - acc: 0.5131 - val_loss: 0.7346 - val_acc: 0.5208\n",
            "Epoch 7/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 1.1618 - acc: 0.4984 - val_loss: 0.7202 - val_acc: 0.5729\n",
            "Epoch 8/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1.0436 - acc: 0.5196 - val_loss: 0.7347 - val_acc: 0.5729\n",
            "Epoch 9/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1.0942 - acc: 0.5310 - val_loss: 0.7569 - val_acc: 0.5312\n",
            "Epoch 10/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 1.0578 - acc: 0.5458 - val_loss: 0.7311 - val_acc: 0.5521\n",
            "Epoch 11/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1.0111 - acc: 0.5392 - val_loss: 0.7490 - val_acc: 0.5312\n",
            "Epoch 12/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.9642 - acc: 0.5523 - val_loss: 0.7331 - val_acc: 0.5312\n",
            "Epoch 13/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 1.0627 - acc: 0.5441 - val_loss: 0.7476 - val_acc: 0.5521\n",
            "Epoch 14/1000\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 1.0318 - acc: 0.5441 - val_loss: 0.7637 - val_acc: 0.5521\n",
            "Epoch 15/1000\n",
            "39/39 [==============================] - 8s 199ms/step - loss: 1.0074 - acc: 0.5752 - val_loss: 0.7538 - val_acc: 0.5000\n",
            "Epoch 16/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 1.0285 - acc: 0.5539 - val_loss: 0.7600 - val_acc: 0.5000\n",
            "Epoch 17/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.9713 - acc: 0.5784 - val_loss: 0.7699 - val_acc: 0.5208\n",
            "Epoch 18/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.9591 - acc: 0.5686 - val_loss: 0.7844 - val_acc: 0.5000\n",
            "Epoch 19/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 1.0432 - acc: 0.5180 - val_loss: 0.7742 - val_acc: 0.5521\n",
            "Epoch 20/1000\n",
            "39/39 [==============================] - 3s 79ms/step - loss: 0.9864 - acc: 0.5392 - val_loss: 0.7648 - val_acc: 0.4583\n",
            "Epoch 21/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.9500 - acc: 0.5408 - val_loss: 0.7939 - val_acc: 0.5000\n",
            "Epoch 22/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.9994 - acc: 0.5441 - val_loss: 0.7829 - val_acc: 0.4896\n",
            "Epoch 23/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.9115 - acc: 0.6046 - val_loss: 0.7643 - val_acc: 0.5000\n",
            "Epoch 24/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 1.0459 - acc: 0.5131 - val_loss: 0.7974 - val_acc: 0.4583\n",
            "Epoch 25/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.8955 - acc: 0.5768 - val_loss: 0.8054 - val_acc: 0.4583\n",
            "Epoch 26/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.9285 - acc: 0.5588 - val_loss: 0.7936 - val_acc: 0.4792\n",
            "Epoch 27/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.9032 - acc: 0.5768 - val_loss: 0.7910 - val_acc: 0.4792\n",
            "Epoch 28/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.9286 - acc: 0.5686 - val_loss: 0.7835 - val_acc: 0.5104\n",
            "Epoch 29/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.9264 - acc: 0.5654 - val_loss: 0.8028 - val_acc: 0.4688\n",
            "Epoch 30/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.9139 - acc: 0.5605 - val_loss: 0.7901 - val_acc: 0.4896\n",
            "Epoch 31/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.8707 - acc: 0.5850 - val_loss: 0.7907 - val_acc: 0.4688\n",
            "Epoch 32/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.9275 - acc: 0.5490 - val_loss: 0.7821 - val_acc: 0.5104\n",
            "Epoch 33/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.9344 - acc: 0.5490 - val_loss: 0.8050 - val_acc: 0.4792\n",
            "Epoch 34/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.9653 - acc: 0.5850 - val_loss: 0.7930 - val_acc: 0.4896\n",
            "Epoch 35/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.8923 - acc: 0.5899 - val_loss: 0.7965 - val_acc: 0.5312\n",
            "Epoch 36/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.8982 - acc: 0.5703 - val_loss: 0.7796 - val_acc: 0.5000\n",
            "Epoch 37/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.8974 - acc: 0.5866 - val_loss: 0.7887 - val_acc: 0.5000\n",
            "Epoch 38/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.8514 - acc: 0.5882 - val_loss: 0.7439 - val_acc: 0.5104\n",
            "Epoch 39/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.8150 - acc: 0.5997 - val_loss: 0.7662 - val_acc: 0.5312\n",
            "Epoch 40/1000\n",
            "39/39 [==============================] - 3s 79ms/step - loss: 0.9011 - acc: 0.5833 - val_loss: 0.7432 - val_acc: 0.5312\n",
            "Epoch 41/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.9048 - acc: 0.5833 - val_loss: 0.7558 - val_acc: 0.5000\n",
            "Epoch 42/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.8749 - acc: 0.5948 - val_loss: 0.7807 - val_acc: 0.4896\n",
            "Epoch 43/1000\n",
            "39/39 [==============================] - 3s 75ms/step - loss: 0.8842 - acc: 0.5539 - val_loss: 0.7791 - val_acc: 0.5000\n",
            "Epoch 44/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.9213 - acc: 0.5621 - val_loss: 0.7791 - val_acc: 0.5312\n",
            "Epoch 45/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.7984 - acc: 0.6013 - val_loss: 0.7931 - val_acc: 0.5312\n",
            "Epoch 46/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.8220 - acc: 0.6193 - val_loss: 0.7854 - val_acc: 0.5312\n",
            "Epoch 47/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.8962 - acc: 0.5572 - val_loss: 0.7803 - val_acc: 0.5104\n",
            "Epoch 48/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.8038 - acc: 0.6029 - val_loss: 0.7757 - val_acc: 0.5312\n",
            "Epoch 49/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.8034 - acc: 0.6258 - val_loss: 0.7697 - val_acc: 0.4896\n",
            "Epoch 50/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.8327 - acc: 0.5801 - val_loss: 0.7744 - val_acc: 0.5104\n",
            "Epoch 51/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.8874 - acc: 0.5997 - val_loss: 0.7994 - val_acc: 0.4688\n",
            "Epoch 52/1000\n",
            "39/39 [==============================] - 8s 203ms/step - loss: 0.8241 - acc: 0.6078 - val_loss: 0.7538 - val_acc: 0.5625\n",
            "Epoch 53/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.7719 - acc: 0.6095 - val_loss: 0.7786 - val_acc: 0.4688\n",
            "Epoch 54/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.8549 - acc: 0.5980 - val_loss: 0.7802 - val_acc: 0.4896\n",
            "Epoch 55/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.8141 - acc: 0.5931 - val_loss: 0.7984 - val_acc: 0.4271\n",
            "Epoch 56/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.7836 - acc: 0.5964 - val_loss: 0.7889 - val_acc: 0.4583\n",
            "Epoch 57/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.8461 - acc: 0.5801 - val_loss: 0.7690 - val_acc: 0.4583\n",
            "Epoch 58/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 0.8312 - acc: 0.6160 - val_loss: 0.7841 - val_acc: 0.4792\n",
            "Epoch 59/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.8193 - acc: 0.6176 - val_loss: 0.7852 - val_acc: 0.4583\n",
            "Epoch 60/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.7736 - acc: 0.6242 - val_loss: 0.7766 - val_acc: 0.5208\n",
            "Epoch 61/1000\n",
            "39/39 [==============================] - 4s 78ms/step - loss: 0.8423 - acc: 0.5899 - val_loss: 0.7382 - val_acc: 0.5625\n",
            "Epoch 62/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.8268 - acc: 0.5899 - val_loss: 0.7638 - val_acc: 0.5208\n",
            "Epoch 63/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.8239 - acc: 0.6013 - val_loss: 0.7524 - val_acc: 0.5312\n",
            "Epoch 64/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.8443 - acc: 0.5735 - val_loss: 0.7387 - val_acc: 0.5729\n",
            "Epoch 65/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.9217 - acc: 0.5507 - val_loss: 0.7530 - val_acc: 0.5208\n",
            "Epoch 66/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.7853 - acc: 0.6176 - val_loss: 0.7575 - val_acc: 0.5417\n",
            "Epoch 67/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.8188 - acc: 0.5882 - val_loss: 0.7636 - val_acc: 0.5104\n",
            "Epoch 68/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.8175 - acc: 0.5980 - val_loss: 0.7755 - val_acc: 0.4896\n",
            "Epoch 69/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.7051 - acc: 0.6601 - val_loss: 0.7527 - val_acc: 0.4792\n",
            "Epoch 70/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.8026 - acc: 0.6111 - val_loss: 0.7635 - val_acc: 0.4792\n",
            "Epoch 71/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.7366 - acc: 0.6225 - val_loss: 0.7626 - val_acc: 0.4583\n",
            "Epoch 72/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.7412 - acc: 0.6258 - val_loss: 0.7784 - val_acc: 0.5208\n",
            "Epoch 73/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.7252 - acc: 0.6405 - val_loss: 0.7575 - val_acc: 0.4792\n",
            "Epoch 74/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.7170 - acc: 0.6618 - val_loss: 0.7605 - val_acc: 0.5000\n",
            "Epoch 75/1000\n",
            "39/39 [==============================] - 9s 206ms/step - loss: 0.7708 - acc: 0.6275 - val_loss: 0.7692 - val_acc: 0.5000\n",
            "Epoch 76/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.8186 - acc: 0.5719 - val_loss: 0.7506 - val_acc: 0.4792\n",
            "Epoch 77/1000\n",
            "39/39 [==============================] - 5s 97ms/step - loss: 0.7555 - acc: 0.6324 - val_loss: 0.7486 - val_acc: 0.5208\n",
            "Epoch 78/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.7264 - acc: 0.6438 - val_loss: 0.7464 - val_acc: 0.5312\n",
            "Epoch 79/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.7831 - acc: 0.5866 - val_loss: 0.7439 - val_acc: 0.4583\n",
            "Epoch 80/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.7207 - acc: 0.6405 - val_loss: 0.7558 - val_acc: 0.4792\n",
            "Epoch 81/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.6833 - acc: 0.6601 - val_loss: 0.7551 - val_acc: 0.5208\n",
            "Epoch 82/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.7672 - acc: 0.6438 - val_loss: 0.7517 - val_acc: 0.5521\n",
            "Epoch 83/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.7392 - acc: 0.6356 - val_loss: 0.7527 - val_acc: 0.5417\n",
            "Epoch 84/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.7129 - acc: 0.6307 - val_loss: 0.7607 - val_acc: 0.4688\n",
            "Epoch 85/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.7784 - acc: 0.6307 - val_loss: 0.7337 - val_acc: 0.5417\n",
            "Epoch 86/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.7395 - acc: 0.6127 - val_loss: 0.7408 - val_acc: 0.5312\n",
            "Epoch 87/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.7615 - acc: 0.6405 - val_loss: 0.7424 - val_acc: 0.5104\n",
            "Epoch 88/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.7155 - acc: 0.6373 - val_loss: 0.7227 - val_acc: 0.5312\n",
            "Epoch 89/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.7241 - acc: 0.6520 - val_loss: 0.7384 - val_acc: 0.5104\n",
            "Epoch 90/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.6903 - acc: 0.6487 - val_loss: 0.7391 - val_acc: 0.4896\n",
            "Epoch 91/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.7171 - acc: 0.6585 - val_loss: 0.7438 - val_acc: 0.5208\n",
            "Epoch 92/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.6918 - acc: 0.6552 - val_loss: 0.7289 - val_acc: 0.4792\n",
            "Epoch 93/1000\n",
            "39/39 [==============================] - 3s 79ms/step - loss: 0.6823 - acc: 0.6650 - val_loss: 0.7341 - val_acc: 0.5312\n",
            "Epoch 94/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.6925 - acc: 0.6373 - val_loss: 0.7381 - val_acc: 0.5104\n",
            "Epoch 95/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.7299 - acc: 0.6503 - val_loss: 0.7407 - val_acc: 0.5208\n",
            "Epoch 96/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.7004 - acc: 0.6209 - val_loss: 0.7474 - val_acc: 0.4896\n",
            "Epoch 97/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.7323 - acc: 0.6536 - val_loss: 0.7589 - val_acc: 0.4583\n",
            "Epoch 98/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 0.7320 - acc: 0.6111 - val_loss: 0.7739 - val_acc: 0.5104\n",
            "Epoch 99/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.6828 - acc: 0.6373 - val_loss: 0.7518 - val_acc: 0.4688\n",
            "Epoch 100/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.6791 - acc: 0.6667 - val_loss: 0.7688 - val_acc: 0.4792\n",
            "Epoch 101/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.7113 - acc: 0.6699 - val_loss: 0.7573 - val_acc: 0.4792\n",
            "Epoch 102/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.7091 - acc: 0.6454 - val_loss: 0.7837 - val_acc: 0.4896\n",
            "Epoch 103/1000\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.7331 - acc: 0.6193 - val_loss: 0.7442 - val_acc: 0.5000\n",
            "Epoch 104/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.6953 - acc: 0.6356 - val_loss: 0.7400 - val_acc: 0.4896\n",
            "Epoch 105/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.6898 - acc: 0.6454 - val_loss: 0.7733 - val_acc: 0.4792\n",
            "Epoch 106/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.7062 - acc: 0.6389 - val_loss: 0.7618 - val_acc: 0.5104\n",
            "Epoch 107/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.6724 - acc: 0.6634 - val_loss: 0.7802 - val_acc: 0.4688\n",
            "Epoch 108/1000\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 0.6959 - acc: 0.6291 - val_loss: 0.7502 - val_acc: 0.4896\n",
            "Epoch 109/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.6910 - acc: 0.6503 - val_loss: 0.7478 - val_acc: 0.4583\n",
            "Epoch 110/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.6786 - acc: 0.6536 - val_loss: 0.7570 - val_acc: 0.5312\n",
            "Epoch 111/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.6812 - acc: 0.6503 - val_loss: 0.7591 - val_acc: 0.4479\n",
            "Epoch 112/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.6835 - acc: 0.6487 - val_loss: 0.7595 - val_acc: 0.4896\n",
            "Epoch 113/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.6764 - acc: 0.6454 - val_loss: 0.7546 - val_acc: 0.4896\n",
            "Epoch 114/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.7063 - acc: 0.6503 - val_loss: 0.7591 - val_acc: 0.5104\n",
            "Epoch 115/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.6742 - acc: 0.6634 - val_loss: 0.7456 - val_acc: 0.5208\n",
            "Epoch 116/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.6495 - acc: 0.6552 - val_loss: 0.7432 - val_acc: 0.4896\n",
            "Epoch 117/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6592 - acc: 0.6569 - val_loss: 0.7287 - val_acc: 0.5312\n",
            "Epoch 118/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.6831 - acc: 0.6242 - val_loss: 0.7213 - val_acc: 0.5729\n",
            "Epoch 119/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.6948 - acc: 0.6585 - val_loss: 0.7413 - val_acc: 0.4792\n",
            "Epoch 120/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.6626 - acc: 0.6634 - val_loss: 0.7293 - val_acc: 0.5417\n",
            "Epoch 121/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.6400 - acc: 0.6569 - val_loss: 0.7362 - val_acc: 0.5000\n",
            "Epoch 122/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.6439 - acc: 0.6634 - val_loss: 0.7204 - val_acc: 0.5625\n",
            "Epoch 123/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.7159 - acc: 0.6438 - val_loss: 0.7338 - val_acc: 0.4792\n",
            "Epoch 124/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.6611 - acc: 0.6683 - val_loss: 0.7449 - val_acc: 0.5312\n",
            "Epoch 125/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6269 - acc: 0.6683 - val_loss: 0.7289 - val_acc: 0.5521\n",
            "Epoch 126/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.6435 - acc: 0.6863 - val_loss: 0.7187 - val_acc: 0.5521\n",
            "Epoch 127/1000\n",
            "39/39 [==============================] - 8s 213ms/step - loss: 0.6542 - acc: 0.6601 - val_loss: 0.7305 - val_acc: 0.4896\n",
            "Epoch 128/1000\n",
            "39/39 [==============================] - 10s 241ms/step - loss: 0.6320 - acc: 0.6928 - val_loss: 0.7400 - val_acc: 0.4688\n",
            "Epoch 129/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.6970 - acc: 0.6389 - val_loss: 0.7181 - val_acc: 0.5104\n",
            "Epoch 130/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.6277 - acc: 0.6863 - val_loss: 0.7055 - val_acc: 0.5938\n",
            "Epoch 131/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6315 - acc: 0.6863 - val_loss: 0.7269 - val_acc: 0.5938\n",
            "Epoch 132/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.6197 - acc: 0.6944 - val_loss: 0.7185 - val_acc: 0.5417\n",
            "Epoch 133/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.6314 - acc: 0.6520 - val_loss: 0.7184 - val_acc: 0.5625\n",
            "Epoch 134/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.6733 - acc: 0.6340 - val_loss: 0.7454 - val_acc: 0.5312\n",
            "Epoch 135/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.6662 - acc: 0.6634 - val_loss: 0.7258 - val_acc: 0.5312\n",
            "Epoch 136/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.6117 - acc: 0.6846 - val_loss: 0.7191 - val_acc: 0.5312\n",
            "Epoch 137/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.6709 - acc: 0.6487 - val_loss: 0.7130 - val_acc: 0.5521\n",
            "Epoch 138/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.6428 - acc: 0.6814 - val_loss: 0.7228 - val_acc: 0.5521\n",
            "Epoch 139/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.6075 - acc: 0.6814 - val_loss: 0.7178 - val_acc: 0.5312\n",
            "Epoch 140/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 0.6212 - acc: 0.6797 - val_loss: 0.7164 - val_acc: 0.5312\n",
            "Epoch 141/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 0.6171 - acc: 0.6830 - val_loss: 0.7167 - val_acc: 0.5625\n",
            "Epoch 142/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.6774 - acc: 0.6699 - val_loss: 0.7089 - val_acc: 0.5625\n",
            "Epoch 143/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.6086 - acc: 0.6814 - val_loss: 0.7084 - val_acc: 0.5521\n",
            "Epoch 144/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.6295 - acc: 0.6716 - val_loss: 0.7122 - val_acc: 0.5729\n",
            "Epoch 145/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.6711 - acc: 0.6520 - val_loss: 0.7053 - val_acc: 0.5521\n",
            "Epoch 146/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.6413 - acc: 0.6683 - val_loss: 0.7128 - val_acc: 0.5521\n",
            "Epoch 147/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.6487 - acc: 0.6781 - val_loss: 0.7227 - val_acc: 0.5417\n",
            "Epoch 148/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.6413 - acc: 0.6471 - val_loss: 0.7006 - val_acc: 0.5625\n",
            "Epoch 149/1000\n",
            "39/39 [==============================] - 8s 213ms/step - loss: 0.6158 - acc: 0.6650 - val_loss: 0.6687 - val_acc: 0.5625\n",
            "Epoch 150/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5936 - acc: 0.6928 - val_loss: 0.7031 - val_acc: 0.5625\n",
            "Epoch 151/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5891 - acc: 0.7026 - val_loss: 0.6910 - val_acc: 0.5521\n",
            "Epoch 152/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.6107 - acc: 0.6601 - val_loss: 0.7158 - val_acc: 0.5833\n",
            "Epoch 153/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.6629 - acc: 0.6569 - val_loss: 0.6924 - val_acc: 0.5729\n",
            "Epoch 154/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.6274 - acc: 0.6781 - val_loss: 0.7103 - val_acc: 0.5625\n",
            "Epoch 155/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.6430 - acc: 0.6732 - val_loss: 0.6983 - val_acc: 0.5417\n",
            "Epoch 156/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.6315 - acc: 0.6781 - val_loss: 0.6768 - val_acc: 0.5833\n",
            "Epoch 157/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.6660 - acc: 0.6618 - val_loss: 0.6719 - val_acc: 0.5625\n",
            "Epoch 158/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.6249 - acc: 0.6814 - val_loss: 0.6881 - val_acc: 0.5833\n",
            "Epoch 159/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.6178 - acc: 0.6683 - val_loss: 0.6814 - val_acc: 0.5625\n",
            "Epoch 160/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.6240 - acc: 0.6879 - val_loss: 0.6777 - val_acc: 0.5625\n",
            "Epoch 161/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5887 - acc: 0.6993 - val_loss: 0.6830 - val_acc: 0.5625\n",
            "Epoch 162/1000\n",
            "39/39 [==============================] - 8s 203ms/step - loss: 0.5901 - acc: 0.7010 - val_loss: 0.6928 - val_acc: 0.5312\n",
            "Epoch 163/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 0.6521 - acc: 0.6536 - val_loss: 0.6831 - val_acc: 0.5938\n",
            "Epoch 164/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.6144 - acc: 0.6895 - val_loss: 0.6961 - val_acc: 0.5521\n",
            "Epoch 165/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.6016 - acc: 0.6977 - val_loss: 0.7038 - val_acc: 0.5521\n",
            "Epoch 166/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.6027 - acc: 0.6993 - val_loss: 0.6847 - val_acc: 0.5729\n",
            "Epoch 167/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5971 - acc: 0.7092 - val_loss: 0.6806 - val_acc: 0.5833\n",
            "Epoch 168/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.6075 - acc: 0.6961 - val_loss: 0.6878 - val_acc: 0.5938\n",
            "Epoch 169/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5987 - acc: 0.7010 - val_loss: 0.6878 - val_acc: 0.5729\n",
            "Epoch 170/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6138 - acc: 0.6536 - val_loss: 0.6872 - val_acc: 0.5729\n",
            "Epoch 171/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5917 - acc: 0.6912 - val_loss: 0.6881 - val_acc: 0.5312\n",
            "Epoch 172/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.6287 - acc: 0.6797 - val_loss: 0.6833 - val_acc: 0.5729\n",
            "Epoch 173/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5844 - acc: 0.6814 - val_loss: 0.6769 - val_acc: 0.5521\n",
            "Epoch 174/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.6051 - acc: 0.6618 - val_loss: 0.6735 - val_acc: 0.5417\n",
            "Epoch 175/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5825 - acc: 0.7026 - val_loss: 0.6908 - val_acc: 0.5312\n",
            "Epoch 176/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5939 - acc: 0.6781 - val_loss: 0.6905 - val_acc: 0.5729\n",
            "Epoch 177/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5617 - acc: 0.7337 - val_loss: 0.7035 - val_acc: 0.5625\n",
            "Epoch 178/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.6096 - acc: 0.6732 - val_loss: 0.6763 - val_acc: 0.5625\n",
            "Epoch 179/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 0.6157 - acc: 0.6797 - val_loss: 0.6854 - val_acc: 0.5833\n",
            "Epoch 180/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5573 - acc: 0.7190 - val_loss: 0.6688 - val_acc: 0.6042\n",
            "Epoch 181/1000\n",
            "39/39 [==============================] - 8s 214ms/step - loss: 0.5969 - acc: 0.6895 - val_loss: 0.6733 - val_acc: 0.5938\n",
            "Epoch 182/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 0.6160 - acc: 0.6716 - val_loss: 0.6884 - val_acc: 0.5625\n",
            "Epoch 183/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5704 - acc: 0.7010 - val_loss: 0.6866 - val_acc: 0.5729\n",
            "Epoch 184/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.6111 - acc: 0.6716 - val_loss: 0.6676 - val_acc: 0.6042\n",
            "Epoch 185/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5936 - acc: 0.6748 - val_loss: 0.6643 - val_acc: 0.5833\n",
            "Epoch 186/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.6091 - acc: 0.6863 - val_loss: 0.6900 - val_acc: 0.5729\n",
            "Epoch 187/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5819 - acc: 0.7010 - val_loss: 0.6774 - val_acc: 0.5417\n",
            "Epoch 188/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.6189 - acc: 0.6716 - val_loss: 0.6679 - val_acc: 0.5729\n",
            "Epoch 189/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.6055 - acc: 0.6569 - val_loss: 0.6636 - val_acc: 0.5833\n",
            "Epoch 190/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.5603 - acc: 0.7206 - val_loss: 0.6581 - val_acc: 0.5729\n",
            "Epoch 191/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.5444 - acc: 0.7075 - val_loss: 0.6574 - val_acc: 0.5729\n",
            "Epoch 192/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 0.6030 - acc: 0.6797 - val_loss: 0.6470 - val_acc: 0.6042\n",
            "Epoch 193/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5970 - acc: 0.6879 - val_loss: 0.6604 - val_acc: 0.5625\n",
            "Epoch 194/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.6213 - acc: 0.6781 - val_loss: 0.6816 - val_acc: 0.5938\n",
            "Epoch 195/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.6399 - acc: 0.6618 - val_loss: 0.6805 - val_acc: 0.5208\n",
            "Epoch 196/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5799 - acc: 0.6977 - val_loss: 0.6665 - val_acc: 0.5938\n",
            "Epoch 197/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 0.6020 - acc: 0.6993 - val_loss: 0.6926 - val_acc: 0.5833\n",
            "Epoch 198/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 0.5699 - acc: 0.7092 - val_loss: 0.6792 - val_acc: 0.5312\n",
            "Epoch 199/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5859 - acc: 0.7010 - val_loss: 0.6798 - val_acc: 0.5312\n",
            "Epoch 200/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6031 - acc: 0.6993 - val_loss: 0.6714 - val_acc: 0.5417\n",
            "Epoch 201/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5681 - acc: 0.7042 - val_loss: 0.6744 - val_acc: 0.5417\n",
            "Epoch 202/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5645 - acc: 0.6912 - val_loss: 0.6763 - val_acc: 0.5625\n",
            "Epoch 203/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5670 - acc: 0.7124 - val_loss: 0.6613 - val_acc: 0.5833\n",
            "Epoch 204/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.5797 - acc: 0.7353 - val_loss: 0.6690 - val_acc: 0.5417\n",
            "Epoch 205/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.5886 - acc: 0.7026 - val_loss: 0.6584 - val_acc: 0.5521\n",
            "Epoch 206/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5625 - acc: 0.7190 - val_loss: 0.6571 - val_acc: 0.5729\n",
            "Epoch 207/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5951 - acc: 0.7010 - val_loss: 0.6648 - val_acc: 0.5417\n",
            "Epoch 208/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.6149 - acc: 0.6830 - val_loss: 0.6638 - val_acc: 0.5521\n",
            "Epoch 209/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5968 - acc: 0.6863 - val_loss: 0.6637 - val_acc: 0.5521\n",
            "Epoch 210/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5902 - acc: 0.6961 - val_loss: 0.6661 - val_acc: 0.5521\n",
            "Epoch 211/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5891 - acc: 0.6971 - val_loss: 0.6592 - val_acc: 0.5833\n",
            "Epoch 212/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.6047 - acc: 0.6765 - val_loss: 0.6658 - val_acc: 0.5833\n",
            "Epoch 213/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5601 - acc: 0.6912 - val_loss: 0.6677 - val_acc: 0.5417\n",
            "Epoch 214/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.6138 - acc: 0.6977 - val_loss: 0.6824 - val_acc: 0.5417\n",
            "Epoch 215/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5760 - acc: 0.6977 - val_loss: 0.6737 - val_acc: 0.5417\n",
            "Epoch 216/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5640 - acc: 0.7173 - val_loss: 0.6591 - val_acc: 0.5833\n",
            "Epoch 217/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5652 - acc: 0.6830 - val_loss: 0.6627 - val_acc: 0.5521\n",
            "Epoch 218/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5438 - acc: 0.7190 - val_loss: 0.6624 - val_acc: 0.5521\n",
            "Epoch 219/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.6150 - acc: 0.6748 - val_loss: 0.6606 - val_acc: 0.5521\n",
            "Epoch 220/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5821 - acc: 0.6765 - val_loss: 0.6526 - val_acc: 0.5625\n",
            "Epoch 221/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5988 - acc: 0.6895 - val_loss: 0.6524 - val_acc: 0.5625\n",
            "Epoch 222/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5425 - acc: 0.7320 - val_loss: 0.6671 - val_acc: 0.5729\n",
            "Epoch 223/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.5871 - acc: 0.7059 - val_loss: 0.6614 - val_acc: 0.5729\n",
            "Epoch 224/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5598 - acc: 0.7124 - val_loss: 0.6799 - val_acc: 0.5417\n",
            "Epoch 225/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 0.5599 - acc: 0.7042 - val_loss: 0.6693 - val_acc: 0.5938\n",
            "Epoch 226/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5826 - acc: 0.6977 - val_loss: 0.6682 - val_acc: 0.5833\n",
            "Epoch 227/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5828 - acc: 0.6732 - val_loss: 0.6830 - val_acc: 0.5521\n",
            "Epoch 228/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5763 - acc: 0.6961 - val_loss: 0.6764 - val_acc: 0.5417\n",
            "Epoch 229/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5941 - acc: 0.7042 - val_loss: 0.6842 - val_acc: 0.5312\n",
            "Epoch 230/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5487 - acc: 0.7124 - val_loss: 0.6655 - val_acc: 0.5521\n",
            "Epoch 231/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5670 - acc: 0.6977 - val_loss: 0.6705 - val_acc: 0.5625\n",
            "Epoch 232/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5281 - acc: 0.7288 - val_loss: 0.6730 - val_acc: 0.5521\n",
            "Epoch 233/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 0.5406 - acc: 0.7244 - val_loss: 0.6677 - val_acc: 0.5833\n",
            "Epoch 234/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5615 - acc: 0.7124 - val_loss: 0.6779 - val_acc: 0.5521\n",
            "Epoch 235/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5600 - acc: 0.7026 - val_loss: 0.6482 - val_acc: 0.5625\n",
            "Epoch 236/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.5525 - acc: 0.7141 - val_loss: 0.6673 - val_acc: 0.5417\n",
            "Epoch 237/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.5701 - acc: 0.6928 - val_loss: 0.6610 - val_acc: 0.5417\n",
            "Epoch 238/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5538 - acc: 0.7222 - val_loss: 0.6565 - val_acc: 0.5938\n",
            "Epoch 239/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5244 - acc: 0.7108 - val_loss: 0.6689 - val_acc: 0.5312\n",
            "Epoch 240/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 0.5702 - acc: 0.7173 - val_loss: 0.6577 - val_acc: 0.5625\n",
            "Epoch 241/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5821 - acc: 0.6830 - val_loss: 0.6549 - val_acc: 0.5729\n",
            "Epoch 242/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.5307 - acc: 0.7582 - val_loss: 0.6679 - val_acc: 0.5938\n",
            "Epoch 243/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.5552 - acc: 0.7092 - val_loss: 0.6725 - val_acc: 0.5208\n",
            "Epoch 244/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5400 - acc: 0.7402 - val_loss: 0.6618 - val_acc: 0.5521\n",
            "Epoch 245/1000\n",
            "39/39 [==============================] - 8s 204ms/step - loss: 0.5349 - acc: 0.7337 - val_loss: 0.6616 - val_acc: 0.5729\n",
            "Epoch 246/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5479 - acc: 0.6977 - val_loss: 0.6615 - val_acc: 0.5312\n",
            "Epoch 247/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5722 - acc: 0.7092 - val_loss: 0.6553 - val_acc: 0.5729\n",
            "Epoch 248/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 0.5422 - acc: 0.7271 - val_loss: 0.6613 - val_acc: 0.5729\n",
            "Epoch 249/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5407 - acc: 0.7337 - val_loss: 0.6737 - val_acc: 0.5417\n",
            "Epoch 250/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.5704 - acc: 0.7075 - val_loss: 0.6754 - val_acc: 0.5521\n",
            "Epoch 251/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5733 - acc: 0.7075 - val_loss: 0.6486 - val_acc: 0.6042\n",
            "Epoch 252/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.5593 - acc: 0.7092 - val_loss: 0.6647 - val_acc: 0.5833\n",
            "Epoch 253/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.5794 - acc: 0.7124 - val_loss: 0.6497 - val_acc: 0.5729\n",
            "Epoch 254/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5503 - acc: 0.7288 - val_loss: 0.6667 - val_acc: 0.5521\n",
            "Epoch 255/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5684 - acc: 0.7206 - val_loss: 0.6530 - val_acc: 0.5521\n",
            "Epoch 256/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5713 - acc: 0.7026 - val_loss: 0.6487 - val_acc: 0.5521\n",
            "Epoch 257/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5576 - acc: 0.7108 - val_loss: 0.6545 - val_acc: 0.5521\n",
            "Epoch 258/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 0.5553 - acc: 0.7157 - val_loss: 0.6505 - val_acc: 0.5833\n",
            "Epoch 259/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.5574 - acc: 0.7108 - val_loss: 0.6635 - val_acc: 0.5729\n",
            "Epoch 260/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5432 - acc: 0.7190 - val_loss: 0.6620 - val_acc: 0.5625\n",
            "Epoch 261/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5765 - acc: 0.6955 - val_loss: 0.6532 - val_acc: 0.5729\n",
            "Epoch 262/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.5768 - acc: 0.7059 - val_loss: 0.6673 - val_acc: 0.5625\n",
            "Epoch 263/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.5373 - acc: 0.7435 - val_loss: 0.6604 - val_acc: 0.5833\n",
            "Epoch 264/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5696 - acc: 0.7092 - val_loss: 0.6461 - val_acc: 0.5833\n",
            "Epoch 265/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5376 - acc: 0.7304 - val_loss: 0.6505 - val_acc: 0.5833\n",
            "Epoch 266/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 0.5674 - acc: 0.6977 - val_loss: 0.6681 - val_acc: 0.5729\n",
            "Epoch 267/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5530 - acc: 0.7026 - val_loss: 0.6416 - val_acc: 0.5625\n",
            "Epoch 268/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 0.5336 - acc: 0.7320 - val_loss: 0.6597 - val_acc: 0.5729\n",
            "Epoch 269/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5525 - acc: 0.7222 - val_loss: 0.6391 - val_acc: 0.5625\n",
            "Epoch 270/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.5252 - acc: 0.7451 - val_loss: 0.6618 - val_acc: 0.5625\n",
            "Epoch 271/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 0.5346 - acc: 0.7369 - val_loss: 0.6455 - val_acc: 0.5833\n",
            "Epoch 272/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 0.5472 - acc: 0.7124 - val_loss: 0.6730 - val_acc: 0.5208\n",
            "Epoch 273/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5479 - acc: 0.7304 - val_loss: 0.6580 - val_acc: 0.5312\n",
            "Epoch 274/1000\n",
            "39/39 [==============================] - 4s 79ms/step - loss: 0.5354 - acc: 0.7369 - val_loss: 0.6728 - val_acc: 0.5938\n",
            "Epoch 275/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5864 - acc: 0.6912 - val_loss: 0.6744 - val_acc: 0.5417\n",
            "Epoch 276/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5376 - acc: 0.7418 - val_loss: 0.6509 - val_acc: 0.5729\n",
            "Epoch 277/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5646 - acc: 0.6944 - val_loss: 0.6739 - val_acc: 0.5625\n",
            "Epoch 278/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.6043 - acc: 0.6863 - val_loss: 0.6651 - val_acc: 0.5417\n",
            "Epoch 279/1000\n",
            "39/39 [==============================] - 10s 229ms/step - loss: 0.5636 - acc: 0.7141 - val_loss: 0.6737 - val_acc: 0.5104\n",
            "Epoch 280/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5910 - acc: 0.6944 - val_loss: 0.6775 - val_acc: 0.5104\n",
            "Epoch 281/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5109 - acc: 0.7369 - val_loss: 0.6706 - val_acc: 0.5417\n",
            "Epoch 282/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5478 - acc: 0.7320 - val_loss: 0.6707 - val_acc: 0.5312\n",
            "Epoch 283/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5273 - acc: 0.7288 - val_loss: 0.6735 - val_acc: 0.5625\n",
            "Epoch 284/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5722 - acc: 0.6928 - val_loss: 0.6749 - val_acc: 0.5729\n",
            "Epoch 285/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5818 - acc: 0.6944 - val_loss: 0.6751 - val_acc: 0.5833\n",
            "Epoch 286/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5504 - acc: 0.7271 - val_loss: 0.6511 - val_acc: 0.6042\n",
            "Epoch 287/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 0.5344 - acc: 0.7516 - val_loss: 0.6670 - val_acc: 0.5625\n",
            "Epoch 288/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5633 - acc: 0.7157 - val_loss: 0.6482 - val_acc: 0.5625\n",
            "Epoch 289/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5415 - acc: 0.7308 - val_loss: 0.6522 - val_acc: 0.5729\n",
            "Epoch 290/1000\n",
            "39/39 [==============================] - 6s 132ms/step - loss: 0.5326 - acc: 0.7304 - val_loss: 0.6417 - val_acc: 0.5625\n",
            "Epoch 291/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.5260 - acc: 0.7092 - val_loss: 0.6614 - val_acc: 0.5625\n",
            "Epoch 292/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 0.5326 - acc: 0.6977 - val_loss: 0.6609 - val_acc: 0.5729\n",
            "Epoch 293/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5614 - acc: 0.7075 - val_loss: 0.6610 - val_acc: 0.5625\n",
            "Epoch 294/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5570 - acc: 0.7108 - val_loss: 0.6757 - val_acc: 0.5521\n",
            "Epoch 295/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5600 - acc: 0.6846 - val_loss: 0.6593 - val_acc: 0.5729\n",
            "Epoch 296/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5252 - acc: 0.7369 - val_loss: 0.6580 - val_acc: 0.5625\n",
            "Epoch 297/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5639 - acc: 0.7108 - val_loss: 0.6636 - val_acc: 0.5312\n",
            "Epoch 298/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5285 - acc: 0.7239 - val_loss: 0.6595 - val_acc: 0.6042\n",
            "Epoch 299/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5446 - acc: 0.7190 - val_loss: 0.6592 - val_acc: 0.5625\n",
            "Epoch 300/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5543 - acc: 0.7124 - val_loss: 0.6652 - val_acc: 0.5417\n",
            "Epoch 301/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5604 - acc: 0.7388 - val_loss: 0.6616 - val_acc: 0.6042\n",
            "Epoch 302/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.5470 - acc: 0.7304 - val_loss: 0.6627 - val_acc: 0.5833\n",
            "Epoch 303/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5138 - acc: 0.7582 - val_loss: 0.6594 - val_acc: 0.5729\n",
            "Epoch 304/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5578 - acc: 0.7353 - val_loss: 0.6297 - val_acc: 0.6042\n",
            "Epoch 305/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 0.5529 - acc: 0.7239 - val_loss: 0.6388 - val_acc: 0.6458\n",
            "Epoch 306/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5402 - acc: 0.7288 - val_loss: 0.6423 - val_acc: 0.5729\n",
            "Epoch 307/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.4970 - acc: 0.7614 - val_loss: 0.6389 - val_acc: 0.5833\n",
            "Epoch 308/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5415 - acc: 0.7484 - val_loss: 0.6592 - val_acc: 0.5833\n",
            "Epoch 309/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5597 - acc: 0.7157 - val_loss: 0.6302 - val_acc: 0.5833\n",
            "Epoch 310/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5431 - acc: 0.7467 - val_loss: 0.6788 - val_acc: 0.6354\n",
            "Epoch 311/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 0.5402 - acc: 0.7222 - val_loss: 0.6694 - val_acc: 0.6042\n",
            "Epoch 312/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5393 - acc: 0.7255 - val_loss: 0.6561 - val_acc: 0.5729\n",
            "Epoch 313/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5728 - acc: 0.7092 - val_loss: 0.6357 - val_acc: 0.5833\n",
            "Epoch 314/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5492 - acc: 0.7206 - val_loss: 0.6347 - val_acc: 0.5729\n",
            "Epoch 315/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5543 - acc: 0.7035 - val_loss: 0.6395 - val_acc: 0.5938\n",
            "Epoch 316/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5519 - acc: 0.7190 - val_loss: 0.6450 - val_acc: 0.5729\n",
            "Epoch 317/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5707 - acc: 0.7124 - val_loss: 0.6550 - val_acc: 0.5938\n",
            "Epoch 318/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 0.5135 - acc: 0.7451 - val_loss: 0.6480 - val_acc: 0.5625\n",
            "Epoch 319/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5450 - acc: 0.7075 - val_loss: 0.6261 - val_acc: 0.5938\n",
            "Epoch 320/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5572 - acc: 0.7304 - val_loss: 0.6244 - val_acc: 0.5833\n",
            "Epoch 321/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5468 - acc: 0.7059 - val_loss: 0.6330 - val_acc: 0.5729\n",
            "Epoch 322/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 0.5510 - acc: 0.7260 - val_loss: 0.6249 - val_acc: 0.5833\n",
            "Epoch 323/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5510 - acc: 0.7356 - val_loss: 0.6204 - val_acc: 0.5938\n",
            "Epoch 324/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5359 - acc: 0.7369 - val_loss: 0.6216 - val_acc: 0.5729\n",
            "Epoch 325/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 0.5056 - acc: 0.7369 - val_loss: 0.6215 - val_acc: 0.5938\n",
            "Epoch 326/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5510 - acc: 0.7239 - val_loss: 0.6239 - val_acc: 0.6250\n",
            "Epoch 327/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5352 - acc: 0.7157 - val_loss: 0.6282 - val_acc: 0.6042\n",
            "Epoch 328/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5345 - acc: 0.7108 - val_loss: 0.6112 - val_acc: 0.5729\n",
            "Epoch 329/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5113 - acc: 0.7614 - val_loss: 0.6119 - val_acc: 0.5625\n",
            "Epoch 330/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 0.5193 - acc: 0.7239 - val_loss: 0.6078 - val_acc: 0.6250\n",
            "Epoch 331/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5225 - acc: 0.7369 - val_loss: 0.6006 - val_acc: 0.6146\n",
            "Epoch 332/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5549 - acc: 0.7059 - val_loss: 0.6136 - val_acc: 0.6042\n",
            "Epoch 333/1000\n",
            "39/39 [==============================] - 6s 124ms/step - loss: 0.5154 - acc: 0.7614 - val_loss: 0.6417 - val_acc: 0.6250\n",
            "Epoch 334/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5475 - acc: 0.7276 - val_loss: 0.6204 - val_acc: 0.6667\n",
            "Epoch 335/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5363 - acc: 0.7292 - val_loss: 0.6222 - val_acc: 0.6146\n",
            "Epoch 336/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5259 - acc: 0.7292 - val_loss: 0.6256 - val_acc: 0.6146\n",
            "Epoch 337/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5204 - acc: 0.7451 - val_loss: 0.6245 - val_acc: 0.6042\n",
            "Epoch 338/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5586 - acc: 0.7206 - val_loss: 0.6309 - val_acc: 0.6042\n",
            "Epoch 339/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 0.5448 - acc: 0.7288 - val_loss: 0.6266 - val_acc: 0.6562\n",
            "Epoch 340/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5389 - acc: 0.7353 - val_loss: 0.6329 - val_acc: 0.6146\n",
            "Epoch 341/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 0.5351 - acc: 0.7386 - val_loss: 0.6204 - val_acc: 0.5729\n",
            "Epoch 342/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5330 - acc: 0.7353 - val_loss: 0.6162 - val_acc: 0.6250\n",
            "Epoch 343/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5529 - acc: 0.7124 - val_loss: 0.6079 - val_acc: 0.5938\n",
            "Epoch 344/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5332 - acc: 0.7484 - val_loss: 0.6348 - val_acc: 0.5938\n",
            "Epoch 345/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5506 - acc: 0.7190 - val_loss: 0.6313 - val_acc: 0.6042\n",
            "Epoch 346/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.5496 - acc: 0.7173 - val_loss: 0.6276 - val_acc: 0.6146\n",
            "Epoch 347/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5524 - acc: 0.7271 - val_loss: 0.6136 - val_acc: 0.5938\n",
            "Epoch 348/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5339 - acc: 0.7255 - val_loss: 0.6446 - val_acc: 0.6354\n",
            "Epoch 349/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5562 - acc: 0.7173 - val_loss: 0.6379 - val_acc: 0.5625\n",
            "Epoch 350/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.5076 - acc: 0.7320 - val_loss: 0.6570 - val_acc: 0.6354\n",
            "Epoch 351/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5255 - acc: 0.7320 - val_loss: 0.6191 - val_acc: 0.5938\n",
            "Epoch 352/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 0.5449 - acc: 0.7271 - val_loss: 0.6151 - val_acc: 0.5729\n",
            "Epoch 353/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5098 - acc: 0.7467 - val_loss: 0.6230 - val_acc: 0.6250\n",
            "Epoch 354/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.4873 - acc: 0.7631 - val_loss: 0.6526 - val_acc: 0.6562\n",
            "Epoch 355/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.5264 - acc: 0.7418 - val_loss: 0.6274 - val_acc: 0.5521\n",
            "Epoch 356/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.5434 - acc: 0.7141 - val_loss: 0.6419 - val_acc: 0.5938\n",
            "Epoch 357/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5237 - acc: 0.7549 - val_loss: 0.6277 - val_acc: 0.5833\n",
            "Epoch 358/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5232 - acc: 0.7124 - val_loss: 0.6153 - val_acc: 0.5938\n",
            "Epoch 359/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5426 - acc: 0.7418 - val_loss: 0.6011 - val_acc: 0.6042\n",
            "Epoch 360/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5238 - acc: 0.7304 - val_loss: 0.6297 - val_acc: 0.6146\n",
            "Epoch 361/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5493 - acc: 0.7173 - val_loss: 0.6328 - val_acc: 0.5938\n",
            "Epoch 362/1000\n",
            "39/39 [==============================] - 10s 230ms/step - loss: 0.5248 - acc: 0.7141 - val_loss: 0.6457 - val_acc: 0.6354\n",
            "Epoch 363/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5372 - acc: 0.7124 - val_loss: 0.6095 - val_acc: 0.6458\n",
            "Epoch 364/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5231 - acc: 0.7288 - val_loss: 0.6217 - val_acc: 0.6354\n",
            "Epoch 365/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5208 - acc: 0.7435 - val_loss: 0.6068 - val_acc: 0.5938\n",
            "Epoch 366/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5297 - acc: 0.7255 - val_loss: 0.6048 - val_acc: 0.6250\n",
            "Epoch 367/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5157 - acc: 0.7386 - val_loss: 0.6198 - val_acc: 0.6042\n",
            "Epoch 368/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5107 - acc: 0.7402 - val_loss: 0.6213 - val_acc: 0.6458\n",
            "Epoch 369/1000\n",
            "39/39 [==============================] - 9s 233ms/step - loss: 0.4981 - acc: 0.7435 - val_loss: 0.6305 - val_acc: 0.6146\n",
            "Epoch 370/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5128 - acc: 0.7304 - val_loss: 0.6231 - val_acc: 0.6250\n",
            "Epoch 371/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 0.5297 - acc: 0.7353 - val_loss: 0.6072 - val_acc: 0.5938\n",
            "Epoch 372/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 0.5273 - acc: 0.7206 - val_loss: 0.6076 - val_acc: 0.6146\n",
            "Epoch 373/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5393 - acc: 0.7141 - val_loss: 0.6348 - val_acc: 0.6354\n",
            "Epoch 374/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.5119 - acc: 0.7271 - val_loss: 0.6317 - val_acc: 0.6250\n",
            "Epoch 375/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.4954 - acc: 0.7565 - val_loss: 0.6289 - val_acc: 0.5729\n",
            "Epoch 376/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5725 - acc: 0.7141 - val_loss: 0.6179 - val_acc: 0.6146\n",
            "Epoch 377/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5308 - acc: 0.7467 - val_loss: 0.6366 - val_acc: 0.6354\n",
            "Epoch 378/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5210 - acc: 0.7435 - val_loss: 0.6324 - val_acc: 0.6562\n",
            "Epoch 379/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5437 - acc: 0.7157 - val_loss: 0.6283 - val_acc: 0.6562\n",
            "Epoch 380/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5295 - acc: 0.7190 - val_loss: 0.6211 - val_acc: 0.6146\n",
            "Epoch 381/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5239 - acc: 0.7435 - val_loss: 0.6291 - val_acc: 0.6562\n",
            "Epoch 382/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5482 - acc: 0.7467 - val_loss: 0.6344 - val_acc: 0.6562\n",
            "Epoch 383/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5394 - acc: 0.7141 - val_loss: 0.6244 - val_acc: 0.6667\n",
            "Epoch 384/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5638 - acc: 0.7075 - val_loss: 0.6105 - val_acc: 0.6042\n",
            "Epoch 385/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5507 - acc: 0.7304 - val_loss: 0.6216 - val_acc: 0.6042\n",
            "Epoch 386/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.5264 - acc: 0.7435 - val_loss: 0.6049 - val_acc: 0.6458\n",
            "Epoch 387/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5139 - acc: 0.7320 - val_loss: 0.6236 - val_acc: 0.6250\n",
            "Epoch 388/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 0.5393 - acc: 0.7141 - val_loss: 0.6156 - val_acc: 0.5938\n",
            "Epoch 389/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 0.5062 - acc: 0.7565 - val_loss: 0.6214 - val_acc: 0.5833\n",
            "Epoch 390/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5287 - acc: 0.7435 - val_loss: 0.6138 - val_acc: 0.6250\n",
            "Epoch 391/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 0.5329 - acc: 0.7451 - val_loss: 0.6395 - val_acc: 0.6458\n",
            "Epoch 392/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5269 - acc: 0.7222 - val_loss: 0.6359 - val_acc: 0.6458\n",
            "Epoch 393/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5001 - acc: 0.7337 - val_loss: 0.6177 - val_acc: 0.6562\n",
            "Epoch 394/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5280 - acc: 0.7320 - val_loss: 0.6078 - val_acc: 0.6458\n",
            "Epoch 395/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5370 - acc: 0.7255 - val_loss: 0.5985 - val_acc: 0.5938\n",
            "Epoch 396/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.4920 - acc: 0.7582 - val_loss: 0.6219 - val_acc: 0.6667\n",
            "Epoch 397/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5358 - acc: 0.7157 - val_loss: 0.6047 - val_acc: 0.6250\n",
            "Epoch 398/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 0.5427 - acc: 0.7271 - val_loss: 0.6482 - val_acc: 0.6458\n",
            "Epoch 399/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5489 - acc: 0.7255 - val_loss: 0.6308 - val_acc: 0.6562\n",
            "Epoch 400/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5225 - acc: 0.7418 - val_loss: 0.6063 - val_acc: 0.6250\n",
            "Epoch 401/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5472 - acc: 0.7255 - val_loss: 0.5994 - val_acc: 0.6042\n",
            "Epoch 402/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5280 - acc: 0.7320 - val_loss: 0.6206 - val_acc: 0.6146\n",
            "Epoch 403/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.4981 - acc: 0.7484 - val_loss: 0.6314 - val_acc: 0.6562\n",
            "Epoch 404/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.5375 - acc: 0.7288 - val_loss: 0.6156 - val_acc: 0.6146\n",
            "Epoch 405/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5338 - acc: 0.7484 - val_loss: 0.6409 - val_acc: 0.6354\n",
            "Epoch 406/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5389 - acc: 0.7173 - val_loss: 0.6207 - val_acc: 0.6667\n",
            "Epoch 407/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5340 - acc: 0.7320 - val_loss: 0.6063 - val_acc: 0.6354\n",
            "Epoch 408/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5161 - acc: 0.7451 - val_loss: 0.6111 - val_acc: 0.5833\n",
            "Epoch 409/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.5273 - acc: 0.7435 - val_loss: 0.6091 - val_acc: 0.6354\n",
            "Epoch 410/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5241 - acc: 0.7369 - val_loss: 0.6237 - val_acc: 0.6562\n",
            "Epoch 411/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5183 - acc: 0.7484 - val_loss: 0.6021 - val_acc: 0.6146\n",
            "Epoch 412/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.5345 - acc: 0.7418 - val_loss: 0.6025 - val_acc: 0.6250\n",
            "Epoch 413/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5220 - acc: 0.7304 - val_loss: 0.6420 - val_acc: 0.6354\n",
            "Epoch 414/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 0.5026 - acc: 0.7404 - val_loss: 0.6061 - val_acc: 0.6562\n",
            "Epoch 415/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5534 - acc: 0.7190 - val_loss: 0.6102 - val_acc: 0.6562\n",
            "Epoch 416/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5308 - acc: 0.7337 - val_loss: 0.6045 - val_acc: 0.6146\n",
            "Epoch 417/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 0.5425 - acc: 0.7255 - val_loss: 0.5987 - val_acc: 0.6250\n",
            "Epoch 418/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 0.5172 - acc: 0.7467 - val_loss: 0.6184 - val_acc: 0.6562\n",
            "Epoch 419/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5189 - acc: 0.7451 - val_loss: 0.6023 - val_acc: 0.6250\n",
            "Epoch 420/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5358 - acc: 0.7353 - val_loss: 0.6032 - val_acc: 0.6354\n",
            "Epoch 421/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5249 - acc: 0.7271 - val_loss: 0.6154 - val_acc: 0.6458\n",
            "Epoch 422/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5378 - acc: 0.7369 - val_loss: 0.6302 - val_acc: 0.6354\n",
            "Epoch 423/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5240 - acc: 0.7190 - val_loss: 0.6189 - val_acc: 0.5938\n",
            "Epoch 424/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5298 - acc: 0.7320 - val_loss: 0.6072 - val_acc: 0.6042\n",
            "Epoch 425/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5448 - acc: 0.7271 - val_loss: 0.6067 - val_acc: 0.6250\n",
            "Epoch 426/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 0.5327 - acc: 0.7157 - val_loss: 0.6492 - val_acc: 0.6250\n",
            "Epoch 427/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 0.5499 - acc: 0.7157 - val_loss: 0.6264 - val_acc: 0.6562\n",
            "Epoch 428/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5312 - acc: 0.7402 - val_loss: 0.6331 - val_acc: 0.6146\n",
            "Epoch 429/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5121 - acc: 0.7484 - val_loss: 0.6196 - val_acc: 0.6146\n",
            "Epoch 430/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5380 - acc: 0.7484 - val_loss: 0.5982 - val_acc: 0.6042\n",
            "Epoch 431/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5237 - acc: 0.7451 - val_loss: 0.6043 - val_acc: 0.5833\n",
            "Epoch 432/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 0.5178 - acc: 0.7467 - val_loss: 0.6055 - val_acc: 0.6042\n",
            "Epoch 433/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.5192 - acc: 0.7418 - val_loss: 0.6022 - val_acc: 0.6042\n",
            "Epoch 434/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5366 - acc: 0.7190 - val_loss: 0.6278 - val_acc: 0.5938\n",
            "Epoch 435/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5424 - acc: 0.7255 - val_loss: 0.6190 - val_acc: 0.5833\n",
            "Epoch 436/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.5154 - acc: 0.7402 - val_loss: 0.6166 - val_acc: 0.6146\n",
            "Epoch 437/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 0.5215 - acc: 0.7353 - val_loss: 0.6273 - val_acc: 0.6146\n",
            "Epoch 438/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5558 - acc: 0.7124 - val_loss: 0.6148 - val_acc: 0.5938\n",
            "Epoch 439/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5137 - acc: 0.7598 - val_loss: 0.6152 - val_acc: 0.5938\n",
            "Epoch 440/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5063 - acc: 0.7467 - val_loss: 0.6065 - val_acc: 0.6354\n",
            "Epoch 441/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 0.5206 - acc: 0.7467 - val_loss: 0.6122 - val_acc: 0.6250\n",
            "Epoch 442/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5347 - acc: 0.7353 - val_loss: 0.6073 - val_acc: 0.6354\n",
            "Epoch 443/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5189 - acc: 0.7288 - val_loss: 0.6524 - val_acc: 0.6667\n",
            "Epoch 444/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5300 - acc: 0.7173 - val_loss: 0.5950 - val_acc: 0.6250\n",
            "Epoch 445/1000\n",
            "39/39 [==============================] - 6s 140ms/step - loss: 0.5299 - acc: 0.7304 - val_loss: 0.6081 - val_acc: 0.6667\n",
            "Epoch 446/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 0.5428 - acc: 0.7271 - val_loss: 0.6001 - val_acc: 0.6771\n",
            "Epoch 447/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5346 - acc: 0.7288 - val_loss: 0.6176 - val_acc: 0.6146\n",
            "Epoch 448/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5317 - acc: 0.7173 - val_loss: 0.6005 - val_acc: 0.5938\n",
            "Epoch 449/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5345 - acc: 0.7288 - val_loss: 0.5957 - val_acc: 0.6146\n",
            "Epoch 450/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 0.5268 - acc: 0.7369 - val_loss: 0.5850 - val_acc: 0.6667\n",
            "Epoch 451/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5018 - acc: 0.7467 - val_loss: 0.6057 - val_acc: 0.6146\n",
            "Epoch 452/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5241 - acc: 0.7157 - val_loss: 0.6072 - val_acc: 0.6354\n",
            "Epoch 453/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.4819 - acc: 0.7810 - val_loss: 0.6107 - val_acc: 0.6458\n",
            "Epoch 454/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5251 - acc: 0.7467 - val_loss: 0.6078 - val_acc: 0.6458\n",
            "Epoch 455/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5251 - acc: 0.7206 - val_loss: 0.6262 - val_acc: 0.6250\n",
            "Epoch 456/1000\n",
            "39/39 [==============================] - 9s 207ms/step - loss: 0.5432 - acc: 0.7075 - val_loss: 0.5938 - val_acc: 0.6562\n",
            "Epoch 457/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5062 - acc: 0.7320 - val_loss: 0.6201 - val_acc: 0.6250\n",
            "Epoch 458/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5229 - acc: 0.7255 - val_loss: 0.6002 - val_acc: 0.6458\n",
            "Epoch 459/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5096 - acc: 0.7451 - val_loss: 0.6071 - val_acc: 0.6771\n",
            "Epoch 460/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5366 - acc: 0.7271 - val_loss: 0.6020 - val_acc: 0.6250\n",
            "Epoch 461/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5386 - acc: 0.7173 - val_loss: 0.6253 - val_acc: 0.6354\n",
            "Epoch 462/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 0.5275 - acc: 0.7369 - val_loss: 0.6053 - val_acc: 0.6458\n",
            "Epoch 463/1000\n",
            "39/39 [==============================] - 6s 133ms/step - loss: 0.5275 - acc: 0.7337 - val_loss: 0.6192 - val_acc: 0.6458\n",
            "Epoch 464/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5424 - acc: 0.7271 - val_loss: 0.6129 - val_acc: 0.6042\n",
            "Epoch 465/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 0.5367 - acc: 0.7418 - val_loss: 0.6083 - val_acc: 0.6562\n",
            "Epoch 466/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.4845 - acc: 0.7533 - val_loss: 0.6187 - val_acc: 0.6354\n",
            "Epoch 467/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 0.5670 - acc: 0.7075 - val_loss: 0.6072 - val_acc: 0.6042\n",
            "Epoch 468/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5247 - acc: 0.7239 - val_loss: 0.6189 - val_acc: 0.6562\n",
            "Epoch 469/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.4861 - acc: 0.7680 - val_loss: 0.6071 - val_acc: 0.5938\n",
            "Epoch 470/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5312 - acc: 0.7386 - val_loss: 0.6123 - val_acc: 0.6458\n",
            "Epoch 471/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5061 - acc: 0.7484 - val_loss: 0.6062 - val_acc: 0.6354\n",
            "Epoch 472/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5275 - acc: 0.7141 - val_loss: 0.6213 - val_acc: 0.6250\n",
            "Epoch 473/1000\n",
            "39/39 [==============================] - 5s 96ms/step - loss: 0.5201 - acc: 0.7451 - val_loss: 0.6072 - val_acc: 0.6146\n",
            "Epoch 474/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5139 - acc: 0.7353 - val_loss: 0.6064 - val_acc: 0.6458\n",
            "Epoch 475/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.5167 - acc: 0.7418 - val_loss: 0.6126 - val_acc: 0.6354\n",
            "Epoch 476/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 0.5326 - acc: 0.7173 - val_loss: 0.6021 - val_acc: 0.6250\n",
            "Epoch 477/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5468 - acc: 0.7190 - val_loss: 0.6198 - val_acc: 0.5833\n",
            "Epoch 478/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5153 - acc: 0.7271 - val_loss: 0.6217 - val_acc: 0.6458\n",
            "Epoch 479/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.5396 - acc: 0.7239 - val_loss: 0.6057 - val_acc: 0.6250\n",
            "Epoch 480/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5102 - acc: 0.7451 - val_loss: 0.6344 - val_acc: 0.6771\n",
            "Epoch 481/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5215 - acc: 0.7271 - val_loss: 0.6188 - val_acc: 0.6354\n",
            "Epoch 482/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 0.5276 - acc: 0.7402 - val_loss: 0.6345 - val_acc: 0.6562\n",
            "Epoch 483/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.5265 - acc: 0.7418 - val_loss: 0.6246 - val_acc: 0.6146\n",
            "Epoch 484/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5598 - acc: 0.7222 - val_loss: 0.6140 - val_acc: 0.6146\n",
            "Epoch 485/1000\n",
            "39/39 [==============================] - 10s 250ms/step - loss: 0.5312 - acc: 0.7075 - val_loss: 0.6201 - val_acc: 0.6458\n",
            "Epoch 486/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5261 - acc: 0.7386 - val_loss: 0.6217 - val_acc: 0.6354\n",
            "Epoch 487/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5256 - acc: 0.7549 - val_loss: 0.6155 - val_acc: 0.5729\n",
            "Epoch 488/1000\n",
            "39/39 [==============================] - 6s 132ms/step - loss: 0.5295 - acc: 0.7598 - val_loss: 0.6252 - val_acc: 0.6250\n",
            "Epoch 489/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.5093 - acc: 0.7402 - val_loss: 0.6529 - val_acc: 0.6250\n",
            "Epoch 490/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5193 - acc: 0.7418 - val_loss: 0.6351 - val_acc: 0.6146\n",
            "Epoch 491/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 0.5386 - acc: 0.7173 - val_loss: 0.6479 - val_acc: 0.6250\n",
            "Epoch 492/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5312 - acc: 0.7239 - val_loss: 0.6219 - val_acc: 0.6458\n",
            "Epoch 493/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5429 - acc: 0.7271 - val_loss: 0.6503 - val_acc: 0.6458\n",
            "Epoch 494/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5273 - acc: 0.7059 - val_loss: 0.6196 - val_acc: 0.6354\n",
            "Epoch 495/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5300 - acc: 0.7369 - val_loss: 0.6031 - val_acc: 0.6250\n",
            "Epoch 496/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 0.5187 - acc: 0.7386 - val_loss: 0.5983 - val_acc: 0.6458\n",
            "Epoch 497/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5315 - acc: 0.7304 - val_loss: 0.6217 - val_acc: 0.6250\n",
            "Epoch 498/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5085 - acc: 0.7353 - val_loss: 0.6095 - val_acc: 0.6458\n",
            "Epoch 499/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5427 - acc: 0.7059 - val_loss: 0.6043 - val_acc: 0.6458\n",
            "Epoch 500/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.4995 - acc: 0.7353 - val_loss: 0.6138 - val_acc: 0.6667\n",
            "Epoch 501/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5281 - acc: 0.7337 - val_loss: 0.5990 - val_acc: 0.6354\n",
            "Epoch 502/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5131 - acc: 0.7451 - val_loss: 0.5983 - val_acc: 0.6146\n",
            "Epoch 503/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5369 - acc: 0.7288 - val_loss: 0.5986 - val_acc: 0.6562\n",
            "Epoch 504/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.4970 - acc: 0.7582 - val_loss: 0.6159 - val_acc: 0.6458\n",
            "Epoch 505/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5298 - acc: 0.7190 - val_loss: 0.6235 - val_acc: 0.6875\n",
            "Epoch 506/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.4838 - acc: 0.7778 - val_loss: 0.5881 - val_acc: 0.6146\n",
            "Epoch 507/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 0.5489 - acc: 0.7190 - val_loss: 0.5956 - val_acc: 0.6146\n",
            "Epoch 508/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5084 - acc: 0.7533 - val_loss: 0.6160 - val_acc: 0.6667\n",
            "Epoch 509/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5247 - acc: 0.7549 - val_loss: 0.6229 - val_acc: 0.6458\n",
            "Epoch 510/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5257 - acc: 0.7402 - val_loss: 0.6079 - val_acc: 0.6146\n",
            "Epoch 511/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 0.5201 - acc: 0.7222 - val_loss: 0.6061 - val_acc: 0.6562\n",
            "Epoch 512/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5241 - acc: 0.7206 - val_loss: 0.6046 - val_acc: 0.6667\n",
            "Epoch 513/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5242 - acc: 0.7402 - val_loss: 0.6038 - val_acc: 0.6354\n",
            "Epoch 514/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5063 - acc: 0.7467 - val_loss: 0.5973 - val_acc: 0.6354\n",
            "Epoch 515/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5248 - acc: 0.7418 - val_loss: 0.6078 - val_acc: 0.6146\n",
            "Epoch 516/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5115 - acc: 0.7337 - val_loss: 0.6155 - val_acc: 0.6667\n",
            "Epoch 517/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5230 - acc: 0.7239 - val_loss: 0.6023 - val_acc: 0.6667\n",
            "Epoch 518/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5712 - acc: 0.6961 - val_loss: 0.6117 - val_acc: 0.6354\n",
            "Epoch 519/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5161 - acc: 0.7418 - val_loss: 0.5902 - val_acc: 0.6250\n",
            "Epoch 520/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5186 - acc: 0.7402 - val_loss: 0.6091 - val_acc: 0.6354\n",
            "Epoch 521/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5116 - acc: 0.7304 - val_loss: 0.6131 - val_acc: 0.5938\n",
            "Epoch 522/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5078 - acc: 0.7663 - val_loss: 0.6206 - val_acc: 0.6562\n",
            "Epoch 523/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5552 - acc: 0.7026 - val_loss: 0.5873 - val_acc: 0.6354\n",
            "Epoch 524/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5075 - acc: 0.7484 - val_loss: 0.6097 - val_acc: 0.6146\n",
            "Epoch 525/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5023 - acc: 0.7467 - val_loss: 0.5922 - val_acc: 0.6146\n",
            "Epoch 526/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5191 - acc: 0.7108 - val_loss: 0.6152 - val_acc: 0.6458\n",
            "Epoch 527/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5208 - acc: 0.7288 - val_loss: 0.6148 - val_acc: 0.6562\n",
            "Epoch 528/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5301 - acc: 0.7320 - val_loss: 0.6057 - val_acc: 0.6354\n",
            "Epoch 529/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5415 - acc: 0.7418 - val_loss: 0.5977 - val_acc: 0.6250\n",
            "Epoch 530/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5151 - acc: 0.7484 - val_loss: 0.5947 - val_acc: 0.6562\n",
            "Epoch 531/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.5065 - acc: 0.7435 - val_loss: 0.6158 - val_acc: 0.6562\n",
            "Epoch 532/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5151 - acc: 0.7467 - val_loss: 0.6048 - val_acc: 0.6667\n",
            "Epoch 533/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5053 - acc: 0.7500 - val_loss: 0.6197 - val_acc: 0.6771\n",
            "Epoch 534/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 0.4714 - acc: 0.7565 - val_loss: 0.6326 - val_acc: 0.6458\n",
            "Epoch 535/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5093 - acc: 0.7451 - val_loss: 0.6095 - val_acc: 0.6562\n",
            "Epoch 536/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5108 - acc: 0.7533 - val_loss: 0.6131 - val_acc: 0.6562\n",
            "Epoch 537/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 0.5389 - acc: 0.7402 - val_loss: 0.6218 - val_acc: 0.6458\n",
            "Epoch 538/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.5005 - acc: 0.7435 - val_loss: 0.6257 - val_acc: 0.6458\n",
            "Epoch 539/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5118 - acc: 0.7451 - val_loss: 0.6101 - val_acc: 0.6458\n",
            "Epoch 540/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5011 - acc: 0.7533 - val_loss: 0.6327 - val_acc: 0.6562\n",
            "Epoch 541/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5150 - acc: 0.7369 - val_loss: 0.6183 - val_acc: 0.6562\n",
            "Epoch 542/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 0.5270 - acc: 0.7369 - val_loss: 0.6301 - val_acc: 0.6354\n",
            "Epoch 543/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5586 - acc: 0.7402 - val_loss: 0.5888 - val_acc: 0.6562\n",
            "Epoch 544/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5349 - acc: 0.7337 - val_loss: 0.6137 - val_acc: 0.6458\n",
            "Epoch 545/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5295 - acc: 0.7320 - val_loss: 0.6134 - val_acc: 0.6562\n",
            "Epoch 546/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.4819 - acc: 0.7647 - val_loss: 0.6212 - val_acc: 0.6562\n",
            "Epoch 547/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 0.5144 - acc: 0.7418 - val_loss: 0.5993 - val_acc: 0.6354\n",
            "Epoch 548/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5147 - acc: 0.7386 - val_loss: 0.6061 - val_acc: 0.6562\n",
            "Epoch 549/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.4897 - acc: 0.7516 - val_loss: 0.6162 - val_acc: 0.6250\n",
            "Epoch 550/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5091 - acc: 0.7418 - val_loss: 0.6151 - val_acc: 0.6354\n",
            "Epoch 551/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5255 - acc: 0.7320 - val_loss: 0.6199 - val_acc: 0.6458\n",
            "Epoch 552/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 0.5188 - acc: 0.7255 - val_loss: 0.5951 - val_acc: 0.6250\n",
            "Epoch 553/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5312 - acc: 0.7516 - val_loss: 0.6031 - val_acc: 0.6458\n",
            "Epoch 554/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 0.5196 - acc: 0.7386 - val_loss: 0.5992 - val_acc: 0.6250\n",
            "Epoch 555/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5490 - acc: 0.7190 - val_loss: 0.6233 - val_acc: 0.6562\n",
            "Epoch 556/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.4917 - acc: 0.7533 - val_loss: 0.5832 - val_acc: 0.6771\n",
            "Epoch 557/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5441 - acc: 0.7386 - val_loss: 0.5858 - val_acc: 0.6146\n",
            "Epoch 558/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5413 - acc: 0.7418 - val_loss: 0.6092 - val_acc: 0.6250\n",
            "Epoch 559/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5136 - acc: 0.7402 - val_loss: 0.6243 - val_acc: 0.6458\n",
            "Epoch 560/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 0.5189 - acc: 0.7500 - val_loss: 0.5939 - val_acc: 0.6562\n",
            "Epoch 561/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 0.4834 - acc: 0.7647 - val_loss: 0.6240 - val_acc: 0.6458\n",
            "Epoch 562/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.4888 - acc: 0.7614 - val_loss: 0.6283 - val_acc: 0.6771\n",
            "Epoch 563/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.4926 - acc: 0.7647 - val_loss: 0.6484 - val_acc: 0.6667\n",
            "Epoch 564/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5308 - acc: 0.7239 - val_loss: 0.6300 - val_acc: 0.6458\n",
            "Epoch 565/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.4952 - acc: 0.7614 - val_loss: 0.6531 - val_acc: 0.6771\n",
            "Epoch 566/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 0.5532 - acc: 0.7075 - val_loss: 0.6223 - val_acc: 0.6979\n",
            "Epoch 567/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5339 - acc: 0.7337 - val_loss: 0.6349 - val_acc: 0.6458\n",
            "Epoch 568/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 0.5132 - acc: 0.7435 - val_loss: 0.6272 - val_acc: 0.6354\n",
            "Epoch 569/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5113 - acc: 0.7451 - val_loss: 0.6656 - val_acc: 0.6875\n",
            "Epoch 570/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5490 - acc: 0.7222 - val_loss: 0.6178 - val_acc: 0.5938\n",
            "Epoch 571/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5348 - acc: 0.7369 - val_loss: 0.6338 - val_acc: 0.6458\n",
            "Epoch 572/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5370 - acc: 0.7353 - val_loss: 0.6241 - val_acc: 0.6562\n",
            "Epoch 573/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5282 - acc: 0.7386 - val_loss: 0.6114 - val_acc: 0.5938\n",
            "Epoch 574/1000\n",
            "39/39 [==============================] - 11s 254ms/step - loss: 0.5029 - acc: 0.7614 - val_loss: 0.6152 - val_acc: 0.6042\n",
            "Epoch 575/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5507 - acc: 0.7353 - val_loss: 0.5957 - val_acc: 0.6562\n",
            "Epoch 576/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5151 - acc: 0.7436 - val_loss: 0.6100 - val_acc: 0.6250\n",
            "Epoch 577/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5537 - acc: 0.7108 - val_loss: 0.6377 - val_acc: 0.6667\n",
            "Epoch 578/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5035 - acc: 0.7484 - val_loss: 0.6136 - val_acc: 0.6667\n",
            "Epoch 579/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5233 - acc: 0.7516 - val_loss: 0.6060 - val_acc: 0.6562\n",
            "Epoch 580/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.4983 - acc: 0.7304 - val_loss: 0.6071 - val_acc: 0.6667\n",
            "Epoch 581/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 0.4927 - acc: 0.7533 - val_loss: 0.5989 - val_acc: 0.6250\n",
            "Epoch 582/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 0.4967 - acc: 0.7467 - val_loss: 0.6075 - val_acc: 0.6562\n",
            "Epoch 583/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5138 - acc: 0.7386 - val_loss: 0.6056 - val_acc: 0.5833\n",
            "Epoch 584/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 0.5070 - acc: 0.7467 - val_loss: 0.6110 - val_acc: 0.6250\n",
            "Epoch 585/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.4950 - acc: 0.7386 - val_loss: 0.5905 - val_acc: 0.6250\n",
            "Epoch 586/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5213 - acc: 0.7484 - val_loss: 0.6075 - val_acc: 0.6354\n",
            "Epoch 587/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5032 - acc: 0.7565 - val_loss: 0.6277 - val_acc: 0.6562\n",
            "Epoch 588/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5294 - acc: 0.7239 - val_loss: 0.6340 - val_acc: 0.6354\n",
            "Epoch 589/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5226 - acc: 0.7418 - val_loss: 0.6030 - val_acc: 0.6042\n",
            "Epoch 590/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5178 - acc: 0.7516 - val_loss: 0.5911 - val_acc: 0.6458\n",
            "Epoch 591/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5033 - acc: 0.7598 - val_loss: 0.6177 - val_acc: 0.6354\n",
            "Epoch 592/1000\n",
            "39/39 [==============================] - 6s 129ms/step - loss: 0.5057 - acc: 0.7663 - val_loss: 0.6043 - val_acc: 0.6250\n",
            "Epoch 593/1000\n",
            "39/39 [==============================] - 5s 128ms/step - loss: 0.5100 - acc: 0.7320 - val_loss: 0.6119 - val_acc: 0.6250\n",
            "Epoch 594/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5275 - acc: 0.7320 - val_loss: 0.6166 - val_acc: 0.6562\n",
            "Epoch 595/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 0.5322 - acc: 0.7467 - val_loss: 0.6039 - val_acc: 0.6458\n",
            "Epoch 596/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5364 - acc: 0.7222 - val_loss: 0.6057 - val_acc: 0.6458\n",
            "Epoch 597/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5011 - acc: 0.7500 - val_loss: 0.6207 - val_acc: 0.6979\n",
            "Epoch 598/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 0.5405 - acc: 0.7549 - val_loss: 0.6071 - val_acc: 0.6771\n",
            "Epoch 599/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5270 - acc: 0.7451 - val_loss: 0.5841 - val_acc: 0.6667\n",
            "Epoch 600/1000\n",
            "39/39 [==============================] - 7s 159ms/step - loss: 0.4849 - acc: 0.7467 - val_loss: 0.5809 - val_acc: 0.6667\n",
            "Epoch 601/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5083 - acc: 0.7451 - val_loss: 0.6000 - val_acc: 0.6771\n",
            "Epoch 602/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 0.5072 - acc: 0.7516 - val_loss: 0.5921 - val_acc: 0.5938\n",
            "Epoch 603/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5027 - acc: 0.7729 - val_loss: 0.6012 - val_acc: 0.6562\n",
            "Epoch 604/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5451 - acc: 0.7418 - val_loss: 0.6277 - val_acc: 0.6458\n",
            "Epoch 605/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5174 - acc: 0.7222 - val_loss: 0.6215 - val_acc: 0.6354\n",
            "Epoch 606/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 0.5118 - acc: 0.7565 - val_loss: 0.5966 - val_acc: 0.6458\n",
            "Epoch 607/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5136 - acc: 0.7451 - val_loss: 0.6097 - val_acc: 0.6667\n",
            "Epoch 608/1000\n",
            "39/39 [==============================] - 5s 95ms/step - loss: 0.4922 - acc: 0.7467 - val_loss: 0.6027 - val_acc: 0.6354\n",
            "Epoch 609/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 0.4859 - acc: 0.7598 - val_loss: 0.5973 - val_acc: 0.6771\n",
            "Epoch 610/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5101 - acc: 0.7451 - val_loss: 0.6058 - val_acc: 0.6458\n",
            "Epoch 611/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.5094 - acc: 0.7451 - val_loss: 0.6052 - val_acc: 0.6354\n",
            "Epoch 612/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.4987 - acc: 0.7516 - val_loss: 0.5960 - val_acc: 0.6354\n",
            "Epoch 613/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5352 - acc: 0.7353 - val_loss: 0.5918 - val_acc: 0.6458\n",
            "Epoch 614/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5460 - acc: 0.7222 - val_loss: 0.6062 - val_acc: 0.5938\n",
            "Epoch 615/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 0.5401 - acc: 0.7255 - val_loss: 0.6021 - val_acc: 0.5729\n",
            "Epoch 616/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5198 - acc: 0.7418 - val_loss: 0.6137 - val_acc: 0.6771\n",
            "Epoch 617/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.5357 - acc: 0.7337 - val_loss: 0.6105 - val_acc: 0.6354\n",
            "Epoch 618/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5032 - acc: 0.7386 - val_loss: 0.6204 - val_acc: 0.6667\n",
            "Epoch 619/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5363 - acc: 0.7337 - val_loss: 0.6123 - val_acc: 0.6667\n",
            "Epoch 620/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 0.5165 - acc: 0.7500 - val_loss: 0.6111 - val_acc: 0.6667\n",
            "Epoch 621/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.4930 - acc: 0.7647 - val_loss: 0.6275 - val_acc: 0.6562\n",
            "Epoch 622/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 0.5290 - acc: 0.7451 - val_loss: 0.6075 - val_acc: 0.6667\n",
            "Epoch 623/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5232 - acc: 0.7222 - val_loss: 0.6047 - val_acc: 0.6875\n",
            "Epoch 624/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5165 - acc: 0.7598 - val_loss: 0.6046 - val_acc: 0.6354\n",
            "Epoch 625/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5095 - acc: 0.7516 - val_loss: 0.6030 - val_acc: 0.6562\n",
            "Epoch 626/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.4948 - acc: 0.7255 - val_loss: 0.6008 - val_acc: 0.6875\n",
            "Epoch 627/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5262 - acc: 0.7418 - val_loss: 0.6290 - val_acc: 0.6875\n",
            "Epoch 628/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5114 - acc: 0.7533 - val_loss: 0.6233 - val_acc: 0.6562\n",
            "Epoch 629/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.4737 - acc: 0.7696 - val_loss: 0.6564 - val_acc: 0.6667\n",
            "Epoch 630/1000\n",
            "39/39 [==============================] - 6s 134ms/step - loss: 0.5401 - acc: 0.7288 - val_loss: 0.6053 - val_acc: 0.6562\n",
            "Epoch 631/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5043 - acc: 0.7222 - val_loss: 0.6049 - val_acc: 0.6562\n",
            "Epoch 632/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5071 - acc: 0.7324 - val_loss: 0.6036 - val_acc: 0.6667\n",
            "Epoch 633/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5034 - acc: 0.7369 - val_loss: 0.6000 - val_acc: 0.6354\n",
            "Epoch 634/1000\n",
            "39/39 [==============================] - 6s 141ms/step - loss: 0.5513 - acc: 0.7206 - val_loss: 0.6305 - val_acc: 0.6667\n",
            "Epoch 635/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5035 - acc: 0.7451 - val_loss: 0.6049 - val_acc: 0.6458\n",
            "Epoch 636/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.4867 - acc: 0.7729 - val_loss: 0.5968 - val_acc: 0.6771\n",
            "Epoch 637/1000\n",
            "39/39 [==============================] - 6s 132ms/step - loss: 0.5313 - acc: 0.7288 - val_loss: 0.6192 - val_acc: 0.6146\n",
            "Epoch 638/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5109 - acc: 0.7451 - val_loss: 0.6231 - val_acc: 0.6562\n",
            "Epoch 639/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 0.5184 - acc: 0.7320 - val_loss: 0.6071 - val_acc: 0.6354\n",
            "Epoch 640/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5489 - acc: 0.7271 - val_loss: 0.6011 - val_acc: 0.6354\n",
            "Epoch 641/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5099 - acc: 0.7353 - val_loss: 0.6220 - val_acc: 0.6458\n",
            "Epoch 642/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5278 - acc: 0.7386 - val_loss: 0.6377 - val_acc: 0.6667\n",
            "Epoch 643/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5440 - acc: 0.7304 - val_loss: 0.6279 - val_acc: 0.6667\n",
            "Epoch 644/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5168 - acc: 0.7451 - val_loss: 0.5934 - val_acc: 0.6250\n",
            "Epoch 645/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5206 - acc: 0.7173 - val_loss: 0.5912 - val_acc: 0.6146\n",
            "Epoch 646/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.4771 - acc: 0.7778 - val_loss: 0.5743 - val_acc: 0.6771\n",
            "Epoch 647/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5077 - acc: 0.7467 - val_loss: 0.5902 - val_acc: 0.6354\n",
            "Epoch 648/1000\n",
            "39/39 [==============================] - 6s 144ms/step - loss: 0.5146 - acc: 0.7386 - val_loss: 0.5955 - val_acc: 0.6562\n",
            "Epoch 649/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.5240 - acc: 0.7288 - val_loss: 0.5991 - val_acc: 0.6562\n",
            "Epoch 650/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5218 - acc: 0.7369 - val_loss: 0.5869 - val_acc: 0.6667\n",
            "Epoch 651/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5137 - acc: 0.7435 - val_loss: 0.5923 - val_acc: 0.6250\n",
            "Epoch 652/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5162 - acc: 0.7239 - val_loss: 0.5931 - val_acc: 0.6458\n",
            "Epoch 653/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5409 - acc: 0.7190 - val_loss: 0.6066 - val_acc: 0.6667\n",
            "Epoch 654/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 0.5362 - acc: 0.7320 - val_loss: 0.5975 - val_acc: 0.6458\n",
            "Epoch 655/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5588 - acc: 0.7386 - val_loss: 0.5918 - val_acc: 0.6562\n",
            "Epoch 656/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5199 - acc: 0.7418 - val_loss: 0.5865 - val_acc: 0.6562\n",
            "Epoch 657/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.4948 - acc: 0.7582 - val_loss: 0.5873 - val_acc: 0.6042\n",
            "Epoch 658/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5396 - acc: 0.7356 - val_loss: 0.5882 - val_acc: 0.6562\n",
            "Epoch 659/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.4979 - acc: 0.7451 - val_loss: 0.5803 - val_acc: 0.6562\n",
            "Epoch 660/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 0.5369 - acc: 0.7402 - val_loss: 0.5845 - val_acc: 0.6667\n",
            "Epoch 661/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5022 - acc: 0.7500 - val_loss: 0.6136 - val_acc: 0.6562\n",
            "Epoch 662/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 0.4982 - acc: 0.7582 - val_loss: 0.6175 - val_acc: 0.6562\n",
            "Epoch 663/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5245 - acc: 0.7386 - val_loss: 0.5985 - val_acc: 0.6146\n",
            "Epoch 664/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 0.5233 - acc: 0.7598 - val_loss: 0.6260 - val_acc: 0.6458\n",
            "Epoch 665/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.4896 - acc: 0.7712 - val_loss: 0.6279 - val_acc: 0.6458\n",
            "Epoch 666/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5357 - acc: 0.7467 - val_loss: 0.6081 - val_acc: 0.6667\n",
            "Epoch 667/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5485 - acc: 0.7157 - val_loss: 0.6227 - val_acc: 0.7083\n",
            "Epoch 668/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5184 - acc: 0.7451 - val_loss: 0.6027 - val_acc: 0.6250\n",
            "Epoch 669/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 0.5240 - acc: 0.7353 - val_loss: 0.6088 - val_acc: 0.6771\n",
            "Epoch 670/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5274 - acc: 0.7239 - val_loss: 0.5864 - val_acc: 0.6458\n",
            "Epoch 671/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.5194 - acc: 0.7451 - val_loss: 0.6149 - val_acc: 0.6667\n",
            "Epoch 672/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.5217 - acc: 0.7484 - val_loss: 0.5952 - val_acc: 0.6458\n",
            "Epoch 673/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5037 - acc: 0.7418 - val_loss: 0.5729 - val_acc: 0.6562\n",
            "Epoch 674/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5386 - acc: 0.7222 - val_loss: 0.5856 - val_acc: 0.6458\n",
            "Epoch 675/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.4795 - acc: 0.7598 - val_loss: 0.5704 - val_acc: 0.6979\n",
            "Epoch 676/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5245 - acc: 0.7451 - val_loss: 0.5882 - val_acc: 0.6458\n",
            "Epoch 677/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5033 - acc: 0.7402 - val_loss: 0.5935 - val_acc: 0.6979\n",
            "Epoch 678/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 0.4997 - acc: 0.7288 - val_loss: 0.5921 - val_acc: 0.6458\n",
            "Epoch 679/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5328 - acc: 0.7320 - val_loss: 0.5998 - val_acc: 0.6354\n",
            "Epoch 680/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5444 - acc: 0.7190 - val_loss: 0.5892 - val_acc: 0.6562\n",
            "Epoch 681/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.5085 - acc: 0.7598 - val_loss: 0.5748 - val_acc: 0.6667\n",
            "Epoch 682/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5291 - acc: 0.7271 - val_loss: 0.5888 - val_acc: 0.6458\n",
            "Epoch 683/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.4687 - acc: 0.7614 - val_loss: 0.5863 - val_acc: 0.6458\n",
            "Epoch 684/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5165 - acc: 0.7320 - val_loss: 0.6034 - val_acc: 0.6354\n",
            "Epoch 685/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5455 - acc: 0.7075 - val_loss: 0.6034 - val_acc: 0.6667\n",
            "Epoch 686/1000\n",
            "39/39 [==============================] - 6s 133ms/step - loss: 0.5262 - acc: 0.7320 - val_loss: 0.6212 - val_acc: 0.6875\n",
            "Epoch 687/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.4920 - acc: 0.7451 - val_loss: 0.5735 - val_acc: 0.6562\n",
            "Epoch 688/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5033 - acc: 0.7582 - val_loss: 0.5801 - val_acc: 0.6458\n",
            "Epoch 689/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5243 - acc: 0.7206 - val_loss: 0.5773 - val_acc: 0.6667\n",
            "Epoch 690/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.5153 - acc: 0.7484 - val_loss: 0.5780 - val_acc: 0.6875\n",
            "Epoch 691/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5100 - acc: 0.7533 - val_loss: 0.5680 - val_acc: 0.6875\n",
            "Epoch 692/1000\n",
            "39/39 [==============================] - 10s 256ms/step - loss: 0.4834 - acc: 0.7794 - val_loss: 0.5499 - val_acc: 0.6562\n",
            "Epoch 693/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5071 - acc: 0.7598 - val_loss: 0.5799 - val_acc: 0.6667\n",
            "Epoch 694/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.5144 - acc: 0.7402 - val_loss: 0.5738 - val_acc: 0.6771\n",
            "Epoch 695/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.4892 - acc: 0.7500 - val_loss: 0.5888 - val_acc: 0.6562\n",
            "Epoch 696/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.4894 - acc: 0.7647 - val_loss: 0.5878 - val_acc: 0.6771\n",
            "Epoch 697/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.4759 - acc: 0.7859 - val_loss: 0.5978 - val_acc: 0.6458\n",
            "Epoch 698/1000\n",
            "39/39 [==============================] - 6s 153ms/step - loss: 0.4988 - acc: 0.7598 - val_loss: 0.5888 - val_acc: 0.6458\n",
            "Epoch 699/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5032 - acc: 0.7484 - val_loss: 0.6008 - val_acc: 0.6562\n",
            "Epoch 700/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 0.5048 - acc: 0.7467 - val_loss: 0.5953 - val_acc: 0.6458\n",
            "Epoch 701/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.4958 - acc: 0.7549 - val_loss: 0.5834 - val_acc: 0.6250\n",
            "Epoch 702/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.5115 - acc: 0.7451 - val_loss: 0.5867 - val_acc: 0.6354\n",
            "Epoch 703/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5086 - acc: 0.7402 - val_loss: 0.6097 - val_acc: 0.6458\n",
            "Epoch 704/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5381 - acc: 0.7304 - val_loss: 0.6173 - val_acc: 0.6771\n",
            "Epoch 705/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5090 - acc: 0.7549 - val_loss: 0.6118 - val_acc: 0.6458\n",
            "Epoch 706/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.4857 - acc: 0.7631 - val_loss: 0.6150 - val_acc: 0.6667\n",
            "Epoch 707/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.4994 - acc: 0.7598 - val_loss: 0.6041 - val_acc: 0.6458\n",
            "Epoch 708/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5073 - acc: 0.7500 - val_loss: 0.5907 - val_acc: 0.6667\n",
            "Epoch 709/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.5336 - acc: 0.7190 - val_loss: 0.6070 - val_acc: 0.6458\n",
            "Epoch 710/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5410 - acc: 0.7320 - val_loss: 0.6022 - val_acc: 0.6562\n",
            "Epoch 711/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5427 - acc: 0.7239 - val_loss: 0.6013 - val_acc: 0.6667\n",
            "Epoch 712/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5057 - acc: 0.7712 - val_loss: 0.5947 - val_acc: 0.6667\n",
            "Epoch 713/1000\n",
            "39/39 [==============================] - 5s 126ms/step - loss: 0.4642 - acc: 0.7810 - val_loss: 0.6212 - val_acc: 0.6354\n",
            "Epoch 714/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5217 - acc: 0.7402 - val_loss: 0.6105 - val_acc: 0.6458\n",
            "Epoch 715/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5452 - acc: 0.7337 - val_loss: 0.5933 - val_acc: 0.6250\n",
            "Epoch 716/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 0.4728 - acc: 0.7843 - val_loss: 0.5994 - val_acc: 0.6458\n",
            "Epoch 717/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5073 - acc: 0.7500 - val_loss: 0.5980 - val_acc: 0.6458\n",
            "Epoch 718/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5374 - acc: 0.7386 - val_loss: 0.5960 - val_acc: 0.6771\n",
            "Epoch 719/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5034 - acc: 0.7598 - val_loss: 0.6161 - val_acc: 0.6667\n",
            "Epoch 720/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5279 - acc: 0.7271 - val_loss: 0.5931 - val_acc: 0.6562\n",
            "Epoch 721/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5501 - acc: 0.7059 - val_loss: 0.6091 - val_acc: 0.6771\n",
            "Epoch 722/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 0.5390 - acc: 0.7565 - val_loss: 0.6199 - val_acc: 0.6354\n",
            "Epoch 723/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5162 - acc: 0.7549 - val_loss: 0.6024 - val_acc: 0.6667\n",
            "Epoch 724/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5385 - acc: 0.7173 - val_loss: 0.5936 - val_acc: 0.6667\n",
            "Epoch 725/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5026 - acc: 0.7467 - val_loss: 0.5842 - val_acc: 0.6146\n",
            "Epoch 726/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 0.5009 - acc: 0.7500 - val_loss: 0.5862 - val_acc: 0.6667\n",
            "Epoch 727/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5041 - acc: 0.7565 - val_loss: 0.6055 - val_acc: 0.6458\n",
            "Epoch 728/1000\n",
            "39/39 [==============================] - 6s 127ms/step - loss: 0.4904 - acc: 0.7680 - val_loss: 0.5994 - val_acc: 0.6667\n",
            "Epoch 729/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5084 - acc: 0.7467 - val_loss: 0.6171 - val_acc: 0.6667\n",
            "Epoch 730/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5126 - acc: 0.7304 - val_loss: 0.6215 - val_acc: 0.6562\n",
            "Epoch 731/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5372 - acc: 0.7647 - val_loss: 0.6155 - val_acc: 0.6354\n",
            "Epoch 732/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5278 - acc: 0.7402 - val_loss: 0.6133 - val_acc: 0.6562\n",
            "Epoch 733/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.4776 - acc: 0.7631 - val_loss: 0.6011 - val_acc: 0.5938\n",
            "Epoch 734/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5020 - acc: 0.7582 - val_loss: 0.5929 - val_acc: 0.6562\n",
            "Epoch 735/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.4988 - acc: 0.7565 - val_loss: 0.6085 - val_acc: 0.6562\n",
            "Epoch 736/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5350 - acc: 0.7402 - val_loss: 0.6173 - val_acc: 0.6771\n",
            "Epoch 737/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.4895 - acc: 0.7533 - val_loss: 0.6219 - val_acc: 0.6667\n",
            "Epoch 738/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.4877 - acc: 0.7484 - val_loss: 0.6068 - val_acc: 0.6250\n",
            "Epoch 739/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 0.5119 - acc: 0.7271 - val_loss: 0.6199 - val_acc: 0.6875\n",
            "Epoch 740/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5039 - acc: 0.7582 - val_loss: 0.6532 - val_acc: 0.6667\n",
            "Epoch 741/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5062 - acc: 0.7516 - val_loss: 0.6099 - val_acc: 0.6354\n",
            "Epoch 742/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5380 - acc: 0.7173 - val_loss: 0.5994 - val_acc: 0.6875\n",
            "Epoch 743/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5307 - acc: 0.7467 - val_loss: 0.6026 - val_acc: 0.6250\n",
            "Epoch 744/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5128 - acc: 0.7337 - val_loss: 0.5778 - val_acc: 0.6667\n",
            "Epoch 745/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5094 - acc: 0.7484 - val_loss: 0.5941 - val_acc: 0.6562\n",
            "Epoch 746/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.4835 - acc: 0.7696 - val_loss: 0.6348 - val_acc: 0.6562\n",
            "Epoch 747/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5253 - acc: 0.7386 - val_loss: 0.5961 - val_acc: 0.5729\n",
            "Epoch 748/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.4996 - acc: 0.7484 - val_loss: 0.6345 - val_acc: 0.6875\n",
            "Epoch 749/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5636 - acc: 0.7157 - val_loss: 0.6016 - val_acc: 0.6458\n",
            "Epoch 750/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.4897 - acc: 0.7761 - val_loss: 0.6170 - val_acc: 0.6354\n",
            "Epoch 751/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5130 - acc: 0.7353 - val_loss: 0.6168 - val_acc: 0.6458\n",
            "Epoch 752/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.4977 - acc: 0.7516 - val_loss: 0.5812 - val_acc: 0.6771\n",
            "Epoch 753/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 0.4991 - acc: 0.7614 - val_loss: 0.5774 - val_acc: 0.6667\n",
            "Epoch 754/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5034 - acc: 0.7402 - val_loss: 0.5766 - val_acc: 0.6667\n",
            "Epoch 755/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5104 - acc: 0.7320 - val_loss: 0.6027 - val_acc: 0.6250\n",
            "Epoch 756/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 0.5093 - acc: 0.7647 - val_loss: 0.5998 - val_acc: 0.5833\n",
            "Epoch 757/1000\n",
            "39/39 [==============================] - 6s 143ms/step - loss: 0.5614 - acc: 0.7108 - val_loss: 0.5961 - val_acc: 0.6354\n",
            "Epoch 758/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5420 - acc: 0.7320 - val_loss: 0.6187 - val_acc: 0.6771\n",
            "Epoch 759/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 0.5138 - acc: 0.7516 - val_loss: 0.5923 - val_acc: 0.6875\n",
            "Epoch 760/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5518 - acc: 0.7369 - val_loss: 0.6339 - val_acc: 0.6667\n",
            "Epoch 761/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.4921 - acc: 0.7663 - val_loss: 0.5949 - val_acc: 0.6562\n",
            "Epoch 762/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.4955 - acc: 0.7631 - val_loss: 0.5901 - val_acc: 0.6250\n",
            "Epoch 763/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5263 - acc: 0.7386 - val_loss: 0.5903 - val_acc: 0.6146\n",
            "Epoch 764/1000\n",
            "39/39 [==============================] - 9s 235ms/step - loss: 0.5412 - acc: 0.7288 - val_loss: 0.5853 - val_acc: 0.6979\n",
            "Epoch 765/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5211 - acc: 0.7500 - val_loss: 0.6048 - val_acc: 0.6562\n",
            "Epoch 766/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 0.5343 - acc: 0.7435 - val_loss: 0.5788 - val_acc: 0.6458\n",
            "Epoch 767/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5126 - acc: 0.7337 - val_loss: 0.6071 - val_acc: 0.6562\n",
            "Epoch 768/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 0.5322 - acc: 0.7353 - val_loss: 0.6098 - val_acc: 0.6667\n",
            "Epoch 769/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5487 - acc: 0.7222 - val_loss: 0.6027 - val_acc: 0.6354\n",
            "Epoch 770/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5367 - acc: 0.7435 - val_loss: 0.6180 - val_acc: 0.6562\n",
            "Epoch 771/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.4983 - acc: 0.7418 - val_loss: 0.5970 - val_acc: 0.6458\n",
            "Epoch 772/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5187 - acc: 0.7222 - val_loss: 0.6005 - val_acc: 0.6562\n",
            "Epoch 773/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5291 - acc: 0.7418 - val_loss: 0.5951 - val_acc: 0.6042\n",
            "Epoch 774/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.4845 - acc: 0.7549 - val_loss: 0.6047 - val_acc: 0.6146\n",
            "Epoch 775/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5301 - acc: 0.7484 - val_loss: 0.6117 - val_acc: 0.6979\n",
            "Epoch 776/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 0.5207 - acc: 0.7451 - val_loss: 0.5931 - val_acc: 0.6250\n",
            "Epoch 777/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.5395 - acc: 0.7308 - val_loss: 0.5859 - val_acc: 0.6771\n",
            "Epoch 778/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 0.5282 - acc: 0.7369 - val_loss: 0.5881 - val_acc: 0.6458\n",
            "Epoch 779/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5165 - acc: 0.7353 - val_loss: 0.5708 - val_acc: 0.6875\n",
            "Epoch 780/1000\n",
            "39/39 [==============================] - 5s 126ms/step - loss: 0.5091 - acc: 0.7451 - val_loss: 0.6041 - val_acc: 0.6458\n",
            "Epoch 781/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.4806 - acc: 0.7794 - val_loss: 0.5907 - val_acc: 0.6875\n",
            "Epoch 782/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5118 - acc: 0.7663 - val_loss: 0.5824 - val_acc: 0.6562\n",
            "Epoch 783/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5196 - acc: 0.7614 - val_loss: 0.5987 - val_acc: 0.6562\n",
            "Epoch 784/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.5123 - acc: 0.7500 - val_loss: 0.6172 - val_acc: 0.7083\n",
            "Epoch 785/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5172 - acc: 0.7402 - val_loss: 0.6099 - val_acc: 0.6562\n",
            "Epoch 786/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.4826 - acc: 0.7663 - val_loss: 0.5828 - val_acc: 0.6667\n",
            "Epoch 787/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5276 - acc: 0.7337 - val_loss: 0.6209 - val_acc: 0.6979\n",
            "Epoch 788/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5027 - acc: 0.7451 - val_loss: 0.5925 - val_acc: 0.6667\n",
            "Epoch 789/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 0.5201 - acc: 0.7500 - val_loss: 0.5934 - val_acc: 0.6771\n",
            "Epoch 790/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.4773 - acc: 0.7778 - val_loss: 0.5971 - val_acc: 0.6250\n",
            "Epoch 791/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 0.4981 - acc: 0.7404 - val_loss: 0.6073 - val_acc: 0.6458\n",
            "Epoch 792/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 0.5305 - acc: 0.7353 - val_loss: 0.5869 - val_acc: 0.6458\n",
            "Epoch 793/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5066 - acc: 0.7614 - val_loss: 0.6032 - val_acc: 0.6562\n",
            "Epoch 794/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 0.5403 - acc: 0.7369 - val_loss: 0.5819 - val_acc: 0.6667\n",
            "Epoch 795/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5090 - acc: 0.7516 - val_loss: 0.5861 - val_acc: 0.6458\n",
            "Epoch 796/1000\n",
            "39/39 [==============================] - 6s 131ms/step - loss: 0.5433 - acc: 0.7059 - val_loss: 0.6089 - val_acc: 0.6667\n",
            "Epoch 797/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5017 - acc: 0.7598 - val_loss: 0.5852 - val_acc: 0.6771\n",
            "Epoch 798/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5114 - acc: 0.7467 - val_loss: 0.6070 - val_acc: 0.6354\n",
            "Epoch 799/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.4950 - acc: 0.7484 - val_loss: 0.6047 - val_acc: 0.6562\n",
            "Epoch 800/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.4926 - acc: 0.7467 - val_loss: 0.5736 - val_acc: 0.6771\n",
            "Epoch 801/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.4970 - acc: 0.7614 - val_loss: 0.6048 - val_acc: 0.6354\n",
            "Epoch 802/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5414 - acc: 0.7141 - val_loss: 0.5758 - val_acc: 0.6250\n",
            "Epoch 803/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5027 - acc: 0.7565 - val_loss: 0.6029 - val_acc: 0.6562\n",
            "Epoch 804/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5267 - acc: 0.7386 - val_loss: 0.6002 - val_acc: 0.6458\n",
            "Epoch 805/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.4978 - acc: 0.7582 - val_loss: 0.5915 - val_acc: 0.6771\n",
            "Epoch 806/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5128 - acc: 0.7614 - val_loss: 0.6222 - val_acc: 0.6562\n",
            "Epoch 807/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 0.5040 - acc: 0.7516 - val_loss: 0.5902 - val_acc: 0.6354\n",
            "Epoch 808/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5077 - acc: 0.7386 - val_loss: 0.6109 - val_acc: 0.6562\n",
            "Epoch 809/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.4793 - acc: 0.7696 - val_loss: 0.6067 - val_acc: 0.6771\n",
            "Epoch 810/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.4838 - acc: 0.7778 - val_loss: 0.6083 - val_acc: 0.6771\n",
            "Epoch 811/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.5427 - acc: 0.7190 - val_loss: 0.5960 - val_acc: 0.6354\n",
            "Epoch 812/1000\n",
            "39/39 [==============================] - 5s 128ms/step - loss: 0.4943 - acc: 0.7794 - val_loss: 0.5909 - val_acc: 0.6875\n",
            "Epoch 813/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.5048 - acc: 0.7516 - val_loss: 0.5878 - val_acc: 0.6458\n",
            "Epoch 814/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5121 - acc: 0.7304 - val_loss: 0.5965 - val_acc: 0.6771\n",
            "Epoch 815/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 0.5110 - acc: 0.7386 - val_loss: 0.6038 - val_acc: 0.6771\n",
            "Epoch 816/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 0.5562 - acc: 0.7019 - val_loss: 0.6127 - val_acc: 0.6562\n",
            "Epoch 817/1000\n",
            "39/39 [==============================] - 10s 250ms/step - loss: 0.5124 - acc: 0.7484 - val_loss: 0.6214 - val_acc: 0.6562\n",
            "Epoch 818/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5411 - acc: 0.7484 - val_loss: 0.5947 - val_acc: 0.6458\n",
            "Epoch 819/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.4987 - acc: 0.7647 - val_loss: 0.5979 - val_acc: 0.6875\n",
            "Epoch 820/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5215 - acc: 0.7565 - val_loss: 0.6027 - val_acc: 0.6771\n",
            "Epoch 821/1000\n",
            "39/39 [==============================] - 6s 141ms/step - loss: 0.5003 - acc: 0.7631 - val_loss: 0.5855 - val_acc: 0.6875\n",
            "Epoch 822/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5313 - acc: 0.7369 - val_loss: 0.6024 - val_acc: 0.6979\n",
            "Epoch 823/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 0.5071 - acc: 0.7614 - val_loss: 0.6004 - val_acc: 0.6875\n",
            "Epoch 824/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5613 - acc: 0.7173 - val_loss: 0.5937 - val_acc: 0.6875\n",
            "Epoch 825/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5130 - acc: 0.7402 - val_loss: 0.5926 - val_acc: 0.6771\n",
            "Epoch 826/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 0.5148 - acc: 0.7533 - val_loss: 0.6080 - val_acc: 0.6875\n",
            "Epoch 827/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 0.5448 - acc: 0.7173 - val_loss: 0.6069 - val_acc: 0.6875\n",
            "Epoch 828/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 0.5142 - acc: 0.7337 - val_loss: 0.6004 - val_acc: 0.6875\n",
            "Epoch 829/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5417 - acc: 0.7304 - val_loss: 0.5962 - val_acc: 0.6354\n",
            "Epoch 830/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.4946 - acc: 0.7598 - val_loss: 0.6040 - val_acc: 0.6667\n",
            "Epoch 831/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.5211 - acc: 0.7353 - val_loss: 0.5769 - val_acc: 0.6562\n",
            "Epoch 832/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.4954 - acc: 0.7565 - val_loss: 0.5994 - val_acc: 0.6667\n",
            "Epoch 833/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5102 - acc: 0.7418 - val_loss: 0.6026 - val_acc: 0.6354\n",
            "Epoch 834/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 0.5022 - acc: 0.7386 - val_loss: 0.5848 - val_acc: 0.6562\n",
            "Epoch 835/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 0.5069 - acc: 0.7369 - val_loss: 0.6143 - val_acc: 0.6667\n",
            "Epoch 836/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5268 - acc: 0.7369 - val_loss: 0.5831 - val_acc: 0.6562\n",
            "Epoch 837/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5223 - acc: 0.7484 - val_loss: 0.5995 - val_acc: 0.6458\n",
            "Epoch 838/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5173 - acc: 0.7596 - val_loss: 0.5996 - val_acc: 0.6667\n",
            "Epoch 839/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 0.5334 - acc: 0.7320 - val_loss: 0.5944 - val_acc: 0.6458\n",
            "Epoch 840/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5205 - acc: 0.7435 - val_loss: 0.5785 - val_acc: 0.6458\n",
            "Epoch 841/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 0.5276 - acc: 0.7467 - val_loss: 0.5754 - val_acc: 0.6354\n",
            "Epoch 842/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5153 - acc: 0.7386 - val_loss: 0.6106 - val_acc: 0.6979\n",
            "Epoch 843/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.5304 - acc: 0.7173 - val_loss: 0.5990 - val_acc: 0.6354\n",
            "Epoch 844/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 0.5174 - acc: 0.7516 - val_loss: 0.5737 - val_acc: 0.6667\n",
            "Epoch 845/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.4930 - acc: 0.7614 - val_loss: 0.5947 - val_acc: 0.6875\n",
            "Epoch 846/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5367 - acc: 0.7288 - val_loss: 0.5756 - val_acc: 0.6875\n",
            "Epoch 847/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5162 - acc: 0.7484 - val_loss: 0.5858 - val_acc: 0.6771\n",
            "Epoch 848/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 0.4972 - acc: 0.7451 - val_loss: 0.5836 - val_acc: 0.6250\n",
            "Epoch 849/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.5361 - acc: 0.7320 - val_loss: 0.5902 - val_acc: 0.6250\n",
            "Epoch 850/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5188 - acc: 0.7304 - val_loss: 0.5855 - val_acc: 0.6875\n",
            "Epoch 851/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5038 - acc: 0.7761 - val_loss: 0.5995 - val_acc: 0.6250\n",
            "Epoch 852/1000\n",
            "39/39 [==============================] - 7s 152ms/step - loss: 0.5053 - acc: 0.7549 - val_loss: 0.6056 - val_acc: 0.6667\n",
            "Epoch 853/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 0.4667 - acc: 0.7582 - val_loss: 0.6130 - val_acc: 0.6875\n",
            "Epoch 854/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 0.5191 - acc: 0.7190 - val_loss: 0.5934 - val_acc: 0.6354\n",
            "Epoch 855/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 0.4980 - acc: 0.7451 - val_loss: 0.5752 - val_acc: 0.6458\n",
            "Epoch 856/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5137 - acc: 0.7500 - val_loss: 0.5968 - val_acc: 0.6146\n",
            "Epoch 857/1000\n",
            "39/39 [==============================] - 6s 134ms/step - loss: 0.5439 - acc: 0.7288 - val_loss: 0.5807 - val_acc: 0.6562\n",
            "Epoch 858/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5186 - acc: 0.7548 - val_loss: 0.5942 - val_acc: 0.6562\n",
            "Epoch 859/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5131 - acc: 0.7324 - val_loss: 0.5891 - val_acc: 0.6458\n",
            "Epoch 860/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5140 - acc: 0.7402 - val_loss: 0.5777 - val_acc: 0.6562\n",
            "Epoch 861/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5242 - acc: 0.7353 - val_loss: 0.5691 - val_acc: 0.6875\n",
            "Epoch 862/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5171 - acc: 0.7533 - val_loss: 0.5888 - val_acc: 0.6042\n",
            "Epoch 863/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.4933 - acc: 0.7631 - val_loss: 0.5976 - val_acc: 0.6458\n",
            "Epoch 864/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 0.5287 - acc: 0.7386 - val_loss: 0.6080 - val_acc: 0.6667\n",
            "Epoch 865/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 0.5199 - acc: 0.7386 - val_loss: 0.6064 - val_acc: 0.6354\n",
            "Epoch 866/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 0.5201 - acc: 0.7435 - val_loss: 0.5831 - val_acc: 0.6771\n",
            "Epoch 867/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5654 - acc: 0.6977 - val_loss: 0.5846 - val_acc: 0.6458\n",
            "Epoch 868/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 0.5236 - acc: 0.7500 - val_loss: 0.5814 - val_acc: 0.6250\n",
            "Epoch 869/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.5189 - acc: 0.7356 - val_loss: 0.5959 - val_acc: 0.6667\n",
            "Epoch 870/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5193 - acc: 0.7271 - val_loss: 0.6429 - val_acc: 0.6771\n",
            "Epoch 871/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5458 - acc: 0.7386 - val_loss: 0.6120 - val_acc: 0.6875\n",
            "Epoch 872/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5033 - acc: 0.7369 - val_loss: 0.6019 - val_acc: 0.6042\n",
            "Epoch 873/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 0.4817 - acc: 0.7810 - val_loss: 0.5760 - val_acc: 0.6458\n",
            "Epoch 874/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.5055 - acc: 0.7467 - val_loss: 0.5950 - val_acc: 0.6458\n",
            "Epoch 875/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5218 - acc: 0.7288 - val_loss: 0.6130 - val_acc: 0.6354\n",
            "Epoch 876/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.5053 - acc: 0.7631 - val_loss: 0.6160 - val_acc: 0.6458\n",
            "Epoch 877/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.5260 - acc: 0.7631 - val_loss: 0.6006 - val_acc: 0.6667\n",
            "Epoch 878/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5001 - acc: 0.7500 - val_loss: 0.5896 - val_acc: 0.6667\n",
            "Epoch 879/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.5267 - acc: 0.7337 - val_loss: 0.5989 - val_acc: 0.6562\n",
            "Epoch 880/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.5223 - acc: 0.7533 - val_loss: 0.6249 - val_acc: 0.6562\n",
            "Epoch 881/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5097 - acc: 0.7533 - val_loss: 0.6095 - val_acc: 0.6771\n",
            "Epoch 882/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 0.5055 - acc: 0.7696 - val_loss: 0.6163 - val_acc: 0.6667\n",
            "Epoch 883/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5468 - acc: 0.7059 - val_loss: 0.5899 - val_acc: 0.6875\n",
            "Epoch 884/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5126 - acc: 0.7353 - val_loss: 0.6038 - val_acc: 0.6771\n",
            "Epoch 885/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 0.4896 - acc: 0.7794 - val_loss: 0.6089 - val_acc: 0.6562\n",
            "Epoch 886/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.4914 - acc: 0.7843 - val_loss: 0.6464 - val_acc: 0.6771\n",
            "Epoch 887/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 0.5206 - acc: 0.7418 - val_loss: 0.5798 - val_acc: 0.6562\n",
            "Epoch 888/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5157 - acc: 0.7418 - val_loss: 0.6289 - val_acc: 0.6875\n",
            "Epoch 889/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5013 - acc: 0.7467 - val_loss: 0.6040 - val_acc: 0.6771\n",
            "Epoch 890/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 0.4846 - acc: 0.7580 - val_loss: 0.6433 - val_acc: 0.6771\n",
            "Epoch 891/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 0.5435 - acc: 0.7500 - val_loss: 0.5889 - val_acc: 0.6875\n",
            "Epoch 892/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.4796 - acc: 0.7631 - val_loss: 0.5746 - val_acc: 0.6979\n",
            "Epoch 893/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 0.5409 - acc: 0.7173 - val_loss: 0.6190 - val_acc: 0.6562\n",
            "Epoch 894/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.4773 - acc: 0.7582 - val_loss: 0.5998 - val_acc: 0.6562\n",
            "Epoch 895/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.4978 - acc: 0.7794 - val_loss: 0.6213 - val_acc: 0.6562\n",
            "Epoch 896/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.4923 - acc: 0.7761 - val_loss: 0.6123 - val_acc: 0.6458\n",
            "Epoch 897/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 0.5210 - acc: 0.7402 - val_loss: 0.6042 - val_acc: 0.6354\n",
            "Epoch 898/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.4820 - acc: 0.7712 - val_loss: 0.6406 - val_acc: 0.6458\n",
            "Epoch 899/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5557 - acc: 0.7157 - val_loss: 0.6277 - val_acc: 0.6667\n",
            "Epoch 900/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.4724 - acc: 0.7794 - val_loss: 0.6264 - val_acc: 0.6562\n",
            "Epoch 901/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.4726 - acc: 0.7696 - val_loss: 0.6227 - val_acc: 0.6562\n",
            "Epoch 902/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5338 - acc: 0.7337 - val_loss: 0.6193 - val_acc: 0.6667\n",
            "Epoch 903/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5182 - acc: 0.7353 - val_loss: 0.6262 - val_acc: 0.6667\n",
            "Epoch 904/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 0.5062 - acc: 0.7712 - val_loss: 0.6087 - val_acc: 0.6562\n",
            "Epoch 905/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5005 - acc: 0.7598 - val_loss: 0.6037 - val_acc: 0.6458\n",
            "Epoch 906/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5353 - acc: 0.7157 - val_loss: 0.6215 - val_acc: 0.6667\n",
            "Epoch 907/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 0.5131 - acc: 0.7320 - val_loss: 0.6098 - val_acc: 0.6146\n",
            "Epoch 908/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.5284 - acc: 0.7467 - val_loss: 0.6328 - val_acc: 0.6771\n",
            "Epoch 909/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5051 - acc: 0.7467 - val_loss: 0.6088 - val_acc: 0.5938\n",
            "Epoch 910/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.4868 - acc: 0.7533 - val_loss: 0.6171 - val_acc: 0.6667\n",
            "Epoch 911/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.4866 - acc: 0.7740 - val_loss: 0.6170 - val_acc: 0.6771\n",
            "Epoch 912/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 0.5081 - acc: 0.7484 - val_loss: 0.6084 - val_acc: 0.6875\n",
            "Epoch 913/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5452 - acc: 0.7239 - val_loss: 0.6018 - val_acc: 0.6667\n",
            "Epoch 914/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5067 - acc: 0.7484 - val_loss: 0.6068 - val_acc: 0.6875\n",
            "Epoch 915/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 0.5057 - acc: 0.7451 - val_loss: 0.6008 - val_acc: 0.6562\n",
            "Epoch 916/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 0.5179 - acc: 0.7418 - val_loss: 0.6165 - val_acc: 0.6562\n",
            "Epoch 917/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.5313 - acc: 0.7288 - val_loss: 0.6141 - val_acc: 0.6875\n",
            "Epoch 918/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5031 - acc: 0.7565 - val_loss: 0.5961 - val_acc: 0.6667\n",
            "Epoch 919/1000\n",
            "39/39 [==============================] - 10s 258ms/step - loss: 0.5085 - acc: 0.7612 - val_loss: 0.6044 - val_acc: 0.6562\n",
            "Epoch 920/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 0.5345 - acc: 0.7353 - val_loss: 0.5952 - val_acc: 0.6562\n",
            "Epoch 921/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 0.5160 - acc: 0.7418 - val_loss: 0.5892 - val_acc: 0.6146\n",
            "Epoch 922/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 0.5262 - acc: 0.7320 - val_loss: 0.6135 - val_acc: 0.6458\n",
            "Epoch 923/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5178 - acc: 0.7500 - val_loss: 0.5864 - val_acc: 0.6771\n",
            "Epoch 924/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5128 - acc: 0.7353 - val_loss: 0.5985 - val_acc: 0.6562\n",
            "Epoch 925/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.5172 - acc: 0.7549 - val_loss: 0.6057 - val_acc: 0.6667\n",
            "Epoch 926/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5185 - acc: 0.7369 - val_loss: 0.6054 - val_acc: 0.6354\n",
            "Epoch 927/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5119 - acc: 0.7680 - val_loss: 0.6104 - val_acc: 0.6771\n",
            "Epoch 928/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 0.5296 - acc: 0.7353 - val_loss: 0.5871 - val_acc: 0.6458\n",
            "Epoch 929/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.4907 - acc: 0.7598 - val_loss: 0.6048 - val_acc: 0.6562\n",
            "Epoch 930/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.4849 - acc: 0.7582 - val_loss: 0.6043 - val_acc: 0.6667\n",
            "Epoch 931/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5153 - acc: 0.7532 - val_loss: 0.5818 - val_acc: 0.6458\n",
            "Epoch 932/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 0.5142 - acc: 0.7435 - val_loss: 0.6003 - val_acc: 0.6667\n",
            "Epoch 933/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 0.5158 - acc: 0.7386 - val_loss: 0.6139 - val_acc: 0.6250\n",
            "Epoch 934/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.4860 - acc: 0.7565 - val_loss: 0.6316 - val_acc: 0.6667\n",
            "Epoch 935/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.4923 - acc: 0.7516 - val_loss: 0.6189 - val_acc: 0.6771\n",
            "Epoch 936/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5458 - acc: 0.7288 - val_loss: 0.6149 - val_acc: 0.6562\n",
            "Epoch 937/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 0.5096 - acc: 0.7484 - val_loss: 0.6177 - val_acc: 0.6354\n",
            "Epoch 938/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.4885 - acc: 0.7810 - val_loss: 0.6105 - val_acc: 0.6667\n",
            "Epoch 939/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.4946 - acc: 0.7500 - val_loss: 0.6323 - val_acc: 0.6458\n",
            "Epoch 940/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 0.5204 - acc: 0.7386 - val_loss: 0.6048 - val_acc: 0.6562\n",
            "Epoch 941/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5142 - acc: 0.7320 - val_loss: 0.6029 - val_acc: 0.6250\n",
            "Epoch 942/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 0.4945 - acc: 0.7533 - val_loss: 0.5898 - val_acc: 0.6771\n",
            "Epoch 943/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 0.5147 - acc: 0.7598 - val_loss: 0.6189 - val_acc: 0.6562\n",
            "Epoch 944/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5214 - acc: 0.7369 - val_loss: 0.6007 - val_acc: 0.6771\n",
            "Epoch 945/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 0.4900 - acc: 0.7712 - val_loss: 0.6142 - val_acc: 0.6875\n",
            "Epoch 946/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 0.4967 - acc: 0.7288 - val_loss: 0.6036 - val_acc: 0.6771\n",
            "Epoch 947/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 0.4923 - acc: 0.7516 - val_loss: 0.6249 - val_acc: 0.6562\n",
            "Epoch 948/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 0.5121 - acc: 0.7353 - val_loss: 0.6322 - val_acc: 0.6354\n",
            "Epoch 949/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 0.4971 - acc: 0.7418 - val_loss: 0.6137 - val_acc: 0.6458\n",
            "Epoch 950/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5496 - acc: 0.7042 - val_loss: 0.6012 - val_acc: 0.6667\n",
            "Epoch 951/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 0.5182 - acc: 0.7404 - val_loss: 0.6128 - val_acc: 0.6667\n",
            "Epoch 952/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 0.5234 - acc: 0.7435 - val_loss: 0.6266 - val_acc: 0.6875\n",
            "Epoch 953/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5106 - acc: 0.7647 - val_loss: 0.6205 - val_acc: 0.6562\n",
            "Epoch 954/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 0.5320 - acc: 0.7141 - val_loss: 0.6207 - val_acc: 0.6562\n",
            "Epoch 955/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 0.5343 - acc: 0.7206 - val_loss: 0.6111 - val_acc: 0.6562\n",
            "Epoch 956/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 0.4966 - acc: 0.7598 - val_loss: 0.6090 - val_acc: 0.6979\n",
            "Epoch 957/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 0.4993 - acc: 0.7647 - val_loss: 0.6183 - val_acc: 0.6667\n",
            "Epoch 958/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.5007 - acc: 0.7500 - val_loss: 0.6115 - val_acc: 0.6562\n",
            "Epoch 959/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.4959 - acc: 0.7810 - val_loss: 0.5923 - val_acc: 0.6354\n",
            "Epoch 960/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 0.5251 - acc: 0.7402 - val_loss: 0.6196 - val_acc: 0.6458\n",
            "Epoch 961/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 0.5111 - acc: 0.7484 - val_loss: 0.6119 - val_acc: 0.6250\n",
            "Epoch 962/1000\n",
            "39/39 [==============================] - 5s 97ms/step - loss: 0.4729 - acc: 0.7680 - val_loss: 0.6024 - val_acc: 0.6354\n",
            "Epoch 963/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5321 - acc: 0.7304 - val_loss: 0.6208 - val_acc: 0.6562\n",
            "Epoch 964/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 0.5213 - acc: 0.7337 - val_loss: 0.6064 - val_acc: 0.6562\n",
            "Epoch 965/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 0.4894 - acc: 0.7402 - val_loss: 0.5813 - val_acc: 0.5938\n",
            "Epoch 966/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 0.4952 - acc: 0.7696 - val_loss: 0.5943 - val_acc: 0.6562\n",
            "Epoch 967/1000\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.5575 - acc: 0.7304 - val_loss: 0.5913 - val_acc: 0.6667\n",
            "Epoch 968/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5174 - acc: 0.7484 - val_loss: 0.6150 - val_acc: 0.6562\n",
            "Epoch 969/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 0.5217 - acc: 0.7484 - val_loss: 0.6016 - val_acc: 0.6667\n",
            "Epoch 970/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 0.5144 - acc: 0.7516 - val_loss: 0.6485 - val_acc: 0.6875\n",
            "Epoch 971/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 0.5307 - acc: 0.7271 - val_loss: 0.6060 - val_acc: 0.6875\n",
            "Epoch 972/1000\n",
            "39/39 [==============================] - 6s 142ms/step - loss: 0.5238 - acc: 0.7484 - val_loss: 0.6172 - val_acc: 0.6458\n",
            "Epoch 973/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.4977 - acc: 0.7435 - val_loss: 0.5866 - val_acc: 0.6146\n",
            "Epoch 974/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 0.5171 - acc: 0.7435 - val_loss: 0.6232 - val_acc: 0.6875\n",
            "Epoch 975/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 0.5193 - acc: 0.7386 - val_loss: 0.6048 - val_acc: 0.6771\n",
            "Epoch 976/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 0.5269 - acc: 0.7484 - val_loss: 0.5914 - val_acc: 0.6667\n",
            "Epoch 977/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 0.5294 - acc: 0.7255 - val_loss: 0.6274 - val_acc: 0.6875\n",
            "Epoch 978/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 0.5159 - acc: 0.7435 - val_loss: 0.5786 - val_acc: 0.6667\n",
            "Epoch 979/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.4966 - acc: 0.7533 - val_loss: 0.6095 - val_acc: 0.6771\n",
            "Epoch 980/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 0.5307 - acc: 0.7533 - val_loss: 0.6179 - val_acc: 0.6667\n",
            "Epoch 981/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 0.5632 - acc: 0.7141 - val_loss: 0.6059 - val_acc: 0.6771\n",
            "Epoch 982/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 0.5223 - acc: 0.7402 - val_loss: 0.6176 - val_acc: 0.6667\n",
            "Epoch 983/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 0.5295 - acc: 0.7353 - val_loss: 0.5928 - val_acc: 0.6562\n",
            "Epoch 984/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5388 - acc: 0.7157 - val_loss: 0.5880 - val_acc: 0.6771\n",
            "Epoch 985/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 0.5103 - acc: 0.7388 - val_loss: 0.6093 - val_acc: 0.6979\n",
            "Epoch 986/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 0.4780 - acc: 0.7663 - val_loss: 0.6113 - val_acc: 0.6979\n",
            "Epoch 987/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 0.5359 - acc: 0.7353 - val_loss: 0.5868 - val_acc: 0.6562\n",
            "Epoch 988/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5198 - acc: 0.7369 - val_loss: 0.6348 - val_acc: 0.6875\n",
            "Epoch 989/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 0.5307 - acc: 0.7369 - val_loss: 0.5868 - val_acc: 0.6979\n",
            "Epoch 990/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 0.5290 - acc: 0.7418 - val_loss: 0.6204 - val_acc: 0.6771\n",
            "Epoch 991/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 0.5083 - acc: 0.7451 - val_loss: 0.6036 - val_acc: 0.6667\n",
            "Epoch 992/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 0.5027 - acc: 0.7500 - val_loss: 0.6024 - val_acc: 0.6979\n",
            "Epoch 993/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 0.5376 - acc: 0.7353 - val_loss: 0.6211 - val_acc: 0.6875\n",
            "Epoch 994/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 0.4866 - acc: 0.7549 - val_loss: 0.6079 - val_acc: 0.6979\n",
            "Epoch 995/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 0.5119 - acc: 0.7271 - val_loss: 0.6175 - val_acc: 0.6875\n",
            "Epoch 996/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 0.5306 - acc: 0.7484 - val_loss: 0.6102 - val_acc: 0.6562\n",
            "Epoch 997/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 0.5295 - acc: 0.7467 - val_loss: 0.6123 - val_acc: 0.6771\n",
            "Epoch 998/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 0.5294 - acc: 0.7516 - val_loss: 0.6024 - val_acc: 0.6875\n",
            "Epoch 999/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 0.5221 - acc: 0.7467 - val_loss: 0.6207 - val_acc: 0.6979\n",
            "Epoch 1000/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 0.5049 - acc: 0.7876 - val_loss: 0.5981 - val_acc: 0.6562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "24bf21c6-3223-4e41-fdb2-f73a4a2969f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.660213589668274,\n",
              "  1.2081280946731567,\n",
              "  1.0858962535858154,\n",
              "  1.2016924619674683,\n",
              "  1.0693899393081665,\n",
              "  1.1307828426361084,\n",
              "  1.1617796421051025,\n",
              "  1.0436463356018066,\n",
              "  1.094193458557129,\n",
              "  1.057809591293335,\n",
              "  1.011149525642395,\n",
              "  0.9641730785369873,\n",
              "  1.0627361536026,\n",
              "  1.0317883491516113,\n",
              "  1.007372498512268,\n",
              "  1.028462529182434,\n",
              "  0.9713352918624878,\n",
              "  0.9591334462165833,\n",
              "  1.0432111024856567,\n",
              "  0.9863999485969543,\n",
              "  0.9500178694725037,\n",
              "  0.999410092830658,\n",
              "  0.9115477204322815,\n",
              "  1.0458524227142334,\n",
              "  0.8954987525939941,\n",
              "  0.9285101890563965,\n",
              "  0.9031503796577454,\n",
              "  0.9285926818847656,\n",
              "  0.9264349937438965,\n",
              "  0.9139406085014343,\n",
              "  0.8706537485122681,\n",
              "  0.927528440952301,\n",
              "  0.9343733191490173,\n",
              "  0.9653283357620239,\n",
              "  0.892278254032135,\n",
              "  0.8982186913490295,\n",
              "  0.8973689675331116,\n",
              "  0.8513821959495544,\n",
              "  0.8149961829185486,\n",
              "  0.9011431932449341,\n",
              "  0.9047644138336182,\n",
              "  0.8749381899833679,\n",
              "  0.8842104077339172,\n",
              "  0.9212508797645569,\n",
              "  0.7984350919723511,\n",
              "  0.8219897747039795,\n",
              "  0.8962447047233582,\n",
              "  0.8038317561149597,\n",
              "  0.8033618927001953,\n",
              "  0.8326990604400635,\n",
              "  0.887363612651825,\n",
              "  0.8241269588470459,\n",
              "  0.7718585729598999,\n",
              "  0.8549391031265259,\n",
              "  0.8140557408332825,\n",
              "  0.7835889458656311,\n",
              "  0.8461213707923889,\n",
              "  0.8312077522277832,\n",
              "  0.8193376064300537,\n",
              "  0.7735725045204163,\n",
              "  0.8423069715499878,\n",
              "  0.8268276453018188,\n",
              "  0.8239150047302246,\n",
              "  0.844275951385498,\n",
              "  0.9216710329055786,\n",
              "  0.7853295207023621,\n",
              "  0.8187629580497742,\n",
              "  0.8174574375152588,\n",
              "  0.7051106095314026,\n",
              "  0.8025749325752258,\n",
              "  0.7365893721580505,\n",
              "  0.7411670684814453,\n",
              "  0.7251676917076111,\n",
              "  0.7169526815414429,\n",
              "  0.7707510590553284,\n",
              "  0.8186048269271851,\n",
              "  0.7555414438247681,\n",
              "  0.7263771295547485,\n",
              "  0.7831274271011353,\n",
              "  0.7206966280937195,\n",
              "  0.6832778453826904,\n",
              "  0.7671546339988708,\n",
              "  0.7391737103462219,\n",
              "  0.7129311561584473,\n",
              "  0.7784016132354736,\n",
              "  0.7394923567771912,\n",
              "  0.7614887356758118,\n",
              "  0.7154662609100342,\n",
              "  0.7241052389144897,\n",
              "  0.6903049945831299,\n",
              "  0.7171446084976196,\n",
              "  0.6917773485183716,\n",
              "  0.6823078989982605,\n",
              "  0.6924917697906494,\n",
              "  0.7299288511276245,\n",
              "  0.7004422545433044,\n",
              "  0.7323173880577087,\n",
              "  0.7320374250411987,\n",
              "  0.6827753782272339,\n",
              "  0.6790528893470764,\n",
              "  0.7113394737243652,\n",
              "  0.7090807557106018,\n",
              "  0.7330906391143799,\n",
              "  0.6953089833259583,\n",
              "  0.6898155808448792,\n",
              "  0.7061667442321777,\n",
              "  0.6723835468292236,\n",
              "  0.6958674788475037,\n",
              "  0.690964937210083,\n",
              "  0.6785584092140198,\n",
              "  0.6812097430229187,\n",
              "  0.6834903955459595,\n",
              "  0.6764431595802307,\n",
              "  0.7062811255455017,\n",
              "  0.6741909384727478,\n",
              "  0.6494671106338501,\n",
              "  0.6591679453849792,\n",
              "  0.6830743551254272,\n",
              "  0.6947709918022156,\n",
              "  0.6626371145248413,\n",
              "  0.6400133967399597,\n",
              "  0.643908679485321,\n",
              "  0.7158858180046082,\n",
              "  0.6610695123672485,\n",
              "  0.6268585324287415,\n",
              "  0.6434709429740906,\n",
              "  0.6541855931282043,\n",
              "  0.6319748759269714,\n",
              "  0.6970391273498535,\n",
              "  0.6277055740356445,\n",
              "  0.6314857006072998,\n",
              "  0.6196580529212952,\n",
              "  0.6314436197280884,\n",
              "  0.6733419895172119,\n",
              "  0.6662388443946838,\n",
              "  0.6117488145828247,\n",
              "  0.670913577079773,\n",
              "  0.6428144574165344,\n",
              "  0.6075078248977661,\n",
              "  0.6212146878242493,\n",
              "  0.6171159744262695,\n",
              "  0.6774184703826904,\n",
              "  0.6085728406906128,\n",
              "  0.6295456290245056,\n",
              "  0.6710696220397949,\n",
              "  0.6412674784660339,\n",
              "  0.648684561252594,\n",
              "  0.641287624835968,\n",
              "  0.6158186197280884,\n",
              "  0.5936060547828674,\n",
              "  0.5890917181968689,\n",
              "  0.6107069253921509,\n",
              "  0.662869393825531,\n",
              "  0.6273680329322815,\n",
              "  0.6429575681686401,\n",
              "  0.6315159797668457,\n",
              "  0.6659737825393677,\n",
              "  0.6248947978019714,\n",
              "  0.6178077459335327,\n",
              "  0.6239807605743408,\n",
              "  0.5887196660041809,\n",
              "  0.5900776982307434,\n",
              "  0.6521298885345459,\n",
              "  0.614352285861969,\n",
              "  0.6015875935554504,\n",
              "  0.6026503443717957,\n",
              "  0.5970926880836487,\n",
              "  0.6075377464294434,\n",
              "  0.5986535549163818,\n",
              "  0.6138277649879456,\n",
              "  0.5917196869850159,\n",
              "  0.6287403702735901,\n",
              "  0.5843765735626221,\n",
              "  0.6050989627838135,\n",
              "  0.5824800133705139,\n",
              "  0.5938735008239746,\n",
              "  0.5616676211357117,\n",
              "  0.6095556616783142,\n",
              "  0.6157258749008179,\n",
              "  0.5573476552963257,\n",
              "  0.5968902111053467,\n",
              "  0.6159529089927673,\n",
              "  0.5703950524330139,\n",
              "  0.6111172437667847,\n",
              "  0.5935789346694946,\n",
              "  0.6091444492340088,\n",
              "  0.5818716883659363,\n",
              "  0.6189103126525879,\n",
              "  0.6054656505584717,\n",
              "  0.5602818131446838,\n",
              "  0.5444233417510986,\n",
              "  0.6030006408691406,\n",
              "  0.5969829559326172,\n",
              "  0.6212795972824097,\n",
              "  0.6398612856864929,\n",
              "  0.5799120664596558,\n",
              "  0.6020479798316956,\n",
              "  0.5699467658996582,\n",
              "  0.5859119296073914,\n",
              "  0.6031160354614258,\n",
              "  0.5680785179138184,\n",
              "  0.5645487904548645,\n",
              "  0.5669907927513123,\n",
              "  0.5797058939933777,\n",
              "  0.5885847806930542,\n",
              "  0.5624767541885376,\n",
              "  0.5950739979743958,\n",
              "  0.6149478554725647,\n",
              "  0.5967592000961304,\n",
              "  0.5902197360992432,\n",
              "  0.5891492962837219,\n",
              "  0.6046962738037109,\n",
              "  0.5600564479827881,\n",
              "  0.613766610622406,\n",
              "  0.5759661197662354,\n",
              "  0.564021646976471,\n",
              "  0.5651658773422241,\n",
              "  0.5437816381454468,\n",
              "  0.614966094493866,\n",
              "  0.5821070075035095,\n",
              "  0.5987709760665894,\n",
              "  0.5424971580505371,\n",
              "  0.5870540142059326,\n",
              "  0.5597865581512451,\n",
              "  0.5599086284637451,\n",
              "  0.5825573205947876,\n",
              "  0.5828206539154053,\n",
              "  0.5763021111488342,\n",
              "  0.5940960645675659,\n",
              "  0.5486583113670349,\n",
              "  0.566987931728363,\n",
              "  0.528056263923645,\n",
              "  0.5405945777893066,\n",
              "  0.5615291595458984,\n",
              "  0.5600273013114929,\n",
              "  0.5525320172309875,\n",
              "  0.5700940489768982,\n",
              "  0.5538382530212402,\n",
              "  0.5244022607803345,\n",
              "  0.5702013373374939,\n",
              "  0.582065999507904,\n",
              "  0.5307145714759827,\n",
              "  0.5552419424057007,\n",
              "  0.5400156378746033,\n",
              "  0.5348942875862122,\n",
              "  0.5479035973548889,\n",
              "  0.5721898674964905,\n",
              "  0.5421885848045349,\n",
              "  0.5407446026802063,\n",
              "  0.5703954696655273,\n",
              "  0.5732695460319519,\n",
              "  0.5593451261520386,\n",
              "  0.5793883800506592,\n",
              "  0.5502812266349792,\n",
              "  0.5683592557907104,\n",
              "  0.5713342428207397,\n",
              "  0.5575720071792603,\n",
              "  0.5552998185157776,\n",
              "  0.5574313402175903,\n",
              "  0.5431696176528931,\n",
              "  0.5764543414115906,\n",
              "  0.5768018364906311,\n",
              "  0.5372653603553772,\n",
              "  0.5695634484291077,\n",
              "  0.5376270413398743,\n",
              "  0.5673675537109375,\n",
              "  0.5530052781105042,\n",
              "  0.5336033701896667,\n",
              "  0.5524717569351196,\n",
              "  0.5252229571342468,\n",
              "  0.534553587436676,\n",
              "  0.54715895652771,\n",
              "  0.5478647351264954,\n",
              "  0.5354002714157104,\n",
              "  0.5863790512084961,\n",
              "  0.5375532507896423,\n",
              "  0.5645848512649536,\n",
              "  0.6042550206184387,\n",
              "  0.5635770559310913,\n",
              "  0.5909700393676758,\n",
              "  0.5109314918518066,\n",
              "  0.5477559566497803,\n",
              "  0.5273200869560242,\n",
              "  0.572223961353302,\n",
              "  0.5818172693252563,\n",
              "  0.550448477268219,\n",
              "  0.5344048738479614,\n",
              "  0.5632944107055664,\n",
              "  0.5415307283401489,\n",
              "  0.5325620770454407,\n",
              "  0.5260235071182251,\n",
              "  0.5325616598129272,\n",
              "  0.5613880157470703,\n",
              "  0.5570077896118164,\n",
              "  0.5600190162658691,\n",
              "  0.5251511931419373,\n",
              "  0.5639194846153259,\n",
              "  0.5285072922706604,\n",
              "  0.544625461101532,\n",
              "  0.5543215870857239,\n",
              "  0.5603625178337097,\n",
              "  0.5470103025436401,\n",
              "  0.5138353109359741,\n",
              "  0.5577646493911743,\n",
              "  0.5529364347457886,\n",
              "  0.5402399897575378,\n",
              "  0.4970380961894989,\n",
              "  0.5414780974388123,\n",
              "  0.5597403049468994,\n",
              "  0.5430770516395569,\n",
              "  0.5402015447616577,\n",
              "  0.539279043674469,\n",
              "  0.5727701783180237,\n",
              "  0.5491624474525452,\n",
              "  0.5543047785758972,\n",
              "  0.5518742203712463,\n",
              "  0.5707070827484131,\n",
              "  0.5134952664375305,\n",
              "  0.5449736714363098,\n",
              "  0.5571588277816772,\n",
              "  0.5467650294303894,\n",
              "  0.5510354042053223,\n",
              "  0.5510345697402954,\n",
              "  0.5358502864837646,\n",
              "  0.5056025981903076,\n",
              "  0.5509671568870544,\n",
              "  0.5351580381393433,\n",
              "  0.5345279574394226,\n",
              "  0.5112936496734619,\n",
              "  0.5192817449569702,\n",
              "  0.5225381255149841,\n",
              "  0.5548631548881531,\n",
              "  0.5154170393943787,\n",
              "  0.5474974513053894,\n",
              "  0.5363466739654541,\n",
              "  0.5259084701538086,\n",
              "  0.5204172730445862,\n",
              "  0.5586235523223877,\n",
              "  0.5447632670402527,\n",
              "  0.5388695001602173,\n",
              "  0.5351232886314392,\n",
              "  0.5329826474189758,\n",
              "  0.5529218912124634,\n",
              "  0.533208966255188,\n",
              "  0.5506205558776855,\n",
              "  0.5496441125869751,\n",
              "  0.5524237751960754,\n",
              "  0.5339288115501404,\n",
              "  0.5562391877174377,\n",
              "  0.5075956583023071,\n",
              "  0.5255125761032104,\n",
              "  0.5448899865150452,\n",
              "  0.5097672939300537,\n",
              "  0.48730283975601196,\n",
              "  0.5264168977737427,\n",
              "  0.5434498190879822,\n",
              "  0.5236830115318298,\n",
              "  0.5231946110725403,\n",
              "  0.5425639748573303,\n",
              "  0.5238081812858582,\n",
              "  0.5492900013923645,\n",
              "  0.5247976183891296,\n",
              "  0.5371895432472229,\n",
              "  0.5230770111083984,\n",
              "  0.5207746624946594,\n",
              "  0.5296952128410339,\n",
              "  0.5156986713409424,\n",
              "  0.5106721520423889,\n",
              "  0.49806761741638184,\n",
              "  0.5127769708633423,\n",
              "  0.5296631455421448,\n",
              "  0.5273398160934448,\n",
              "  0.5392835736274719,\n",
              "  0.51187664270401,\n",
              "  0.4954100549221039,\n",
              "  0.5724897980690002,\n",
              "  0.530789315700531,\n",
              "  0.520986795425415,\n",
              "  0.5437301397323608,\n",
              "  0.5294945240020752,\n",
              "  0.5239163637161255,\n",
              "  0.5482054948806763,\n",
              "  0.5394486784934998,\n",
              "  0.5638313889503479,\n",
              "  0.550690770149231,\n",
              "  0.5264065861701965,\n",
              "  0.5139093995094299,\n",
              "  0.5392672419548035,\n",
              "  0.5062082409858704,\n",
              "  0.5286896824836731,\n",
              "  0.5329110026359558,\n",
              "  0.526904821395874,\n",
              "  0.5000776648521423,\n",
              "  0.5280154347419739,\n",
              "  0.5369964838027954,\n",
              "  0.4919678866863251,\n",
              "  0.53581303358078,\n",
              "  0.5426525473594666,\n",
              "  0.5489445924758911,\n",
              "  0.522502601146698,\n",
              "  0.5472422242164612,\n",
              "  0.5280301570892334,\n",
              "  0.4981468617916107,\n",
              "  0.5374577045440674,\n",
              "  0.5338121652603149,\n",
              "  0.5389180183410645,\n",
              "  0.5339667797088623,\n",
              "  0.5161007642745972,\n",
              "  0.5272877812385559,\n",
              "  0.524080753326416,\n",
              "  0.5182837247848511,\n",
              "  0.5344836115837097,\n",
              "  0.5219566226005554,\n",
              "  0.5026325583457947,\n",
              "  0.5533620119094849,\n",
              "  0.5308180451393127,\n",
              "  0.5425479412078857,\n",
              "  0.5171747207641602,\n",
              "  0.5189221501350403,\n",
              "  0.535773754119873,\n",
              "  0.5248663425445557,\n",
              "  0.5377745032310486,\n",
              "  0.5240440964698792,\n",
              "  0.529755711555481,\n",
              "  0.544776439666748,\n",
              "  0.5327162146568298,\n",
              "  0.5498654842376709,\n",
              "  0.5312002897262573,\n",
              "  0.5121328234672546,\n",
              "  0.5379968285560608,\n",
              "  0.5236925482749939,\n",
              "  0.5177634358406067,\n",
              "  0.519158124923706,\n",
              "  0.5365682244300842,\n",
              "  0.5424426794052124,\n",
              "  0.5154420137405396,\n",
              "  0.5214712023735046,\n",
              "  0.5557569861412048,\n",
              "  0.5136959552764893,\n",
              "  0.5063228011131287,\n",
              "  0.5206185579299927,\n",
              "  0.5346555709838867,\n",
              "  0.5189236402511597,\n",
              "  0.5299760699272156,\n",
              "  0.5298845171928406,\n",
              "  0.5428346991539001,\n",
              "  0.5345927476882935,\n",
              "  0.5317292809486389,\n",
              "  0.5344930291175842,\n",
              "  0.526813805103302,\n",
              "  0.5017794370651245,\n",
              "  0.5241004228591919,\n",
              "  0.4819371700286865,\n",
              "  0.5250639319419861,\n",
              "  0.5250648260116577,\n",
              "  0.5431985259056091,\n",
              "  0.5061542987823486,\n",
              "  0.5228577256202698,\n",
              "  0.5096157789230347,\n",
              "  0.5366001725196838,\n",
              "  0.5386442542076111,\n",
              "  0.5275358557701111,\n",
              "  0.5275129675865173,\n",
              "  0.5424468517303467,\n",
              "  0.5367303490638733,\n",
              "  0.48452553153038025,\n",
              "  0.5669564008712769,\n",
              "  0.5246574282646179,\n",
              "  0.486106812953949,\n",
              "  0.5311722159385681,\n",
              "  0.5061094164848328,\n",
              "  0.5275439620018005,\n",
              "  0.5200727581977844,\n",
              "  0.5139371156692505,\n",
              "  0.5166774988174438,\n",
              "  0.5325665473937988,\n",
              "  0.5468088388442993,\n",
              "  0.5152968168258667,\n",
              "  0.5395920872688293,\n",
              "  0.5101635456085205,\n",
              "  0.5215041637420654,\n",
              "  0.527625560760498,\n",
              "  0.5264788866043091,\n",
              "  0.5598324537277222,\n",
              "  0.5312260389328003,\n",
              "  0.5260547995567322,\n",
              "  0.5255915522575378,\n",
              "  0.529509425163269,\n",
              "  0.5092508792877197,\n",
              "  0.5193420052528381,\n",
              "  0.538623571395874,\n",
              "  0.5311538577079773,\n",
              "  0.5428910255432129,\n",
              "  0.5272846817970276,\n",
              "  0.5300083160400391,\n",
              "  0.518674910068512,\n",
              "  0.5315107107162476,\n",
              "  0.5085037350654602,\n",
              "  0.5427047610282898,\n",
              "  0.4995131194591522,\n",
              "  0.528134286403656,\n",
              "  0.5130792856216431,\n",
              "  0.5368805527687073,\n",
              "  0.49696311354637146,\n",
              "  0.5297797918319702,\n",
              "  0.48382025957107544,\n",
              "  0.5489354729652405,\n",
              "  0.5084105134010315,\n",
              "  0.5247262120246887,\n",
              "  0.5257198810577393,\n",
              "  0.520103394985199,\n",
              "  0.5241493582725525,\n",
              "  0.5242372155189514,\n",
              "  0.5062954425811768,\n",
              "  0.5248021483421326,\n",
              "  0.5115225315093994,\n",
              "  0.5229631066322327,\n",
              "  0.5712419152259827,\n",
              "  0.5161072611808777,\n",
              "  0.5185667872428894,\n",
              "  0.5115541219711304,\n",
              "  0.5077647566795349,\n",
              "  0.555160641670227,\n",
              "  0.5074523091316223,\n",
              "  0.502303421497345,\n",
              "  0.519113302230835,\n",
              "  0.5208224654197693,\n",
              "  0.5300728678703308,\n",
              "  0.5414976477622986,\n",
              "  0.5151153206825256,\n",
              "  0.5065137147903442,\n",
              "  0.5151153206825256,\n",
              "  0.5052714347839355,\n",
              "  0.47143715620040894,\n",
              "  0.509346067905426,\n",
              "  0.5108438730239868,\n",
              "  0.5389277338981628,\n",
              "  0.5004787445068359,\n",
              "  0.5118317604064941,\n",
              "  0.5010835528373718,\n",
              "  0.5150413513183594,\n",
              "  0.5269546508789062,\n",
              "  0.558628499507904,\n",
              "  0.5348924398422241,\n",
              "  0.5294543504714966,\n",
              "  0.4818529188632965,\n",
              "  0.5143684148788452,\n",
              "  0.5146931409835815,\n",
              "  0.4896966218948364,\n",
              "  0.5090941786766052,\n",
              "  0.5254520177841187,\n",
              "  0.5187948942184448,\n",
              "  0.5311780571937561,\n",
              "  0.5195638537406921,\n",
              "  0.5489960312843323,\n",
              "  0.491725891828537,\n",
              "  0.5441098809242249,\n",
              "  0.5413019061088562,\n",
              "  0.5136235356330872,\n",
              "  0.5189173221588135,\n",
              "  0.4833880662918091,\n",
              "  0.4887843430042267,\n",
              "  0.492640882730484,\n",
              "  0.5307588577270508,\n",
              "  0.49521005153656006,\n",
              "  0.5532236099243164,\n",
              "  0.5338812470436096,\n",
              "  0.5131849050521851,\n",
              "  0.5113359689712524,\n",
              "  0.5489954948425293,\n",
              "  0.5347502827644348,\n",
              "  0.5369579792022705,\n",
              "  0.5282150506973267,\n",
              "  0.5029257535934448,\n",
              "  0.5507208108901978,\n",
              "  0.515082597732544,\n",
              "  0.5536928176879883,\n",
              "  0.5035450458526611,\n",
              "  0.5233015418052673,\n",
              "  0.4982680678367615,\n",
              "  0.4927240014076233,\n",
              "  0.49669966101646423,\n",
              "  0.5138381719589233,\n",
              "  0.507003903388977,\n",
              "  0.4950481355190277,\n",
              "  0.5212858319282532,\n",
              "  0.5032085180282593,\n",
              "  0.5294302105903625,\n",
              "  0.5226303935050964,\n",
              "  0.5177940726280212,\n",
              "  0.503295361995697,\n",
              "  0.5056934952735901,\n",
              "  0.5099815130233765,\n",
              "  0.5274598598480225,\n",
              "  0.5322079658508301,\n",
              "  0.5363671183586121,\n",
              "  0.5011082291603088,\n",
              "  0.5405390858650208,\n",
              "  0.5269677639007568,\n",
              "  0.4849334955215454,\n",
              "  0.508280873298645,\n",
              "  0.5071991682052612,\n",
              "  0.5026977062225342,\n",
              "  0.5450753569602966,\n",
              "  0.5173523426055908,\n",
              "  0.5118468403816223,\n",
              "  0.5135835409164429,\n",
              "  0.4922187626361847,\n",
              "  0.48586827516555786,\n",
              "  0.5101169943809509,\n",
              "  0.5094178915023804,\n",
              "  0.49868708848953247,\n",
              "  0.535181999206543,\n",
              "  0.546048104763031,\n",
              "  0.5401099324226379,\n",
              "  0.5197635293006897,\n",
              "  0.5356830954551697,\n",
              "  0.503186821937561,\n",
              "  0.5362628698348999,\n",
              "  0.5164860486984253,\n",
              "  0.49302348494529724,\n",
              "  0.5290307998657227,\n",
              "  0.5231581330299377,\n",
              "  0.5164803862571716,\n",
              "  0.5094534158706665,\n",
              "  0.49475347995758057,\n",
              "  0.5261824131011963,\n",
              "  0.5114039182662964,\n",
              "  0.47365450859069824,\n",
              "  0.540112316608429,\n",
              "  0.5043059587478638,\n",
              "  0.5071374177932739,\n",
              "  0.5034245848655701,\n",
              "  0.5513142347335815,\n",
              "  0.5034720301628113,\n",
              "  0.4867195785045624,\n",
              "  0.531302273273468,\n",
              "  0.5108534097671509,\n",
              "  0.5184248685836792,\n",
              "  0.548888623714447,\n",
              "  0.5099018216133118,\n",
              "  0.5277607440948486,\n",
              "  0.544040858745575,\n",
              "  0.5168079733848572,\n",
              "  0.5205768346786499,\n",
              "  0.4770720303058624,\n",
              "  0.5077217221260071,\n",
              "  0.5146098732948303,\n",
              "  0.5239707827568054,\n",
              "  0.5217511057853699,\n",
              "  0.5136981010437012,\n",
              "  0.516209065914154,\n",
              "  0.5409043431282043,\n",
              "  0.5361638069152832,\n",
              "  0.5588484406471252,\n",
              "  0.5199415683746338,\n",
              "  0.4948248863220215,\n",
              "  0.5395727753639221,\n",
              "  0.4979311227798462,\n",
              "  0.5369116067886353,\n",
              "  0.5022450685501099,\n",
              "  0.49818092584609985,\n",
              "  0.5244718194007874,\n",
              "  0.5233113765716553,\n",
              "  0.48963966965675354,\n",
              "  0.535697340965271,\n",
              "  0.5484959483146667,\n",
              "  0.5184051394462585,\n",
              "  0.5239595174789429,\n",
              "  0.5274296998977661,\n",
              "  0.5193855166435242,\n",
              "  0.5217106342315674,\n",
              "  0.5037451982498169,\n",
              "  0.5386132001876831,\n",
              "  0.47951728105545044,\n",
              "  0.5244737267494202,\n",
              "  0.5032675266265869,\n",
              "  0.49974772334098816,\n",
              "  0.5327723026275635,\n",
              "  0.5444411635398865,\n",
              "  0.5084855556488037,\n",
              "  0.5291013717651367,\n",
              "  0.46865326166152954,\n",
              "  0.5164536833763123,\n",
              "  0.5454704165458679,\n",
              "  0.5262082815170288,\n",
              "  0.4920096695423126,\n",
              "  0.5032868385314941,\n",
              "  0.5242581963539124,\n",
              "  0.5152559280395508,\n",
              "  0.5099575519561768,\n",
              "  0.48343056440353394,\n",
              "  0.5071178078651428,\n",
              "  0.5143980979919434,\n",
              "  0.4891856610774994,\n",
              "  0.4894137382507324,\n",
              "  0.47589120268821716,\n",
              "  0.49878448247909546,\n",
              "  0.5032330751419067,\n",
              "  0.5047566294670105,\n",
              "  0.49576592445373535,\n",
              "  0.5115237236022949,\n",
              "  0.5085800290107727,\n",
              "  0.538122832775116,\n",
              "  0.5090068578720093,\n",
              "  0.4856771230697632,\n",
              "  0.49944090843200684,\n",
              "  0.5073097348213196,\n",
              "  0.5336076617240906,\n",
              "  0.5410415530204773,\n",
              "  0.542725682258606,\n",
              "  0.5057030320167542,\n",
              "  0.4641791582107544,\n",
              "  0.5216789841651917,\n",
              "  0.5452237129211426,\n",
              "  0.47281891107559204,\n",
              "  0.5072984099388123,\n",
              "  0.537365734577179,\n",
              "  0.5034123063087463,\n",
              "  0.5279138088226318,\n",
              "  0.5501052141189575,\n",
              "  0.5390446186065674,\n",
              "  0.5161573886871338,\n",
              "  0.5384557247161865,\n",
              "  0.5026167631149292,\n",
              "  0.5008565783500671,\n",
              "  0.5040524005889893,\n",
              "  0.4904463589191437,\n",
              "  0.508374810218811,\n",
              "  0.5125641822814941,\n",
              "  0.5371525287628174,\n",
              "  0.5277906060218811,\n",
              "  0.4775504171848297,\n",
              "  0.501986026763916,\n",
              "  0.49882441759109497,\n",
              "  0.5350308418273926,\n",
              "  0.4894539415836334,\n",
              "  0.48769015073776245,\n",
              "  0.5119154453277588,\n",
              "  0.5039360523223877,\n",
              "  0.5062499046325684,\n",
              "  0.5380399227142334,\n",
              "  0.5306518077850342,\n",
              "  0.512773871421814,\n",
              "  0.509416401386261,\n",
              "  0.4835103452205658,\n",
              "  0.5253141522407532,\n",
              "  0.49959009885787964,\n",
              "  0.5635853409767151,\n",
              "  0.48971307277679443,\n",
              "  0.5130031108856201,\n",
              "  0.4977249503135681,\n",
              "  0.4991130530834198,\n",
              "  0.5034439563751221,\n",
              "  0.51039719581604,\n",
              "  0.5092822313308716,\n",
              "  0.5613793134689331,\n",
              "  0.542048990726471,\n",
              "  0.5138192772865295,\n",
              "  0.5517870783805847,\n",
              "  0.492114782333374,\n",
              "  0.49549227952957153,\n",
              "  0.5262974500656128,\n",
              "  0.5411673188209534,\n",
              "  0.52107834815979,\n",
              "  0.5342991948127747,\n",
              "  0.5126022696495056,\n",
              "  0.5321603417396545,\n",
              "  0.5486596822738647,\n",
              "  0.5366967916488647,\n",
              "  0.4983142018318176,\n",
              "  0.5187307596206665,\n",
              "  0.5290566682815552,\n",
              "  0.4844973683357239,\n",
              "  0.5300581455230713,\n",
              "  0.5206625461578369,\n",
              "  0.5395270586013794,\n",
              "  0.528221845626831,\n",
              "  0.5165308117866516,\n",
              "  0.5091459155082703,\n",
              "  0.48064109683036804,\n",
              "  0.5117620825767517,\n",
              "  0.5195584893226624,\n",
              "  0.5122842192649841,\n",
              "  0.517178475856781,\n",
              "  0.4825887680053711,\n",
              "  0.5276094675064087,\n",
              "  0.5026562213897705,\n",
              "  0.5201033353805542,\n",
              "  0.47726449370384216,\n",
              "  0.4981067478656769,\n",
              "  0.5304753184318542,\n",
              "  0.5066054463386536,\n",
              "  0.5403066873550415,\n",
              "  0.5090456008911133,\n",
              "  0.5432688593864441,\n",
              "  0.5017085671424866,\n",
              "  0.5114451050758362,\n",
              "  0.49499571323394775,\n",
              "  0.4925891160964966,\n",
              "  0.49699726700782776,\n",
              "  0.541404664516449,\n",
              "  0.5027444362640381,\n",
              "  0.5266615152359009,\n",
              "  0.49775436520576477,\n",
              "  0.5127962827682495,\n",
              "  0.5040401220321655,\n",
              "  0.5076687335968018,\n",
              "  0.47927263379096985,\n",
              "  0.4837992191314697,\n",
              "  0.5426673889160156,\n",
              "  0.4942852854728699,\n",
              "  0.5047839283943176,\n",
              "  0.5121378898620605,\n",
              "  0.5109846591949463,\n",
              "  0.5562281012535095,\n",
              "  0.5123724937438965,\n",
              "  0.5410969257354736,\n",
              "  0.49874159693717957,\n",
              "  0.5214998126029968,\n",
              "  0.5002756118774414,\n",
              "  0.531278669834137,\n",
              "  0.5071027278900146,\n",
              "  0.5612545609474182,\n",
              "  0.5129634737968445,\n",
              "  0.5148202180862427,\n",
              "  0.5448179244995117,\n",
              "  0.5141849517822266,\n",
              "  0.5417413711547852,\n",
              "  0.49456787109375,\n",
              "  0.5210914611816406,\n",
              "  0.4954145848751068,\n",
              "  0.510152280330658,\n",
              "  0.5022484660148621,\n",
              "  0.5068622827529907,\n",
              "  0.5268028974533081,\n",
              "  0.5222856402397156,\n",
              "  0.5173042416572571,\n",
              "  0.533449649810791,\n",
              "  0.5204782485961914,\n",
              "  0.5276379585266113,\n",
              "  0.5153157114982605,\n",
              "  0.5304248929023743,\n",
              "  0.5174356698989868,\n",
              "  0.49298399686813354,\n",
              "  0.5367111563682556,\n",
              "  0.5162018537521362,\n",
              "  0.49718379974365234,\n",
              "  0.5361267328262329,\n",
              "  0.5188003778457642,\n",
              "  0.5038375854492188,\n",
              "  0.5052507519721985,\n",
              "  0.46666043996810913,\n",
              "  0.5191060304641724,\n",
              "  0.49801766872406006,\n",
              "  0.5136944651603699,\n",
              "  0.5438551306724548,\n",
              "  0.5185520052909851,\n",
              "  0.5130504965782166,\n",
              "  0.5139771699905396,\n",
              "  0.5242407321929932,\n",
              "  0.5171195864677429,\n",
              "  0.4933036267757416,\n",
              "  0.5287148356437683,\n",
              "  0.5198835730552673,\n",
              "  0.5201338529586792,\n",
              "  0.5653835535049438,\n",
              "  0.5236137509346008,\n",
              "  0.5188915133476257,\n",
              "  0.5192552804946899,\n",
              "  0.5457935333251953,\n",
              "  0.5033453702926636,\n",
              "  0.48172760009765625,\n",
              "  0.5054808855056763,\n",
              "  0.5218273401260376,\n",
              "  0.5052804946899414,\n",
              "  0.526019811630249,\n",
              "  0.5001097321510315,\n",
              "  0.5267015099525452,\n",
              "  0.522329568862915,\n",
              "  0.5097461342811584,\n",
              "  0.5055366158485413,\n",
              "  0.546798825263977,\n",
              "  0.5126012563705444,\n",
              "  0.48959410190582275,\n",
              "  0.49139517545700073,\n",
              "  0.520620584487915,\n",
              "  0.5157122015953064,\n",
              "  0.5012810230255127,\n",
              "  0.48456284403800964,\n",
              "  0.5434572696685791,\n",
              "  0.4796367883682251,\n",
              "  0.5408955216407776,\n",
              "  0.4773021936416626,\n",
              "  0.49775582551956177,\n",
              "  0.4922504723072052,\n",
              "  0.5209577679634094,\n",
              "  0.4820344150066376,\n",
              "  0.5556955337524414,\n",
              "  0.4723910093307495,\n",
              "  0.47256410121917725,\n",
              "  0.5338026881217957,\n",
              "  0.5182087421417236,\n",
              "  0.5061712861061096,\n",
              "  0.5004514455795288,\n",
              "  0.5352582931518555,\n",
              "  0.5131071209907532,\n",
              "  0.5284276008605957,\n",
              "  0.5051265954971313,\n",
              "  0.48681730031967163,\n",
              "  0.4865681529045105,\n",
              "  0.5080878138542175,\n",
              "  0.5451796054840088,\n",
              "  0.506673276424408,\n",
              "  0.5056766271591187,\n",
              "  0.5178649425506592,\n",
              "  0.5313239693641663,\n",
              "  0.5030767321586609,\n",
              "  0.5085251927375793,\n",
              "  0.5344566106796265,\n",
              "  0.5160227417945862,\n",
              "  0.5261589884757996,\n",
              "  0.5177819728851318,\n",
              "  0.5128114223480225,\n",
              "  0.5171849727630615,\n",
              "  0.518546462059021,\n",
              "  0.5118973851203918,\n",
              "  0.5296456813812256,\n",
              "  0.49073463678359985,\n",
              "  0.484935998916626,\n",
              "  0.5152937769889832,\n",
              "  0.5142163634300232,\n",
              "  0.515761137008667,\n",
              "  0.48602256178855896,\n",
              "  0.4922889173030853,\n",
              "  0.5457625389099121,\n",
              "  0.509640097618103,\n",
              "  0.48852089047431946,\n",
              "  0.49464860558509827,\n",
              "  0.5203661322593689,\n",
              "  0.514228343963623,\n",
              "  0.49447745084762573,\n",
              "  0.5146914720535278,\n",
              "  0.5214067101478577,\n",
              "  0.49003151059150696,\n",
              "  0.4966830611228943,\n",
              "  0.492342472076416,\n",
              "  0.5120964050292969,\n",
              "  0.4971197843551636,\n",
              "  0.5495607852935791,\n",
              "  0.5182215571403503,\n",
              "  0.5234003663063049,\n",
              "  0.5105573534965515,\n",
              "  0.5320279002189636,\n",
              "  0.5343455672264099,\n",
              "  0.4966059625148773,\n",
              "  0.4993255138397217,\n",
              "  0.5007103085517883,\n",
              "  0.4959299564361572,\n",
              "  0.5250970125198364,\n",
              "  0.5111261010169983,\n",
              "  0.47285929322242737,\n",
              "  0.5320611596107483,\n",
              "  0.521282970905304,\n",
              "  0.48936381936073303,\n",
              "  0.4952428340911865,\n",
              "  0.5574703812599182,\n",
              "  0.5174373388290405,\n",
              "  0.5217061638832092,\n",
              "  0.5143866539001465,\n",
              "  0.5306932330131531,\n",
              "  0.5238165855407715,\n",
              "  0.497709184885025,\n",
              "  0.5170665383338928,\n",
              "  0.5193484425544739,\n",
              "  0.526852548122406,\n",
              "  0.5294416546821594,\n",
              "  0.5158780217170715,\n",
              "  0.49660077691078186,\n",
              "  0.5306752324104309,\n",
              "  0.5631784200668335,\n",
              "  0.5222861170768738,\n",
              "  0.5295321345329285,\n",
              "  0.5387864708900452,\n",
              "  0.5103186368942261,\n",
              "  0.47803786396980286,\n",
              "  0.5359333753585815,\n",
              "  0.5197610855102539,\n",
              "  0.5306633114814758,\n",
              "  0.529040515422821,\n",
              "  0.5082557201385498,\n",
              "  0.5026850700378418,\n",
              "  0.5376042127609253,\n",
              "  0.48662057518959045,\n",
              "  0.5118849873542786,\n",
              "  0.5306341052055359,\n",
              "  0.5294701457023621,\n",
              "  0.5294267535209656,\n",
              "  0.5221260190010071,\n",
              "  0.5049365162849426],\n",
              " 'acc': [0.5212418437004089,\n",
              "  0.4950980246067047,\n",
              "  0.5506535768508911,\n",
              "  0.5359477400779724,\n",
              "  0.5261437892913818,\n",
              "  0.5130718946456909,\n",
              "  0.49836599826812744,\n",
              "  0.5196078419685364,\n",
              "  0.5310457348823547,\n",
              "  0.5457516312599182,\n",
              "  0.5392156839370728,\n",
              "  0.5522875785827637,\n",
              "  0.5441176295280457,\n",
              "  0.5441176295280457,\n",
              "  0.5751634240150452,\n",
              "  0.5539215803146362,\n",
              "  0.5784313678741455,\n",
              "  0.5686274766921997,\n",
              "  0.5179738402366638,\n",
              "  0.5392156839370728,\n",
              "  0.5408496856689453,\n",
              "  0.5441176295280457,\n",
              "  0.6045751571655273,\n",
              "  0.5130718946456909,\n",
              "  0.576797366142273,\n",
              "  0.5588235259056091,\n",
              "  0.576797366142273,\n",
              "  0.5686274766921997,\n",
              "  0.5653594732284546,\n",
              "  0.5604575276374817,\n",
              "  0.584967315196991,\n",
              "  0.5490196347236633,\n",
              "  0.5490196347236633,\n",
              "  0.584967315196991,\n",
              "  0.5898692607879639,\n",
              "  0.5702614188194275,\n",
              "  0.5866013169288635,\n",
              "  0.5882353186607361,\n",
              "  0.5996732115745544,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5947712659835815,\n",
              "  0.5539215803146362,\n",
              "  0.5620915293693542,\n",
              "  0.601307213306427,\n",
              "  0.6192810535430908,\n",
              "  0.5571895241737366,\n",
              "  0.6029411554336548,\n",
              "  0.6258170008659363,\n",
              "  0.5800653696060181,\n",
              "  0.5996732115745544,\n",
              "  0.6078431606292725,\n",
              "  0.6094771027565002,\n",
              "  0.5980392098426819,\n",
              "  0.593137264251709,\n",
              "  0.5964052081108093,\n",
              "  0.5800653696060181,\n",
              "  0.6160130500793457,\n",
              "  0.6176470518112183,\n",
              "  0.6241829991340637,\n",
              "  0.5898692607879639,\n",
              "  0.5898692607879639,\n",
              "  0.601307213306427,\n",
              "  0.5735294222831726,\n",
              "  0.5506535768508911,\n",
              "  0.6176470518112183,\n",
              "  0.5882353186607361,\n",
              "  0.5980392098426819,\n",
              "  0.6601307392120361,\n",
              "  0.6111111044883728,\n",
              "  0.6225489974021912,\n",
              "  0.6258170008659363,\n",
              "  0.6405228972434998,\n",
              "  0.6617646813392639,\n",
              "  0.6274510025978088,\n",
              "  0.5718954205513,\n",
              "  0.6323529481887817,\n",
              "  0.6437908411026001,\n",
              "  0.5866013169288635,\n",
              "  0.6405228972434998,\n",
              "  0.6601307392120361,\n",
              "  0.6437908411026001,\n",
              "  0.6356208920478821,\n",
              "  0.6307189464569092,\n",
              "  0.6307189464569092,\n",
              "  0.6127451062202454,\n",
              "  0.6405228972434998,\n",
              "  0.6372548937797546,\n",
              "  0.6519607901573181,\n",
              "  0.648692786693573,\n",
              "  0.6584967374801636,\n",
              "  0.6552287340164185,\n",
              "  0.665032684803009,\n",
              "  0.6372548937797546,\n",
              "  0.6503267884254456,\n",
              "  0.6209150552749634,\n",
              "  0.6535947918891907,\n",
              "  0.6111111044883728,\n",
              "  0.6372548937797546,\n",
              "  0.6666666865348816,\n",
              "  0.6699346303939819,\n",
              "  0.6454248428344727,\n",
              "  0.6192810535430908,\n",
              "  0.6356208920478821,\n",
              "  0.6454248428344727,\n",
              "  0.6388888955116272,\n",
              "  0.6633986830711365,\n",
              "  0.6290849447250366,\n",
              "  0.6503267884254456,\n",
              "  0.6535947918891907,\n",
              "  0.6503267884254456,\n",
              "  0.648692786693573,\n",
              "  0.6454248428344727,\n",
              "  0.6503267884254456,\n",
              "  0.6633986830711365,\n",
              "  0.6552287340164185,\n",
              "  0.656862735748291,\n",
              "  0.6241829991340637,\n",
              "  0.6584967374801636,\n",
              "  0.6633986830711365,\n",
              "  0.656862735748291,\n",
              "  0.6633986830711365,\n",
              "  0.6437908411026001,\n",
              "  0.6683006286621094,\n",
              "  0.6683006286621094,\n",
              "  0.686274528503418,\n",
              "  0.6601307392120361,\n",
              "  0.6928104758262634,\n",
              "  0.6388888955116272,\n",
              "  0.686274528503418,\n",
              "  0.686274528503418,\n",
              "  0.6944444179534912,\n",
              "  0.6519607901573181,\n",
              "  0.6339869499206543,\n",
              "  0.6633986830711365,\n",
              "  0.6846405267715454,\n",
              "  0.648692786693573,\n",
              "  0.6813725233078003,\n",
              "  0.6813725233078003,\n",
              "  0.6797385811805725,\n",
              "  0.6830065250396729,\n",
              "  0.6699346303939819,\n",
              "  0.6813725233078003,\n",
              "  0.6715686321258545,\n",
              "  0.6519607901573181,\n",
              "  0.6683006286621094,\n",
              "  0.6781045794487,\n",
              "  0.6470588445663452,\n",
              "  0.665032684803009,\n",
              "  0.6928104758262634,\n",
              "  0.7026143670082092,\n",
              "  0.6601307392120361,\n",
              "  0.656862735748291,\n",
              "  0.6781045794487,\n",
              "  0.673202633857727,\n",
              "  0.6781045794487,\n",
              "  0.6617646813392639,\n",
              "  0.6813725233078003,\n",
              "  0.6683006286621094,\n",
              "  0.6879084706306458,\n",
              "  0.6993464231491089,\n",
              "  0.7009803652763367,\n",
              "  0.6535947918891907,\n",
              "  0.6895424723625183,\n",
              "  0.6977124214172363,\n",
              "  0.6993464231491089,\n",
              "  0.7091503143310547,\n",
              "  0.6960784196853638,\n",
              "  0.7009803652763367,\n",
              "  0.6535947918891907,\n",
              "  0.6911764740943909,\n",
              "  0.6797385811805725,\n",
              "  0.6813725233078003,\n",
              "  0.6617646813392639,\n",
              "  0.7026143670082092,\n",
              "  0.6781045794487,\n",
              "  0.733660101890564,\n",
              "  0.673202633857727,\n",
              "  0.6797385811805725,\n",
              "  0.7189542651176453,\n",
              "  0.6895424723625183,\n",
              "  0.6715686321258545,\n",
              "  0.7009803652763367,\n",
              "  0.6715686321258545,\n",
              "  0.6748365759849548,\n",
              "  0.686274528503418,\n",
              "  0.7009803652763367,\n",
              "  0.6715686321258545,\n",
              "  0.656862735748291,\n",
              "  0.720588207244873,\n",
              "  0.7075163125991821,\n",
              "  0.6797385811805725,\n",
              "  0.6879084706306458,\n",
              "  0.6781045794487,\n",
              "  0.6617646813392639,\n",
              "  0.6977124214172363,\n",
              "  0.6993464231491089,\n",
              "  0.7091503143310547,\n",
              "  0.7009803652763367,\n",
              "  0.6993464231491089,\n",
              "  0.7042483687400818,\n",
              "  0.6911764740943909,\n",
              "  0.7124183177947998,\n",
              "  0.7352941036224365,\n",
              "  0.7026143670082092,\n",
              "  0.7189542651176453,\n",
              "  0.7009803652763367,\n",
              "  0.6830065250396729,\n",
              "  0.686274528503418,\n",
              "  0.6960784196853638,\n",
              "  0.6971153616905212,\n",
              "  0.6764705777168274,\n",
              "  0.6911764740943909,\n",
              "  0.6977124214172363,\n",
              "  0.6977124214172363,\n",
              "  0.7173202633857727,\n",
              "  0.6830065250396729,\n",
              "  0.7189542651176453,\n",
              "  0.6748365759849548,\n",
              "  0.6764705777168274,\n",
              "  0.6895424723625183,\n",
              "  0.7320261597633362,\n",
              "  0.7058823704719543,\n",
              "  0.7124183177947998,\n",
              "  0.7042483687400818,\n",
              "  0.6977124214172363,\n",
              "  0.673202633857727,\n",
              "  0.6960784196853638,\n",
              "  0.7042483687400818,\n",
              "  0.7124183177947998,\n",
              "  0.6977124214172363,\n",
              "  0.7287581562995911,\n",
              "  0.7243589758872986,\n",
              "  0.7124183177947998,\n",
              "  0.7026143670082092,\n",
              "  0.7140522599220276,\n",
              "  0.6928104758262634,\n",
              "  0.7222222089767456,\n",
              "  0.7107843160629272,\n",
              "  0.7173202633857727,\n",
              "  0.6830065250396729,\n",
              "  0.758169949054718,\n",
              "  0.7091503143310547,\n",
              "  0.7401960492134094,\n",
              "  0.733660101890564,\n",
              "  0.6977124214172363,\n",
              "  0.7091503143310547,\n",
              "  0.7271241545677185,\n",
              "  0.733660101890564,\n",
              "  0.7075163125991821,\n",
              "  0.7075163125991821,\n",
              "  0.7091503143310547,\n",
              "  0.7124183177947998,\n",
              "  0.7287581562995911,\n",
              "  0.720588207244873,\n",
              "  0.7026143670082092,\n",
              "  0.7107843160629272,\n",
              "  0.7156862616539001,\n",
              "  0.7107843160629272,\n",
              "  0.7189542651176453,\n",
              "  0.6955128312110901,\n",
              "  0.7058823704719543,\n",
              "  0.7434640526771545,\n",
              "  0.7091503143310547,\n",
              "  0.7303921580314636,\n",
              "  0.6977124214172363,\n",
              "  0.7026143670082092,\n",
              "  0.7320261597633362,\n",
              "  0.7222222089767456,\n",
              "  0.7450980544090271,\n",
              "  0.7369281053543091,\n",
              "  0.7124183177947998,\n",
              "  0.7303921580314636,\n",
              "  0.7369281053543091,\n",
              "  0.6911764740943909,\n",
              "  0.741830050945282,\n",
              "  0.6944444179534912,\n",
              "  0.686274528503418,\n",
              "  0.7140522599220276,\n",
              "  0.6944444179534912,\n",
              "  0.7369281053543091,\n",
              "  0.7320261597633362,\n",
              "  0.7287581562995911,\n",
              "  0.6928104758262634,\n",
              "  0.6944444179534912,\n",
              "  0.7271241545677185,\n",
              "  0.7516340017318726,\n",
              "  0.7156862616539001,\n",
              "  0.7307692170143127,\n",
              "  0.7303921580314636,\n",
              "  0.7091503143310547,\n",
              "  0.6977124214172363,\n",
              "  0.7075163125991821,\n",
              "  0.7107843160629272,\n",
              "  0.6846405267715454,\n",
              "  0.7369281053543091,\n",
              "  0.7107843160629272,\n",
              "  0.7238562107086182,\n",
              "  0.7189542651176453,\n",
              "  0.7124183177947998,\n",
              "  0.7387820482254028,\n",
              "  0.7303921580314636,\n",
              "  0.758169949054718,\n",
              "  0.7352941036224365,\n",
              "  0.7238562107086182,\n",
              "  0.7287581562995911,\n",
              "  0.7614378929138184,\n",
              "  0.7483659982681274,\n",
              "  0.7156862616539001,\n",
              "  0.7467319965362549,\n",
              "  0.7222222089767456,\n",
              "  0.7254902124404907,\n",
              "  0.7091503143310547,\n",
              "  0.720588207244873,\n",
              "  0.7035256624221802,\n",
              "  0.7189542651176453,\n",
              "  0.7124183177947998,\n",
              "  0.7450980544090271,\n",
              "  0.7075163125991821,\n",
              "  0.7303921580314636,\n",
              "  0.7058823704719543,\n",
              "  0.7259615659713745,\n",
              "  0.7355769276618958,\n",
              "  0.7369281053543091,\n",
              "  0.7369281053543091,\n",
              "  0.7238562107086182,\n",
              "  0.7156862616539001,\n",
              "  0.7107843160629272,\n",
              "  0.7614378929138184,\n",
              "  0.7238562107086182,\n",
              "  0.7369281053543091,\n",
              "  0.7058823704719543,\n",
              "  0.7614378929138184,\n",
              "  0.7275640964508057,\n",
              "  0.7291666865348816,\n",
              "  0.7291666865348816,\n",
              "  0.7450980544090271,\n",
              "  0.720588207244873,\n",
              "  0.7287581562995911,\n",
              "  0.7352941036224365,\n",
              "  0.7385621070861816,\n",
              "  0.7352941036224365,\n",
              "  0.7124183177947998,\n",
              "  0.7483659982681274,\n",
              "  0.7189542651176453,\n",
              "  0.7173202633857727,\n",
              "  0.7271241545677185,\n",
              "  0.7254902124404907,\n",
              "  0.7173202633857727,\n",
              "  0.7320261597633362,\n",
              "  0.7320261597633362,\n",
              "  0.7271241545677185,\n",
              "  0.7467319965362549,\n",
              "  0.7630718946456909,\n",
              "  0.741830050945282,\n",
              "  0.7140522599220276,\n",
              "  0.7549019455909729,\n",
              "  0.7124183177947998,\n",
              "  0.741830050945282,\n",
              "  0.7303921580314636,\n",
              "  0.7173202633857727,\n",
              "  0.7140522599220276,\n",
              "  0.7124183177947998,\n",
              "  0.7287581562995911,\n",
              "  0.7434640526771545,\n",
              "  0.7254902124404907,\n",
              "  0.7385621070861816,\n",
              "  0.7401960492134094,\n",
              "  0.7434640526771545,\n",
              "  0.7303921580314636,\n",
              "  0.7352941036224365,\n",
              "  0.720588207244873,\n",
              "  0.7140522599220276,\n",
              "  0.7271241545677185,\n",
              "  0.7565359473228455,\n",
              "  0.7140522599220276,\n",
              "  0.7467319965362549,\n",
              "  0.7434640526771545,\n",
              "  0.7156862616539001,\n",
              "  0.7189542651176453,\n",
              "  0.7434640526771545,\n",
              "  0.7467319965362549,\n",
              "  0.7140522599220276,\n",
              "  0.7075163125991821,\n",
              "  0.7303921580314636,\n",
              "  0.7434640526771545,\n",
              "  0.7320261597633362,\n",
              "  0.7140522599220276,\n",
              "  0.7565359473228455,\n",
              "  0.7434640526771545,\n",
              "  0.7450980544090271,\n",
              "  0.7222222089767456,\n",
              "  0.733660101890564,\n",
              "  0.7320261597633362,\n",
              "  0.7254902124404907,\n",
              "  0.758169949054718,\n",
              "  0.7156862616539001,\n",
              "  0.7271241545677185,\n",
              "  0.7254902124404907,\n",
              "  0.741830050945282,\n",
              "  0.7254902124404907,\n",
              "  0.7320261597633362,\n",
              "  0.7483659982681274,\n",
              "  0.7287581562995911,\n",
              "  0.7483659982681274,\n",
              "  0.7173202633857727,\n",
              "  0.7320261597633362,\n",
              "  0.7450980544090271,\n",
              "  0.7434640526771545,\n",
              "  0.7369281053543091,\n",
              "  0.7483659982681274,\n",
              "  0.741830050945282,\n",
              "  0.7303921580314636,\n",
              "  0.7403846383094788,\n",
              "  0.7189542651176453,\n",
              "  0.733660101890564,\n",
              "  0.7254902124404907,\n",
              "  0.7467319965362549,\n",
              "  0.7450980544090271,\n",
              "  0.7352941036224365,\n",
              "  0.7271241545677185,\n",
              "  0.7369281053543091,\n",
              "  0.7189542651176453,\n",
              "  0.7320261597633362,\n",
              "  0.7271241545677185,\n",
              "  0.7156862616539001,\n",
              "  0.7156862616539001,\n",
              "  0.7401960492134094,\n",
              "  0.7483659982681274,\n",
              "  0.7483659982681274,\n",
              "  0.7450980544090271,\n",
              "  0.7467319965362549,\n",
              "  0.741830050945282,\n",
              "  0.7189542651176453,\n",
              "  0.7254902124404907,\n",
              "  0.7401960492134094,\n",
              "  0.7352941036224365,\n",
              "  0.7124183177947998,\n",
              "  0.7598039507865906,\n",
              "  0.7467319965362549,\n",
              "  0.7467319965362549,\n",
              "  0.7352941036224365,\n",
              "  0.7287581562995911,\n",
              "  0.7173202633857727,\n",
              "  0.7303921580314636,\n",
              "  0.7271241545677185,\n",
              "  0.7287581562995911,\n",
              "  0.7173202633857727,\n",
              "  0.7287581562995911,\n",
              "  0.7369281053543091,\n",
              "  0.7467319965362549,\n",
              "  0.7156862616539001,\n",
              "  0.7810457348823547,\n",
              "  0.7467319965362549,\n",
              "  0.720588207244873,\n",
              "  0.7075163125991821,\n",
              "  0.7320261597633362,\n",
              "  0.7254902124404907,\n",
              "  0.7450980544090271,\n",
              "  0.7271241545677185,\n",
              "  0.7173202633857727,\n",
              "  0.7369281053543091,\n",
              "  0.733660101890564,\n",
              "  0.7271241545677185,\n",
              "  0.741830050945282,\n",
              "  0.7532680034637451,\n",
              "  0.7075163125991821,\n",
              "  0.7238562107086182,\n",
              "  0.7679738402366638,\n",
              "  0.7385621070861816,\n",
              "  0.7483659982681274,\n",
              "  0.7140522599220276,\n",
              "  0.7450980544090271,\n",
              "  0.7352941036224365,\n",
              "  0.741830050945282,\n",
              "  0.7173202633857727,\n",
              "  0.7189542651176453,\n",
              "  0.7271241545677185,\n",
              "  0.7238562107086182,\n",
              "  0.7450980544090271,\n",
              "  0.7271241545677185,\n",
              "  0.7401960492134094,\n",
              "  0.741830050945282,\n",
              "  0.7222222089767456,\n",
              "  0.7075163125991821,\n",
              "  0.7385621070861816,\n",
              "  0.7549019455909729,\n",
              "  0.7598039507865906,\n",
              "  0.7401960492134094,\n",
              "  0.741830050945282,\n",
              "  0.7173202633857727,\n",
              "  0.7238562107086182,\n",
              "  0.7271241545677185,\n",
              "  0.7058823704719543,\n",
              "  0.7369281053543091,\n",
              "  0.7385621070861816,\n",
              "  0.7303921580314636,\n",
              "  0.7352941036224365,\n",
              "  0.7058823704719543,\n",
              "  0.7352941036224365,\n",
              "  0.733660101890564,\n",
              "  0.7450980544090271,\n",
              "  0.7287581562995911,\n",
              "  0.758169949054718,\n",
              "  0.7189542651176453,\n",
              "  0.7777777910232544,\n",
              "  0.7189542651176453,\n",
              "  0.7532680034637451,\n",
              "  0.7549019455909729,\n",
              "  0.7401960492134094,\n",
              "  0.7222222089767456,\n",
              "  0.720588207244873,\n",
              "  0.7401960492134094,\n",
              "  0.7467319965362549,\n",
              "  0.741830050945282,\n",
              "  0.733660101890564,\n",
              "  0.7238562107086182,\n",
              "  0.6960784196853638,\n",
              "  0.741830050945282,\n",
              "  0.7401960492134094,\n",
              "  0.7303921580314636,\n",
              "  0.766339898109436,\n",
              "  0.7026143670082092,\n",
              "  0.7483659982681274,\n",
              "  0.7467319965362549,\n",
              "  0.7107843160629272,\n",
              "  0.7287581562995911,\n",
              "  0.7320261597633362,\n",
              "  0.741830050945282,\n",
              "  0.7483659982681274,\n",
              "  0.7434640526771545,\n",
              "  0.7467319965362549,\n",
              "  0.75,\n",
              "  0.7565359473228455,\n",
              "  0.7450980544090271,\n",
              "  0.7532680034637451,\n",
              "  0.7401960492134094,\n",
              "  0.7434640526771545,\n",
              "  0.7450980544090271,\n",
              "  0.7532680034637451,\n",
              "  0.7369281053543091,\n",
              "  0.7369281053543091,\n",
              "  0.7401960492134094,\n",
              "  0.733660101890564,\n",
              "  0.7320261597633362,\n",
              "  0.7647058963775635,\n",
              "  0.741830050945282,\n",
              "  0.7385621070861816,\n",
              "  0.7516340017318726,\n",
              "  0.741830050945282,\n",
              "  0.7320261597633362,\n",
              "  0.7254902124404907,\n",
              "  0.7516340017318726,\n",
              "  0.7385621070861816,\n",
              "  0.7189542651176453,\n",
              "  0.7532680034637451,\n",
              "  0.7385621070861816,\n",
              "  0.741830050945282,\n",
              "  0.7401960492134094,\n",
              "  0.75,\n",
              "  0.7647058963775635,\n",
              "  0.7614378929138184,\n",
              "  0.7647058963775635,\n",
              "  0.7238562107086182,\n",
              "  0.7614378929138184,\n",
              "  0.7075163125991821,\n",
              "  0.733660101890564,\n",
              "  0.7434640526771545,\n",
              "  0.7450980544090271,\n",
              "  0.7222222089767456,\n",
              "  0.7369281053543091,\n",
              "  0.7352941036224365,\n",
              "  0.7385621070861816,\n",
              "  0.7614378929138184,\n",
              "  0.7352941036224365,\n",
              "  0.7435897588729858,\n",
              "  0.7107843160629272,\n",
              "  0.7483659982681274,\n",
              "  0.7516340017318726,\n",
              "  0.7303921580314636,\n",
              "  0.7532680034637451,\n",
              "  0.7467319965362549,\n",
              "  0.7385621070861816,\n",
              "  0.7467319965362549,\n",
              "  0.7385621070861816,\n",
              "  0.7483659982681274,\n",
              "  0.7565359473228455,\n",
              "  0.7238562107086182,\n",
              "  0.741830050945282,\n",
              "  0.7516340017318726,\n",
              "  0.7598039507865906,\n",
              "  0.766339898109436,\n",
              "  0.7320261597633362,\n",
              "  0.7320261597633362,\n",
              "  0.7467319965362549,\n",
              "  0.7222222089767456,\n",
              "  0.75,\n",
              "  0.7549019455909729,\n",
              "  0.7450980544090271,\n",
              "  0.7467319965362549,\n",
              "  0.7450980544090271,\n",
              "  0.7516340017318726,\n",
              "  0.7728758454322815,\n",
              "  0.741830050945282,\n",
              "  0.7222222089767456,\n",
              "  0.7565359473228455,\n",
              "  0.7450980544090271,\n",
              "  0.7467319965362549,\n",
              "  0.7598039507865906,\n",
              "  0.7450980544090271,\n",
              "  0.7450980544090271,\n",
              "  0.7516340017318726,\n",
              "  0.7352941036224365,\n",
              "  0.7222222089767456,\n",
              "  0.7254902124404907,\n",
              "  0.741830050945282,\n",
              "  0.733660101890564,\n",
              "  0.7385621070861816,\n",
              "  0.733660101890564,\n",
              "  0.75,\n",
              "  0.7647058963775635,\n",
              "  0.7450980544090271,\n",
              "  0.7222222089767456,\n",
              "  0.7598039507865906,\n",
              "  0.7516340017318726,\n",
              "  0.7254902124404907,\n",
              "  0.741830050945282,\n",
              "  0.7532680034637451,\n",
              "  0.7696078419685364,\n",
              "  0.7287581562995911,\n",
              "  0.7222222089767456,\n",
              "  0.7323718070983887,\n",
              "  0.7369281053543091,\n",
              "  0.720588207244873,\n",
              "  0.7450980544090271,\n",
              "  0.7728758454322815,\n",
              "  0.7287581562995911,\n",
              "  0.7450980544090271,\n",
              "  0.7320261597633362,\n",
              "  0.7271241545677185,\n",
              "  0.7352941036224365,\n",
              "  0.7385621070861816,\n",
              "  0.7303921580314636,\n",
              "  0.7450980544090271,\n",
              "  0.7173202633857727,\n",
              "  0.7777777910232544,\n",
              "  0.7467319965362549,\n",
              "  0.7385621070861816,\n",
              "  0.7287581562995911,\n",
              "  0.7369281053543091,\n",
              "  0.7434640526771545,\n",
              "  0.7238562107086182,\n",
              "  0.7189542651176453,\n",
              "  0.7320261597633362,\n",
              "  0.7385621070861816,\n",
              "  0.741830050945282,\n",
              "  0.758169949054718,\n",
              "  0.7355769276618958,\n",
              "  0.7450980544090271,\n",
              "  0.7401960492134094,\n",
              "  0.75,\n",
              "  0.758169949054718,\n",
              "  0.7385621070861816,\n",
              "  0.7598039507865906,\n",
              "  0.7712418437004089,\n",
              "  0.7467319965362549,\n",
              "  0.7156862616539001,\n",
              "  0.7450980544090271,\n",
              "  0.7352941036224365,\n",
              "  0.7238562107086182,\n",
              "  0.7450980544090271,\n",
              "  0.7483659982681274,\n",
              "  0.741830050945282,\n",
              "  0.7222222089767456,\n",
              "  0.7598039507865906,\n",
              "  0.7450980544090271,\n",
              "  0.7401960492134094,\n",
              "  0.7287581562995911,\n",
              "  0.7320261597633362,\n",
              "  0.7189542651176453,\n",
              "  0.7598039507865906,\n",
              "  0.7271241545677185,\n",
              "  0.7614378929138184,\n",
              "  0.7320261597633362,\n",
              "  0.7075163125991821,\n",
              "  0.7320261597633362,\n",
              "  0.7450980544090271,\n",
              "  0.758169949054718,\n",
              "  0.720588207244873,\n",
              "  0.7483659982681274,\n",
              "  0.7532680034637451,\n",
              "  0.779411792755127,\n",
              "  0.7598039507865906,\n",
              "  0.7401960492134094,\n",
              "  0.75,\n",
              "  0.7647058963775635,\n",
              "  0.7859477400779724,\n",
              "  0.7598039507865906,\n",
              "  0.7483659982681274,\n",
              "  0.7467319965362549,\n",
              "  0.7549019455909729,\n",
              "  0.7450980544090271,\n",
              "  0.7401960492134094,\n",
              "  0.7303921580314636,\n",
              "  0.7549019455909729,\n",
              "  0.7630718946456909,\n",
              "  0.7598039507865906,\n",
              "  0.75,\n",
              "  0.7189542651176453,\n",
              "  0.7320261597633362,\n",
              "  0.7238562107086182,\n",
              "  0.7712418437004089,\n",
              "  0.7810457348823547,\n",
              "  0.7401960492134094,\n",
              "  0.733660101890564,\n",
              "  0.7843137383460999,\n",
              "  0.75,\n",
              "  0.7385621070861816,\n",
              "  0.7598039507865906,\n",
              "  0.7271241545677185,\n",
              "  0.7058823704719543,\n",
              "  0.7565359473228455,\n",
              "  0.7549019455909729,\n",
              "  0.7173202633857727,\n",
              "  0.7467319965362549,\n",
              "  0.75,\n",
              "  0.7565359473228455,\n",
              "  0.7679738402366638,\n",
              "  0.7467319965362549,\n",
              "  0.7303921580314636,\n",
              "  0.7647058963775635,\n",
              "  0.7401960492134094,\n",
              "  0.7630718946456909,\n",
              "  0.758169949054718,\n",
              "  0.7565359473228455,\n",
              "  0.7401960492134094,\n",
              "  0.7532680034637451,\n",
              "  0.7483659982681274,\n",
              "  0.7271241545677185,\n",
              "  0.758169949054718,\n",
              "  0.7516340017318726,\n",
              "  0.7173202633857727,\n",
              "  0.7467319965362549,\n",
              "  0.733660101890564,\n",
              "  0.7483659982681274,\n",
              "  0.7696078419685364,\n",
              "  0.7385621070861816,\n",
              "  0.7483659982681274,\n",
              "  0.7156862616539001,\n",
              "  0.7761437892913818,\n",
              "  0.7352941036224365,\n",
              "  0.7516340017318726,\n",
              "  0.7614378929138184,\n",
              "  0.7401960492134094,\n",
              "  0.7320261597633362,\n",
              "  0.7647058963775635,\n",
              "  0.7107843160629272,\n",
              "  0.7320261597633362,\n",
              "  0.7516340017318726,\n",
              "  0.7369281053543091,\n",
              "  0.766339898109436,\n",
              "  0.7630718946456909,\n",
              "  0.7385621070861816,\n",
              "  0.7287581562995911,\n",
              "  0.75,\n",
              "  0.7434640526771545,\n",
              "  0.733660101890564,\n",
              "  0.7352941036224365,\n",
              "  0.7222222089767456,\n",
              "  0.7434640526771545,\n",
              "  0.741830050945282,\n",
              "  0.7222222089767456,\n",
              "  0.741830050945282,\n",
              "  0.7549019455909729,\n",
              "  0.7483659982681274,\n",
              "  0.7450980544090271,\n",
              "  0.7307692170143127,\n",
              "  0.7369281053543091,\n",
              "  0.7352941036224365,\n",
              "  0.7450980544090271,\n",
              "  0.779411792755127,\n",
              "  0.766339898109436,\n",
              "  0.7614378929138184,\n",
              "  0.75,\n",
              "  0.7401960492134094,\n",
              "  0.766339898109436,\n",
              "  0.733660101890564,\n",
              "  0.7450980544090271,\n",
              "  0.75,\n",
              "  0.7777777910232544,\n",
              "  0.7403846383094788,\n",
              "  0.7352941036224365,\n",
              "  0.7614378929138184,\n",
              "  0.7369281053543091,\n",
              "  0.7516340017318726,\n",
              "  0.7058823704719543,\n",
              "  0.7598039507865906,\n",
              "  0.7467319965362549,\n",
              "  0.7483659982681274,\n",
              "  0.7467319965362549,\n",
              "  0.7614378929138184,\n",
              "  0.7140522599220276,\n",
              "  0.7565359473228455,\n",
              "  0.7385621070861816,\n",
              "  0.758169949054718,\n",
              "  0.7614378929138184,\n",
              "  0.7516340017318726,\n",
              "  0.7385621070861816,\n",
              "  0.7696078419685364,\n",
              "  0.7777777910232544,\n",
              "  0.7189542651176453,\n",
              "  0.779411792755127,\n",
              "  0.7516340017318726,\n",
              "  0.7303921580314636,\n",
              "  0.7385621070861816,\n",
              "  0.7019230723381042,\n",
              "  0.7483659982681274,\n",
              "  0.7483659982681274,\n",
              "  0.7647058963775635,\n",
              "  0.7565359473228455,\n",
              "  0.7630718946456909,\n",
              "  0.7369281053543091,\n",
              "  0.7614378929138184,\n",
              "  0.7173202633857727,\n",
              "  0.7401960492134094,\n",
              "  0.7532680034637451,\n",
              "  0.7173202633857727,\n",
              "  0.733660101890564,\n",
              "  0.7303921580314636,\n",
              "  0.7598039507865906,\n",
              "  0.7352941036224365,\n",
              "  0.7565359473228455,\n",
              "  0.741830050945282,\n",
              "  0.7385621070861816,\n",
              "  0.7369281053543091,\n",
              "  0.7369281053543091,\n",
              "  0.7483659982681274,\n",
              "  0.7596153616905212,\n",
              "  0.7320261597633362,\n",
              "  0.7434640526771545,\n",
              "  0.7467319965362549,\n",
              "  0.7385621070861816,\n",
              "  0.7173202633857727,\n",
              "  0.7516340017318726,\n",
              "  0.7614378929138184,\n",
              "  0.7287581562995911,\n",
              "  0.7483659982681274,\n",
              "  0.7450980544090271,\n",
              "  0.7320261597633362,\n",
              "  0.7303921580314636,\n",
              "  0.7761437892913818,\n",
              "  0.7549019455909729,\n",
              "  0.758169949054718,\n",
              "  0.7189542651176453,\n",
              "  0.7450980544090271,\n",
              "  0.75,\n",
              "  0.7287581562995911,\n",
              "  0.754807710647583,\n",
              "  0.7323718070983887,\n",
              "  0.7401960492134094,\n",
              "  0.7352941036224365,\n",
              "  0.7532680034637451,\n",
              "  0.7630718946456909,\n",
              "  0.7385621070861816,\n",
              "  0.7385621070861816,\n",
              "  0.7434640526771545,\n",
              "  0.6977124214172363,\n",
              "  0.75,\n",
              "  0.7355769276618958,\n",
              "  0.7271241545677185,\n",
              "  0.7385621070861816,\n",
              "  0.7369281053543091,\n",
              "  0.7810457348823547,\n",
              "  0.7467319965362549,\n",
              "  0.7287581562995911,\n",
              "  0.7630718946456909,\n",
              "  0.7630718946456909,\n",
              "  0.75,\n",
              "  0.733660101890564,\n",
              "  0.7532680034637451,\n",
              "  0.7532680034637451,\n",
              "  0.7696078419685364,\n",
              "  0.7058823704719543,\n",
              "  0.7352941036224365,\n",
              "  0.779411792755127,\n",
              "  0.7843137383460999,\n",
              "  0.741830050945282,\n",
              "  0.741830050945282,\n",
              "  0.7467319965362549,\n",
              "  0.7580128312110901,\n",
              "  0.75,\n",
              "  0.7630718946456909,\n",
              "  0.7173202633857727,\n",
              "  0.758169949054718,\n",
              "  0.779411792755127,\n",
              "  0.7761437892913818,\n",
              "  0.7401960492134094,\n",
              "  0.7712418437004089,\n",
              "  0.7156862616539001,\n",
              "  0.779411792755127,\n",
              "  0.7696078419685364,\n",
              "  0.733660101890564,\n",
              "  0.7352941036224365,\n",
              "  0.7712418437004089,\n",
              "  0.7598039507865906,\n",
              "  0.7156862616539001,\n",
              "  0.7320261597633362,\n",
              "  0.7467319965362549,\n",
              "  0.7467319965362549,\n",
              "  0.7532680034637451,\n",
              "  0.7740384340286255,\n",
              "  0.7483659982681274,\n",
              "  0.7238562107086182,\n",
              "  0.7483659982681274,\n",
              "  0.7450980544090271,\n",
              "  0.741830050945282,\n",
              "  0.7287581562995911,\n",
              "  0.7565359473228455,\n",
              "  0.7612179517745972,\n",
              "  0.7352941036224365,\n",
              "  0.741830050945282,\n",
              "  0.7320261597633362,\n",
              "  0.75,\n",
              "  0.7352941036224365,\n",
              "  0.7549019455909729,\n",
              "  0.7369281053543091,\n",
              "  0.7679738402366638,\n",
              "  0.7352941036224365,\n",
              "  0.7598039507865906,\n",
              "  0.758169949054718,\n",
              "  0.7532051205635071,\n",
              "  0.7434640526771545,\n",
              "  0.7385621070861816,\n",
              "  0.7565359473228455,\n",
              "  0.7516340017318726,\n",
              "  0.7287581562995911,\n",
              "  0.7483659982681274,\n",
              "  0.7810457348823547,\n",
              "  0.75,\n",
              "  0.7385621070861816,\n",
              "  0.7320261597633362,\n",
              "  0.7532680034637451,\n",
              "  0.7598039507865906,\n",
              "  0.7369281053543091,\n",
              "  0.7712418437004089,\n",
              "  0.7287581562995911,\n",
              "  0.7516340017318726,\n",
              "  0.7352941036224365,\n",
              "  0.741830050945282,\n",
              "  0.7042483687400818,\n",
              "  0.7403846383094788,\n",
              "  0.7434640526771545,\n",
              "  0.7647058963775635,\n",
              "  0.7140522599220276,\n",
              "  0.720588207244873,\n",
              "  0.7598039507865906,\n",
              "  0.7647058963775635,\n",
              "  0.75,\n",
              "  0.7810457348823547,\n",
              "  0.7401960492134094,\n",
              "  0.7483659982681274,\n",
              "  0.7679738402366638,\n",
              "  0.7303921580314636,\n",
              "  0.733660101890564,\n",
              "  0.7401960492134094,\n",
              "  0.7696078419685364,\n",
              "  0.7303921580314636,\n",
              "  0.7483659982681274,\n",
              "  0.7483659982681274,\n",
              "  0.7516340017318726,\n",
              "  0.7271241545677185,\n",
              "  0.7483659982681274,\n",
              "  0.7434640526771545,\n",
              "  0.7434640526771545,\n",
              "  0.7385621070861816,\n",
              "  0.7483659982681274,\n",
              "  0.7254902124404907,\n",
              "  0.7434640526771545,\n",
              "  0.7532680034637451,\n",
              "  0.7532680034637451,\n",
              "  0.7140522599220276,\n",
              "  0.7401960492134094,\n",
              "  0.7352941036224365,\n",
              "  0.7156862616539001,\n",
              "  0.7387820482254028,\n",
              "  0.766339898109436,\n",
              "  0.7352941036224365,\n",
              "  0.7369281053543091,\n",
              "  0.7369281053543091,\n",
              "  0.741830050945282,\n",
              "  0.7450980544090271,\n",
              "  0.75,\n",
              "  0.7352941036224365,\n",
              "  0.7549019455909729,\n",
              "  0.7271241545677185,\n",
              "  0.7483659982681274,\n",
              "  0.7467319965362549,\n",
              "  0.7516340017318726,\n",
              "  0.7467319965362549,\n",
              "  0.7875816822052002],\n",
              " 'val_loss': [0.8831025958061218,\n",
              "  0.6843276023864746,\n",
              "  0.6871941685676575,\n",
              "  0.7099586129188538,\n",
              "  0.6968650221824646,\n",
              "  0.7345741391181946,\n",
              "  0.720155656337738,\n",
              "  0.7347332835197449,\n",
              "  0.7568842768669128,\n",
              "  0.7311334609985352,\n",
              "  0.7489802241325378,\n",
              "  0.7330553531646729,\n",
              "  0.7475540637969971,\n",
              "  0.7637426853179932,\n",
              "  0.7537629008293152,\n",
              "  0.7600343823432922,\n",
              "  0.7698876261711121,\n",
              "  0.7843698859214783,\n",
              "  0.7742084860801697,\n",
              "  0.764798641204834,\n",
              "  0.7938513159751892,\n",
              "  0.7829164862632751,\n",
              "  0.7643401622772217,\n",
              "  0.7974057197570801,\n",
              "  0.8054285049438477,\n",
              "  0.7935588955879211,\n",
              "  0.7909867763519287,\n",
              "  0.7834737300872803,\n",
              "  0.8028038144111633,\n",
              "  0.7900583744049072,\n",
              "  0.7906760573387146,\n",
              "  0.7821176052093506,\n",
              "  0.8050256371498108,\n",
              "  0.7930488586425781,\n",
              "  0.7965070605278015,\n",
              "  0.7795648574829102,\n",
              "  0.7886654734611511,\n",
              "  0.7439467906951904,\n",
              "  0.7662143707275391,\n",
              "  0.7432268261909485,\n",
              "  0.7558166980743408,\n",
              "  0.7806660532951355,\n",
              "  0.779052197933197,\n",
              "  0.7791228890419006,\n",
              "  0.793107807636261,\n",
              "  0.7853636145591736,\n",
              "  0.7803354859352112,\n",
              "  0.7756636738777161,\n",
              "  0.769709587097168,\n",
              "  0.7744455337524414,\n",
              "  0.7994444370269775,\n",
              "  0.7538430690765381,\n",
              "  0.7786040306091309,\n",
              "  0.7801938652992249,\n",
              "  0.7983892560005188,\n",
              "  0.788902759552002,\n",
              "  0.7689728140830994,\n",
              "  0.7840723991394043,\n",
              "  0.7852433323860168,\n",
              "  0.7766082882881165,\n",
              "  0.738165557384491,\n",
              "  0.7638273239135742,\n",
              "  0.7523552775382996,\n",
              "  0.7387053370475769,\n",
              "  0.7529858946800232,\n",
              "  0.757516622543335,\n",
              "  0.7636105418205261,\n",
              "  0.7755348086357117,\n",
              "  0.7526885867118835,\n",
              "  0.7634618282318115,\n",
              "  0.7626020312309265,\n",
              "  0.7783851623535156,\n",
              "  0.7575086951255798,\n",
              "  0.7605352401733398,\n",
              "  0.7692034244537354,\n",
              "  0.7506365776062012,\n",
              "  0.7485793232917786,\n",
              "  0.7463940978050232,\n",
              "  0.7438532710075378,\n",
              "  0.7557811141014099,\n",
              "  0.755124568939209,\n",
              "  0.751700222492218,\n",
              "  0.7526981234550476,\n",
              "  0.7606651782989502,\n",
              "  0.7337023615837097,\n",
              "  0.7407940030097961,\n",
              "  0.7423704266548157,\n",
              "  0.7227081656455994,\n",
              "  0.7383795380592346,\n",
              "  0.7391254305839539,\n",
              "  0.7438235282897949,\n",
              "  0.728949248790741,\n",
              "  0.7340896129608154,\n",
              "  0.7381041646003723,\n",
              "  0.7406712174415588,\n",
              "  0.7474389672279358,\n",
              "  0.7588641047477722,\n",
              "  0.773932933807373,\n",
              "  0.7517575621604919,\n",
              "  0.7687798142433167,\n",
              "  0.7572632431983948,\n",
              "  0.7836979031562805,\n",
              "  0.7442232966423035,\n",
              "  0.7400157451629639,\n",
              "  0.7732763290405273,\n",
              "  0.7618407607078552,\n",
              "  0.7801721096038818,\n",
              "  0.7501973509788513,\n",
              "  0.7478253245353699,\n",
              "  0.757000207901001,\n",
              "  0.7590628266334534,\n",
              "  0.7595381736755371,\n",
              "  0.7546272873878479,\n",
              "  0.7590668201446533,\n",
              "  0.7455887198448181,\n",
              "  0.743203341960907,\n",
              "  0.7286631464958191,\n",
              "  0.7212727665901184,\n",
              "  0.7412969470024109,\n",
              "  0.7292569279670715,\n",
              "  0.7361536026000977,\n",
              "  0.7203710079193115,\n",
              "  0.7338401675224304,\n",
              "  0.7449238896369934,\n",
              "  0.7289029955863953,\n",
              "  0.7187435030937195,\n",
              "  0.7304766774177551,\n",
              "  0.7400050163269043,\n",
              "  0.7181283831596375,\n",
              "  0.7055085301399231,\n",
              "  0.7268543243408203,\n",
              "  0.7184925079345703,\n",
              "  0.7183899283409119,\n",
              "  0.7453635334968567,\n",
              "  0.725836455821991,\n",
              "  0.7190575003623962,\n",
              "  0.713036835193634,\n",
              "  0.722791850566864,\n",
              "  0.7177814841270447,\n",
              "  0.7163822650909424,\n",
              "  0.7166953086853027,\n",
              "  0.7089056968688965,\n",
              "  0.7083768248558044,\n",
              "  0.7122304439544678,\n",
              "  0.7053446769714355,\n",
              "  0.7127552032470703,\n",
              "  0.7226929664611816,\n",
              "  0.7006118893623352,\n",
              "  0.6686692237854004,\n",
              "  0.7030565142631531,\n",
              "  0.6910221576690674,\n",
              "  0.7158095836639404,\n",
              "  0.6923885345458984,\n",
              "  0.7103314399719238,\n",
              "  0.6983103156089783,\n",
              "  0.6768301129341125,\n",
              "  0.6719030737876892,\n",
              "  0.6881385445594788,\n",
              "  0.681448757648468,\n",
              "  0.6776642799377441,\n",
              "  0.6829610466957092,\n",
              "  0.6928279995918274,\n",
              "  0.6831390857696533,\n",
              "  0.696142852306366,\n",
              "  0.7038351893424988,\n",
              "  0.6846561431884766,\n",
              "  0.6806497573852539,\n",
              "  0.687789261341095,\n",
              "  0.6878395676612854,\n",
              "  0.687160313129425,\n",
              "  0.6881375908851624,\n",
              "  0.6833088994026184,\n",
              "  0.6768888831138611,\n",
              "  0.6735315918922424,\n",
              "  0.6908051371574402,\n",
              "  0.6905446648597717,\n",
              "  0.7034845352172852,\n",
              "  0.6763166785240173,\n",
              "  0.6853769421577454,\n",
              "  0.6687877774238586,\n",
              "  0.6733043789863586,\n",
              "  0.6883618235588074,\n",
              "  0.6866412162780762,\n",
              "  0.6676137447357178,\n",
              "  0.6643381714820862,\n",
              "  0.6899576783180237,\n",
              "  0.677378237247467,\n",
              "  0.6678716540336609,\n",
              "  0.6636478304862976,\n",
              "  0.6581123471260071,\n",
              "  0.6573633551597595,\n",
              "  0.6470067501068115,\n",
              "  0.6604189276695251,\n",
              "  0.6815558075904846,\n",
              "  0.6805016398429871,\n",
              "  0.6664716601371765,\n",
              "  0.6925650238990784,\n",
              "  0.6792276501655579,\n",
              "  0.6797794699668884,\n",
              "  0.6714115738868713,\n",
              "  0.6743662357330322,\n",
              "  0.6763315200805664,\n",
              "  0.6612539887428284,\n",
              "  0.6689965128898621,\n",
              "  0.658438503742218,\n",
              "  0.6571435332298279,\n",
              "  0.6647602915763855,\n",
              "  0.6637977957725525,\n",
              "  0.6636897325515747,\n",
              "  0.6661351323127747,\n",
              "  0.6592170596122742,\n",
              "  0.6657875776290894,\n",
              "  0.6677215695381165,\n",
              "  0.6823822855949402,\n",
              "  0.6736777424812317,\n",
              "  0.659055769443512,\n",
              "  0.6627498269081116,\n",
              "  0.6623547673225403,\n",
              "  0.6606223583221436,\n",
              "  0.652599036693573,\n",
              "  0.6523723602294922,\n",
              "  0.6671034693717957,\n",
              "  0.6613756418228149,\n",
              "  0.6798513531684875,\n",
              "  0.6693320274353027,\n",
              "  0.668156623840332,\n",
              "  0.6829926371574402,\n",
              "  0.6764132380485535,\n",
              "  0.6841744780540466,\n",
              "  0.6654835343360901,\n",
              "  0.6704927086830139,\n",
              "  0.6729755401611328,\n",
              "  0.6677225232124329,\n",
              "  0.6778538823127747,\n",
              "  0.6482083797454834,\n",
              "  0.6673294901847839,\n",
              "  0.6610058546066284,\n",
              "  0.6564823985099792,\n",
              "  0.6688852310180664,\n",
              "  0.6577474474906921,\n",
              "  0.6549420952796936,\n",
              "  0.6678722500801086,\n",
              "  0.6724824905395508,\n",
              "  0.6618478298187256,\n",
              "  0.6616215109825134,\n",
              "  0.6614586710929871,\n",
              "  0.6552742123603821,\n",
              "  0.6613068580627441,\n",
              "  0.6737115979194641,\n",
              "  0.6754209399223328,\n",
              "  0.6485856175422668,\n",
              "  0.6647332310676575,\n",
              "  0.6496978402137756,\n",
              "  0.6666905283927917,\n",
              "  0.6529527306556702,\n",
              "  0.6486805081367493,\n",
              "  0.6544525027275085,\n",
              "  0.6505129933357239,\n",
              "  0.6635255217552185,\n",
              "  0.6620439887046814,\n",
              "  0.653231680393219,\n",
              "  0.6673172116279602,\n",
              "  0.6603535413742065,\n",
              "  0.6460751295089722,\n",
              "  0.6504732966423035,\n",
              "  0.6681241989135742,\n",
              "  0.6416240334510803,\n",
              "  0.659727156162262,\n",
              "  0.639117956161499,\n",
              "  0.6617655158042908,\n",
              "  0.6455349326133728,\n",
              "  0.6730115413665771,\n",
              "  0.6579557061195374,\n",
              "  0.6728215217590332,\n",
              "  0.6744458675384521,\n",
              "  0.6509212851524353,\n",
              "  0.6739121079444885,\n",
              "  0.6651419997215271,\n",
              "  0.6736963391304016,\n",
              "  0.6775309443473816,\n",
              "  0.6706108450889587,\n",
              "  0.670688807964325,\n",
              "  0.6735479831695557,\n",
              "  0.6749293208122253,\n",
              "  0.675121009349823,\n",
              "  0.6511446833610535,\n",
              "  0.6670243740081787,\n",
              "  0.6481884121894836,\n",
              "  0.6521694660186768,\n",
              "  0.6416628956794739,\n",
              "  0.6614162921905518,\n",
              "  0.6608884334564209,\n",
              "  0.660998523235321,\n",
              "  0.6756551265716553,\n",
              "  0.6592981815338135,\n",
              "  0.6580353379249573,\n",
              "  0.6636278033256531,\n",
              "  0.6594546437263489,\n",
              "  0.6592298150062561,\n",
              "  0.6651761531829834,\n",
              "  0.6615684628486633,\n",
              "  0.6627330183982849,\n",
              "  0.6593891382217407,\n",
              "  0.6296976208686829,\n",
              "  0.638836681842804,\n",
              "  0.6422880291938782,\n",
              "  0.6388678550720215,\n",
              "  0.6591810584068298,\n",
              "  0.6302300691604614,\n",
              "  0.6787512898445129,\n",
              "  0.6694421768188477,\n",
              "  0.6560562252998352,\n",
              "  0.6357107162475586,\n",
              "  0.6346535086631775,\n",
              "  0.63949054479599,\n",
              "  0.6449694037437439,\n",
              "  0.6550266742706299,\n",
              "  0.6480015516281128,\n",
              "  0.6261008381843567,\n",
              "  0.6243906021118164,\n",
              "  0.6330254673957825,\n",
              "  0.6249292492866516,\n",
              "  0.6203702092170715,\n",
              "  0.621596097946167,\n",
              "  0.6214993596076965,\n",
              "  0.6239489912986755,\n",
              "  0.6282177567481995,\n",
              "  0.6112260222434998,\n",
              "  0.6118675470352173,\n",
              "  0.6078457236289978,\n",
              "  0.6005614995956421,\n",
              "  0.613595724105835,\n",
              "  0.6416593194007874,\n",
              "  0.6203940510749817,\n",
              "  0.6221864819526672,\n",
              "  0.6256372928619385,\n",
              "  0.6244606971740723,\n",
              "  0.6308796405792236,\n",
              "  0.6266219019889832,\n",
              "  0.6329060196876526,\n",
              "  0.6204114556312561,\n",
              "  0.6161510944366455,\n",
              "  0.6079424619674683,\n",
              "  0.6347967982292175,\n",
              "  0.6312710642814636,\n",
              "  0.6276416182518005,\n",
              "  0.6135511994361877,\n",
              "  0.6446478962898254,\n",
              "  0.6379036903381348,\n",
              "  0.6570397019386292,\n",
              "  0.619120180606842,\n",
              "  0.6150975227355957,\n",
              "  0.6229649186134338,\n",
              "  0.6525581479072571,\n",
              "  0.6273878812789917,\n",
              "  0.6418513655662537,\n",
              "  0.6277335286140442,\n",
              "  0.615261435508728,\n",
              "  0.601139485836029,\n",
              "  0.6297044157981873,\n",
              "  0.6328287124633789,\n",
              "  0.6457430124282837,\n",
              "  0.6095166802406311,\n",
              "  0.6217254996299744,\n",
              "  0.6067704558372498,\n",
              "  0.6047603487968445,\n",
              "  0.619759738445282,\n",
              "  0.6213071942329407,\n",
              "  0.6304706931114197,\n",
              "  0.6230607032775879,\n",
              "  0.607231855392456,\n",
              "  0.6076421141624451,\n",
              "  0.6347807049751282,\n",
              "  0.6316909193992615,\n",
              "  0.6288571357727051,\n",
              "  0.6178635954856873,\n",
              "  0.6366090178489685,\n",
              "  0.6324020624160767,\n",
              "  0.6282989978790283,\n",
              "  0.6210963129997253,\n",
              "  0.6291112899780273,\n",
              "  0.6344313025474548,\n",
              "  0.6244075298309326,\n",
              "  0.6105216145515442,\n",
              "  0.6215788125991821,\n",
              "  0.6048888564109802,\n",
              "  0.6235788464546204,\n",
              "  0.6155837178230286,\n",
              "  0.6213595271110535,\n",
              "  0.6138492226600647,\n",
              "  0.6394914984703064,\n",
              "  0.6359476447105408,\n",
              "  0.6176623106002808,\n",
              "  0.607753336429596,\n",
              "  0.5985265970230103,\n",
              "  0.621880054473877,\n",
              "  0.6046648621559143,\n",
              "  0.6481609344482422,\n",
              "  0.6307808756828308,\n",
              "  0.6063416600227356,\n",
              "  0.5994141101837158,\n",
              "  0.6206352114677429,\n",
              "  0.631357729434967,\n",
              "  0.615581214427948,\n",
              "  0.6408941149711609,\n",
              "  0.620688259601593,\n",
              "  0.606252133846283,\n",
              "  0.611103355884552,\n",
              "  0.6091230511665344,\n",
              "  0.6236897110939026,\n",
              "  0.6021261811256409,\n",
              "  0.6025336384773254,\n",
              "  0.6420490741729736,\n",
              "  0.6061325669288635,\n",
              "  0.6101751923561096,\n",
              "  0.6044551730155945,\n",
              "  0.5987131595611572,\n",
              "  0.6183555722236633,\n",
              "  0.6023375988006592,\n",
              "  0.6032046675682068,\n",
              "  0.615389883518219,\n",
              "  0.6302170157432556,\n",
              "  0.6188883185386658,\n",
              "  0.607219934463501,\n",
              "  0.6067348122596741,\n",
              "  0.6492271423339844,\n",
              "  0.6264023780822754,\n",
              "  0.6331271529197693,\n",
              "  0.6196482181549072,\n",
              "  0.59818035364151,\n",
              "  0.6042511463165283,\n",
              "  0.6055271625518799,\n",
              "  0.6021801829338074,\n",
              "  0.6278029680252075,\n",
              "  0.619011640548706,\n",
              "  0.6166109442710876,\n",
              "  0.6273293495178223,\n",
              "  0.6148073673248291,\n",
              "  0.6151960492134094,\n",
              "  0.6065385341644287,\n",
              "  0.6121696829795837,\n",
              "  0.6073157787322998,\n",
              "  0.6524222493171692,\n",
              "  0.5949974060058594,\n",
              "  0.6081438064575195,\n",
              "  0.600139319896698,\n",
              "  0.6175863742828369,\n",
              "  0.6005372405052185,\n",
              "  0.5956742167472839,\n",
              "  0.5849930644035339,\n",
              "  0.605665385723114,\n",
              "  0.6071659922599792,\n",
              "  0.610726535320282,\n",
              "  0.60782790184021,\n",
              "  0.6262370944023132,\n",
              "  0.5937793254852295,\n",
              "  0.6200992465019226,\n",
              "  0.6001546382904053,\n",
              "  0.6071024537086487,\n",
              "  0.6020469069480896,\n",
              "  0.6253336071968079,\n",
              "  0.6053274869918823,\n",
              "  0.6191999316215515,\n",
              "  0.6128600239753723,\n",
              "  0.6083136200904846,\n",
              "  0.6186584830284119,\n",
              "  0.6071653962135315,\n",
              "  0.6189079880714417,\n",
              "  0.6070820689201355,\n",
              "  0.6122776865959167,\n",
              "  0.6061525940895081,\n",
              "  0.6212983727455139,\n",
              "  0.6071552634239197,\n",
              "  0.6063901782035828,\n",
              "  0.6125879883766174,\n",
              "  0.6021302938461304,\n",
              "  0.619833767414093,\n",
              "  0.6217156052589417,\n",
              "  0.6057308912277222,\n",
              "  0.6344165205955505,\n",
              "  0.6188284754753113,\n",
              "  0.6345124244689941,\n",
              "  0.6246150732040405,\n",
              "  0.6139779090881348,\n",
              "  0.6200870275497437,\n",
              "  0.6217474341392517,\n",
              "  0.6155003905296326,\n",
              "  0.6251831650733948,\n",
              "  0.6528679728507996,\n",
              "  0.6351339817047119,\n",
              "  0.6479007601737976,\n",
              "  0.621894896030426,\n",
              "  0.6503308415412903,\n",
              "  0.619550883769989,\n",
              "  0.6031198501586914,\n",
              "  0.5982916355133057,\n",
              "  0.6217421889305115,\n",
              "  0.6095141768455505,\n",
              "  0.6042801737785339,\n",
              "  0.6137577295303345,\n",
              "  0.5989806056022644,\n",
              "  0.5983405113220215,\n",
              "  0.5986377000808716,\n",
              "  0.6159404516220093,\n",
              "  0.623466968536377,\n",
              "  0.5881406664848328,\n",
              "  0.5956177115440369,\n",
              "  0.6160246133804321,\n",
              "  0.6228601336479187,\n",
              "  0.6078893542289734,\n",
              "  0.6060611605644226,\n",
              "  0.6046003103256226,\n",
              "  0.6038109660148621,\n",
              "  0.5972504019737244,\n",
              "  0.6078157424926758,\n",
              "  0.6155219674110413,\n",
              "  0.6022579073905945,\n",
              "  0.6116950511932373,\n",
              "  0.5902095437049866,\n",
              "  0.6091190576553345,\n",
              "  0.6131333708763123,\n",
              "  0.6205583214759827,\n",
              "  0.587291419506073,\n",
              "  0.609687864780426,\n",
              "  0.5921774506568909,\n",
              "  0.6152377128601074,\n",
              "  0.6148340106010437,\n",
              "  0.6057429909706116,\n",
              "  0.5977430939674377,\n",
              "  0.594735860824585,\n",
              "  0.6158203482627869,\n",
              "  0.6048229336738586,\n",
              "  0.6196616291999817,\n",
              "  0.6325835585594177,\n",
              "  0.6094881892204285,\n",
              "  0.6130931377410889,\n",
              "  0.621799647808075,\n",
              "  0.6257475018501282,\n",
              "  0.6101038455963135,\n",
              "  0.632748544216156,\n",
              "  0.6183273792266846,\n",
              "  0.630088746547699,\n",
              "  0.5887729525566101,\n",
              "  0.6137005090713501,\n",
              "  0.6134119629859924,\n",
              "  0.6212430596351624,\n",
              "  0.599307656288147,\n",
              "  0.6061100363731384,\n",
              "  0.6161683201789856,\n",
              "  0.6151108741760254,\n",
              "  0.6199217438697815,\n",
              "  0.59514981508255,\n",
              "  0.6031251549720764,\n",
              "  0.5992364287376404,\n",
              "  0.62330162525177,\n",
              "  0.5832017064094543,\n",
              "  0.585815966129303,\n",
              "  0.6092327237129211,\n",
              "  0.6242564916610718,\n",
              "  0.5939006209373474,\n",
              "  0.6239860653877258,\n",
              "  0.6282917857170105,\n",
              "  0.6483649611473083,\n",
              "  0.6300457715988159,\n",
              "  0.6531147360801697,\n",
              "  0.6223430633544922,\n",
              "  0.6349353790283203,\n",
              "  0.6271820664405823,\n",
              "  0.6656249165534973,\n",
              "  0.6178366541862488,\n",
              "  0.6338475346565247,\n",
              "  0.6240555644035339,\n",
              "  0.6113901734352112,\n",
              "  0.6151502728462219,\n",
              "  0.5957473516464233,\n",
              "  0.6099814772605896,\n",
              "  0.6377051472663879,\n",
              "  0.6135634183883667,\n",
              "  0.6060029864311218,\n",
              "  0.6070883870124817,\n",
              "  0.5988599061965942,\n",
              "  0.6074506044387817,\n",
              "  0.605556070804596,\n",
              "  0.6109927296638489,\n",
              "  0.5904889106750488,\n",
              "  0.6074856519699097,\n",
              "  0.6277186870574951,\n",
              "  0.6340399384498596,\n",
              "  0.6029653549194336,\n",
              "  0.5910646915435791,\n",
              "  0.6176976561546326,\n",
              "  0.6043176054954529,\n",
              "  0.6118759512901306,\n",
              "  0.6165758967399597,\n",
              "  0.6038811802864075,\n",
              "  0.6056803464889526,\n",
              "  0.6206889748573303,\n",
              "  0.6071026921272278,\n",
              "  0.5841333270072937,\n",
              "  0.5808647871017456,\n",
              "  0.6000385284423828,\n",
              "  0.592107355594635,\n",
              "  0.60121089220047,\n",
              "  0.6276583671569824,\n",
              "  0.6214929223060608,\n",
              "  0.5965921878814697,\n",
              "  0.6096580624580383,\n",
              "  0.6026541590690613,\n",
              "  0.597283661365509,\n",
              "  0.6058197617530823,\n",
              "  0.6052172183990479,\n",
              "  0.5960192680358887,\n",
              "  0.5917564034461975,\n",
              "  0.6062458157539368,\n",
              "  0.6021032929420471,\n",
              "  0.6136637330055237,\n",
              "  0.6104615330696106,\n",
              "  0.6203608512878418,\n",
              "  0.6122645735740662,\n",
              "  0.6110639572143555,\n",
              "  0.6274928450584412,\n",
              "  0.6074602603912354,\n",
              "  0.6046949028968811,\n",
              "  0.6045562624931335,\n",
              "  0.6029793620109558,\n",
              "  0.6008092761039734,\n",
              "  0.6289780735969543,\n",
              "  0.6233084201812744,\n",
              "  0.6564292311668396,\n",
              "  0.6053377389907837,\n",
              "  0.604926347732544,\n",
              "  0.603587806224823,\n",
              "  0.600013256072998,\n",
              "  0.6305258870124817,\n",
              "  0.6048802137374878,\n",
              "  0.5967774391174316,\n",
              "  0.6192161440849304,\n",
              "  0.6231382489204407,\n",
              "  0.6070881485939026,\n",
              "  0.6010645031929016,\n",
              "  0.6219971179962158,\n",
              "  0.6377042531967163,\n",
              "  0.6279457211494446,\n",
              "  0.5933997631072998,\n",
              "  0.5912277102470398,\n",
              "  0.5742896199226379,\n",
              "  0.5901930928230286,\n",
              "  0.5955156683921814,\n",
              "  0.5990603566169739,\n",
              "  0.5868911147117615,\n",
              "  0.5922970771789551,\n",
              "  0.5930667519569397,\n",
              "  0.6066272854804993,\n",
              "  0.597545325756073,\n",
              "  0.591756284236908,\n",
              "  0.5864887237548828,\n",
              "  0.5873355865478516,\n",
              "  0.5881574749946594,\n",
              "  0.580341637134552,\n",
              "  0.5844689607620239,\n",
              "  0.6136490702629089,\n",
              "  0.617530107498169,\n",
              "  0.5984645485877991,\n",
              "  0.6260005831718445,\n",
              "  0.6278951168060303,\n",
              "  0.6080529689788818,\n",
              "  0.6227355003356934,\n",
              "  0.6027181148529053,\n",
              "  0.60882967710495,\n",
              "  0.5863897204399109,\n",
              "  0.6148896813392639,\n",
              "  0.595163881778717,\n",
              "  0.5728981494903564,\n",
              "  0.5856475234031677,\n",
              "  0.5704171061515808,\n",
              "  0.5882248282432556,\n",
              "  0.5934718251228333,\n",
              "  0.5921183228492737,\n",
              "  0.5998157858848572,\n",
              "  0.5892336964607239,\n",
              "  0.5748379826545715,\n",
              "  0.5888040661811829,\n",
              "  0.5863232016563416,\n",
              "  0.603408932685852,\n",
              "  0.6034470796585083,\n",
              "  0.6211956739425659,\n",
              "  0.5734981894493103,\n",
              "  0.5801476240158081,\n",
              "  0.577269434928894,\n",
              "  0.5779547095298767,\n",
              "  0.5679720640182495,\n",
              "  0.5498555898666382,\n",
              "  0.5798593163490295,\n",
              "  0.5737623572349548,\n",
              "  0.5887547135353088,\n",
              "  0.5877614617347717,\n",
              "  0.5978197455406189,\n",
              "  0.5887972712516785,\n",
              "  0.600756824016571,\n",
              "  0.5953448414802551,\n",
              "  0.5834331512451172,\n",
              "  0.5866764187812805,\n",
              "  0.6096922755241394,\n",
              "  0.6173414587974548,\n",
              "  0.6117743253707886,\n",
              "  0.6150251030921936,\n",
              "  0.6040624380111694,\n",
              "  0.5907424688339233,\n",
              "  0.6070489287376404,\n",
              "  0.6022329926490784,\n",
              "  0.6012691855430603,\n",
              "  0.5946967005729675,\n",
              "  0.6212106347084045,\n",
              "  0.6104761958122253,\n",
              "  0.5932729840278625,\n",
              "  0.5993804335594177,\n",
              "  0.5980486869812012,\n",
              "  0.5959559679031372,\n",
              "  0.616093099117279,\n",
              "  0.593082845211029,\n",
              "  0.6091274619102478,\n",
              "  0.6198999285697937,\n",
              "  0.6024094223976135,\n",
              "  0.5935556888580322,\n",
              "  0.5842180848121643,\n",
              "  0.5861647129058838,\n",
              "  0.605535626411438,\n",
              "  0.5994303822517395,\n",
              "  0.6170749664306641,\n",
              "  0.6215093731880188,\n",
              "  0.6155054569244385,\n",
              "  0.6133062839508057,\n",
              "  0.6010844111442566,\n",
              "  0.592921257019043,\n",
              "  0.6084961295127869,\n",
              "  0.617254912853241,\n",
              "  0.6218567490577698,\n",
              "  0.6068360209465027,\n",
              "  0.6199018955230713,\n",
              "  0.6531886458396912,\n",
              "  0.6099316477775574,\n",
              "  0.5993826985359192,\n",
              "  0.6026204824447632,\n",
              "  0.5778105854988098,\n",
              "  0.5940943360328674,\n",
              "  0.634799063205719,\n",
              "  0.5960620045661926,\n",
              "  0.6344881653785706,\n",
              "  0.6016101837158203,\n",
              "  0.6169993877410889,\n",
              "  0.616780698299408,\n",
              "  0.5812065005302429,\n",
              "  0.5774206519126892,\n",
              "  0.5765723586082458,\n",
              "  0.6027144193649292,\n",
              "  0.5997572541236877,\n",
              "  0.5960646271705627,\n",
              "  0.6186773777008057,\n",
              "  0.5923192501068115,\n",
              "  0.6339334845542908,\n",
              "  0.5949406623840332,\n",
              "  0.5900605320930481,\n",
              "  0.590332567691803,\n",
              "  0.5852601528167725,\n",
              "  0.6047949194908142,\n",
              "  0.5788006782531738,\n",
              "  0.6071231961250305,\n",
              "  0.6097899079322815,\n",
              "  0.6026816964149475,\n",
              "  0.6180494427680969,\n",
              "  0.596978485584259,\n",
              "  0.6005023717880249,\n",
              "  0.5951021313667297,\n",
              "  0.6046690344810486,\n",
              "  0.6117083430290222,\n",
              "  0.5931240916252136,\n",
              "  0.5859255194664001,\n",
              "  0.5880651473999023,\n",
              "  0.5708035826683044,\n",
              "  0.6040716767311096,\n",
              "  0.5906710028648376,\n",
              "  0.5824306607246399,\n",
              "  0.5987321734428406,\n",
              "  0.6171531677246094,\n",
              "  0.6098858714103699,\n",
              "  0.5828028917312622,\n",
              "  0.6209412813186646,\n",
              "  0.5925166606903076,\n",
              "  0.593416154384613,\n",
              "  0.5971166491508484,\n",
              "  0.6073176264762878,\n",
              "  0.5868866443634033,\n",
              "  0.603202223777771,\n",
              "  0.5818581581115723,\n",
              "  0.586107075214386,\n",
              "  0.6088886857032776,\n",
              "  0.5851704478263855,\n",
              "  0.606982409954071,\n",
              "  0.6046504378318787,\n",
              "  0.5736365914344788,\n",
              "  0.6047857403755188,\n",
              "  0.5757593512535095,\n",
              "  0.6028895974159241,\n",
              "  0.6002292037010193,\n",
              "  0.5915417075157166,\n",
              "  0.6221840381622314,\n",
              "  0.5901803374290466,\n",
              "  0.6108972430229187,\n",
              "  0.6067206263542175,\n",
              "  0.6082704067230225,\n",
              "  0.596017062664032,\n",
              "  0.5908558964729309,\n",
              "  0.5878474712371826,\n",
              "  0.5965167880058289,\n",
              "  0.6038346886634827,\n",
              "  0.6127442717552185,\n",
              "  0.6213727593421936,\n",
              "  0.5947192311286926,\n",
              "  0.5978652238845825,\n",
              "  0.6026636362075806,\n",
              "  0.5855027437210083,\n",
              "  0.6024246215820312,\n",
              "  0.6004331111907959,\n",
              "  0.5937370657920837,\n",
              "  0.5925662517547607,\n",
              "  0.6080176830291748,\n",
              "  0.6068903803825378,\n",
              "  0.6003645658493042,\n",
              "  0.5962163805961609,\n",
              "  0.6040104031562805,\n",
              "  0.5769334435462952,\n",
              "  0.5993751883506775,\n",
              "  0.6025968194007874,\n",
              "  0.584814727306366,\n",
              "  0.6143181324005127,\n",
              "  0.5831174254417419,\n",
              "  0.5994630455970764,\n",
              "  0.5996150970458984,\n",
              "  0.5944069027900696,\n",
              "  0.5785251259803772,\n",
              "  0.5753639936447144,\n",
              "  0.6105883121490479,\n",
              "  0.5989786982536316,\n",
              "  0.5737234950065613,\n",
              "  0.594692051410675,\n",
              "  0.5755710005760193,\n",
              "  0.5857824683189392,\n",
              "  0.5836295485496521,\n",
              "  0.5902404189109802,\n",
              "  0.5855381488800049,\n",
              "  0.5994582176208496,\n",
              "  0.6056446433067322,\n",
              "  0.6130096912384033,\n",
              "  0.593403160572052,\n",
              "  0.5751599669456482,\n",
              "  0.5968475937843323,\n",
              "  0.5807227492332458,\n",
              "  0.5942147970199585,\n",
              "  0.5891088843345642,\n",
              "  0.57774418592453,\n",
              "  0.5691257119178772,\n",
              "  0.5887892246246338,\n",
              "  0.5976000428199768,\n",
              "  0.6079533100128174,\n",
              "  0.6064101457595825,\n",
              "  0.5830982327461243,\n",
              "  0.5845960974693298,\n",
              "  0.5814260840415955,\n",
              "  0.5959349274635315,\n",
              "  0.6428664326667786,\n",
              "  0.6120228171348572,\n",
              "  0.6019009947776794,\n",
              "  0.5760278105735779,\n",
              "  0.595029890537262,\n",
              "  0.6130203008651733,\n",
              "  0.6159533858299255,\n",
              "  0.6006088852882385,\n",
              "  0.5896046161651611,\n",
              "  0.5989305377006531,\n",
              "  0.6249311566352844,\n",
              "  0.6095272302627563,\n",
              "  0.6163319945335388,\n",
              "  0.5898844599723816,\n",
              "  0.6037749648094177,\n",
              "  0.6088701486587524,\n",
              "  0.646388053894043,\n",
              "  0.5797591805458069,\n",
              "  0.6288672089576721,\n",
              "  0.6039783358573914,\n",
              "  0.643254816532135,\n",
              "  0.5888612270355225,\n",
              "  0.5746234059333801,\n",
              "  0.6190189123153687,\n",
              "  0.5997922420501709,\n",
              "  0.621328592300415,\n",
              "  0.6122789978981018,\n",
              "  0.6042308807373047,\n",
              "  0.640592634677887,\n",
              "  0.6276788115501404,\n",
              "  0.6264323592185974,\n",
              "  0.6226934194564819,\n",
              "  0.6193169951438904,\n",
              "  0.6261953711509705,\n",
              "  0.6086865067481995,\n",
              "  0.6037240624427795,\n",
              "  0.6214611530303955,\n",
              "  0.6097986698150635,\n",
              "  0.6328017711639404,\n",
              "  0.6087997555732727,\n",
              "  0.6170961856842041,\n",
              "  0.6170428991317749,\n",
              "  0.6083842515945435,\n",
              "  0.6017552018165588,\n",
              "  0.6067503690719604,\n",
              "  0.6008097529411316,\n",
              "  0.6165032982826233,\n",
              "  0.6140890717506409,\n",
              "  0.5960600972175598,\n",
              "  0.6044190526008606,\n",
              "  0.5952155590057373,\n",
              "  0.5892089009284973,\n",
              "  0.6135430335998535,\n",
              "  0.5863781571388245,\n",
              "  0.5985188484191895,\n",
              "  0.6057212352752686,\n",
              "  0.6053723096847534,\n",
              "  0.6104055047035217,\n",
              "  0.5871322751045227,\n",
              "  0.6047926545143127,\n",
              "  0.6043140888214111,\n",
              "  0.5817617774009705,\n",
              "  0.6003467440605164,\n",
              "  0.6139435768127441,\n",
              "  0.6315854787826538,\n",
              "  0.6189059615135193,\n",
              "  0.6149325966835022,\n",
              "  0.6177429556846619,\n",
              "  0.6105446815490723,\n",
              "  0.6322628855705261,\n",
              "  0.6048046946525574,\n",
              "  0.6029171943664551,\n",
              "  0.5897778868675232,\n",
              "  0.6189150214195251,\n",
              "  0.6007384657859802,\n",
              "  0.61416095495224,\n",
              "  0.603590190410614,\n",
              "  0.6248781085014343,\n",
              "  0.6322119832038879,\n",
              "  0.6136845946311951,\n",
              "  0.6012123227119446,\n",
              "  0.6127716898918152,\n",
              "  0.6265565752983093,\n",
              "  0.6205211281776428,\n",
              "  0.6206516623497009,\n",
              "  0.611128032207489,\n",
              "  0.6090156435966492,\n",
              "  0.618264377117157,\n",
              "  0.6115105748176575,\n",
              "  0.5922556519508362,\n",
              "  0.6196132302284241,\n",
              "  0.6119105815887451,\n",
              "  0.6024302840232849,\n",
              "  0.6207862496376038,\n",
              "  0.606426477432251,\n",
              "  0.5813382267951965,\n",
              "  0.5943333506584167,\n",
              "  0.5912936329841614,\n",
              "  0.6150159239768982,\n",
              "  0.6016348004341125,\n",
              "  0.6484591364860535,\n",
              "  0.6059961318969727,\n",
              "  0.6172335743904114,\n",
              "  0.586571991443634,\n",
              "  0.6231937408447266,\n",
              "  0.6048415303230286,\n",
              "  0.5913755893707275,\n",
              "  0.6274314522743225,\n",
              "  0.578635573387146,\n",
              "  0.6095462441444397,\n",
              "  0.6178699135780334,\n",
              "  0.6059162616729736,\n",
              "  0.6175685524940491,\n",
              "  0.5927522778511047,\n",
              "  0.588023841381073,\n",
              "  0.609324038028717,\n",
              "  0.6112791895866394,\n",
              "  0.5868450999259949,\n",
              "  0.6348095536231995,\n",
              "  0.5867671370506287,\n",
              "  0.6203892827033997,\n",
              "  0.6035873293876648,\n",
              "  0.6024184823036194,\n",
              "  0.621091365814209,\n",
              "  0.6079075336456299,\n",
              "  0.6175025105476379,\n",
              "  0.6101720929145813,\n",
              "  0.6123304963111877,\n",
              "  0.6024177670478821,\n",
              "  0.6206907629966736,\n",
              "  0.5980700850486755],\n",
              " 'val_acc': [0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5208333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5208333134651184,\n",
              "  0.5,\n",
              "  0.5520833134651184,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.5104166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.46875,\n",
              "  0.5625,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.4270833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.5625,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.4583333432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.46875,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.5208333134651184,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5104166865348816,\n",
              "  0.46875,\n",
              "  0.4791666567325592,\n",
              "  0.4791666567325592,\n",
              "  0.4895833432674408,\n",
              "  0.5,\n",
              "  0.4895833432674408,\n",
              "  0.4791666567325592,\n",
              "  0.5104166865348816,\n",
              "  0.46875,\n",
              "  0.4895833432674408,\n",
              "  0.4583333432674408,\n",
              "  0.53125,\n",
              "  0.4479166567325592,\n",
              "  0.4895833432674408,\n",
              "  0.4895833432674408,\n",
              "  0.5104166865348816,\n",
              "  0.5208333134651184,\n",
              "  0.4895833432674408,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.4791666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5625,\n",
              "  0.4791666567325592,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.4895833432674408,\n",
              "  0.46875,\n",
              "  0.5104166865348816,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.59375,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.5625,\n",
              "  0.59375,\n",
              "  0.5208333134651184,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.53125,\n",
              "  0.53125,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.59375,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5208333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.53125,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5520833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5520833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.53125,\n",
              "  0.59375,\n",
              "  0.5416666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.5416666865348816,\n",
              "  0.53125,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5520833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.53125,\n",
              "  0.6041666865348816,\n",
              "  0.5625,\n",
              "  0.5416666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.5729166865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.5625,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.5520833134651184,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.5729166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.59375,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.5833333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.59375,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.5833333134651184,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.5729166865348816,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.5833333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.65625]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "8ec9877b-515d-43bd-80cb-9d2017887533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_datanew.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_datanew.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "e6d9e6f4-585a-43fe-a24d-dd83f8250747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c2027cf-9e51-4e5c-91ba-4d7cf3084508\", \"2Class_datanew.h5\", 16604304)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}