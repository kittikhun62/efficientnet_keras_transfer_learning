{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOtRQNdidqUtSAZ5jUdZ8QX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "b3a2c502-0bed-4de1-cbcd-31ed4ef015ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "0922a0e5-b953-4552-b5b9-fb880ce7b63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83d72be1-cb76-413f-ae4b-5a6919476536\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83d72be1-cb76-413f-ae4b-5a6919476536')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83d72be1-cb76-413f-ae4b-5a6919476536 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83d72be1-cb76-413f-ae4b-5a6919476536');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "fb00b981-d210-400e-e274-7745fbff252c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "ee369175-ce0e-47ec-a01a-2fc50534ecec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "49e27fb1-d5c7-4c82-82d6-b9e0207ddac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "8c0eced9-928b-4194-fe56-cd5fc026041f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "3d925055-e80b-45a2-9f46-78a9e9888a35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 19.38 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "b56cc9f1-f1fc-4867-88b9-7fc1f45d93a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "69a64966-357d-4fec-b3ba-ff35de2af969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "535ff430-7268-4aef-df93-0a4cfe4cc167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "556a7ffa-fc31-494b-c8f8-cc74373e18f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "f33a3bad-6124-4166-9986-378163f36701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=2e-1),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd2e176-1c70-4e5e-ad35-8cbbce0ec05c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-121c6c007c10>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 110s 2s/step - loss: 1514168.5000 - mae: 1007.1418 - val_loss: 505376.5938 - val_mae: 539.9007\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1456889.2500 - mae: 980.6069 - val_loss: 506327.6562 - val_mae: 538.2715\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1423011.2500 - mae: 963.9703 - val_loss: 495178.0000 - val_mae: 526.0750\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1418767.0000 - mae: 960.3832 - val_loss: 476352.2188 - val_mae: 509.6674\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 1376103.6250 - mae: 939.2373 - val_loss: 458213.5000 - val_mae: 493.6855\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1362459.6250 - mae: 932.0406 - val_loss: 466477.7812 - val_mae: 501.2784\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1319068.6250 - mae: 914.5460 - val_loss: 417299.0000 - val_mae: 456.3140\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1314850.2500 - mae: 910.6311 - val_loss: 415539.4062 - val_mae: 461.3646\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1291375.5000 - mae: 899.1074 - val_loss: 406253.8750 - val_mae: 467.4276\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1259353.5000 - mae: 881.7862 - val_loss: 388275.8438 - val_mae: 459.6146\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 1232607.6250 - mae: 868.6125 - val_loss: 377240.3438 - val_mae: 459.6146\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1214216.6250 - mae: 860.1555 - val_loss: 366519.7188 - val_mae: 459.3333\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1196777.3750 - mae: 853.2051 - val_loss: 371217.8750 - val_mae: 475.0363\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1148452.0000 - mae: 830.5190 - val_loss: 355756.8750 - val_mae: 468.0705\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1164706.8750 - mae: 837.7263 - val_loss: 359662.1562 - val_mae: 482.1231\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1112662.5000 - mae: 811.2751 - val_loss: 333741.0938 - val_mae: 465.4956\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1109996.7500 - mae: 811.0192 - val_loss: 326569.8438 - val_mae: 463.1146\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1092298.7500 - mae: 801.1519 - val_loss: 315178.7812 - val_mae: 461.3646\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1063570.1250 - mae: 787.9611 - val_loss: 311762.8438 - val_mae: 466.7241\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1028683.1250 - mae: 770.5102 - val_loss: 311272.6875 - val_mae: 473.0369\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1037024.3125 - mae: 775.2031 - val_loss: 307648.7500 - val_mae: 477.3535\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1006996.8750 - mae: 758.3714 - val_loss: 287156.6250 - val_mae: 463.6358\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 989888.3125 - mae: 750.9750 - val_loss: 293147.8750 - val_mae: 475.2558\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 980071.9375 - mae: 742.7682 - val_loss: 274250.9062 - val_mae: 461.3646\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 966486.0000 - mae: 735.6287 - val_loss: 271642.3750 - val_mae: 464.9070\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 956147.8750 - mae: 732.6595 - val_loss: 263167.3750 - val_mae: 461.0833\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 935170.0000 - mae: 721.3767 - val_loss: 255776.3125 - val_mae: 458.3265\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 916342.6250 - mae: 711.4188 - val_loss: 258698.9219 - val_mae: 467.5127\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 906117.0625 - mae: 709.5656 - val_loss: 258538.7031 - val_mae: 471.5769\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 874913.8125 - mae: 692.3651 - val_loss: 247828.5781 - val_mae: 463.3958\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 867551.6250 - mae: 692.0518 - val_loss: 243568.9531 - val_mae: 462.7500\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 865776.0625 - mae: 691.2906 - val_loss: 239797.5469 - val_mae: 463.1925\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 849572.6250 - mae: 683.2105 - val_loss: 236659.6250 - val_mae: 463.2378\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 828443.5000 - mae: 672.6437 - val_loss: 225129.9531 - val_mae: 454.2990\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 815961.0625 - mae: 667.7579 - val_loss: 225194.2500 - val_mae: 457.3021\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 800505.9375 - mae: 664.9661 - val_loss: 224210.4531 - val_mae: 458.1538\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 792873.8750 - mae: 659.1449 - val_loss: 225681.3594 - val_mae: 461.6458\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 778869.2500 - mae: 660.1914 - val_loss: 221731.3906 - val_mae: 459.6146\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 772258.2500 - mae: 656.8176 - val_loss: 219780.9531 - val_mae: 459.1038\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 764886.6250 - mae: 654.0199 - val_loss: 216392.8750 - val_mae: 457.0208\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 743778.1250 - mae: 644.7374 - val_loss: 217716.7031 - val_mae: 459.3333\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 746680.3750 - mae: 651.0627 - val_loss: 218777.3281 - val_mae: 461.2192\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 719345.2500 - mae: 639.4258 - val_loss: 216022.8906 - val_mae: 458.9688\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 727682.0625 - mae: 648.7530 - val_loss: 214468.5625 - val_mae: 457.8616\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 708564.6250 - mae: 639.7833 - val_loss: 216789.0469 - val_mae: 460.3118\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 696917.6875 - mae: 636.4517 - val_loss: 214758.0156 - val_mae: 458.0834\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 688612.8750 - mae: 636.1147 - val_loss: 216619.9219 - val_mae: 459.9077\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 677672.4375 - mae: 633.2697 - val_loss: 217093.6406 - val_mae: 459.9847\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 685484.5625 - mae: 641.5410 - val_loss: 215638.8594 - val_mae: 457.6489\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 668596.4375 - mae: 632.8033 - val_loss: 216024.8594 - val_mae: 457.2576\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 668770.8125 - mae: 638.1011 - val_loss: 216722.2969 - val_mae: 457.0208\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 638188.5625 - mae: 621.6059 - val_loss: 221386.7500 - val_mae: 461.3646\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 640000.8750 - mae: 625.3432 - val_loss: 220864.8750 - val_mae: 459.3182\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 647285.2500 - mae: 633.6044 - val_loss: 219379.3906 - val_mae: 456.0602\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 625032.4375 - mae: 621.3972 - val_loss: 226690.4844 - val_mae: 462.6183\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 628293.0000 - mae: 624.2100 - val_loss: 224788.1406 - val_mae: 458.7688\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 624572.5000 - mae: 624.7975 - val_loss: 227129.3750 - val_mae: 459.3333\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 613310.0000 - mae: 620.1594 - val_loss: 227477.9219 - val_mae: 457.8452\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 607055.1875 - mae: 620.7290 - val_loss: 232382.2969 - val_mae: 460.7131\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 603042.5000 - mae: 618.4454 - val_loss: 234775.7500 - val_mae: 461.6458\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 598925.3125 - mae: 622.7308 - val_loss: 231972.7969 - val_mae: 455.2960\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 593132.1250 - mae: 618.3306 - val_loss: 231945.0781 - val_mae: 452.9480\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 579199.2500 - mae: 614.2065 - val_loss: 236235.5781 - val_mae: 455.5075\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 584957.1250 - mae: 616.7177 - val_loss: 238032.1094 - val_mae: 454.9043\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 587660.1250 - mae: 622.0159 - val_loss: 244956.5469 - val_mae: 459.0521\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 574856.9375 - mae: 616.3300 - val_loss: 243209.2500 - val_mae: 454.5187\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 568002.3125 - mae: 613.5740 - val_loss: 250319.6719 - val_mae: 459.0521\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 565798.8125 - mae: 614.2130 - val_loss: 243843.2031 - val_mae: 448.7245\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 553425.8125 - mae: 608.4218 - val_loss: 253816.1719 - val_mae: 456.6022\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 559918.8750 - mae: 616.7681 - val_loss: 256440.5781 - val_mae: 456.4619\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 556565.6875 - mae: 616.3830 - val_loss: 254236.2031 - val_mae: 450.4184\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 552776.0000 - mae: 616.5801 - val_loss: 265540.8750 - val_mae: 459.3333\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 541490.4375 - mae: 613.6533 - val_loss: 278872.3750 - val_mae: 472.8873\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 548371.8750 - mae: 617.3886 - val_loss: 276196.1875 - val_mae: 465.0930\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 544682.3125 - mae: 616.9020 - val_loss: 274503.0000 - val_mae: 459.3333\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 543801.3750 - mae: 617.4371 - val_loss: 282832.4062 - val_mae: 465.6333\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 538274.1250 - mae: 615.8099 - val_loss: 276471.0938 - val_mae: 455.2193\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 536495.3750 - mae: 617.4185 - val_loss: 290906.4062 - val_mae: 469.5891\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 541309.7500 - mae: 623.7850 - val_loss: 282328.7812 - val_mae: 454.6774\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 528533.9375 - mae: 616.0739 - val_loss: 286399.9688 - val_mae: 456.5761\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 523393.9688 - mae: 617.2033 - val_loss: 289421.7812 - val_mae: 456.4567\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 531491.9375 - mae: 622.9401 - val_loss: 291713.3438 - val_mae: 454.3088\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 526741.1875 - mae: 621.2493 - val_loss: 294934.4062 - val_mae: 454.1877\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 515682.8125 - mae: 615.9569 - val_loss: 297232.3125 - val_mae: 452.0467\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 521290.0000 - mae: 620.9916 - val_loss: 306761.4688 - val_mae: 461.0833\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 525159.0000 - mae: 624.6771 - val_loss: 297605.0000 - val_mae: 446.3622\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 523109.4375 - mae: 626.9356 - val_loss: 307104.6250 - val_mae: 451.9949\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 514298.0625 - mae: 621.5747 - val_loss: 316765.2812 - val_mae: 461.3646\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 518538.0625 - mae: 624.5204 - val_loss: 319631.3750 - val_mae: 461.3646\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 512033.4375 - mae: 624.5515 - val_loss: 329318.4688 - val_mae: 468.9173\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 511120.1250 - mae: 623.3167 - val_loss: 306597.7188 - val_mae: 439.6343\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 509933.1875 - mae: 624.4860 - val_loss: 334170.0938 - val_mae: 463.3843\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 511908.2812 - mae: 625.4070 - val_loss: 331966.2188 - val_mae: 461.3646\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 516297.8438 - mae: 630.2643 - val_loss: 321896.6562 - val_mae: 447.3938\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 508249.4062 - mae: 628.7527 - val_loss: 331082.7812 - val_mae: 455.0151\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 515227.7812 - mae: 632.1186 - val_loss: 333619.1562 - val_mae: 454.9369\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 512509.9062 - mae: 632.2613 - val_loss: 343975.8438 - val_mae: 461.3646\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 511676.4375 - mae: 631.4470 - val_loss: 339861.8750 - val_mae: 452.9993\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 507666.6250 - mae: 629.8018 - val_loss: 342190.1562 - val_mae: 452.9304\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 506773.5000 - mae: 630.5042 - val_loss: 352026.7500 - val_mae: 462.9939\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 503875.9688 - mae: 631.1904 - val_loss: 353825.0000 - val_mae: 464.1664\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 501495.1875 - mae: 630.2868 - val_loss: 341589.5312 - val_mae: 448.4929\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501666.6875 - mae: 629.2399 - val_loss: 364819.1562 - val_mae: 470.5227\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 506491.8125 - mae: 633.4014 - val_loss: 346623.1250 - val_mae: 451.8544\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 505625.7188 - mae: 632.8590 - val_loss: 363074.3750 - val_mae: 468.6529\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 502850.6250 - mae: 632.8755 - val_loss: 351287.1562 - val_mae: 456.2584\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 503580.1562 - mae: 633.6025 - val_loss: 353192.2188 - val_mae: 457.4476\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 507251.7500 - mae: 635.9666 - val_loss: 354570.8438 - val_mae: 457.0334\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 502073.0625 - mae: 632.8895 - val_loss: 372523.2500 - val_mae: 477.1755\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 504086.7500 - mae: 634.7698 - val_loss: 374819.2500 - val_mae: 476.1565\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 503651.3125 - mae: 635.0855 - val_loss: 377216.8438 - val_mae: 476.5424\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 500068.2812 - mae: 635.5477 - val_loss: 378131.2500 - val_mae: 478.3120\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501904.3438 - mae: 635.0408 - val_loss: 387715.9688 - val_mae: 487.0098\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 504186.4375 - mae: 637.5452 - val_loss: 366996.5000 - val_mae: 465.9883\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 503174.5312 - mae: 637.8388 - val_loss: 376404.2188 - val_mae: 473.5654\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 498648.7500 - mae: 634.9360 - val_loss: 377472.5938 - val_mae: 475.2531\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 497652.0625 - mae: 634.0916 - val_loss: 379856.7500 - val_mae: 476.7107\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 502562.6875 - mae: 637.7503 - val_loss: 388929.2500 - val_mae: 485.1461\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 495089.1562 - mae: 635.0775 - val_loss: 390154.4062 - val_mae: 487.5771\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 501794.3750 - mae: 637.7603 - val_loss: 391955.2188 - val_mae: 487.8065\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 498736.7812 - mae: 636.2910 - val_loss: 400424.2500 - val_mae: 497.0193\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 504022.0000 - mae: 639.8654 - val_loss: 400888.5000 - val_mae: 494.6707\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 501100.4688 - mae: 637.0277 - val_loss: 402629.9062 - val_mae: 496.4847\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 496620.3750 - mae: 636.6631 - val_loss: 379631.8750 - val_mae: 473.6578\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 501687.0312 - mae: 638.6469 - val_loss: 404661.8750 - val_mae: 498.4597\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 500334.4688 - mae: 639.1794 - val_loss: 405930.5938 - val_mae: 499.8743\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 497492.9062 - mae: 635.5880 - val_loss: 389971.7500 - val_mae: 482.2478\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500769.4375 - mae: 638.9113 - val_loss: 391666.0938 - val_mae: 483.8954\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 499644.3750 - mae: 638.6379 - val_loss: 399810.3438 - val_mae: 491.8518\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 502392.5625 - mae: 640.3132 - val_loss: 400972.6250 - val_mae: 493.1637\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 505786.5625 - mae: 642.4880 - val_loss: 409517.8438 - val_mae: 501.3556\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 493901.1562 - mae: 634.7674 - val_loss: 410458.1250 - val_mae: 500.7596\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 499707.2812 - mae: 637.7758 - val_loss: 403939.5938 - val_mae: 494.3241\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 496998.3750 - mae: 637.6227 - val_loss: 412596.6562 - val_mae: 503.1584\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 499217.2500 - mae: 639.2498 - val_loss: 396909.4062 - val_mae: 487.5277\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 496982.0938 - mae: 636.9177 - val_loss: 396214.4688 - val_mae: 486.6283\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 498951.5312 - mae: 639.8623 - val_loss: 398626.0000 - val_mae: 488.4955\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 495912.2500 - mae: 637.6584 - val_loss: 415999.7500 - val_mae: 505.9554\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 502497.3125 - mae: 640.9584 - val_loss: 390059.7188 - val_mae: 479.7743\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 500244.5000 - mae: 638.9049 - val_loss: 417473.6562 - val_mae: 505.5824\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 502004.5000 - mae: 641.4146 - val_loss: 408171.7188 - val_mae: 497.3157\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 503215.2188 - mae: 642.1957 - val_loss: 409730.1562 - val_mae: 498.2065\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 499291.6562 - mae: 639.8241 - val_loss: 419421.7188 - val_mae: 507.1391\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 503099.4375 - mae: 642.4747 - val_loss: 419811.6562 - val_mae: 507.3644\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 497411.1250 - mae: 639.2731 - val_loss: 403720.4062 - val_mae: 490.9830\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 495207.4688 - mae: 637.0779 - val_loss: 420366.5938 - val_mae: 506.9508\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 504638.5000 - mae: 643.4760 - val_loss: 394508.9688 - val_mae: 481.9707\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 501916.1562 - mae: 641.7097 - val_loss: 412028.5312 - val_mae: 499.8812\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 504516.2812 - mae: 643.9589 - val_loss: 420574.1562 - val_mae: 507.8159\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 500364.4375 - mae: 640.2526 - val_loss: 412570.6250 - val_mae: 499.1456\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 490121.3438 - mae: 636.3510 - val_loss: 421106.4688 - val_mae: 507.7792\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 496520.6250 - mae: 640.0391 - val_loss: 404964.2500 - val_mae: 492.0443\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 501036.0938 - mae: 641.1286 - val_loss: 421868.2188 - val_mae: 508.2114\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 505729.5312 - mae: 644.7575 - val_loss: 421300.3750 - val_mae: 508.5758\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 501870.6250 - mae: 642.8229 - val_loss: 404872.1562 - val_mae: 491.3285\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 498970.2188 - mae: 639.8661 - val_loss: 398439.4062 - val_mae: 484.2110\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 500996.8438 - mae: 640.7716 - val_loss: 405058.0312 - val_mae: 491.7552\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 501954.4062 - mae: 641.9913 - val_loss: 407145.9062 - val_mae: 493.2518\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 500090.6562 - mae: 641.8012 - val_loss: 405728.0000 - val_mae: 491.8391\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 496029.4062 - mae: 639.0284 - val_loss: 405902.0000 - val_mae: 491.9428\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 500216.5938 - mae: 641.1739 - val_loss: 414381.0938 - val_mae: 500.5444\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 498080.2188 - mae: 641.7094 - val_loss: 389270.5312 - val_mae: 475.5628\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 505515.1562 - mae: 644.9988 - val_loss: 389279.6250 - val_mae: 475.5680\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 492733.0312 - mae: 638.8430 - val_loss: 424057.3750 - val_mae: 509.8047\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 506554.2812 - mae: 645.9199 - val_loss: 415353.4062 - val_mae: 501.3857\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 496952.0312 - mae: 639.0121 - val_loss: 407195.0312 - val_mae: 493.2789\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 496833.5312 - mae: 638.3868 - val_loss: 397010.8750 - val_mae: 483.7466\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 499816.8125 - mae: 640.3142 - val_loss: 407123.2500 - val_mae: 492.9312\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 501429.3438 - mae: 641.4187 - val_loss: 407158.9062 - val_mae: 493.5979\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 500962.8125 - mae: 642.2356 - val_loss: 423974.1562 - val_mae: 510.0552\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 497323.2812 - mae: 639.8353 - val_loss: 406456.4688 - val_mae: 492.8712\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 502092.4688 - mae: 641.8228 - val_loss: 415048.7500 - val_mae: 501.2196\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 503707.3750 - mae: 643.7238 - val_loss: 415832.7188 - val_mae: 501.3593\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 502055.5938 - mae: 641.7663 - val_loss: 407337.3438 - val_mae: 493.3575\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 500224.8125 - mae: 640.4030 - val_loss: 424205.3438 - val_mae: 510.1826\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 494027.6562 - mae: 637.8766 - val_loss: 408302.1562 - val_mae: 493.8890\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 500792.5625 - mae: 641.4532 - val_loss: 425970.6562 - val_mae: 510.6405\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 499547.7188 - mae: 640.3781 - val_loss: 416896.5938 - val_mae: 501.9935\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 497633.1562 - mae: 639.3525 - val_loss: 399542.3438 - val_mae: 484.9049\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 502309.7812 - mae: 642.2667 - val_loss: 400600.1562 - val_mae: 485.7532\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 503763.6250 - mae: 644.2358 - val_loss: 408696.0000 - val_mae: 493.8416\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 500636.0938 - mae: 642.6565 - val_loss: 407784.8438 - val_mae: 493.0403\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 498384.0625 - mae: 641.1870 - val_loss: 424719.7500 - val_mae: 509.9226\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 499368.6875 - mae: 640.5800 - val_loss: 408708.3438 - val_mae: 493.5664\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 501012.6875 - mae: 641.1710 - val_loss: 426276.5000 - val_mae: 511.0702\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 495403.4375 - mae: 640.1680 - val_loss: 424038.6562 - val_mae: 509.8041\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 501341.8438 - mae: 641.6013 - val_loss: 425932.1250 - val_mae: 510.6376\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 500993.9375 - mae: 643.6483 - val_loss: 417786.4688 - val_mae: 502.9958\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 505334.7188 - mae: 644.8893 - val_loss: 418556.6250 - val_mae: 503.5179\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 505413.1250 - mae: 645.4199 - val_loss: 409498.2500 - val_mae: 494.8840\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 502093.5312 - mae: 642.2118 - val_loss: 426149.5938 - val_mae: 510.9980\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 500637.6875 - mae: 641.5846 - val_loss: 425131.7500 - val_mae: 510.1658\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 499811.1250 - mae: 640.8092 - val_loss: 416096.1562 - val_mae: 502.0906\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 505154.4688 - mae: 645.1334 - val_loss: 426038.4062 - val_mae: 510.6805\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 501996.0000 - mae: 642.3660 - val_loss: 408729.1250 - val_mae: 493.8707\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 503719.8750 - mae: 643.0190 - val_loss: 416339.7500 - val_mae: 501.9448\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 501719.1562 - mae: 642.2996 - val_loss: 425845.3750 - val_mae: 511.0851\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 499214.2812 - mae: 640.6026 - val_loss: 417335.4062 - val_mae: 502.2331\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 500669.9062 - mae: 641.7998 - val_loss: 408424.5000 - val_mae: 494.0340\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 496082.9688 - mae: 639.8144 - val_loss: 398483.5312 - val_mae: 484.2871\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 501616.3438 - mae: 642.0480 - val_loss: 407155.2812 - val_mae: 492.9712\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 496441.6875 - mae: 639.6047 - val_loss: 431354.1562 - val_mae: 517.4155\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 505495.7500 - mae: 645.1779 - val_loss: 414823.8750 - val_mae: 501.7362\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 504063.0625 - mae: 644.4547 - val_loss: 414367.7500 - val_mae: 500.5366\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 503664.1250 - mae: 642.6403 - val_loss: 398454.8438 - val_mae: 484.2368\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 502000.7812 - mae: 642.1036 - val_loss: 397655.5938 - val_mae: 483.7857\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 505369.3438 - mae: 643.9938 - val_loss: 414106.5938 - val_mae: 500.0496\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 499004.0312 - mae: 642.4274 - val_loss: 406388.5938 - val_mae: 492.8337\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 502591.7188 - mae: 643.4614 - val_loss: 414535.1562 - val_mae: 500.9304\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 505816.9062 - mae: 645.2925 - val_loss: 415922.8438 - val_mae: 501.7058\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 502983.1562 - mae: 642.7413 - val_loss: 425491.8750 - val_mae: 510.8910\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 503544.0938 - mae: 642.9774 - val_loss: 408394.0312 - val_mae: 493.6692\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 502159.2812 - mae: 641.8373 - val_loss: 416488.7812 - val_mae: 502.0284\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 502039.9688 - mae: 642.5760 - val_loss: 417471.8438 - val_mae: 502.3123\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501141.2812 - mae: 642.3251 - val_loss: 433862.6562 - val_mae: 519.4016\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 499728.2188 - mae: 641.0784 - val_loss: 408102.5000 - val_mae: 493.5130\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 499071.7812 - mae: 639.9670 - val_loss: 399576.9062 - val_mae: 485.1796\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 499347.5312 - mae: 640.2180 - val_loss: 424634.8438 - val_mae: 510.4195\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 493267.8438 - mae: 638.4981 - val_loss: 398871.0938 - val_mae: 484.7829\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 495136.8125 - mae: 640.6913 - val_loss: 415397.1250 - val_mae: 501.1212\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 502903.5312 - mae: 642.5192 - val_loss: 407392.5312 - val_mae: 493.1071\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 505524.6250 - mae: 645.5031 - val_loss: 397549.8750 - val_mae: 484.0445\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 501088.5625 - mae: 641.5734 - val_loss: 432873.5000 - val_mae: 519.2066\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 502012.7500 - mae: 642.0801 - val_loss: 415746.1562 - val_mae: 501.6117\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 500253.2500 - mae: 640.8815 - val_loss: 416698.0312 - val_mae: 502.1457\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 502681.5625 - mae: 642.2247 - val_loss: 416410.5000 - val_mae: 501.9845\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 498681.6250 - mae: 640.1042 - val_loss: 408383.7188 - val_mae: 493.6736\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 501479.1875 - mae: 641.5718 - val_loss: 408200.2500 - val_mae: 493.5688\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 500404.4688 - mae: 642.0613 - val_loss: 416753.7188 - val_mae: 502.1816\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 503300.5625 - mae: 643.2680 - val_loss: 416489.7188 - val_mae: 501.7572\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 502218.7812 - mae: 642.9381 - val_loss: 399094.9062 - val_mae: 484.9088\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 502248.2500 - mae: 642.2152 - val_loss: 416271.9062 - val_mae: 502.1798\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 498351.6562 - mae: 639.2678 - val_loss: 415413.5000 - val_mae: 501.7160\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498801.7500 - mae: 640.8516 - val_loss: 415064.4062 - val_mae: 500.6287\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 500684.6562 - mae: 641.0623 - val_loss: 415759.6562 - val_mae: 501.6193\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 501997.5312 - mae: 643.3037 - val_loss: 407653.8438 - val_mae: 492.9839\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 496913.5312 - mae: 639.9758 - val_loss: 424996.9688 - val_mae: 510.6189\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 501736.3125 - mae: 641.7324 - val_loss: 406991.3750 - val_mae: 493.1666\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 500092.5000 - mae: 640.9816 - val_loss: 407046.8438 - val_mae: 492.8983\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 500631.5938 - mae: 641.5363 - val_loss: 398959.6250 - val_mae: 484.8271\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 496716.6562 - mae: 638.4023 - val_loss: 424194.0000 - val_mae: 510.1766\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 502624.1562 - mae: 642.2307 - val_loss: 415996.4688 - val_mae: 501.7522\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 499155.7812 - mae: 641.3962 - val_loss: 399214.9062 - val_mae: 484.9708\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 500732.2188 - mae: 641.1904 - val_loss: 407465.2500 - val_mae: 492.8506\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 498824.4062 - mae: 639.5029 - val_loss: 415262.5312 - val_mae: 501.6450\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 504347.5000 - mae: 644.6766 - val_loss: 424029.6562 - val_mae: 509.4942\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 494424.5625 - mae: 638.7500 - val_loss: 414478.5000 - val_mae: 500.6013\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 502097.2188 - mae: 642.0461 - val_loss: 398225.2500 - val_mae: 484.1361\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501739.5312 - mae: 642.1422 - val_loss: 397990.8438 - val_mae: 484.0160\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501060.2188 - mae: 641.7406 - val_loss: 390673.0938 - val_mae: 476.3573\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 501157.5625 - mae: 640.8053 - val_loss: 417281.7188 - val_mae: 502.2170\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 500513.5000 - mae: 641.5203 - val_loss: 409574.7188 - val_mae: 494.3327\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 495772.0938 - mae: 638.6669 - val_loss: 408668.7500 - val_mae: 493.8260\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 505058.0938 - mae: 644.5699 - val_loss: 425021.9062 - val_mae: 510.9011\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 499049.7188 - mae: 640.3315 - val_loss: 416908.7500 - val_mae: 501.7397\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 501511.1562 - mae: 641.3347 - val_loss: 406706.7812 - val_mae: 492.4216\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 497394.8125 - mae: 639.2733 - val_loss: 406995.2188 - val_mae: 492.5714\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501180.7500 - mae: 641.1181 - val_loss: 414854.2500 - val_mae: 500.8046\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 500639.4062 - mae: 640.8580 - val_loss: 417148.7188 - val_mae: 502.3934\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 502167.3125 - mae: 642.6476 - val_loss: 416259.2500 - val_mae: 502.2329\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 496293.3750 - mae: 638.4919 - val_loss: 402107.4688 - val_mae: 486.3636\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 504115.4688 - mae: 643.8282 - val_loss: 417508.8438 - val_mae: 502.3488\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 501270.5000 - mae: 641.2130 - val_loss: 416443.5938 - val_mae: 502.0030\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 503422.4062 - mae: 643.2991 - val_loss: 425820.2500 - val_mae: 511.3234\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 505707.5312 - mae: 645.0921 - val_loss: 407307.1250 - val_mae: 493.0582\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 503791.7812 - mae: 643.9896 - val_loss: 407655.5938 - val_mae: 493.5330\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 498308.1875 - mae: 640.5764 - val_loss: 424226.1562 - val_mae: 509.6105\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 498540.4688 - mae: 640.1615 - val_loss: 406698.5938 - val_mae: 492.7096\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 493682.3750 - mae: 637.5472 - val_loss: 407512.0938 - val_mae: 493.1648\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 500626.5938 - mae: 641.0716 - val_loss: 416713.2188 - val_mae: 502.1494\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 499857.9688 - mae: 640.4060 - val_loss: 416440.0938 - val_mae: 502.2764\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 493975.5938 - mae: 638.7010 - val_loss: 408196.6562 - val_mae: 493.8310\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 490553.4062 - mae: 637.1199 - val_loss: 413706.5000 - val_mae: 500.1337\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 505140.4688 - mae: 644.3163 - val_loss: 406202.7812 - val_mae: 492.4139\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 504221.1875 - mae: 644.4218 - val_loss: 424120.5938 - val_mae: 509.8408\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 500068.3438 - mae: 640.5212 - val_loss: 414775.3750 - val_mae: 501.0657\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499817.9375 - mae: 640.0436 - val_loss: 398413.3438 - val_mae: 484.2461\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 496746.3438 - mae: 640.5480 - val_loss: 398127.7500 - val_mae: 484.0792\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 503872.0625 - mae: 643.9303 - val_loss: 415285.2188 - val_mae: 501.0559\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 498404.0000 - mae: 639.2438 - val_loss: 405512.6250 - val_mae: 492.0399\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 503435.3750 - mae: 643.3508 - val_loss: 406005.5000 - val_mae: 492.3118\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 505818.1250 - mae: 645.7398 - val_loss: 407614.8750 - val_mae: 493.2236\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 499126.9688 - mae: 642.2108 - val_loss: 399245.0938 - val_mae: 484.9823\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 501396.7812 - mae: 641.6460 - val_loss: 415867.7812 - val_mae: 501.6799\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 498629.5938 - mae: 640.2013 - val_loss: 407640.4688 - val_mae: 493.2489\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 492499.4062 - mae: 635.9050 - val_loss: 423636.6562 - val_mae: 509.5742\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 497269.7812 - mae: 638.5178 - val_loss: 415768.6562 - val_mae: 501.6243\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 502813.8125 - mae: 643.4343 - val_loss: 407785.7500 - val_mae: 493.3214\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 503604.6250 - mae: 642.9698 - val_loss: 408755.7500 - val_mae: 493.8756\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 500155.6562 - mae: 640.5730 - val_loss: 408605.3438 - val_mae: 494.0558\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 500276.9062 - mae: 641.7921 - val_loss: 408430.5312 - val_mae: 493.6900\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 492457.6875 - mae: 637.8622 - val_loss: 405854.5312 - val_mae: 492.5379\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 498903.5312 - mae: 639.7891 - val_loss: 399466.3438 - val_mae: 485.1122\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 508452.4062 - mae: 648.0856 - val_loss: 424048.7188 - val_mae: 509.2139\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 506545.5938 - mae: 645.8137 - val_loss: 416313.7812 - val_mae: 502.0071\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 503077.9375 - mae: 643.3583 - val_loss: 408214.5312 - val_mae: 493.5770\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 499310.5000 - mae: 640.6974 - val_loss: 408917.2188 - val_mae: 493.7108\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 501407.7188 - mae: 641.7488 - val_loss: 407771.2188 - val_mae: 493.3236\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 505496.6875 - mae: 644.2422 - val_loss: 424608.7812 - val_mae: 510.4052\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 500237.9688 - mae: 641.2426 - val_loss: 397850.7812 - val_mae: 484.2142\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 492272.4375 - mae: 637.4487 - val_loss: 414501.5938 - val_mae: 500.5986\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 500857.0312 - mae: 641.1896 - val_loss: 414009.4062 - val_mae: 500.2946\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 501584.2812 - mae: 641.1013 - val_loss: 405412.5938 - val_mae: 492.2930\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 496719.6562 - mae: 639.1874 - val_loss: 411555.4062 - val_mae: 498.8711\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 500819.5625 - mae: 642.9850 - val_loss: 420904.8438 - val_mae: 508.6968\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 500600.8125 - mae: 640.9558 - val_loss: 412426.5312 - val_mae: 499.7386\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 502133.8438 - mae: 641.7498 - val_loss: 402951.2812 - val_mae: 490.9271\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 506322.2500 - mae: 646.2064 - val_loss: 396478.0000 - val_mae: 483.4324\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 502359.6562 - mae: 641.6281 - val_loss: 421600.2188 - val_mae: 508.0518\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 502127.7812 - mae: 641.5034 - val_loss: 404997.8750 - val_mae: 492.0630\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 505152.5625 - mae: 643.6409 - val_loss: 414280.8750 - val_mae: 500.7814\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 504389.3750 - mae: 643.1325 - val_loss: 396109.9688 - val_mae: 483.5696\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 492818.3125 - mae: 636.5739 - val_loss: 414995.5000 - val_mae: 500.8710\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 491819.2188 - mae: 636.9094 - val_loss: 397775.5938 - val_mae: 484.1658\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 498210.4688 - mae: 641.1522 - val_loss: 406472.3438 - val_mae: 492.5908\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 501851.7812 - mae: 642.0034 - val_loss: 406776.4062 - val_mae: 492.7432\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 499571.3438 - mae: 640.0912 - val_loss: 422689.0000 - val_mae: 509.3457\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 499909.5938 - mae: 641.0689 - val_loss: 421393.5312 - val_mae: 507.9501\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 500866.7500 - mae: 640.7610 - val_loss: 414859.6562 - val_mae: 501.1131\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 505000.2500 - mae: 644.7593 - val_loss: 405912.7812 - val_mae: 492.9095\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 500319.6250 - mae: 640.2914 - val_loss: 396864.5938 - val_mae: 483.3042\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 501399.1562 - mae: 641.7869 - val_loss: 397164.7500 - val_mae: 483.8209\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 496128.2188 - mae: 639.1967 - val_loss: 414554.3438 - val_mae: 500.6456\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 502135.4375 - mae: 642.3425 - val_loss: 415283.5312 - val_mae: 501.0391\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 503053.9688 - mae: 643.0040 - val_loss: 407201.5938 - val_mae: 492.9869\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 493653.5000 - mae: 636.5895 - val_loss: 407420.5000 - val_mae: 492.8026\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 499841.0312 - mae: 640.7149 - val_loss: 398797.7188 - val_mae: 484.7417\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 489576.0312 - mae: 635.8462 - val_loss: 423939.7188 - val_mae: 510.0362\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 494745.4062 - mae: 637.9414 - val_loss: 415111.2812 - val_mae: 500.9705\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 504730.4375 - mae: 644.2529 - val_loss: 406796.8750 - val_mae: 492.7659\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 500262.5000 - mae: 640.2422 - val_loss: 406046.5312 - val_mae: 492.3353\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 500850.0938 - mae: 641.3025 - val_loss: 407136.7812 - val_mae: 492.9607\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 504028.6875 - mae: 644.1922 - val_loss: 425190.7500 - val_mae: 510.4616\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501142.8125 - mae: 641.4022 - val_loss: 409261.8438 - val_mae: 493.9146\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 497943.8125 - mae: 640.0466 - val_loss: 417480.1250 - val_mae: 502.3321\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501462.2188 - mae: 642.5585 - val_loss: 391053.2500 - val_mae: 476.5865\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 501766.0000 - mae: 642.7770 - val_loss: 414845.1562 - val_mae: 501.1050\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 499122.5312 - mae: 640.7056 - val_loss: 406382.5938 - val_mae: 492.8307\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 500092.4688 - mae: 640.1937 - val_loss: 414250.5000 - val_mae: 500.4682\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 501019.7188 - mae: 641.3033 - val_loss: 407366.5000 - val_mae: 492.7705\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 497476.1250 - mae: 639.4099 - val_loss: 415182.2188 - val_mae: 501.0118\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 506170.9375 - mae: 645.1716 - val_loss: 407935.7812 - val_mae: 493.1298\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 493437.7500 - mae: 638.2073 - val_loss: 407766.0312 - val_mae: 493.2996\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 502678.4375 - mae: 642.7029 - val_loss: 415911.0938 - val_mae: 501.4206\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 502292.5000 - mae: 642.6694 - val_loss: 425323.4688 - val_mae: 510.5275\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 500925.5312 - mae: 641.3504 - val_loss: 407555.6250 - val_mae: 493.1897\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 502291.7812 - mae: 642.1757 - val_loss: 424088.7500 - val_mae: 510.1186\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 499751.0312 - mae: 640.1056 - val_loss: 407536.3438 - val_mae: 493.1894\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 500509.5312 - mae: 640.9100 - val_loss: 400359.7188 - val_mae: 485.6136\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 502511.5000 - mae: 644.2805 - val_loss: 408283.0938 - val_mae: 493.6058\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 501592.0312 - mae: 642.7864 - val_loss: 400062.8750 - val_mae: 485.7899\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 504087.6875 - mae: 643.7633 - val_loss: 423951.6562 - val_mae: 510.0428\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 501115.3125 - mae: 641.1068 - val_loss: 408681.7500 - val_mae: 494.0971\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 498221.6875 - mae: 639.3564 - val_loss: 408437.6562 - val_mae: 493.7044\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 500055.5938 - mae: 641.3386 - val_loss: 436361.7500 - val_mae: 520.3154\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 495048.5625 - mae: 639.9861 - val_loss: 417998.7500 - val_mae: 502.6327\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 502096.4375 - mae: 642.2834 - val_loss: 409299.8438 - val_mae: 494.1859\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 498651.2812 - mae: 639.9991 - val_loss: 399981.2188 - val_mae: 485.4013\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 499697.5312 - mae: 641.3831 - val_loss: 417908.8750 - val_mae: 502.5806\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 499271.5312 - mae: 640.3129 - val_loss: 424722.4688 - val_mae: 510.4673\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 501501.6250 - mae: 642.1495 - val_loss: 400203.5000 - val_mae: 485.5310\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 500328.5312 - mae: 642.1187 - val_loss: 407728.4688 - val_mae: 493.3098\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 502173.5312 - mae: 642.5363 - val_loss: 409290.2812 - val_mae: 494.4320\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 502347.1562 - mae: 642.8546 - val_loss: 425745.5312 - val_mae: 510.5277\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502029.3125 - mae: 641.4742 - val_loss: 399089.0312 - val_mae: 484.9164\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 499493.1562 - mae: 641.6521 - val_loss: 424287.1562 - val_mae: 510.2277\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 501931.5000 - mae: 642.4606 - val_loss: 407837.7812 - val_mae: 493.3617\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 496222.0625 - mae: 637.8276 - val_loss: 398560.6250 - val_mae: 484.3322\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 503621.6562 - mae: 643.8352 - val_loss: 433160.1562 - val_mae: 518.4651\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 504269.0938 - mae: 643.7585 - val_loss: 408757.5000 - val_mae: 493.8665\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 496032.0000 - mae: 639.2551 - val_loss: 396991.7188 - val_mae: 483.4311\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 496631.1562 - mae: 639.8538 - val_loss: 406253.0938 - val_mae: 492.1518\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 503653.4375 - mae: 643.6057 - val_loss: 406804.7188 - val_mae: 492.7813\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 497004.7500 - mae: 639.2181 - val_loss: 414996.5000 - val_mae: 500.6141\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 496806.5000 - mae: 640.2746 - val_loss: 406882.6250 - val_mae: 492.8041\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 500074.4375 - mae: 640.1826 - val_loss: 414568.2812 - val_mae: 500.9491\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 502029.6875 - mae: 642.1623 - val_loss: 415162.5938 - val_mae: 501.2836\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 499440.8750 - mae: 640.8271 - val_loss: 407575.3750 - val_mae: 492.8946\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 498419.7500 - mae: 640.9017 - val_loss: 415479.6562 - val_mae: 501.4619\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 501299.3438 - mae: 641.7464 - val_loss: 405727.5938 - val_mae: 492.4681\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 500277.8438 - mae: 640.3636 - val_loss: 406669.0938 - val_mae: 493.0304\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 500797.4062 - mae: 641.1344 - val_loss: 417137.6562 - val_mae: 502.1183\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 500234.4375 - mae: 641.3942 - val_loss: 391125.2500 - val_mae: 476.6054\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 501972.5625 - mae: 642.6514 - val_loss: 416383.4688 - val_mae: 501.6954\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 499978.8750 - mae: 640.4501 - val_loss: 408625.9062 - val_mae: 493.8016\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 493888.3438 - mae: 638.0427 - val_loss: 408176.2812 - val_mae: 493.5655\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 497155.6875 - mae: 638.7819 - val_loss: 408451.6250 - val_mae: 493.9712\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 505468.4375 - mae: 645.0337 - val_loss: 416795.7500 - val_mae: 502.2004\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 503035.8750 - mae: 642.8893 - val_loss: 416889.4688 - val_mae: 501.9894\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 502291.3125 - mae: 642.2088 - val_loss: 408786.7500 - val_mae: 493.9036\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 499909.8438 - mae: 641.4229 - val_loss: 408677.8750 - val_mae: 493.8312\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 504703.0000 - mae: 644.4905 - val_loss: 392696.2188 - val_mae: 477.5148\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 501623.0312 - mae: 642.9014 - val_loss: 400533.5312 - val_mae: 485.4815\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 493153.9688 - mae: 639.0507 - val_loss: 419566.5000 - val_mae: 503.5387\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 500557.0938 - mae: 641.4387 - val_loss: 411051.9688 - val_mae: 495.1915\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 495818.0625 - mae: 639.1910 - val_loss: 428193.2188 - val_mae: 512.5734\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 495838.9688 - mae: 639.2348 - val_loss: 418422.6562 - val_mae: 503.1100\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 503049.6875 - mae: 643.6202 - val_loss: 419003.3438 - val_mae: 503.1992\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 495527.6562 - mae: 639.0526 - val_loss: 408872.2188 - val_mae: 493.9523\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 493004.3125 - mae: 637.8821 - val_loss: 425612.2500 - val_mae: 510.9564\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 498749.2500 - mae: 640.0908 - val_loss: 401369.2500 - val_mae: 485.9510\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 499391.4062 - mae: 640.1298 - val_loss: 435856.0312 - val_mae: 520.2482\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 503636.5625 - mae: 643.4833 - val_loss: 426102.1562 - val_mae: 510.9804\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 499058.8438 - mae: 639.8332 - val_loss: 425395.4062 - val_mae: 510.8373\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 498313.2812 - mae: 642.0684 - val_loss: 418590.2812 - val_mae: 502.9603\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 489987.3438 - mae: 635.7499 - val_loss: 411275.3438 - val_mae: 495.5201\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 496065.2188 - mae: 640.2244 - val_loss: 408949.1562 - val_mae: 493.7506\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 495056.6562 - mae: 639.3846 - val_loss: 400190.4062 - val_mae: 485.2500\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 502367.6562 - mae: 642.0966 - val_loss: 416667.2812 - val_mae: 502.1236\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 495950.7812 - mae: 640.4801 - val_loss: 408461.5000 - val_mae: 493.7077\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 502677.8750 - mae: 643.3289 - val_loss: 417166.8750 - val_mae: 502.6624\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 502076.3125 - mae: 641.6837 - val_loss: 416007.0312 - val_mae: 501.4764\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 505188.7812 - mae: 645.5713 - val_loss: 417363.5938 - val_mae: 502.2494\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 498818.5938 - mae: 640.6344 - val_loss: 409014.3438 - val_mae: 494.2804\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 502158.3438 - mae: 643.5411 - val_loss: 417829.9688 - val_mae: 502.5349\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 499902.7812 - mae: 640.9230 - val_loss: 415855.2500 - val_mae: 501.6729\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 499345.1562 - mae: 640.2686 - val_loss: 407733.3750 - val_mae: 493.3127\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 497267.7812 - mae: 639.1136 - val_loss: 417188.3438 - val_mae: 502.6799\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 505334.1562 - mae: 645.4400 - val_loss: 415669.8438 - val_mae: 501.2958\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 499497.0312 - mae: 640.4721 - val_loss: 425169.6250 - val_mae: 510.7139\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 502491.5000 - mae: 643.1432 - val_loss: 408729.5000 - val_mae: 494.1240\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 502035.7188 - mae: 642.7858 - val_loss: 408897.5000 - val_mae: 493.9566\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 500277.8750 - mae: 640.5540 - val_loss: 418457.4688 - val_mae: 503.1294\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 493935.7188 - mae: 639.3715 - val_loss: 418557.9062 - val_mae: 502.9562\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 502186.6250 - mae: 642.4219 - val_loss: 427369.8750 - val_mae: 511.9193\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 497629.3125 - mae: 639.5065 - val_loss: 419373.4688 - val_mae: 503.6398\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 494026.1562 - mae: 637.4430 - val_loss: 402113.2188 - val_mae: 486.5943\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 504519.3125 - mae: 645.0045 - val_loss: 419571.3750 - val_mae: 503.7538\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 503410.5938 - mae: 643.6945 - val_loss: 429181.6562 - val_mae: 512.7167\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 495725.3750 - mae: 640.1645 - val_loss: 419471.1250 - val_mae: 503.9065\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 500865.0000 - mae: 641.2923 - val_loss: 419063.1562 - val_mae: 503.4670\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501789.1875 - mae: 642.6055 - val_loss: 410341.0000 - val_mae: 495.0065\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 501838.4062 - mae: 642.6288 - val_loss: 427824.5938 - val_mae: 511.9579\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 503097.1250 - mae: 644.6702 - val_loss: 403184.1562 - val_mae: 486.9717\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 503131.6562 - mae: 644.1949 - val_loss: 419640.5938 - val_mae: 503.9973\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 499120.7812 - mae: 641.0281 - val_loss: 411302.4062 - val_mae: 495.5348\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 495967.7812 - mae: 640.6211 - val_loss: 418104.0938 - val_mae: 502.9364\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 493148.7500 - mae: 637.6729 - val_loss: 418299.5000 - val_mae: 503.0370\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 505884.3750 - mae: 645.5400 - val_loss: 419298.0000 - val_mae: 503.1718\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 503200.2188 - mae: 643.4484 - val_loss: 418945.5000 - val_mae: 503.1803\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 494069.0625 - mae: 638.2478 - val_loss: 419066.5000 - val_mae: 503.4728\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 501026.0312 - mae: 642.4100 - val_loss: 427600.6562 - val_mae: 511.8311\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 499690.8438 - mae: 641.5260 - val_loss: 418920.0000 - val_mae: 503.1510\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 496293.8750 - mae: 638.8539 - val_loss: 418210.7812 - val_mae: 502.9917\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 496829.4062 - mae: 640.3557 - val_loss: 418589.5000 - val_mae: 502.9745\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 495740.8125 - mae: 640.0950 - val_loss: 410516.9688 - val_mae: 495.1040\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 502644.6250 - mae: 642.8346 - val_loss: 419498.9688 - val_mae: 503.4998\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 492254.8438 - mae: 637.4657 - val_loss: 427588.7188 - val_mae: 511.6114\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 500339.4688 - mae: 641.7102 - val_loss: 400941.8438 - val_mae: 485.7029\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 503451.7188 - mae: 643.8062 - val_loss: 417061.5938 - val_mae: 502.1045\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501187.5312 - mae: 640.9180 - val_loss: 409584.2188 - val_mae: 494.1256\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 500536.5625 - mae: 642.0325 - val_loss: 410378.7500 - val_mae: 494.5939\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 504815.4375 - mae: 644.5406 - val_loss: 417355.5000 - val_mae: 502.5138\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 502823.8750 - mae: 642.9514 - val_loss: 427881.2188 - val_mae: 512.2003\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 499647.8438 - mae: 640.7184 - val_loss: 394522.2500 - val_mae: 478.5539\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500630.7188 - mae: 641.6667 - val_loss: 428278.5938 - val_mae: 512.2061\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 501805.6250 - mae: 643.9479 - val_loss: 410396.9688 - val_mae: 494.8199\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 500279.9062 - mae: 641.2137 - val_loss: 426751.5312 - val_mae: 511.1197\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 496834.7500 - mae: 639.3234 - val_loss: 417783.5000 - val_mae: 502.4930\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 507671.1875 - mae: 647.8785 - val_loss: 408878.5938 - val_mae: 494.2045\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 492673.5938 - mae: 637.5264 - val_loss: 400765.7188 - val_mae: 485.5848\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 501997.6875 - mae: 642.5227 - val_loss: 416929.5938 - val_mae: 502.2754\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 498451.1875 - mae: 640.5886 - val_loss: 416132.4062 - val_mae: 501.5649\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 488268.3750 - mae: 635.3581 - val_loss: 390825.0000 - val_mae: 476.4557\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 492382.6250 - mae: 637.8962 - val_loss: 390258.1562 - val_mae: 476.1306\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 500977.0312 - mae: 642.2446 - val_loss: 407383.1250 - val_mae: 493.3827\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 496761.3438 - mae: 638.5040 - val_loss: 415640.1562 - val_mae: 500.9761\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 491027.3438 - mae: 636.8355 - val_loss: 414918.5000 - val_mae: 500.5670\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 498168.4688 - mae: 639.6016 - val_loss: 414759.7188 - val_mae: 500.7494\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 501254.5312 - mae: 641.6550 - val_loss: 399038.6562 - val_mae: 484.8772\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 503625.3438 - mae: 643.6094 - val_loss: 425337.7500 - val_mae: 510.2674\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 497370.2188 - mae: 638.5312 - val_loss: 406955.6562 - val_mae: 492.8568\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 505420.4688 - mae: 644.5840 - val_loss: 406541.2500 - val_mae: 492.6082\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 500624.1562 - mae: 640.6976 - val_loss: 422225.4062 - val_mae: 508.4238\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 502518.7188 - mae: 642.3389 - val_loss: 423751.5000 - val_mae: 509.0320\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 503616.2188 - mae: 643.1487 - val_loss: 415620.4062 - val_mae: 501.8792\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 503252.3438 - mae: 643.1121 - val_loss: 423852.3438 - val_mae: 509.6976\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 493767.3125 - mae: 638.9632 - val_loss: 414665.7500 - val_mae: 500.7107\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 504526.9688 - mae: 644.8341 - val_loss: 406843.1562 - val_mae: 493.0848\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 491317.7812 - mae: 637.2659 - val_loss: 414022.5938 - val_mae: 500.3350\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 498666.8750 - mae: 640.1061 - val_loss: 405260.2500 - val_mae: 491.5379\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 500849.5312 - mae: 641.0805 - val_loss: 397857.8750 - val_mae: 483.9042\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 496884.3438 - mae: 637.9108 - val_loss: 414555.4062 - val_mae: 500.6139\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 493124.1875 - mae: 637.5552 - val_loss: 406286.2500 - val_mae: 492.4730\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 499110.4375 - mae: 642.4222 - val_loss: 414620.2500 - val_mae: 501.2865\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 493846.3125 - mae: 639.6490 - val_loss: 397654.5312 - val_mae: 484.1035\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 499722.5625 - mae: 641.0196 - val_loss: 405961.1250 - val_mae: 491.9560\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 500362.6250 - mae: 641.0310 - val_loss: 398486.5000 - val_mae: 484.2721\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 500439.9062 - mae: 640.8868 - val_loss: 432222.2812 - val_mae: 517.9205\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 498845.5312 - mae: 639.6531 - val_loss: 409260.2500 - val_mae: 494.4155\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 493420.3125 - mae: 636.7965 - val_loss: 417265.7500 - val_mae: 502.2078\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 500327.6562 - mae: 640.8084 - val_loss: 416033.7812 - val_mae: 501.2393\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 495792.8125 - mae: 638.8060 - val_loss: 416870.8750 - val_mae: 502.5147\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 500217.0625 - mae: 640.7069 - val_loss: 400247.5312 - val_mae: 485.2833\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 501833.6250 - mae: 641.9454 - val_loss: 416615.7812 - val_mae: 501.8457\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 500190.0625 - mae: 641.5739 - val_loss: 409025.0000 - val_mae: 494.2856\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 502294.3125 - mae: 642.2586 - val_loss: 409143.1250 - val_mae: 494.1067\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 499923.3438 - mae: 641.7800 - val_loss: 410166.2500 - val_mae: 494.9121\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 494589.1875 - mae: 638.3379 - val_loss: 417859.2812 - val_mae: 502.7955\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 506731.0938 - mae: 646.3752 - val_loss: 408291.5938 - val_mae: 493.6313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "ca319584-130b-40cf-da3e-36bc03755077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1514168.5,\n",
              "  1456889.25,\n",
              "  1423011.25,\n",
              "  1418767.0,\n",
              "  1376103.625,\n",
              "  1362459.625,\n",
              "  1319068.625,\n",
              "  1314850.25,\n",
              "  1291375.5,\n",
              "  1259353.5,\n",
              "  1232607.625,\n",
              "  1214216.625,\n",
              "  1196777.375,\n",
              "  1148452.0,\n",
              "  1164706.875,\n",
              "  1112662.5,\n",
              "  1109996.75,\n",
              "  1092298.75,\n",
              "  1063570.125,\n",
              "  1028683.125,\n",
              "  1037024.3125,\n",
              "  1006996.875,\n",
              "  989888.3125,\n",
              "  980071.9375,\n",
              "  966486.0,\n",
              "  956147.875,\n",
              "  935170.0,\n",
              "  916342.625,\n",
              "  906117.0625,\n",
              "  874913.8125,\n",
              "  867551.625,\n",
              "  865776.0625,\n",
              "  849572.625,\n",
              "  828443.5,\n",
              "  815961.0625,\n",
              "  800505.9375,\n",
              "  792873.875,\n",
              "  778869.25,\n",
              "  772258.25,\n",
              "  764886.625,\n",
              "  743778.125,\n",
              "  746680.375,\n",
              "  719345.25,\n",
              "  727682.0625,\n",
              "  708564.625,\n",
              "  696917.6875,\n",
              "  688612.875,\n",
              "  677672.4375,\n",
              "  685484.5625,\n",
              "  668596.4375,\n",
              "  668770.8125,\n",
              "  638188.5625,\n",
              "  640000.875,\n",
              "  647285.25,\n",
              "  625032.4375,\n",
              "  628293.0,\n",
              "  624572.5,\n",
              "  613310.0,\n",
              "  607055.1875,\n",
              "  603042.5,\n",
              "  598925.3125,\n",
              "  593132.125,\n",
              "  579199.25,\n",
              "  584957.125,\n",
              "  587660.125,\n",
              "  574856.9375,\n",
              "  568002.3125,\n",
              "  565798.8125,\n",
              "  553425.8125,\n",
              "  559918.875,\n",
              "  556565.6875,\n",
              "  552776.0,\n",
              "  541490.4375,\n",
              "  548371.875,\n",
              "  544682.3125,\n",
              "  543801.375,\n",
              "  538274.125,\n",
              "  536495.375,\n",
              "  541309.75,\n",
              "  528533.9375,\n",
              "  523393.96875,\n",
              "  531491.9375,\n",
              "  526741.1875,\n",
              "  515682.8125,\n",
              "  521290.0,\n",
              "  525159.0,\n",
              "  523109.4375,\n",
              "  514298.0625,\n",
              "  518538.0625,\n",
              "  512033.4375,\n",
              "  511120.125,\n",
              "  509933.1875,\n",
              "  511908.28125,\n",
              "  516297.84375,\n",
              "  508249.40625,\n",
              "  515227.78125,\n",
              "  512509.90625,\n",
              "  511676.4375,\n",
              "  507666.625,\n",
              "  506773.5,\n",
              "  503875.96875,\n",
              "  501495.1875,\n",
              "  501666.6875,\n",
              "  506491.8125,\n",
              "  505625.71875,\n",
              "  502850.625,\n",
              "  503580.15625,\n",
              "  507251.75,\n",
              "  502073.0625,\n",
              "  504086.75,\n",
              "  503651.3125,\n",
              "  500068.28125,\n",
              "  501904.34375,\n",
              "  504186.4375,\n",
              "  503174.53125,\n",
              "  498648.75,\n",
              "  497652.0625,\n",
              "  502562.6875,\n",
              "  495089.15625,\n",
              "  501794.375,\n",
              "  498736.78125,\n",
              "  504022.0,\n",
              "  501100.46875,\n",
              "  496620.375,\n",
              "  501687.03125,\n",
              "  500334.46875,\n",
              "  497492.90625,\n",
              "  500769.4375,\n",
              "  499644.375,\n",
              "  502392.5625,\n",
              "  505786.5625,\n",
              "  493901.15625,\n",
              "  499707.28125,\n",
              "  496998.375,\n",
              "  499217.25,\n",
              "  496982.09375,\n",
              "  498951.53125,\n",
              "  495912.25,\n",
              "  502497.3125,\n",
              "  500244.5,\n",
              "  502004.5,\n",
              "  503215.21875,\n",
              "  499291.65625,\n",
              "  503099.4375,\n",
              "  497411.125,\n",
              "  495207.46875,\n",
              "  504638.5,\n",
              "  501916.15625,\n",
              "  504516.28125,\n",
              "  500364.4375,\n",
              "  490121.34375,\n",
              "  496520.625,\n",
              "  501036.09375,\n",
              "  505729.53125,\n",
              "  501870.625,\n",
              "  498970.21875,\n",
              "  500996.84375,\n",
              "  501954.40625,\n",
              "  500090.65625,\n",
              "  496029.40625,\n",
              "  500216.59375,\n",
              "  498080.21875,\n",
              "  505515.15625,\n",
              "  492733.03125,\n",
              "  506554.28125,\n",
              "  496952.03125,\n",
              "  496833.53125,\n",
              "  499816.8125,\n",
              "  501429.34375,\n",
              "  500962.8125,\n",
              "  497323.28125,\n",
              "  502092.46875,\n",
              "  503707.375,\n",
              "  502055.59375,\n",
              "  500224.8125,\n",
              "  494027.65625,\n",
              "  500792.5625,\n",
              "  499547.71875,\n",
              "  497633.15625,\n",
              "  502309.78125,\n",
              "  503763.625,\n",
              "  500636.09375,\n",
              "  498384.0625,\n",
              "  499368.6875,\n",
              "  501012.6875,\n",
              "  495403.4375,\n",
              "  501341.84375,\n",
              "  500993.9375,\n",
              "  505334.71875,\n",
              "  505413.125,\n",
              "  502093.53125,\n",
              "  500637.6875,\n",
              "  499811.125,\n",
              "  505154.46875,\n",
              "  501996.0,\n",
              "  503719.875,\n",
              "  501719.15625,\n",
              "  499214.28125,\n",
              "  500669.90625,\n",
              "  496082.96875,\n",
              "  501616.34375,\n",
              "  496441.6875,\n",
              "  505495.75,\n",
              "  504063.0625,\n",
              "  503664.125,\n",
              "  502000.78125,\n",
              "  505369.34375,\n",
              "  499004.03125,\n",
              "  502591.71875,\n",
              "  505816.90625,\n",
              "  502983.15625,\n",
              "  503544.09375,\n",
              "  502159.28125,\n",
              "  502039.96875,\n",
              "  501141.28125,\n",
              "  499728.21875,\n",
              "  499071.78125,\n",
              "  499347.53125,\n",
              "  493267.84375,\n",
              "  495136.8125,\n",
              "  502903.53125,\n",
              "  505524.625,\n",
              "  501088.5625,\n",
              "  502012.75,\n",
              "  500253.25,\n",
              "  502681.5625,\n",
              "  498681.625,\n",
              "  501479.1875,\n",
              "  500404.46875,\n",
              "  503300.5625,\n",
              "  502218.78125,\n",
              "  502248.25,\n",
              "  498351.65625,\n",
              "  498801.75,\n",
              "  500684.65625,\n",
              "  501997.53125,\n",
              "  496913.53125,\n",
              "  501736.3125,\n",
              "  500092.5,\n",
              "  500631.59375,\n",
              "  496716.65625,\n",
              "  502624.15625,\n",
              "  499155.78125,\n",
              "  500732.21875,\n",
              "  498824.40625,\n",
              "  504347.5,\n",
              "  494424.5625,\n",
              "  502097.21875,\n",
              "  501739.53125,\n",
              "  501060.21875,\n",
              "  501157.5625,\n",
              "  500513.5,\n",
              "  495772.09375,\n",
              "  505058.09375,\n",
              "  499049.71875,\n",
              "  501511.15625,\n",
              "  497394.8125,\n",
              "  501180.75,\n",
              "  500639.40625,\n",
              "  502167.3125,\n",
              "  496293.375,\n",
              "  504115.46875,\n",
              "  501270.5,\n",
              "  503422.40625,\n",
              "  505707.53125,\n",
              "  503791.78125,\n",
              "  498308.1875,\n",
              "  498540.46875,\n",
              "  493682.375,\n",
              "  500626.59375,\n",
              "  499857.96875,\n",
              "  493975.59375,\n",
              "  490553.40625,\n",
              "  505140.46875,\n",
              "  504221.1875,\n",
              "  500068.34375,\n",
              "  499817.9375,\n",
              "  496746.34375,\n",
              "  503872.0625,\n",
              "  498404.0,\n",
              "  503435.375,\n",
              "  505818.125,\n",
              "  499126.96875,\n",
              "  501396.78125,\n",
              "  498629.59375,\n",
              "  492499.40625,\n",
              "  497269.78125,\n",
              "  502813.8125,\n",
              "  503604.625,\n",
              "  500155.65625,\n",
              "  500276.90625,\n",
              "  492457.6875,\n",
              "  498903.53125,\n",
              "  508452.40625,\n",
              "  506545.59375,\n",
              "  503077.9375,\n",
              "  499310.5,\n",
              "  501407.71875,\n",
              "  505496.6875,\n",
              "  500237.96875,\n",
              "  492272.4375,\n",
              "  500857.03125,\n",
              "  501584.28125,\n",
              "  496719.65625,\n",
              "  500819.5625,\n",
              "  500600.8125,\n",
              "  502133.84375,\n",
              "  506322.25,\n",
              "  502359.65625,\n",
              "  502127.78125,\n",
              "  505152.5625,\n",
              "  504389.375,\n",
              "  492818.3125,\n",
              "  491819.21875,\n",
              "  498210.46875,\n",
              "  501851.78125,\n",
              "  499571.34375,\n",
              "  499909.59375,\n",
              "  500866.75,\n",
              "  505000.25,\n",
              "  500319.625,\n",
              "  501399.15625,\n",
              "  496128.21875,\n",
              "  502135.4375,\n",
              "  503053.96875,\n",
              "  493653.5,\n",
              "  499841.03125,\n",
              "  489576.03125,\n",
              "  494745.40625,\n",
              "  504730.4375,\n",
              "  500262.5,\n",
              "  500850.09375,\n",
              "  504028.6875,\n",
              "  501142.8125,\n",
              "  497943.8125,\n",
              "  501462.21875,\n",
              "  501766.0,\n",
              "  499122.53125,\n",
              "  500092.46875,\n",
              "  501019.71875,\n",
              "  497476.125,\n",
              "  506170.9375,\n",
              "  493437.75,\n",
              "  502678.4375,\n",
              "  502292.5,\n",
              "  500925.53125,\n",
              "  502291.78125,\n",
              "  499751.03125,\n",
              "  500509.53125,\n",
              "  502511.5,\n",
              "  501592.03125,\n",
              "  504087.6875,\n",
              "  501115.3125,\n",
              "  498221.6875,\n",
              "  500055.59375,\n",
              "  495048.5625,\n",
              "  502096.4375,\n",
              "  498651.28125,\n",
              "  499697.53125,\n",
              "  499271.53125,\n",
              "  501501.625,\n",
              "  500328.53125,\n",
              "  502173.53125,\n",
              "  502347.15625,\n",
              "  502029.3125,\n",
              "  499493.15625,\n",
              "  501931.5,\n",
              "  496222.0625,\n",
              "  503621.65625,\n",
              "  504269.09375,\n",
              "  496032.0,\n",
              "  496631.15625,\n",
              "  503653.4375,\n",
              "  497004.75,\n",
              "  496806.5,\n",
              "  500074.4375,\n",
              "  502029.6875,\n",
              "  499440.875,\n",
              "  498419.75,\n",
              "  501299.34375,\n",
              "  500277.84375,\n",
              "  500797.40625,\n",
              "  500234.4375,\n",
              "  501972.5625,\n",
              "  499978.875,\n",
              "  493888.34375,\n",
              "  497155.6875,\n",
              "  505468.4375,\n",
              "  503035.875,\n",
              "  502291.3125,\n",
              "  499909.84375,\n",
              "  504703.0,\n",
              "  501623.03125,\n",
              "  493153.96875,\n",
              "  500557.09375,\n",
              "  495818.0625,\n",
              "  495838.96875,\n",
              "  503049.6875,\n",
              "  495527.65625,\n",
              "  493004.3125,\n",
              "  498749.25,\n",
              "  499391.40625,\n",
              "  503636.5625,\n",
              "  499058.84375,\n",
              "  498313.28125,\n",
              "  489987.34375,\n",
              "  496065.21875,\n",
              "  495056.65625,\n",
              "  502367.65625,\n",
              "  495950.78125,\n",
              "  502677.875,\n",
              "  502076.3125,\n",
              "  505188.78125,\n",
              "  498818.59375,\n",
              "  502158.34375,\n",
              "  499902.78125,\n",
              "  499345.15625,\n",
              "  497267.78125,\n",
              "  505334.15625,\n",
              "  499497.03125,\n",
              "  502491.5,\n",
              "  502035.71875,\n",
              "  500277.875,\n",
              "  493935.71875,\n",
              "  502186.625,\n",
              "  497629.3125,\n",
              "  494026.15625,\n",
              "  504519.3125,\n",
              "  503410.59375,\n",
              "  495725.375,\n",
              "  500865.0,\n",
              "  501789.1875,\n",
              "  501838.40625,\n",
              "  503097.125,\n",
              "  503131.65625,\n",
              "  499120.78125,\n",
              "  495967.78125,\n",
              "  493148.75,\n",
              "  505884.375,\n",
              "  503200.21875,\n",
              "  494069.0625,\n",
              "  501026.03125,\n",
              "  499690.84375,\n",
              "  496293.875,\n",
              "  496829.40625,\n",
              "  495740.8125,\n",
              "  502644.625,\n",
              "  492254.84375,\n",
              "  500339.46875,\n",
              "  503451.71875,\n",
              "  501187.53125,\n",
              "  500536.5625,\n",
              "  504815.4375,\n",
              "  502823.875,\n",
              "  499647.84375,\n",
              "  500630.71875,\n",
              "  501805.625,\n",
              "  500279.90625,\n",
              "  496834.75,\n",
              "  507671.1875,\n",
              "  492673.59375,\n",
              "  501997.6875,\n",
              "  498451.1875,\n",
              "  488268.375,\n",
              "  492382.625,\n",
              "  500977.03125,\n",
              "  496761.34375,\n",
              "  491027.34375,\n",
              "  498168.46875,\n",
              "  501254.53125,\n",
              "  503625.34375,\n",
              "  497370.21875,\n",
              "  505420.46875,\n",
              "  500624.15625,\n",
              "  502518.71875,\n",
              "  503616.21875,\n",
              "  503252.34375,\n",
              "  493767.3125,\n",
              "  504526.96875,\n",
              "  491317.78125,\n",
              "  498666.875,\n",
              "  500849.53125,\n",
              "  496884.34375,\n",
              "  493124.1875,\n",
              "  499110.4375,\n",
              "  493846.3125,\n",
              "  499722.5625,\n",
              "  500362.625,\n",
              "  500439.90625,\n",
              "  498845.53125,\n",
              "  493420.3125,\n",
              "  500327.65625,\n",
              "  495792.8125,\n",
              "  500217.0625,\n",
              "  501833.625,\n",
              "  500190.0625,\n",
              "  502294.3125,\n",
              "  499923.34375,\n",
              "  494589.1875,\n",
              "  506731.09375],\n",
              " 'mae': [1007.141845703125,\n",
              "  980.6068725585938,\n",
              "  963.9703369140625,\n",
              "  960.3832397460938,\n",
              "  939.2373046875,\n",
              "  932.0405883789062,\n",
              "  914.5459594726562,\n",
              "  910.631103515625,\n",
              "  899.1073608398438,\n",
              "  881.7861938476562,\n",
              "  868.6124877929688,\n",
              "  860.1554565429688,\n",
              "  853.205078125,\n",
              "  830.51904296875,\n",
              "  837.7262573242188,\n",
              "  811.2750854492188,\n",
              "  811.0192260742188,\n",
              "  801.15185546875,\n",
              "  787.9610595703125,\n",
              "  770.5101928710938,\n",
              "  775.203125,\n",
              "  758.3713989257812,\n",
              "  750.9750366210938,\n",
              "  742.7682495117188,\n",
              "  735.6287231445312,\n",
              "  732.6594848632812,\n",
              "  721.376708984375,\n",
              "  711.4187622070312,\n",
              "  709.5655517578125,\n",
              "  692.3651123046875,\n",
              "  692.0517578125,\n",
              "  691.2905883789062,\n",
              "  683.2105102539062,\n",
              "  672.6436767578125,\n",
              "  667.7579345703125,\n",
              "  664.966064453125,\n",
              "  659.1448974609375,\n",
              "  660.19140625,\n",
              "  656.817626953125,\n",
              "  654.0198974609375,\n",
              "  644.7374267578125,\n",
              "  651.062744140625,\n",
              "  639.42578125,\n",
              "  648.7529907226562,\n",
              "  639.7833251953125,\n",
              "  636.45166015625,\n",
              "  636.1146850585938,\n",
              "  633.2697143554688,\n",
              "  641.541015625,\n",
              "  632.8033447265625,\n",
              "  638.10107421875,\n",
              "  621.6058959960938,\n",
              "  625.3432006835938,\n",
              "  633.6044311523438,\n",
              "  621.397216796875,\n",
              "  624.2099609375,\n",
              "  624.7975463867188,\n",
              "  620.1593627929688,\n",
              "  620.72900390625,\n",
              "  618.4453735351562,\n",
              "  622.7307739257812,\n",
              "  618.3306274414062,\n",
              "  614.2064819335938,\n",
              "  616.7177124023438,\n",
              "  622.015869140625,\n",
              "  616.3299560546875,\n",
              "  613.573974609375,\n",
              "  614.2130126953125,\n",
              "  608.4218139648438,\n",
              "  616.76806640625,\n",
              "  616.3829956054688,\n",
              "  616.5801391601562,\n",
              "  613.6533203125,\n",
              "  617.3886108398438,\n",
              "  616.9019775390625,\n",
              "  617.4371337890625,\n",
              "  615.8098754882812,\n",
              "  617.4185180664062,\n",
              "  623.7850341796875,\n",
              "  616.0739135742188,\n",
              "  617.2033081054688,\n",
              "  622.9401245117188,\n",
              "  621.2493286132812,\n",
              "  615.9569091796875,\n",
              "  620.9916381835938,\n",
              "  624.6771240234375,\n",
              "  626.9356079101562,\n",
              "  621.57470703125,\n",
              "  624.5204467773438,\n",
              "  624.551513671875,\n",
              "  623.316650390625,\n",
              "  624.4859619140625,\n",
              "  625.406982421875,\n",
              "  630.2642822265625,\n",
              "  628.752685546875,\n",
              "  632.1185913085938,\n",
              "  632.2612915039062,\n",
              "  631.447021484375,\n",
              "  629.8018188476562,\n",
              "  630.5042114257812,\n",
              "  631.1904296875,\n",
              "  630.2868041992188,\n",
              "  629.2399291992188,\n",
              "  633.4013671875,\n",
              "  632.8590087890625,\n",
              "  632.8755493164062,\n",
              "  633.6025390625,\n",
              "  635.966552734375,\n",
              "  632.8894653320312,\n",
              "  634.7698364257812,\n",
              "  635.0855102539062,\n",
              "  635.5477294921875,\n",
              "  635.040771484375,\n",
              "  637.5452270507812,\n",
              "  637.8388061523438,\n",
              "  634.9359741210938,\n",
              "  634.091552734375,\n",
              "  637.7503051757812,\n",
              "  635.0775146484375,\n",
              "  637.76025390625,\n",
              "  636.2909545898438,\n",
              "  639.8654174804688,\n",
              "  637.0277099609375,\n",
              "  636.6631469726562,\n",
              "  638.6469116210938,\n",
              "  639.179443359375,\n",
              "  635.5880126953125,\n",
              "  638.9113159179688,\n",
              "  638.637939453125,\n",
              "  640.3131713867188,\n",
              "  642.488037109375,\n",
              "  634.7673950195312,\n",
              "  637.7757568359375,\n",
              "  637.6226806640625,\n",
              "  639.249755859375,\n",
              "  636.917724609375,\n",
              "  639.8623046875,\n",
              "  637.658447265625,\n",
              "  640.9584350585938,\n",
              "  638.9049072265625,\n",
              "  641.4146118164062,\n",
              "  642.1957397460938,\n",
              "  639.8240966796875,\n",
              "  642.4747314453125,\n",
              "  639.2731323242188,\n",
              "  637.077880859375,\n",
              "  643.4760131835938,\n",
              "  641.709716796875,\n",
              "  643.9589233398438,\n",
              "  640.2525634765625,\n",
              "  636.3509521484375,\n",
              "  640.0390625,\n",
              "  641.1286010742188,\n",
              "  644.7575073242188,\n",
              "  642.8228759765625,\n",
              "  639.8661499023438,\n",
              "  640.7716064453125,\n",
              "  641.9913330078125,\n",
              "  641.8012084960938,\n",
              "  639.0284423828125,\n",
              "  641.1738891601562,\n",
              "  641.7093505859375,\n",
              "  644.998779296875,\n",
              "  638.8429565429688,\n",
              "  645.9198608398438,\n",
              "  639.0120849609375,\n",
              "  638.3867797851562,\n",
              "  640.314208984375,\n",
              "  641.418701171875,\n",
              "  642.235595703125,\n",
              "  639.8353271484375,\n",
              "  641.82275390625,\n",
              "  643.7238159179688,\n",
              "  641.7662963867188,\n",
              "  640.4029541015625,\n",
              "  637.8766479492188,\n",
              "  641.4531860351562,\n",
              "  640.3781127929688,\n",
              "  639.3524780273438,\n",
              "  642.2667236328125,\n",
              "  644.2357788085938,\n",
              "  642.656494140625,\n",
              "  641.1869506835938,\n",
              "  640.5800170898438,\n",
              "  641.1710205078125,\n",
              "  640.1680297851562,\n",
              "  641.601318359375,\n",
              "  643.6482543945312,\n",
              "  644.8892822265625,\n",
              "  645.4198608398438,\n",
              "  642.2117919921875,\n",
              "  641.5845947265625,\n",
              "  640.8092041015625,\n",
              "  645.1333618164062,\n",
              "  642.3660278320312,\n",
              "  643.0189819335938,\n",
              "  642.2996215820312,\n",
              "  640.6026000976562,\n",
              "  641.7998046875,\n",
              "  639.8143920898438,\n",
              "  642.0479736328125,\n",
              "  639.6046752929688,\n",
              "  645.1778564453125,\n",
              "  644.4547119140625,\n",
              "  642.6403198242188,\n",
              "  642.1035766601562,\n",
              "  643.9938354492188,\n",
              "  642.4273681640625,\n",
              "  643.46142578125,\n",
              "  645.2925415039062,\n",
              "  642.7412719726562,\n",
              "  642.9773559570312,\n",
              "  641.8372802734375,\n",
              "  642.5760498046875,\n",
              "  642.3251342773438,\n",
              "  641.078369140625,\n",
              "  639.967041015625,\n",
              "  640.2179565429688,\n",
              "  638.4981079101562,\n",
              "  640.6912841796875,\n",
              "  642.5192260742188,\n",
              "  645.5031127929688,\n",
              "  641.5734252929688,\n",
              "  642.080078125,\n",
              "  640.8814697265625,\n",
              "  642.2247314453125,\n",
              "  640.1041870117188,\n",
              "  641.57177734375,\n",
              "  642.0613403320312,\n",
              "  643.2680053710938,\n",
              "  642.9381103515625,\n",
              "  642.2152099609375,\n",
              "  639.267822265625,\n",
              "  640.8515625,\n",
              "  641.062255859375,\n",
              "  643.3037109375,\n",
              "  639.975830078125,\n",
              "  641.732421875,\n",
              "  640.9816284179688,\n",
              "  641.5362548828125,\n",
              "  638.40234375,\n",
              "  642.2306518554688,\n",
              "  641.3961791992188,\n",
              "  641.1904296875,\n",
              "  639.5028686523438,\n",
              "  644.6765747070312,\n",
              "  638.75,\n",
              "  642.046142578125,\n",
              "  642.1422119140625,\n",
              "  641.7406005859375,\n",
              "  640.8052978515625,\n",
              "  641.520263671875,\n",
              "  638.6668701171875,\n",
              "  644.5698852539062,\n",
              "  640.3314819335938,\n",
              "  641.3346557617188,\n",
              "  639.2733154296875,\n",
              "  641.1181030273438,\n",
              "  640.8579711914062,\n",
              "  642.6476440429688,\n",
              "  638.4918823242188,\n",
              "  643.8281860351562,\n",
              "  641.2129516601562,\n",
              "  643.299072265625,\n",
              "  645.0921020507812,\n",
              "  643.9895629882812,\n",
              "  640.5763549804688,\n",
              "  640.1614990234375,\n",
              "  637.5472412109375,\n",
              "  641.0715942382812,\n",
              "  640.406005859375,\n",
              "  638.7010498046875,\n",
              "  637.119873046875,\n",
              "  644.3162841796875,\n",
              "  644.4218139648438,\n",
              "  640.521240234375,\n",
              "  640.0436401367188,\n",
              "  640.5479736328125,\n",
              "  643.9302978515625,\n",
              "  639.2438354492188,\n",
              "  643.3507690429688,\n",
              "  645.7398071289062,\n",
              "  642.2107543945312,\n",
              "  641.64599609375,\n",
              "  640.2012939453125,\n",
              "  635.905029296875,\n",
              "  638.5177612304688,\n",
              "  643.4342651367188,\n",
              "  642.9698486328125,\n",
              "  640.572998046875,\n",
              "  641.7921142578125,\n",
              "  637.8622436523438,\n",
              "  639.7890625,\n",
              "  648.0855712890625,\n",
              "  645.8136596679688,\n",
              "  643.3582763671875,\n",
              "  640.6974487304688,\n",
              "  641.748779296875,\n",
              "  644.2421875,\n",
              "  641.2426147460938,\n",
              "  637.44873046875,\n",
              "  641.1896362304688,\n",
              "  641.1012573242188,\n",
              "  639.1873779296875,\n",
              "  642.9850463867188,\n",
              "  640.955810546875,\n",
              "  641.7498168945312,\n",
              "  646.2063598632812,\n",
              "  641.6281127929688,\n",
              "  641.5033569335938,\n",
              "  643.640869140625,\n",
              "  643.1325073242188,\n",
              "  636.5739135742188,\n",
              "  636.909423828125,\n",
              "  641.1522216796875,\n",
              "  642.0033569335938,\n",
              "  640.0912475585938,\n",
              "  641.0689086914062,\n",
              "  640.760986328125,\n",
              "  644.7593383789062,\n",
              "  640.2913818359375,\n",
              "  641.7869262695312,\n",
              "  639.1966552734375,\n",
              "  642.3424682617188,\n",
              "  643.0039672851562,\n",
              "  636.5895385742188,\n",
              "  640.7149047851562,\n",
              "  635.84619140625,\n",
              "  637.94140625,\n",
              "  644.2529296875,\n",
              "  640.2421875,\n",
              "  641.302490234375,\n",
              "  644.1921997070312,\n",
              "  641.4022216796875,\n",
              "  640.046630859375,\n",
              "  642.5584716796875,\n",
              "  642.7770385742188,\n",
              "  640.70556640625,\n",
              "  640.1937255859375,\n",
              "  641.3033447265625,\n",
              "  639.409912109375,\n",
              "  645.171630859375,\n",
              "  638.2073364257812,\n",
              "  642.7029418945312,\n",
              "  642.66943359375,\n",
              "  641.3504028320312,\n",
              "  642.1757202148438,\n",
              "  640.1055908203125,\n",
              "  640.9099731445312,\n",
              "  644.280517578125,\n",
              "  642.786376953125,\n",
              "  643.7633056640625,\n",
              "  641.1068115234375,\n",
              "  639.3563842773438,\n",
              "  641.338623046875,\n",
              "  639.9861450195312,\n",
              "  642.2833862304688,\n",
              "  639.9990844726562,\n",
              "  641.383056640625,\n",
              "  640.3129272460938,\n",
              "  642.1494750976562,\n",
              "  642.11865234375,\n",
              "  642.5362548828125,\n",
              "  642.8546142578125,\n",
              "  641.4741821289062,\n",
              "  641.652099609375,\n",
              "  642.4605712890625,\n",
              "  637.8275756835938,\n",
              "  643.835205078125,\n",
              "  643.7584838867188,\n",
              "  639.255126953125,\n",
              "  639.853759765625,\n",
              "  643.6056518554688,\n",
              "  639.2180786132812,\n",
              "  640.2745971679688,\n",
              "  640.1825561523438,\n",
              "  642.1622924804688,\n",
              "  640.8271484375,\n",
              "  640.9016723632812,\n",
              "  641.7463989257812,\n",
              "  640.3635864257812,\n",
              "  641.1343994140625,\n",
              "  641.3942260742188,\n",
              "  642.6513671875,\n",
              "  640.4500732421875,\n",
              "  638.0426635742188,\n",
              "  638.7819213867188,\n",
              "  645.03369140625,\n",
              "  642.8893432617188,\n",
              "  642.2088012695312,\n",
              "  641.4228515625,\n",
              "  644.4905395507812,\n",
              "  642.9013671875,\n",
              "  639.0506591796875,\n",
              "  641.438720703125,\n",
              "  639.1910400390625,\n",
              "  639.2348022460938,\n",
              "  643.6202392578125,\n",
              "  639.0526123046875,\n",
              "  637.8821411132812,\n",
              "  640.0907592773438,\n",
              "  640.1298217773438,\n",
              "  643.4833374023438,\n",
              "  639.8331909179688,\n",
              "  642.068359375,\n",
              "  635.7499389648438,\n",
              "  640.224365234375,\n",
              "  639.3846435546875,\n",
              "  642.0966186523438,\n",
              "  640.4801025390625,\n",
              "  643.3289184570312,\n",
              "  641.6837158203125,\n",
              "  645.5712890625,\n",
              "  640.6343994140625,\n",
              "  643.5410766601562,\n",
              "  640.9230346679688,\n",
              "  640.2686157226562,\n",
              "  639.1135864257812,\n",
              "  645.4400024414062,\n",
              "  640.4721069335938,\n",
              "  643.1431884765625,\n",
              "  642.7857666015625,\n",
              "  640.5540161132812,\n",
              "  639.3715209960938,\n",
              "  642.4219360351562,\n",
              "  639.5065307617188,\n",
              "  637.4429931640625,\n",
              "  645.0045166015625,\n",
              "  643.6944580078125,\n",
              "  640.1644897460938,\n",
              "  641.2922973632812,\n",
              "  642.6055297851562,\n",
              "  642.6287841796875,\n",
              "  644.6702270507812,\n",
              "  644.1948852539062,\n",
              "  641.0281372070312,\n",
              "  640.62109375,\n",
              "  637.6729125976562,\n",
              "  645.5399780273438,\n",
              "  643.4484252929688,\n",
              "  638.247802734375,\n",
              "  642.4100341796875,\n",
              "  641.5260009765625,\n",
              "  638.8539428710938,\n",
              "  640.355712890625,\n",
              "  640.0950317382812,\n",
              "  642.8345947265625,\n",
              "  637.4656982421875,\n",
              "  641.710205078125,\n",
              "  643.80615234375,\n",
              "  640.9180297851562,\n",
              "  642.032470703125,\n",
              "  644.5405883789062,\n",
              "  642.951416015625,\n",
              "  640.7183837890625,\n",
              "  641.666748046875,\n",
              "  643.9479370117188,\n",
              "  641.2136840820312,\n",
              "  639.3233642578125,\n",
              "  647.8784790039062,\n",
              "  637.5263671875,\n",
              "  642.522705078125,\n",
              "  640.588623046875,\n",
              "  635.3580932617188,\n",
              "  637.8961791992188,\n",
              "  642.2445678710938,\n",
              "  638.5039672851562,\n",
              "  636.8355102539062,\n",
              "  639.6015625,\n",
              "  641.655029296875,\n",
              "  643.6094360351562,\n",
              "  638.53125,\n",
              "  644.583984375,\n",
              "  640.6975708007812,\n",
              "  642.3388671875,\n",
              "  643.1487426757812,\n",
              "  643.112060546875,\n",
              "  638.9631958007812,\n",
              "  644.8341064453125,\n",
              "  637.2659301757812,\n",
              "  640.1061401367188,\n",
              "  641.0805053710938,\n",
              "  637.9108276367188,\n",
              "  637.55517578125,\n",
              "  642.4221801757812,\n",
              "  639.6490478515625,\n",
              "  641.0195922851562,\n",
              "  641.031005859375,\n",
              "  640.8868408203125,\n",
              "  639.6531372070312,\n",
              "  636.7965087890625,\n",
              "  640.8084106445312,\n",
              "  638.8059692382812,\n",
              "  640.7069091796875,\n",
              "  641.9453735351562,\n",
              "  641.5739135742188,\n",
              "  642.2586059570312,\n",
              "  641.7799682617188,\n",
              "  638.337890625,\n",
              "  646.375244140625],\n",
              " 'val_loss': [505376.59375,\n",
              "  506327.65625,\n",
              "  495178.0,\n",
              "  476352.21875,\n",
              "  458213.5,\n",
              "  466477.78125,\n",
              "  417299.0,\n",
              "  415539.40625,\n",
              "  406253.875,\n",
              "  388275.84375,\n",
              "  377240.34375,\n",
              "  366519.71875,\n",
              "  371217.875,\n",
              "  355756.875,\n",
              "  359662.15625,\n",
              "  333741.09375,\n",
              "  326569.84375,\n",
              "  315178.78125,\n",
              "  311762.84375,\n",
              "  311272.6875,\n",
              "  307648.75,\n",
              "  287156.625,\n",
              "  293147.875,\n",
              "  274250.90625,\n",
              "  271642.375,\n",
              "  263167.375,\n",
              "  255776.3125,\n",
              "  258698.921875,\n",
              "  258538.703125,\n",
              "  247828.578125,\n",
              "  243568.953125,\n",
              "  239797.546875,\n",
              "  236659.625,\n",
              "  225129.953125,\n",
              "  225194.25,\n",
              "  224210.453125,\n",
              "  225681.359375,\n",
              "  221731.390625,\n",
              "  219780.953125,\n",
              "  216392.875,\n",
              "  217716.703125,\n",
              "  218777.328125,\n",
              "  216022.890625,\n",
              "  214468.5625,\n",
              "  216789.046875,\n",
              "  214758.015625,\n",
              "  216619.921875,\n",
              "  217093.640625,\n",
              "  215638.859375,\n",
              "  216024.859375,\n",
              "  216722.296875,\n",
              "  221386.75,\n",
              "  220864.875,\n",
              "  219379.390625,\n",
              "  226690.484375,\n",
              "  224788.140625,\n",
              "  227129.375,\n",
              "  227477.921875,\n",
              "  232382.296875,\n",
              "  234775.75,\n",
              "  231972.796875,\n",
              "  231945.078125,\n",
              "  236235.578125,\n",
              "  238032.109375,\n",
              "  244956.546875,\n",
              "  243209.25,\n",
              "  250319.671875,\n",
              "  243843.203125,\n",
              "  253816.171875,\n",
              "  256440.578125,\n",
              "  254236.203125,\n",
              "  265540.875,\n",
              "  278872.375,\n",
              "  276196.1875,\n",
              "  274503.0,\n",
              "  282832.40625,\n",
              "  276471.09375,\n",
              "  290906.40625,\n",
              "  282328.78125,\n",
              "  286399.96875,\n",
              "  289421.78125,\n",
              "  291713.34375,\n",
              "  294934.40625,\n",
              "  297232.3125,\n",
              "  306761.46875,\n",
              "  297605.0,\n",
              "  307104.625,\n",
              "  316765.28125,\n",
              "  319631.375,\n",
              "  329318.46875,\n",
              "  306597.71875,\n",
              "  334170.09375,\n",
              "  331966.21875,\n",
              "  321896.65625,\n",
              "  331082.78125,\n",
              "  333619.15625,\n",
              "  343975.84375,\n",
              "  339861.875,\n",
              "  342190.15625,\n",
              "  352026.75,\n",
              "  353825.0,\n",
              "  341589.53125,\n",
              "  364819.15625,\n",
              "  346623.125,\n",
              "  363074.375,\n",
              "  351287.15625,\n",
              "  353192.21875,\n",
              "  354570.84375,\n",
              "  372523.25,\n",
              "  374819.25,\n",
              "  377216.84375,\n",
              "  378131.25,\n",
              "  387715.96875,\n",
              "  366996.5,\n",
              "  376404.21875,\n",
              "  377472.59375,\n",
              "  379856.75,\n",
              "  388929.25,\n",
              "  390154.40625,\n",
              "  391955.21875,\n",
              "  400424.25,\n",
              "  400888.5,\n",
              "  402629.90625,\n",
              "  379631.875,\n",
              "  404661.875,\n",
              "  405930.59375,\n",
              "  389971.75,\n",
              "  391666.09375,\n",
              "  399810.34375,\n",
              "  400972.625,\n",
              "  409517.84375,\n",
              "  410458.125,\n",
              "  403939.59375,\n",
              "  412596.65625,\n",
              "  396909.40625,\n",
              "  396214.46875,\n",
              "  398626.0,\n",
              "  415999.75,\n",
              "  390059.71875,\n",
              "  417473.65625,\n",
              "  408171.71875,\n",
              "  409730.15625,\n",
              "  419421.71875,\n",
              "  419811.65625,\n",
              "  403720.40625,\n",
              "  420366.59375,\n",
              "  394508.96875,\n",
              "  412028.53125,\n",
              "  420574.15625,\n",
              "  412570.625,\n",
              "  421106.46875,\n",
              "  404964.25,\n",
              "  421868.21875,\n",
              "  421300.375,\n",
              "  404872.15625,\n",
              "  398439.40625,\n",
              "  405058.03125,\n",
              "  407145.90625,\n",
              "  405728.0,\n",
              "  405902.0,\n",
              "  414381.09375,\n",
              "  389270.53125,\n",
              "  389279.625,\n",
              "  424057.375,\n",
              "  415353.40625,\n",
              "  407195.03125,\n",
              "  397010.875,\n",
              "  407123.25,\n",
              "  407158.90625,\n",
              "  423974.15625,\n",
              "  406456.46875,\n",
              "  415048.75,\n",
              "  415832.71875,\n",
              "  407337.34375,\n",
              "  424205.34375,\n",
              "  408302.15625,\n",
              "  425970.65625,\n",
              "  416896.59375,\n",
              "  399542.34375,\n",
              "  400600.15625,\n",
              "  408696.0,\n",
              "  407784.84375,\n",
              "  424719.75,\n",
              "  408708.34375,\n",
              "  426276.5,\n",
              "  424038.65625,\n",
              "  425932.125,\n",
              "  417786.46875,\n",
              "  418556.625,\n",
              "  409498.25,\n",
              "  426149.59375,\n",
              "  425131.75,\n",
              "  416096.15625,\n",
              "  426038.40625,\n",
              "  408729.125,\n",
              "  416339.75,\n",
              "  425845.375,\n",
              "  417335.40625,\n",
              "  408424.5,\n",
              "  398483.53125,\n",
              "  407155.28125,\n",
              "  431354.15625,\n",
              "  414823.875,\n",
              "  414367.75,\n",
              "  398454.84375,\n",
              "  397655.59375,\n",
              "  414106.59375,\n",
              "  406388.59375,\n",
              "  414535.15625,\n",
              "  415922.84375,\n",
              "  425491.875,\n",
              "  408394.03125,\n",
              "  416488.78125,\n",
              "  417471.84375,\n",
              "  433862.65625,\n",
              "  408102.5,\n",
              "  399576.90625,\n",
              "  424634.84375,\n",
              "  398871.09375,\n",
              "  415397.125,\n",
              "  407392.53125,\n",
              "  397549.875,\n",
              "  432873.5,\n",
              "  415746.15625,\n",
              "  416698.03125,\n",
              "  416410.5,\n",
              "  408383.71875,\n",
              "  408200.25,\n",
              "  416753.71875,\n",
              "  416489.71875,\n",
              "  399094.90625,\n",
              "  416271.90625,\n",
              "  415413.5,\n",
              "  415064.40625,\n",
              "  415759.65625,\n",
              "  407653.84375,\n",
              "  424996.96875,\n",
              "  406991.375,\n",
              "  407046.84375,\n",
              "  398959.625,\n",
              "  424194.0,\n",
              "  415996.46875,\n",
              "  399214.90625,\n",
              "  407465.25,\n",
              "  415262.53125,\n",
              "  424029.65625,\n",
              "  414478.5,\n",
              "  398225.25,\n",
              "  397990.84375,\n",
              "  390673.09375,\n",
              "  417281.71875,\n",
              "  409574.71875,\n",
              "  408668.75,\n",
              "  425021.90625,\n",
              "  416908.75,\n",
              "  406706.78125,\n",
              "  406995.21875,\n",
              "  414854.25,\n",
              "  417148.71875,\n",
              "  416259.25,\n",
              "  402107.46875,\n",
              "  417508.84375,\n",
              "  416443.59375,\n",
              "  425820.25,\n",
              "  407307.125,\n",
              "  407655.59375,\n",
              "  424226.15625,\n",
              "  406698.59375,\n",
              "  407512.09375,\n",
              "  416713.21875,\n",
              "  416440.09375,\n",
              "  408196.65625,\n",
              "  413706.5,\n",
              "  406202.78125,\n",
              "  424120.59375,\n",
              "  414775.375,\n",
              "  398413.34375,\n",
              "  398127.75,\n",
              "  415285.21875,\n",
              "  405512.625,\n",
              "  406005.5,\n",
              "  407614.875,\n",
              "  399245.09375,\n",
              "  415867.78125,\n",
              "  407640.46875,\n",
              "  423636.65625,\n",
              "  415768.65625,\n",
              "  407785.75,\n",
              "  408755.75,\n",
              "  408605.34375,\n",
              "  408430.53125,\n",
              "  405854.53125,\n",
              "  399466.34375,\n",
              "  424048.71875,\n",
              "  416313.78125,\n",
              "  408214.53125,\n",
              "  408917.21875,\n",
              "  407771.21875,\n",
              "  424608.78125,\n",
              "  397850.78125,\n",
              "  414501.59375,\n",
              "  414009.40625,\n",
              "  405412.59375,\n",
              "  411555.40625,\n",
              "  420904.84375,\n",
              "  412426.53125,\n",
              "  402951.28125,\n",
              "  396478.0,\n",
              "  421600.21875,\n",
              "  404997.875,\n",
              "  414280.875,\n",
              "  396109.96875,\n",
              "  414995.5,\n",
              "  397775.59375,\n",
              "  406472.34375,\n",
              "  406776.40625,\n",
              "  422689.0,\n",
              "  421393.53125,\n",
              "  414859.65625,\n",
              "  405912.78125,\n",
              "  396864.59375,\n",
              "  397164.75,\n",
              "  414554.34375,\n",
              "  415283.53125,\n",
              "  407201.59375,\n",
              "  407420.5,\n",
              "  398797.71875,\n",
              "  423939.71875,\n",
              "  415111.28125,\n",
              "  406796.875,\n",
              "  406046.53125,\n",
              "  407136.78125,\n",
              "  425190.75,\n",
              "  409261.84375,\n",
              "  417480.125,\n",
              "  391053.25,\n",
              "  414845.15625,\n",
              "  406382.59375,\n",
              "  414250.5,\n",
              "  407366.5,\n",
              "  415182.21875,\n",
              "  407935.78125,\n",
              "  407766.03125,\n",
              "  415911.09375,\n",
              "  425323.46875,\n",
              "  407555.625,\n",
              "  424088.75,\n",
              "  407536.34375,\n",
              "  400359.71875,\n",
              "  408283.09375,\n",
              "  400062.875,\n",
              "  423951.65625,\n",
              "  408681.75,\n",
              "  408437.65625,\n",
              "  436361.75,\n",
              "  417998.75,\n",
              "  409299.84375,\n",
              "  399981.21875,\n",
              "  417908.875,\n",
              "  424722.46875,\n",
              "  400203.5,\n",
              "  407728.46875,\n",
              "  409290.28125,\n",
              "  425745.53125,\n",
              "  399089.03125,\n",
              "  424287.15625,\n",
              "  407837.78125,\n",
              "  398560.625,\n",
              "  433160.15625,\n",
              "  408757.5,\n",
              "  396991.71875,\n",
              "  406253.09375,\n",
              "  406804.71875,\n",
              "  414996.5,\n",
              "  406882.625,\n",
              "  414568.28125,\n",
              "  415162.59375,\n",
              "  407575.375,\n",
              "  415479.65625,\n",
              "  405727.59375,\n",
              "  406669.09375,\n",
              "  417137.65625,\n",
              "  391125.25,\n",
              "  416383.46875,\n",
              "  408625.90625,\n",
              "  408176.28125,\n",
              "  408451.625,\n",
              "  416795.75,\n",
              "  416889.46875,\n",
              "  408786.75,\n",
              "  408677.875,\n",
              "  392696.21875,\n",
              "  400533.53125,\n",
              "  419566.5,\n",
              "  411051.96875,\n",
              "  428193.21875,\n",
              "  418422.65625,\n",
              "  419003.34375,\n",
              "  408872.21875,\n",
              "  425612.25,\n",
              "  401369.25,\n",
              "  435856.03125,\n",
              "  426102.15625,\n",
              "  425395.40625,\n",
              "  418590.28125,\n",
              "  411275.34375,\n",
              "  408949.15625,\n",
              "  400190.40625,\n",
              "  416667.28125,\n",
              "  408461.5,\n",
              "  417166.875,\n",
              "  416007.03125,\n",
              "  417363.59375,\n",
              "  409014.34375,\n",
              "  417829.96875,\n",
              "  415855.25,\n",
              "  407733.375,\n",
              "  417188.34375,\n",
              "  415669.84375,\n",
              "  425169.625,\n",
              "  408729.5,\n",
              "  408897.5,\n",
              "  418457.46875,\n",
              "  418557.90625,\n",
              "  427369.875,\n",
              "  419373.46875,\n",
              "  402113.21875,\n",
              "  419571.375,\n",
              "  429181.65625,\n",
              "  419471.125,\n",
              "  419063.15625,\n",
              "  410341.0,\n",
              "  427824.59375,\n",
              "  403184.15625,\n",
              "  419640.59375,\n",
              "  411302.40625,\n",
              "  418104.09375,\n",
              "  418299.5,\n",
              "  419298.0,\n",
              "  418945.5,\n",
              "  419066.5,\n",
              "  427600.65625,\n",
              "  418920.0,\n",
              "  418210.78125,\n",
              "  418589.5,\n",
              "  410516.96875,\n",
              "  419498.96875,\n",
              "  427588.71875,\n",
              "  400941.84375,\n",
              "  417061.59375,\n",
              "  409584.21875,\n",
              "  410378.75,\n",
              "  417355.5,\n",
              "  427881.21875,\n",
              "  394522.25,\n",
              "  428278.59375,\n",
              "  410396.96875,\n",
              "  426751.53125,\n",
              "  417783.5,\n",
              "  408878.59375,\n",
              "  400765.71875,\n",
              "  416929.59375,\n",
              "  416132.40625,\n",
              "  390825.0,\n",
              "  390258.15625,\n",
              "  407383.125,\n",
              "  415640.15625,\n",
              "  414918.5,\n",
              "  414759.71875,\n",
              "  399038.65625,\n",
              "  425337.75,\n",
              "  406955.65625,\n",
              "  406541.25,\n",
              "  422225.40625,\n",
              "  423751.5,\n",
              "  415620.40625,\n",
              "  423852.34375,\n",
              "  414665.75,\n",
              "  406843.15625,\n",
              "  414022.59375,\n",
              "  405260.25,\n",
              "  397857.875,\n",
              "  414555.40625,\n",
              "  406286.25,\n",
              "  414620.25,\n",
              "  397654.53125,\n",
              "  405961.125,\n",
              "  398486.5,\n",
              "  432222.28125,\n",
              "  409260.25,\n",
              "  417265.75,\n",
              "  416033.78125,\n",
              "  416870.875,\n",
              "  400247.53125,\n",
              "  416615.78125,\n",
              "  409025.0,\n",
              "  409143.125,\n",
              "  410166.25,\n",
              "  417859.28125,\n",
              "  408291.59375],\n",
              " 'val_mae': [539.9006958007812,\n",
              "  538.271484375,\n",
              "  526.074951171875,\n",
              "  509.6673583984375,\n",
              "  493.685546875,\n",
              "  501.2783508300781,\n",
              "  456.31396484375,\n",
              "  461.3645935058594,\n",
              "  467.4276428222656,\n",
              "  459.6145935058594,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  475.0362854003906,\n",
              "  468.0704650878906,\n",
              "  482.1231384277344,\n",
              "  465.4955749511719,\n",
              "  463.1145935058594,\n",
              "  461.3645935058594,\n",
              "  466.7240905761719,\n",
              "  473.036865234375,\n",
              "  477.3534851074219,\n",
              "  463.6358337402344,\n",
              "  475.2557678222656,\n",
              "  461.3645935058594,\n",
              "  464.906982421875,\n",
              "  461.0833435058594,\n",
              "  458.3265075683594,\n",
              "  467.5127258300781,\n",
              "  471.5768737792969,\n",
              "  463.3958435058594,\n",
              "  462.75,\n",
              "  463.1925354003906,\n",
              "  463.23779296875,\n",
              "  454.2990417480469,\n",
              "  457.3020935058594,\n",
              "  458.1537780761719,\n",
              "  461.6457824707031,\n",
              "  459.6145935058594,\n",
              "  459.103759765625,\n",
              "  457.0208435058594,\n",
              "  459.3333435058594,\n",
              "  461.21923828125,\n",
              "  458.96875,\n",
              "  457.861572265625,\n",
              "  460.311767578125,\n",
              "  458.0833740234375,\n",
              "  459.9076843261719,\n",
              "  459.9847106933594,\n",
              "  457.64892578125,\n",
              "  457.2575988769531,\n",
              "  457.0208435058594,\n",
              "  461.3646240234375,\n",
              "  459.3182067871094,\n",
              "  456.0602111816406,\n",
              "  462.6182556152344,\n",
              "  458.7687683105469,\n",
              "  459.3333435058594,\n",
              "  457.8451843261719,\n",
              "  460.713134765625,\n",
              "  461.6458435058594,\n",
              "  455.2959899902344,\n",
              "  452.9479675292969,\n",
              "  455.5074768066406,\n",
              "  454.9043273925781,\n",
              "  459.0520935058594,\n",
              "  454.5186767578125,\n",
              "  459.0520935058594,\n",
              "  448.7244873046875,\n",
              "  456.6022033691406,\n",
              "  456.4619445800781,\n",
              "  450.4183654785156,\n",
              "  459.3332824707031,\n",
              "  472.8873291015625,\n",
              "  465.093017578125,\n",
              "  459.3333435058594,\n",
              "  465.6333312988281,\n",
              "  455.2192687988281,\n",
              "  469.5891418457031,\n",
              "  454.6773681640625,\n",
              "  456.5760803222656,\n",
              "  456.4566650390625,\n",
              "  454.3088073730469,\n",
              "  454.1876525878906,\n",
              "  452.0467224121094,\n",
              "  461.0833435058594,\n",
              "  446.3621520996094,\n",
              "  451.9949035644531,\n",
              "  461.3646240234375,\n",
              "  461.3645935058594,\n",
              "  468.9172668457031,\n",
              "  439.63427734375,\n",
              "  463.3843078613281,\n",
              "  461.3645935058594,\n",
              "  447.393798828125,\n",
              "  455.01513671875,\n",
              "  454.9368591308594,\n",
              "  461.3645935058594,\n",
              "  452.999267578125,\n",
              "  452.930419921875,\n",
              "  462.993896484375,\n",
              "  464.1663513183594,\n",
              "  448.4928894042969,\n",
              "  470.522705078125,\n",
              "  451.8544006347656,\n",
              "  468.6529235839844,\n",
              "  456.2584228515625,\n",
              "  457.4476013183594,\n",
              "  457.0333557128906,\n",
              "  477.1755065917969,\n",
              "  476.156494140625,\n",
              "  476.5423583984375,\n",
              "  478.31201171875,\n",
              "  487.009765625,\n",
              "  465.98828125,\n",
              "  473.5654296875,\n",
              "  475.2530822753906,\n",
              "  476.710693359375,\n",
              "  485.1460876464844,\n",
              "  487.5770568847656,\n",
              "  487.8065185546875,\n",
              "  497.0192565917969,\n",
              "  494.6707458496094,\n",
              "  496.4847412109375,\n",
              "  473.6578063964844,\n",
              "  498.4597473144531,\n",
              "  499.874267578125,\n",
              "  482.2478332519531,\n",
              "  483.8953552246094,\n",
              "  491.851806640625,\n",
              "  493.1636962890625,\n",
              "  501.3555908203125,\n",
              "  500.7596130371094,\n",
              "  494.3241271972656,\n",
              "  503.158447265625,\n",
              "  487.5277404785156,\n",
              "  486.6282958984375,\n",
              "  488.4954528808594,\n",
              "  505.9553527832031,\n",
              "  479.7742919921875,\n",
              "  505.5823974609375,\n",
              "  497.3157043457031,\n",
              "  498.2065124511719,\n",
              "  507.1390686035156,\n",
              "  507.3644104003906,\n",
              "  490.9830017089844,\n",
              "  506.9508056640625,\n",
              "  481.9706726074219,\n",
              "  499.8811950683594,\n",
              "  507.8159484863281,\n",
              "  499.1455993652344,\n",
              "  507.7792053222656,\n",
              "  492.0442810058594,\n",
              "  508.2113952636719,\n",
              "  508.5757751464844,\n",
              "  491.3285217285156,\n",
              "  484.2110290527344,\n",
              "  491.7551574707031,\n",
              "  493.2518310546875,\n",
              "  491.8391418457031,\n",
              "  491.9428405761719,\n",
              "  500.54443359375,\n",
              "  475.5628356933594,\n",
              "  475.5680236816406,\n",
              "  509.8046569824219,\n",
              "  501.3856506347656,\n",
              "  493.2789306640625,\n",
              "  483.74658203125,\n",
              "  492.9311828613281,\n",
              "  493.597900390625,\n",
              "  510.05517578125,\n",
              "  492.8712463378906,\n",
              "  501.2195739746094,\n",
              "  501.3592834472656,\n",
              "  493.3575134277344,\n",
              "  510.1825866699219,\n",
              "  493.8890075683594,\n",
              "  510.6405029296875,\n",
              "  501.9935302734375,\n",
              "  484.9048767089844,\n",
              "  485.7532043457031,\n",
              "  493.841552734375,\n",
              "  493.040283203125,\n",
              "  509.9225769042969,\n",
              "  493.5664367675781,\n",
              "  511.0701904296875,\n",
              "  509.8041076660156,\n",
              "  510.6376037597656,\n",
              "  502.9957580566406,\n",
              "  503.5179138183594,\n",
              "  494.884033203125,\n",
              "  510.9979553222656,\n",
              "  510.1658020019531,\n",
              "  502.0906066894531,\n",
              "  510.6804504394531,\n",
              "  493.8706970214844,\n",
              "  501.94482421875,\n",
              "  511.0851135253906,\n",
              "  502.2330627441406,\n",
              "  494.0340270996094,\n",
              "  484.2871398925781,\n",
              "  492.97119140625,\n",
              "  517.4154663085938,\n",
              "  501.7362060546875,\n",
              "  500.53662109375,\n",
              "  484.2368469238281,\n",
              "  483.7857360839844,\n",
              "  500.0495910644531,\n",
              "  492.833740234375,\n",
              "  500.930419921875,\n",
              "  501.705810546875,\n",
              "  510.8909606933594,\n",
              "  493.6691589355469,\n",
              "  502.0284118652344,\n",
              "  502.312255859375,\n",
              "  519.401611328125,\n",
              "  493.5129699707031,\n",
              "  485.1795959472656,\n",
              "  510.4195251464844,\n",
              "  484.7829284667969,\n",
              "  501.1212158203125,\n",
              "  493.1070556640625,\n",
              "  484.0445251464844,\n",
              "  519.2066040039062,\n",
              "  501.6116943359375,\n",
              "  502.1456604003906,\n",
              "  501.9845275878906,\n",
              "  493.673583984375,\n",
              "  493.56884765625,\n",
              "  502.181640625,\n",
              "  501.7571716308594,\n",
              "  484.9088134765625,\n",
              "  502.1798400878906,\n",
              "  501.7160339355469,\n",
              "  500.6286926269531,\n",
              "  501.6192626953125,\n",
              "  492.98388671875,\n",
              "  510.6188659667969,\n",
              "  493.1665954589844,\n",
              "  492.8982849121094,\n",
              "  484.8271484375,\n",
              "  510.1766052246094,\n",
              "  501.752197265625,\n",
              "  484.9707946777344,\n",
              "  492.8505859375,\n",
              "  501.64501953125,\n",
              "  509.4942321777344,\n",
              "  500.601318359375,\n",
              "  484.1361389160156,\n",
              "  484.0160217285156,\n",
              "  476.3572692871094,\n",
              "  502.217041015625,\n",
              "  494.3327331542969,\n",
              "  493.8260192871094,\n",
              "  510.901123046875,\n",
              "  501.73974609375,\n",
              "  492.421630859375,\n",
              "  492.5714111328125,\n",
              "  500.8045654296875,\n",
              "  502.3934326171875,\n",
              "  502.23291015625,\n",
              "  486.3636169433594,\n",
              "  502.3487854003906,\n",
              "  502.0030212402344,\n",
              "  511.3233642578125,\n",
              "  493.0581970214844,\n",
              "  493.532958984375,\n",
              "  509.6105041503906,\n",
              "  492.7095642089844,\n",
              "  493.164794921875,\n",
              "  502.1493835449219,\n",
              "  502.2763671875,\n",
              "  493.8309631347656,\n",
              "  500.1336975097656,\n",
              "  492.4139099121094,\n",
              "  509.8407897949219,\n",
              "  501.0657043457031,\n",
              "  484.2461242675781,\n",
              "  484.0791931152344,\n",
              "  501.0559387207031,\n",
              "  492.0399475097656,\n",
              "  492.311767578125,\n",
              "  493.2236328125,\n",
              "  484.9823303222656,\n",
              "  501.679931640625,\n",
              "  493.2488708496094,\n",
              "  509.5742492675781,\n",
              "  501.6242980957031,\n",
              "  493.3213806152344,\n",
              "  493.8756408691406,\n",
              "  494.0557556152344,\n",
              "  493.6900329589844,\n",
              "  492.5379333496094,\n",
              "  485.1121520996094,\n",
              "  509.2138671875,\n",
              "  502.007080078125,\n",
              "  493.5770263671875,\n",
              "  493.7108459472656,\n",
              "  493.3236389160156,\n",
              "  510.4051818847656,\n",
              "  484.2142028808594,\n",
              "  500.5986328125,\n",
              "  500.2945861816406,\n",
              "  492.2929992675781,\n",
              "  498.8711242675781,\n",
              "  508.6968078613281,\n",
              "  499.7386169433594,\n",
              "  490.9271240234375,\n",
              "  483.432373046875,\n",
              "  508.0518493652344,\n",
              "  492.06298828125,\n",
              "  500.7814025878906,\n",
              "  483.5696105957031,\n",
              "  500.8710021972656,\n",
              "  484.165771484375,\n",
              "  492.5908203125,\n",
              "  492.7431640625,\n",
              "  509.3456726074219,\n",
              "  507.9501037597656,\n",
              "  501.1131286621094,\n",
              "  492.9095153808594,\n",
              "  483.3042297363281,\n",
              "  483.8208923339844,\n",
              "  500.6455993652344,\n",
              "  501.0390625,\n",
              "  492.9869384765625,\n",
              "  492.8025817871094,\n",
              "  484.7416687011719,\n",
              "  510.0362243652344,\n",
              "  500.970458984375,\n",
              "  492.765869140625,\n",
              "  492.3353271484375,\n",
              "  492.9606628417969,\n",
              "  510.4615783691406,\n",
              "  493.9146423339844,\n",
              "  502.3321228027344,\n",
              "  476.5864562988281,\n",
              "  501.1050109863281,\n",
              "  492.8307189941406,\n",
              "  500.4681701660156,\n",
              "  492.7705078125,\n",
              "  501.0118103027344,\n",
              "  493.1297912597656,\n",
              "  493.299560546875,\n",
              "  501.4206237792969,\n",
              "  510.5274658203125,\n",
              "  493.189697265625,\n",
              "  510.1186218261719,\n",
              "  493.1893615722656,\n",
              "  485.6136474609375,\n",
              "  493.6058044433594,\n",
              "  485.7898864746094,\n",
              "  510.0428161621094,\n",
              "  494.0970764160156,\n",
              "  493.7043762207031,\n",
              "  520.3153686523438,\n",
              "  502.6326599121094,\n",
              "  494.1859436035156,\n",
              "  485.4012756347656,\n",
              "  502.58056640625,\n",
              "  510.46728515625,\n",
              "  485.5310363769531,\n",
              "  493.309814453125,\n",
              "  494.4319763183594,\n",
              "  510.5277404785156,\n",
              "  484.9163818359375,\n",
              "  510.2276916503906,\n",
              "  493.3616943359375,\n",
              "  484.3321838378906,\n",
              "  518.4651489257812,\n",
              "  493.8665466308594,\n",
              "  483.4310607910156,\n",
              "  492.1517639160156,\n",
              "  492.7813415527344,\n",
              "  500.6141357421875,\n",
              "  492.8041076660156,\n",
              "  500.9490966796875,\n",
              "  501.2835693359375,\n",
              "  492.8945617675781,\n",
              "  501.4618835449219,\n",
              "  492.4681396484375,\n",
              "  493.0304260253906,\n",
              "  502.1182556152344,\n",
              "  476.6053771972656,\n",
              "  501.6954040527344,\n",
              "  493.8016052246094,\n",
              "  493.5655212402344,\n",
              "  493.9712219238281,\n",
              "  502.200439453125,\n",
              "  501.9893798828125,\n",
              "  493.903564453125,\n",
              "  493.8312072753906,\n",
              "  477.5148010253906,\n",
              "  485.4815368652344,\n",
              "  503.5387268066406,\n",
              "  495.1915283203125,\n",
              "  512.5734252929688,\n",
              "  503.1099853515625,\n",
              "  503.1991882324219,\n",
              "  493.9522705078125,\n",
              "  510.9563903808594,\n",
              "  485.9510192871094,\n",
              "  520.2482299804688,\n",
              "  510.9803771972656,\n",
              "  510.8372802734375,\n",
              "  502.9602966308594,\n",
              "  495.5200500488281,\n",
              "  493.7506408691406,\n",
              "  485.25,\n",
              "  502.1236267089844,\n",
              "  493.7076721191406,\n",
              "  502.6623840332031,\n",
              "  501.4764099121094,\n",
              "  502.2494201660156,\n",
              "  494.2804260253906,\n",
              "  502.5348815917969,\n",
              "  501.6728515625,\n",
              "  493.3126525878906,\n",
              "  502.679931640625,\n",
              "  501.2958068847656,\n",
              "  510.7138671875,\n",
              "  494.1239929199219,\n",
              "  493.9566345214844,\n",
              "  503.12939453125,\n",
              "  502.9561767578125,\n",
              "  511.9193420410156,\n",
              "  503.6398010253906,\n",
              "  486.5942687988281,\n",
              "  503.7537536621094,\n",
              "  512.7167358398438,\n",
              "  503.906494140625,\n",
              "  503.467041015625,\n",
              "  495.0064697265625,\n",
              "  511.9579162597656,\n",
              "  486.9717102050781,\n",
              "  503.997314453125,\n",
              "  495.5348205566406,\n",
              "  502.9364318847656,\n",
              "  503.0370178222656,\n",
              "  503.1717529296875,\n",
              "  503.1802673339844,\n",
              "  503.4728088378906,\n",
              "  511.8310546875,\n",
              "  503.1509704589844,\n",
              "  502.9917297363281,\n",
              "  502.9744873046875,\n",
              "  495.1040344238281,\n",
              "  503.499755859375,\n",
              "  511.6113586425781,\n",
              "  485.702880859375,\n",
              "  502.1044921875,\n",
              "  494.1256103515625,\n",
              "  494.5938720703125,\n",
              "  502.5137634277344,\n",
              "  512.2002563476562,\n",
              "  478.5539245605469,\n",
              "  512.2061157226562,\n",
              "  494.8199157714844,\n",
              "  511.1197204589844,\n",
              "  502.4930114746094,\n",
              "  494.2044677734375,\n",
              "  485.5848083496094,\n",
              "  502.2753601074219,\n",
              "  501.56494140625,\n",
              "  476.4556579589844,\n",
              "  476.130615234375,\n",
              "  493.3827209472656,\n",
              "  500.97607421875,\n",
              "  500.5670166015625,\n",
              "  500.7493896484375,\n",
              "  484.877197265625,\n",
              "  510.2673645019531,\n",
              "  492.8568420410156,\n",
              "  492.6082458496094,\n",
              "  508.423828125,\n",
              "  509.0319519042969,\n",
              "  501.8792419433594,\n",
              "  509.6976013183594,\n",
              "  500.7106628417969,\n",
              "  493.0848083496094,\n",
              "  500.3349609375,\n",
              "  491.5379333496094,\n",
              "  483.9042053222656,\n",
              "  500.6139221191406,\n",
              "  492.4729919433594,\n",
              "  501.2864685058594,\n",
              "  484.1035461425781,\n",
              "  491.9559631347656,\n",
              "  484.2720642089844,\n",
              "  517.9205322265625,\n",
              "  494.41552734375,\n",
              "  502.207763671875,\n",
              "  501.2393493652344,\n",
              "  502.5147399902344,\n",
              "  485.2833251953125,\n",
              "  501.8457336425781,\n",
              "  494.2856140136719,\n",
              "  494.106689453125,\n",
              "  494.912109375,\n",
              "  502.7955017089844,\n",
              "  493.63134765625]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "f72a1824-8eea-4e2e-fc09-73b5d590b575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression_unfreeze_1000.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression_unfreeze_1000.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "d84a4766-4be7-46f9-aa22-4d18fe22777e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_645beb71-8ce7-46ac-9984-1d6249ead044\", \"2Class_regression_unfreeze_1000.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}