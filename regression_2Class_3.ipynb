{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMFBx6jr6v7+yviCAL+/gCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "38573101-60a3-4c51-b33d-bb828ad0bcde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "68dec050-44da-494f-945d-d337f22bc715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ffdacc5-8140-419e-976f-2126ea543cd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ffdacc5-8140-419e-976f-2126ea543cd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ffdacc5-8140-419e-976f-2126ea543cd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ffdacc5-8140-419e-976f-2126ea543cd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "d06c5f0a-2931-4d68-80b4-cb480a25b1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "384cd912-2d4e-4a3a-8f06-98371f201bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "9ad9bd9d-a79d-4e26-b2f0-255aaa985e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "b720da04-99b5-4f2b-90d4-2946248b9473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "13354a0a-daa4-4a83-9052-865c0c25c3f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 34.68 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "74e81a77-c272-4b7b-9954-cc1a94f4cbcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "018bdc61-10a8-4689-af16-d9f4fd43321b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "ec6f27e8-bcfe-4b83-8480-ac80c3708ebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "158ce13a-5186-415b-87be-0f75b8dc7bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "ac7bba94-de27-4a72-b05c-6586b4e51550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=2e-3),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df8b315-d58c-48ff-cbdb-54d9d22bee80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-02f782872428>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 64s 1s/step - loss: 1504759.7500 - mae: 1002.1371 - val_loss: 539610.3750 - val_mae: 568.5801\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1514978.0000 - mae: 1007.1339 - val_loss: 534795.5625 - val_mae: 561.7349\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1516002.2500 - mae: 1006.3206 - val_loss: 539222.6250 - val_mae: 568.0063\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1523348.8750 - mae: 1009.6442 - val_loss: 534411.9375 - val_mae: 561.1611\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1504105.8750 - mae: 1003.5939 - val_loss: 543033.2500 - val_mae: 569.7456\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1523431.6250 - mae: 1011.6113 - val_loss: 530001.6250 - val_mae: 559.1228\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 8s 204ms/step - loss: 1496448.5000 - mae: 1002.8279 - val_loss: 560205.5625 - val_mae: 586.6331\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1518940.5000 - mae: 1008.1011 - val_loss: 516074.1250 - val_mae: 547.7060\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1511732.0000 - mae: 1004.5677 - val_loss: 538229.5000 - val_mae: 567.1314\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1513034.7500 - mae: 1007.2527 - val_loss: 559637.2500 - val_mae: 585.9135\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1517300.6250 - mae: 1008.5204 - val_loss: 528394.5625 - val_mae: 557.4675\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1513803.6250 - mae: 1006.3929 - val_loss: 550638.3750 - val_mae: 577.4553\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1514151.7500 - mae: 1005.5891 - val_loss: 540973.0625 - val_mae: 567.9387\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1519794.0000 - mae: 1009.0394 - val_loss: 524512.6875 - val_mae: 555.6484\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1492495.0000 - mae: 997.6647 - val_loss: 524297.0000 - val_mae: 555.2217\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1509161.6250 - mae: 1005.6389 - val_loss: 545730.9375 - val_mae: 574.2866\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1519262.7500 - mae: 1010.2207 - val_loss: 536970.0000 - val_mae: 566.2535\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1478128.7500 - mae: 993.3705 - val_loss: 536751.5000 - val_mae: 565.8269\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1510751.8750 - mae: 1004.8784 - val_loss: 527841.5625 - val_mae: 556.9548\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1497683.3750 - mae: 998.9362 - val_loss: 514801.5938 - val_mae: 546.0499\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1513505.3750 - mae: 1005.5016 - val_loss: 518775.3438 - val_mae: 547.9355\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1504683.7500 - mae: 1001.6332 - val_loss: 518614.1250 - val_mae: 547.7883\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1509060.5000 - mae: 1003.7164 - val_loss: 531691.9375 - val_mae: 562.5071\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1510066.2500 - mae: 1004.2576 - val_loss: 557315.0625 - val_mae: 584.1641\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1503549.6250 - mae: 1001.2180 - val_loss: 557143.2500 - val_mae: 584.0170\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1499279.3750 - mae: 999.8073 - val_loss: 522628.1562 - val_mae: 554.1833\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1495386.3750 - mae: 997.0330 - val_loss: 522467.2812 - val_mae: 554.0381\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1503450.0000 - mae: 1001.0655 - val_loss: 547856.8125 - val_mae: 574.5715\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1523733.7500 - mae: 1011.8954 - val_loss: 556411.7500 - val_mae: 583.1545\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1499161.6250 - mae: 1002.5991 - val_loss: 513209.0000 - val_mae: 544.5896\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1510540.2500 - mae: 1006.1578 - val_loss: 521821.9062 - val_mae: 553.4554\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1515602.7500 - mae: 1007.8544 - val_loss: 521607.4688 - val_mae: 553.0281\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1519522.2500 - mae: 1008.5449 - val_loss: 534230.8750 - val_mae: 563.3606\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1506906.0000 - mae: 1002.5283 - val_loss: 542777.8125 - val_mae: 571.9450\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1479048.7500 - mae: 990.5555 - val_loss: 538030.5625 - val_mae: 565.1035\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1505752.2500 - mae: 1002.6577 - val_loss: 546623.8125 - val_mae: 573.9687\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1493679.1250 - mae: 996.1514 - val_loss: 507980.0000 - val_mae: 541.5422\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1516991.3750 - mae: 1006.7986 - val_loss: 533469.8125 - val_mae: 562.9195\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1515983.2500 - mae: 1006.4183 - val_loss: 524655.6875 - val_mae: 554.3215\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1486610.7500 - mae: 994.1029 - val_loss: 498861.7500 - val_mae: 532.6552\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1466570.8750 - mae: 990.9125 - val_loss: 528856.5000 - val_mae: 560.4507\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1501031.2500 - mae: 999.6010 - val_loss: 528243.1875 - val_mae: 555.6375\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1475969.2500 - mae: 994.4169 - val_loss: 536718.6875 - val_mae: 563.9414\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 1488559.5000 - mae: 995.4155 - val_loss: 532489.4375 - val_mae: 562.0481\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1507924.5000 - mae: 1002.5167 - val_loss: 545075.2500 - val_mae: 572.3819\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 1456835.1250 - mae: 984.9432 - val_loss: 519370.0938 - val_mae: 551.0014\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1499205.5000 - mae: 999.2416 - val_loss: 553371.6250 - val_mae: 580.5421\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1492459.0000 - mae: 997.9146 - val_loss: 523110.2500 - val_mae: 552.4563\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1500114.2500 - mae: 1000.8815 - val_loss: 518887.1562 - val_mae: 550.5630\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1493243.0000 - mae: 996.2526 - val_loss: 531510.8750 - val_mae: 561.1768\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1489096.5000 - mae: 992.7111 - val_loss: 522733.5938 - val_mae: 552.5850\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1492167.7500 - mae: 999.1578 - val_loss: 518405.3750 - val_mae: 550.1253\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1474113.7500 - mae: 989.6393 - val_loss: 509587.0938 - val_mae: 541.2541\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1499736.2500 - mae: 999.1032 - val_loss: 518140.0312 - val_mae: 550.1191\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1498299.2500 - mae: 998.3753 - val_loss: 551969.3125 - val_mae: 579.0956\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1493005.1250 - mae: 998.3788 - val_loss: 530481.5625 - val_mae: 560.0230\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1503428.5000 - mae: 1001.2686 - val_loss: 521664.2188 - val_mae: 551.1459\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1512313.0000 - mae: 1004.5507 - val_loss: 521603.9062 - val_mae: 551.5620\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1496497.3750 - mae: 998.9069 - val_loss: 530038.1875 - val_mae: 559.8632\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1462004.6250 - mae: 983.0818 - val_loss: 529828.1875 - val_mae: 559.4393\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1498811.6250 - mae: 996.9366 - val_loss: 529614.5625 - val_mae: 559.0122\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1490927.5000 - mae: 996.9614 - val_loss: 529551.7500 - val_mae: 559.4286\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1502711.6250 - mae: 1000.6385 - val_loss: 512115.3438 - val_mae: 541.8240\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1498271.8750 - mae: 997.5301 - val_loss: 525119.0000 - val_mae: 557.1064\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1503808.8750 - mae: 999.3278 - val_loss: 532490.1875 - val_mae: 560.6575\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1481025.1250 - mae: 991.8279 - val_loss: 516115.0312 - val_mae: 547.8040\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1493495.7500 - mae: 996.2109 - val_loss: 528738.1250 - val_mae: 558.7009\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1474670.1250 - mae: 988.3721 - val_loss: 519903.2500 - val_mae: 549.5461\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1489930.1250 - mae: 994.9136 - val_loss: 532518.5625 - val_mae: 560.4421\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1506178.8750 - mae: 999.7978 - val_loss: 528151.3750 - val_mae: 557.7019\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1490796.8750 - mae: 994.1727 - val_loss: 528088.6875 - val_mae: 558.1194\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1477348.1250 - mae: 994.0403 - val_loss: 518585.6250 - val_mae: 548.5992\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1478682.8750 - mae: 991.1373 - val_loss: 518474.5938 - val_mae: 548.7343\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1480254.0000 - mae: 994.8882 - val_loss: 531756.3750 - val_mae: 559.9990\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1492507.6250 - mae: 993.8529 - val_loss: 527392.2500 - val_mae: 557.2581\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1483100.5000 - mae: 991.0707 - val_loss: 535884.0625 - val_mae: 566.1242\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1472417.3750 - mae: 986.6052 - val_loss: 527069.7500 - val_mae: 556.9685\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1510450.0000 - mae: 1002.6547 - val_loss: 497109.2188 - val_mae: 529.1650\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1480344.6250 - mae: 989.2316 - val_loss: 526843.4375 - val_mae: 557.2402\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1491564.3750 - mae: 995.0694 - val_loss: 513940.4688 - val_mae: 546.0522\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1481382.0000 - mae: 992.9962 - val_loss: 517875.7500 - val_mae: 547.9350\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1494616.2500 - mae: 994.8013 - val_loss: 526307.0625 - val_mae: 556.5210\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1498536.1250 - mae: 999.5335 - val_loss: 513461.4688 - val_mae: 545.6133\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1468420.3750 - mae: 984.4433 - val_loss: 538663.4375 - val_mae: 566.9918\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1504214.1250 - mae: 1002.5811 - val_loss: 500470.4688 - val_mae: 534.5639\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1506925.0000 - mae: 1001.7734 - val_loss: 538334.8125 - val_mae: 566.7018\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1466476.0000 - mae: 983.9980 - val_loss: 525450.8125 - val_mae: 555.5132\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1485363.3750 - mae: 993.6469 - val_loss: 538003.3750 - val_mae: 566.4094\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1497099.0000 - mae: 997.4699 - val_loss: 508422.0938 - val_mae: 542.7138\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1490540.3750 - mae: 993.6104 - val_loss: 533539.0000 - val_mae: 563.8105\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1479438.8750 - mae: 990.2098 - val_loss: 516338.7812 - val_mae: 546.7681\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1497782.8750 - mae: 998.5144 - val_loss: 507952.5000 - val_mae: 542.2811\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1479834.0000 - mae: 992.0685 - val_loss: 503323.8438 - val_mae: 535.4370\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1493431.3750 - mae: 996.5167 - val_loss: 528463.9375 - val_mae: 556.8130\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1470969.5000 - mae: 991.1308 - val_loss: 524213.4062 - val_mae: 554.6368\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1489084.6250 - mae: 994.5233 - val_loss: 528092.6875 - val_mae: 556.2410\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1499114.5000 - mae: 997.3695 - val_loss: 523793.8438 - val_mae: 553.7813\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1490212.3750 - mae: 994.3483 - val_loss: 485868.7500 - val_mae: 522.1975\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1488277.8750 - mae: 993.5048 - val_loss: 523563.8438 - val_mae: 554.0508\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1465497.7500 - mae: 985.5383 - val_loss: 536030.5625 - val_mae: 564.6652\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1485886.6250 - mae: 991.3664 - val_loss: 513979.0000 - val_mae: 544.3845\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1455664.2500 - mae: 983.6426 - val_loss: 522406.3750 - val_mae: 553.2507\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1481698.6250 - mae: 990.6075 - val_loss: 526959.0000 - val_mae: 555.2209\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1493814.5000 - mae: 996.2985 - val_loss: 522712.4062 - val_mae: 553.0430\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1491745.6250 - mae: 993.1085 - val_loss: 500781.4688 - val_mae: 533.3253\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1473016.8750 - mae: 986.8422 - val_loss: 530918.1250 - val_mae: 561.4814\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1478154.3750 - mae: 990.2911 - val_loss: 530758.4375 - val_mae: 561.3391\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1481398.8750 - mae: 991.0741 - val_loss: 517631.0312 - val_mae: 545.7659\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1488162.8750 - mae: 993.1087 - val_loss: 505233.5312 - val_mae: 539.5300\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1482247.2500 - mae: 990.9498 - val_loss: 521796.2500 - val_mae: 552.4536\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1491415.6250 - mae: 993.5966 - val_loss: 492061.2812 - val_mae: 524.3735\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1475119.8750 - mae: 989.8522 - val_loss: 513013.1250 - val_mae: 543.7185\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1491550.7500 - mae: 993.9395 - val_loss: 521225.3438 - val_mae: 551.4575\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1474224.0000 - mae: 986.2758 - val_loss: 483449.7812 - val_mae: 519.8762\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1483356.7500 - mae: 993.4507 - val_loss: 495879.4062 - val_mae: 530.4902\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1480934.0000 - mae: 992.0444 - val_loss: 533462.1250 - val_mae: 562.6267\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1463666.7500 - mae: 984.3837 - val_loss: 504025.7188 - val_mae: 538.6483\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1496105.3750 - mae: 998.6088 - val_loss: 512018.5312 - val_mae: 542.5639\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1490134.6250 - mae: 992.9703 - val_loss: 516325.0938 - val_mae: 549.3970\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1466297.1250 - mae: 985.7781 - val_loss: 528685.6875 - val_mae: 559.7304\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1474502.7500 - mae: 987.4097 - val_loss: 507427.5938 - val_mae: 539.8165\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1482404.2500 - mae: 990.0681 - val_loss: 528361.5000 - val_mae: 559.4407\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1464349.2500 - mae: 985.2449 - val_loss: 528157.0625 - val_mae: 559.0173\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1467593.8750 - mae: 982.2278 - val_loss: 507003.0000 - val_mae: 539.6624\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1478747.0000 - mae: 988.4426 - val_loss: 494295.0312 - val_mae: 528.7561\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1446594.1250 - mae: 977.2897 - val_loss: 523265.7188 - val_mae: 551.8850\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1465648.3750 - mae: 984.6866 - val_loss: 531534.8125 - val_mae: 560.1877\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1476200.6250 - mae: 987.3849 - val_loss: 493879.5000 - val_mae: 528.6018\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1474668.7500 - mae: 986.1810 - val_loss: 510247.5312 - val_mae: 540.6895\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1459851.1250 - mae: 980.4333 - val_loss: 489178.0000 - val_mae: 521.6169\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1484223.5000 - mae: 992.1954 - val_loss: 497490.1250 - val_mae: 530.2005\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1468360.3750 - mae: 983.5601 - val_loss: 505711.7500 - val_mae: 538.2249\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1466618.5000 - mae: 982.9515 - val_loss: 505554.8750 - val_mae: 538.0790\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1475974.5000 - mae: 986.2804 - val_loss: 496986.5312 - val_mae: 529.4860\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1446934.1250 - mae: 975.3770 - val_loss: 517763.2188 - val_mae: 548.5503\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 1474059.1250 - mae: 989.0073 - val_loss: 501067.0312 - val_mae: 535.8948\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1452668.1250 - mae: 980.6100 - val_loss: 525848.4375 - val_mae: 556.7068\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1458555.6250 - mae: 986.5898 - val_loss: 508885.5312 - val_mae: 539.6690\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1455901.0000 - mae: 979.5950 - val_loss: 512791.2500 - val_mae: 541.5543\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1481769.0000 - mae: 990.6065 - val_loss: 512905.9062 - val_mae: 545.7942\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1462558.0000 - mae: 983.0096 - val_loss: 508415.4062 - val_mae: 539.2332\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1491010.2500 - mae: 994.7679 - val_loss: 529192.3125 - val_mae: 558.5772\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1499231.3750 - mae: 998.2214 - val_loss: 511490.1562 - val_mae: 540.6073\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1500397.2500 - mae: 999.1161 - val_loss: 507984.2500 - val_mae: 539.0742\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 1467309.0000 - mae: 985.3446 - val_loss: 495341.5938 - val_mae: 528.1705\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1466630.5000 - mae: 983.3903 - val_loss: 524482.3125 - val_mae: 555.9630\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1476897.1250 - mae: 987.5349 - val_loss: 519953.0938 - val_mae: 549.1174\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1480542.6250 - mae: 990.8599 - val_loss: 511635.4688 - val_mae: 544.6290\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1475983.6250 - mae: 987.5762 - val_loss: 519677.0938 - val_mae: 549.1079\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1485118.6250 - mae: 991.1478 - val_loss: 507043.0000 - val_mae: 538.2004\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1445423.5000 - mae: 975.8073 - val_loss: 490363.0000 - val_mae: 525.2652\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1477920.5000 - mae: 988.9460 - val_loss: 506689.9688 - val_mae: 537.6309\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1479053.1250 - mae: 987.9567 - val_loss: 510928.2500 - val_mae: 544.4633\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1465289.6250 - mae: 983.2411 - val_loss: 510429.7188 - val_mae: 539.3696\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1485207.2500 - mae: 993.1318 - val_loss: 502124.6562 - val_mae: 534.8821\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1479572.6250 - mae: 988.1992 - val_loss: 506108.3750 - val_mae: 537.3314\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1473596.5000 - mae: 986.0931 - val_loss: 501810.9688 - val_mae: 534.5888\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1449365.2500 - mae: 976.7855 - val_loss: 509847.1562 - val_mae: 539.0712\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1462123.3750 - mae: 980.7277 - val_loss: 509982.7500 - val_mae: 543.5942\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1442844.3750 - mae: 976.0588 - val_loss: 517884.2188 - val_mae: 547.2304\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1466513.5000 - mae: 981.8931 - val_loss: 484748.9062 - val_mae: 521.5021\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1443473.8750 - mae: 974.4980 - val_loss: 525913.5000 - val_mae: 555.3912\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1477556.8750 - mae: 987.1085 - val_loss: 505019.0938 - val_mae: 536.3168\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1473501.3750 - mae: 986.7203 - val_loss: 491766.5000 - val_mae: 525.0469\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1466656.5000 - mae: 983.9065 - val_loss: 513000.4062 - val_mae: 544.1917\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1463888.3750 - mae: 982.2070 - val_loss: 508840.4062 - val_mae: 542.2997\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1470370.3750 - mae: 984.1970 - val_loss: 500265.0312 - val_mae: 533.1409\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1483041.3750 - mae: 991.2889 - val_loss: 504237.3438 - val_mae: 535.5876\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1459905.8750 - mae: 977.8898 - val_loss: 499994.8750 - val_mae: 533.1298\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1444908.3750 - mae: 978.0125 - val_loss: 491478.3438 - val_mae: 524.2587\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1459532.3750 - mae: 978.9716 - val_loss: 487321.8438 - val_mae: 522.3622\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1452172.6250 - mae: 977.2927 - val_loss: 515939.5938 - val_mae: 545.2073\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1459609.1250 - mae: 981.3399 - val_loss: 511779.9062 - val_mae: 543.3125\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1466471.7500 - mae: 981.8850 - val_loss: 507498.7188 - val_mae: 540.5747\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1462355.2500 - mae: 981.4052 - val_loss: 507467.2500 - val_mae: 541.2755\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1444032.3750 - mae: 975.6464 - val_loss: 511269.6250 - val_mae: 542.5991\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 1472448.2500 - mae: 984.6943 - val_loss: 498761.5938 - val_mae: 531.9720\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1478636.1250 - mae: 988.3984 - val_loss: 523337.1250 - val_mae: 553.0668\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1458933.8750 - mae: 981.1059 - val_loss: 494413.2500 - val_mae: 529.6503\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1462015.6250 - mae: 980.8517 - val_loss: 514715.8750 - val_mae: 544.3278\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1469688.3750 - mae: 984.5876 - val_loss: 485766.5938 - val_mae: 520.6291\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1461403.0000 - mae: 983.4230 - val_loss: 502066.3750 - val_mae: 533.5569\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1472173.0000 - mae: 986.8226 - val_loss: 510241.9062 - val_mae: 542.1392\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1471506.2500 - mae: 984.8251 - val_loss: 497678.6562 - val_mae: 530.9531\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1463110.7500 - mae: 980.3151 - val_loss: 501603.1562 - val_mae: 533.1227\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1461637.0000 - mae: 980.1950 - val_loss: 501408.2188 - val_mae: 532.6962\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1430938.6250 - mae: 969.5181 - val_loss: 509533.4062 - val_mae: 540.9969\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1453571.2500 - mae: 977.8618 - val_loss: 488787.0938 - val_mae: 521.9286\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1448330.3750 - mae: 974.1348 - val_loss: 496911.8438 - val_mae: 530.2305\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1450537.6250 - mae: 975.9690 - val_loss: 517302.0312 - val_mae: 548.7324\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1466755.2500 - mae: 983.1252 - val_loss: 492612.5938 - val_mae: 528.1913\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1439859.6250 - mae: 973.1553 - val_loss: 504721.8750 - val_mae: 538.2446\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1468355.3750 - mae: 983.2238 - val_loss: 508596.3438 - val_mae: 540.1301\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1442256.7500 - mae: 976.2955 - val_loss: 496102.9062 - val_mae: 529.2231\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1452780.1250 - mae: 977.1313 - val_loss: 504049.0312 - val_mae: 533.4218\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1452379.5000 - mae: 979.4970 - val_loss: 499905.5312 - val_mae: 531.5281\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1460781.5000 - mae: 981.6178 - val_loss: 491453.5000 - val_mae: 522.6530\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1429370.3750 - mae: 969.4560 - val_loss: 483244.5000 - val_mae: 518.4448\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1466465.2500 - mae: 981.5135 - val_loss: 511721.3750 - val_mae: 541.5701\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1446088.7500 - mae: 972.1821 - val_loss: 503435.0000 - val_mae: 536.8028\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1454437.5000 - mae: 976.9678 - val_loss: 503315.6250 - val_mae: 536.9366\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1413706.7500 - mae: 961.3491 - val_loss: 490932.5312 - val_mae: 526.5986\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1467379.6250 - mae: 982.7286 - val_loss: 498829.0938 - val_mae: 530.5146\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1457158.7500 - mae: 976.5840 - val_loss: 498637.3750 - val_mae: 530.0891\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1468621.3750 - mae: 984.5104 - val_loss: 510744.3438 - val_mae: 540.4218\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1453011.6250 - mae: 977.5321 - val_loss: 506601.8750 - val_mae: 538.5261\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1442303.5000 - mae: 970.7651 - val_loss: 518740.0000 - val_mae: 549.1413\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1458129.6250 - mae: 978.5873 - val_loss: 506293.2500 - val_mae: 538.2397\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1459286.0000 - mae: 980.4560 - val_loss: 514327.7500 - val_mae: 546.2617\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1458996.6250 - mae: 980.2619 - val_loss: 497714.9688 - val_mae: 529.2183\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1438978.2500 - mae: 969.6100 - val_loss: 501841.2812 - val_mae: 536.0533\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1453003.1250 - mae: 976.2446 - val_loss: 497410.7500 - val_mae: 528.9309\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1453839.1250 - mae: 976.6168 - val_loss: 493238.6562 - val_mae: 526.7553\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1453454.1250 - mae: 978.4185 - val_loss: 488888.6250 - val_mae: 520.1935\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1437104.3750 - mae: 968.8583 - val_loss: 496250.9688 - val_mae: 527.8508\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1458036.3750 - mae: 978.2078 - val_loss: 496793.9688 - val_mae: 528.3475\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1439633.5000 - mae: 973.2240 - val_loss: 500870.2812 - val_mae: 534.9008\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1465135.3750 - mae: 979.8340 - val_loss: 525240.6875 - val_mae: 556.2811\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1439219.0000 - mae: 971.3611 - val_loss: 496337.4688 - val_mae: 527.9153\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1433299.7500 - mae: 970.8821 - val_loss: 508436.4062 - val_mae: 538.5288\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1448701.0000 - mae: 974.2112 - val_loss: 512499.3438 - val_mae: 545.0797\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1432186.7500 - mae: 968.5899 - val_loss: 491864.9062 - val_mae: 525.4498\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1461302.2500 - mae: 980.2015 - val_loss: 508005.1562 - val_mae: 538.3750\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1465520.0000 - mae: 981.5382 - val_loss: 499821.6250 - val_mae: 534.1663\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1456699.8750 - mae: 977.2285 - val_loss: 491403.5000 - val_mae: 525.0106\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1438625.7500 - mae: 971.2437 - val_loss: 491249.2812 - val_mae: 524.8636\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1446807.3750 - mae: 973.6849 - val_loss: 498460.7812 - val_mae: 528.4192\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1441115.7500 - mae: 970.1963 - val_loss: 490946.7500 - val_mae: 524.5753\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1449070.6250 - mae: 975.0434 - val_loss: 503019.4062 - val_mae: 535.1897\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 1436953.6250 - mae: 968.1820 - val_loss: 486472.4062 - val_mae: 517.8659\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1433323.6250 - mae: 971.3935 - val_loss: 498664.7500 - val_mae: 532.5881\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1426940.5000 - mae: 965.8183 - val_loss: 502484.1562 - val_mae: 534.1949\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1450622.1250 - mae: 975.4222 - val_loss: 498202.9062 - val_mae: 527.9136\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1460130.6250 - mae: 979.0462 - val_loss: 514490.7500 - val_mae: 545.5066\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1444761.1250 - mae: 973.1262 - val_loss: 489919.0000 - val_mae: 523.8416\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1442342.3750 - mae: 971.5550 - val_loss: 514103.0000 - val_mae: 544.6547\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1442076.7500 - mae: 972.3241 - val_loss: 489576.0312 - val_mae: 523.2672\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1447823.1250 - mae: 974.0462 - val_loss: 493466.4062 - val_mae: 525.4361\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1448706.1250 - mae: 975.8365 - val_loss: 509623.0938 - val_mae: 542.1866\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1447342.1250 - mae: 975.3046 - val_loss: 505282.7500 - val_mae: 535.3448\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1451622.5000 - mae: 977.0440 - val_loss: 509378.0938 - val_mae: 542.4573\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1446259.2500 - mae: 971.9684 - val_loss: 496859.1562 - val_mae: 526.8868\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1443348.1250 - mae: 973.1528 - val_loss: 488702.2812 - val_mae: 522.6790\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1436313.2500 - mae: 969.5925 - val_loss: 496655.6562 - val_mae: 530.6987\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1424367.6250 - mae: 965.7349 - val_loss: 508642.4688 - val_mae: 541.0329\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1447124.1250 - mae: 973.1103 - val_loss: 512521.3750 - val_mae: 543.2009\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1416425.1250 - mae: 962.5013 - val_loss: 500232.5000 - val_mae: 532.5796\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1452709.1250 - mae: 975.9943 - val_loss: 467612.2812 - val_mae: 502.4660\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1452120.2500 - mae: 976.2323 - val_loss: 495854.9688 - val_mae: 529.6954\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1443150.1250 - mae: 970.6121 - val_loss: 511962.6562 - val_mae: 543.1845\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1427983.1250 - mae: 967.3901 - val_loss: 507741.2188 - val_mae: 540.4483\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1437580.3750 - mae: 968.7114 - val_loss: 495430.6250 - val_mae: 529.5432\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1444317.3750 - mae: 972.2664 - val_loss: 491114.8750 - val_mae: 522.6973\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1445754.3750 - mae: 972.0967 - val_loss: 487037.8438 - val_mae: 521.0843\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1434038.8750 - mae: 967.8638 - val_loss: 478579.3438 - val_mae: 507.5464\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1440551.3750 - mae: 970.7657 - val_loss: 470568.0938 - val_mae: 507.7239\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1439478.8750 - mae: 971.9266 - val_loss: 486588.6250 - val_mae: 520.6531\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1427171.3750 - mae: 965.1307 - val_loss: 482409.9062 - val_mae: 518.1951\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1442106.0000 - mae: 973.6623 - val_loss: 490279.1562 - val_mae: 522.3943\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1431822.3750 - mae: 968.7930 - val_loss: 498229.5938 - val_mae: 530.6959\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1440386.7500 - mae: 970.2965 - val_loss: 481957.4062 - val_mae: 517.7582\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1440943.1250 - mae: 971.4697 - val_loss: 501913.4062 - val_mae: 532.4380\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1433205.2500 - mae: 966.1033 - val_loss: 481542.5312 - val_mae: 513.0840\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1444815.1250 - mae: 972.6991 - val_loss: 488796.7812 - val_mae: 520.7420\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1439030.3750 - mae: 970.8777 - val_loss: 493471.4062 - val_mae: 527.9394\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1421017.2500 - mae: 962.0618 - val_loss: 517471.5312 - val_mae: 548.7523\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1438505.8750 - mae: 969.2325 - val_loss: 489033.7500 - val_mae: 520.9518\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1445943.5000 - mae: 972.6245 - val_loss: 472681.2812 - val_mae: 503.6292\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1434143.2500 - mae: 967.3574 - val_loss: 504860.0938 - val_mae: 537.2758\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1431840.2500 - mae: 968.7643 - val_loss: 496658.7188 - val_mae: 528.9639\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1428383.0000 - mae: 965.0532 - val_loss: 484443.2500 - val_mae: 518.3394\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1436761.7500 - mae: 967.8035 - val_loss: 484261.5938 - val_mae: 517.9149\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1420972.3750 - mae: 961.7442 - val_loss: 488129.7188 - val_mae: 520.0834\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1419049.8750 - mae: 964.3832 - val_loss: 491310.6562 - val_mae: 521.6098\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1415062.3750 - mae: 962.7263 - val_loss: 499880.5000 - val_mae: 530.2753\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1411994.0000 - mae: 957.8271 - val_loss: 487713.9062 - val_mae: 519.9332\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1454710.8750 - mae: 977.9000 - val_loss: 479534.6562 - val_mae: 515.1638\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1429500.7500 - mae: 965.7015 - val_loss: 499417.1562 - val_mae: 529.8382\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1433939.5000 - mae: 967.5567 - val_loss: 495283.0312 - val_mae: 527.6621\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1428326.3750 - mae: 964.2526 - val_loss: 487044.1562 - val_mae: 518.7888\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1425361.5000 - mae: 962.7570 - val_loss: 486927.4688 - val_mae: 518.9263\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1425440.1250 - mae: 964.9843 - val_loss: 494859.7500 - val_mae: 527.5115\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1428469.3750 - mae: 965.9167 - val_loss: 494705.6562 - val_mae: 527.3653\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1446758.1250 - mae: 975.1716 - val_loss: 478396.4688 - val_mae: 509.7592\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1425476.0000 - mae: 964.5553 - val_loss: 494433.7812 - val_mae: 527.3584\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1425586.5000 - mae: 963.6453 - val_loss: 466207.5938 - val_mae: 503.6610\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1429070.6250 - mae: 966.0212 - val_loss: 494097.5312 - val_mae: 526.7886\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1428762.6250 - mae: 964.9672 - val_loss: 505920.3438 - val_mae: 536.8388\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1430387.6250 - mae: 965.2114 - val_loss: 485726.9062 - val_mae: 517.7683\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1415363.1250 - mae: 962.2575 - val_loss: 461537.9688 - val_mae: 496.3847\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1445334.1250 - mae: 973.5446 - val_loss: 493451.7188 - val_mae: 525.9238\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 3s 86ms/step - loss: 1422365.8750 - mae: 962.7133 - val_loss: 488596.0312 - val_mae: 519.0010\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1418049.7500 - mae: 961.0659 - val_loss: 481157.7812 - val_mae: 515.1605\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1429901.2500 - mae: 965.0121 - val_loss: 501080.1562 - val_mae: 534.5022\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1438341.8750 - mae: 970.9203 - val_loss: 476812.2188 - val_mae: 508.4534\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1452380.1250 - mae: 977.6208 - val_loss: 484645.9062 - val_mae: 516.4722\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1431544.2500 - mae: 967.5524 - val_loss: 492569.0938 - val_mae: 525.3358\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1423204.6250 - mae: 963.6018 - val_loss: 476467.5000 - val_mae: 512.6802\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1423904.1250 - mae: 963.9756 - val_loss: 492264.1250 - val_mae: 525.0454\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1427932.3750 - mae: 964.8767 - val_loss: 492083.4688 - val_mae: 524.6215\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1437545.0000 - mae: 968.9324 - val_loss: 467959.9062 - val_mae: 503.2375\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1434597.1250 - mae: 967.3624 - val_loss: 491809.3750 - val_mae: 524.6121\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1433315.0000 - mae: 968.2248 - val_loss: 483627.6250 - val_mae: 515.7371\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1417215.5000 - mae: 960.4207 - val_loss: 511461.9688 - val_mae: 543.2490\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1436855.6250 - mae: 969.1222 - val_loss: 467402.0000 - val_mae: 502.9338\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1432977.0000 - mae: 967.2528 - val_loss: 495131.3750 - val_mae: 525.7783\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1415208.6250 - mae: 962.2586 - val_loss: 478971.7188 - val_mae: 508.4555\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1418740.8750 - mae: 963.8391 - val_loss: 478913.7500 - val_mae: 512.9778\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1418543.1250 - mae: 961.7051 - val_loss: 502686.9062 - val_mae: 534.0718\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1423775.1250 - mae: 964.5492 - val_loss: 486568.0312 - val_mae: 520.8554\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1420337.1250 - mae: 963.6025 - val_loss: 478497.3438 - val_mae: 512.8238\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1439556.1250 - mae: 968.8350 - val_loss: 470343.0000 - val_mae: 503.9499\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1423188.5000 - mae: 963.2767 - val_loss: 462242.3438 - val_mae: 499.4575\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1405922.5000 - mae: 955.2545 - val_loss: 478051.8438 - val_mae: 512.3892\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 4s 111ms/step - loss: 1415765.1250 - mae: 958.9399 - val_loss: 489802.8750 - val_mae: 522.4434\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1424006.0000 - mae: 962.1052 - val_loss: 485688.9062 - val_mae: 520.2637\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1409494.6250 - mae: 960.4421 - val_loss: 485569.2812 - val_mae: 520.4016\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 1434424.1250 - mae: 967.0858 - val_loss: 485372.4688 - val_mae: 515.8712\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1408589.1250 - mae: 956.6573 - val_loss: 477281.1562 - val_mae: 511.3841\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1412739.2500 - mae: 958.1935 - val_loss: 485043.5938 - val_mae: 515.2994\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1408231.6250 - mae: 957.8798 - val_loss: 492881.5938 - val_mae: 523.8878\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1411323.7500 - mae: 956.1421 - val_loss: 500711.7500 - val_mae: 532.4734\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1413499.6250 - mae: 958.1428 - val_loss: 476692.5938 - val_mae: 510.8083\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1414932.7500 - mae: 958.9291 - val_loss: 475920.5938 - val_mae: 510.5786\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1402643.8750 - mae: 956.4180 - val_loss: 492247.2188 - val_mae: 523.0283\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1422676.1250 - mae: 962.7136 - val_loss: 476216.2500 - val_mae: 510.0887\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1427759.1250 - mae: 964.5027 - val_loss: 491969.2500 - val_mae: 523.0164\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1417095.2500 - mae: 960.1959 - val_loss: 495804.0938 - val_mae: 529.2895\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1429411.0000 - mae: 964.6725 - val_loss: 471792.0000 - val_mae: 503.2394\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1413798.0000 - mae: 958.1050 - val_loss: 471704.4062 - val_mae: 507.7605\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 1414661.0000 - mae: 958.3802 - val_loss: 471529.1562 - val_mae: 503.2314\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1399775.0000 - mae: 951.5892 - val_loss: 499113.7188 - val_mae: 530.4614\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1410900.7500 - mae: 956.9714 - val_loss: 498961.1562 - val_mae: 530.3176\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1423935.1250 - mae: 962.4592 - val_loss: 475096.2188 - val_mae: 509.4970\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1415743.5000 - mae: 960.1080 - val_loss: 478869.1562 - val_mae: 511.1029\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1433083.0000 - mae: 967.2377 - val_loss: 466174.9688 - val_mae: 499.8293\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1412919.3750 - mae: 961.9160 - val_loss: 474622.7812 - val_mae: 508.7782\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1415777.0000 - mae: 958.4419 - val_loss: 482339.7812 - val_mae: 512.4150\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1421306.0000 - mae: 962.8705 - val_loss: 494122.9062 - val_mae: 527.6989\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1410573.2500 - mae: 956.6936 - val_loss: 466273.6250 - val_mae: 499.8961\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1421322.3750 - mae: 960.4594 - val_loss: 489801.9062 - val_mae: 520.4306\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1416772.1250 - mae: 960.9366 - val_loss: 481790.6562 - val_mae: 516.5037\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1401615.6250 - mae: 951.7775 - val_loss: 489502.0312 - val_mae: 520.1423\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1410553.3750 - mae: 957.1779 - val_loss: 477511.7812 - val_mae: 509.5188\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1436584.6250 - mae: 969.7958 - val_loss: 485339.4062 - val_mae: 518.6641\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1392240.0000 - mae: 950.2154 - val_loss: 469321.6562 - val_mae: 500.7791\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1409163.8750 - mae: 958.5913 - val_loss: 461285.8438 - val_mae: 492.1858\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1416909.3750 - mae: 961.1587 - val_loss: 496737.7500 - val_mae: 528.9844\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1432463.7500 - mae: 966.6667 - val_loss: 476765.7812 - val_mae: 508.7862\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1409985.8750 - mae: 955.9167 - val_loss: 472733.6562 - val_mae: 507.1732\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1421104.7500 - mae: 960.6654 - val_loss: 488343.0938 - val_mae: 519.5381\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1420907.2500 - mae: 959.7347 - val_loss: 468450.2188 - val_mae: 504.2909\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1407408.7500 - mae: 955.8405 - val_loss: 464392.6562 - val_mae: 498.0111\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1416089.3750 - mae: 960.8388 - val_loss: 491805.8438 - val_mae: 525.2427\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 1408614.8750 - mae: 957.0038 - val_loss: 479846.5938 - val_mae: 514.6182\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1413788.7500 - mae: 957.9163 - val_loss: 475763.0000 - val_mae: 508.0553\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1406373.7500 - mae: 953.2367 - val_loss: 483507.5938 - val_mae: 516.6392\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1400663.3750 - mae: 950.9167 - val_loss: 463646.9688 - val_mae: 497.0069\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 1420914.3750 - mae: 960.8331 - val_loss: 491046.5938 - val_mae: 524.5195\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1408004.3750 - mae: 956.9112 - val_loss: 475177.5312 - val_mae: 507.4786\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 8s 225ms/step - loss: 1409843.1250 - mae: 956.5645 - val_loss: 471122.8438 - val_mae: 505.5826\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1412244.7500 - mae: 956.5750 - val_loss: 482709.8750 - val_mae: 515.3541\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1418941.5000 - mae: 961.0211 - val_loss: 482610.5000 - val_mae: 515.7703\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1425659.6250 - mae: 963.5710 - val_loss: 474586.6562 - val_mae: 506.8962\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1411667.5000 - mae: 956.8288 - val_loss: 482283.0312 - val_mae: 515.1963\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1410733.0000 - mae: 958.2936 - val_loss: 474313.4688 - val_mae: 506.8827\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1383343.6250 - mae: 948.4902 - val_loss: 470212.7188 - val_mae: 504.4256\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1407601.5000 - mae: 955.3968 - val_loss: 466131.7812 - val_mae: 497.5839\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1407433.3750 - mae: 953.4749 - val_loss: 485613.5312 - val_mae: 516.6477\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1398425.2500 - mae: 953.0639 - val_loss: 473729.7812 - val_mae: 506.3067\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1414694.3750 - mae: 961.7367 - val_loss: 485314.5312 - val_mae: 516.3583\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1406289.6250 - mae: 953.6250 - val_loss: 485191.3750 - val_mae: 516.4961\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1402598.7500 - mae: 954.6027 - val_loss: 488967.5312 - val_mae: 523.0488\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1394189.1250 - mae: 952.2667 - val_loss: 492667.1562 - val_mae: 524.0922\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1390265.1250 - mae: 950.0608 - val_loss: 465126.9688 - val_mae: 496.5732\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1413626.2500 - mae: 958.6147 - val_loss: 484545.6250 - val_mae: 515.3558\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1410635.7500 - mae: 958.3315 - val_loss: 480521.6250 - val_mae: 513.7413\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1423991.5000 - mae: 963.4584 - val_loss: 468609.5312 - val_mae: 502.8341\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1382968.5000 - mae: 949.2332 - val_loss: 468438.7812 - val_mae: 502.4072\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1408043.1250 - mae: 955.9997 - val_loss: 480098.5938 - val_mae: 513.5870\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1421501.6250 - mae: 959.8346 - val_loss: 464249.7812 - val_mae: 500.3660\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1413028.6250 - mae: 956.7412 - val_loss: 487531.2188 - val_mae: 520.8996\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1414503.0000 - mae: 957.4468 - val_loss: 487424.4688 - val_mae: 521.3134\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1375160.5000 - mae: 942.8056 - val_loss: 455991.5938 - val_mae: 491.2022\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1414268.6250 - mae: 958.9310 - val_loss: 475433.1250 - val_mae: 510.8278\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1404957.0000 - mae: 954.1461 - val_loss: 471360.5938 - val_mae: 503.7039\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1410770.7500 - mae: 957.0806 - val_loss: 455564.9062 - val_mae: 490.7677\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1426166.8750 - mae: 963.5967 - val_loss: 482822.5938 - val_mae: 514.4561\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1418017.7500 - mae: 958.2989 - val_loss: 463089.5312 - val_mae: 499.2052\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1386136.2500 - mae: 947.3211 - val_loss: 474671.5312 - val_mae: 509.8236\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1407145.8750 - mae: 955.1336 - val_loss: 482329.7188 - val_mae: 513.4599\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1401889.0000 - mae: 952.9448 - val_loss: 481514.1562 - val_mae: 512.6699\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1404780.1250 - mae: 954.1050 - val_loss: 458628.0312 - val_mae: 491.9318\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1396570.8750 - mae: 949.9473 - val_loss: 474081.9062 - val_mae: 509.2450\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1396921.1250 - mae: 951.9370 - val_loss: 462205.0000 - val_mae: 498.0604\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1406908.7500 - mae: 954.6166 - val_loss: 485493.9688 - val_mae: 519.7177\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1386404.7500 - mae: 945.6721 - val_loss: 469781.0000 - val_mae: 502.3919\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1382578.7500 - mae: 944.9536 - val_loss: 481291.6562 - val_mae: 512.4482\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1384159.7500 - mae: 945.9766 - val_loss: 473405.1562 - val_mae: 504.1386\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1404689.7500 - mae: 953.5395 - val_loss: 480994.5938 - val_mae: 512.1581\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1406197.2500 - mae: 953.4667 - val_loss: 461386.2500 - val_mae: 497.7547\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1392810.7500 - mae: 947.1992 - val_loss: 457376.5312 - val_mae: 490.9164\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1394015.0000 - mae: 953.7569 - val_loss: 472800.5000 - val_mae: 503.2796\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1399996.0000 - mae: 949.4207 - val_loss: 464237.5938 - val_mae: 494.0399\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1402890.1250 - mae: 953.6387 - val_loss: 464716.5000 - val_mae: 499.2069\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 1386279.7500 - mae: 943.2211 - val_loss: 480128.4688 - val_mae: 511.5714\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1374008.0000 - mae: 940.5537 - val_loss: 468315.9688 - val_mae: 500.6726\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1404036.3750 - mae: 954.4016 - val_loss: 487577.5938 - val_mae: 519.7339\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1398317.0000 - mae: 953.1236 - val_loss: 452455.3438 - val_mae: 487.5894\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1396209.8750 - mae: 948.7582 - val_loss: 475615.1250 - val_mae: 508.6840\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1406467.5000 - mae: 952.6090 - val_loss: 459977.0000 - val_mae: 491.3603\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1387625.1250 - mae: 946.9420 - val_loss: 475321.6250 - val_mae: 508.3954\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1383416.2500 - mae: 944.6638 - val_loss: 467441.4062 - val_mae: 499.7984\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1392830.6250 - mae: 948.3177 - val_loss: 486652.0938 - val_mae: 518.5823\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1384133.6250 - mae: 947.9426 - val_loss: 463272.9688 - val_mae: 497.7589\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1398933.2500 - mae: 951.1802 - val_loss: 466365.2500 - val_mae: 498.9990\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1378535.0000 - mae: 941.2243 - val_loss: 443604.9688 - val_mae: 477.9799\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1408781.8750 - mae: 955.2056 - val_loss: 478353.6250 - val_mae: 509.8337\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1387680.8750 - mae: 946.6202 - val_loss: 454960.5000 - val_mae: 488.4496\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1365112.0000 - mae: 936.3561 - val_loss: 474163.2500 - val_mae: 507.5154\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1398581.8750 - mae: 950.9800 - val_loss: 470204.0000 - val_mae: 500.9537\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1407848.6250 - mae: 954.1583 - val_loss: 473845.8438 - val_mae: 506.9419\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1394144.8750 - mae: 950.2250 - val_loss: 465994.9062 - val_mae: 498.3492\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1386497.1250 - mae: 945.5547 - val_loss: 473573.8438 - val_mae: 506.9343\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1388987.5000 - mae: 948.0690 - val_loss: 461835.2188 - val_mae: 496.3126\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1400631.0000 - mae: 951.3314 - val_loss: 465543.2812 - val_mae: 497.6353\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1392782.2500 - mae: 949.6382 - val_loss: 473134.5312 - val_mae: 506.5009\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1370874.5000 - mae: 940.0259 - val_loss: 465276.6562 - val_mae: 497.6282\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1399772.7500 - mae: 953.5173 - val_loss: 468384.2188 - val_mae: 499.1495\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1384002.3750 - mae: 944.9365 - val_loss: 464966.3438 - val_mae: 497.0553\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1401080.3750 - mae: 951.9167 - val_loss: 476438.0938 - val_mae: 507.9517\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1373835.8750 - mae: 939.1484 - val_loss: 480042.0000 - val_mae: 513.6606\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1390028.5000 - mae: 945.9833 - val_loss: 472233.8750 - val_mae: 505.3495\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1382227.5000 - mae: 943.5891 - val_loss: 463773.8438 - val_mae: 496.3956\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1390653.2500 - mae: 945.9904 - val_loss: 460399.5938 - val_mae: 494.8642\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1387115.3750 - mae: 946.3008 - val_loss: 471838.0938 - val_mae: 505.4811\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1392828.5000 - mae: 946.6280 - val_loss: 452407.6562 - val_mae: 485.5683\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1356187.5000 - mae: 933.9287 - val_loss: 467642.4062 - val_mae: 502.8827\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1382949.0000 - mae: 943.3896 - val_loss: 475269.8438 - val_mae: 506.8004\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1397299.3750 - mae: 949.9702 - val_loss: 463574.3438 - val_mae: 496.1764\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1378838.2500 - mae: 941.9768 - val_loss: 478753.7812 - val_mae: 512.9299\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1370680.1250 - mae: 937.5292 - val_loss: 478624.8438 - val_mae: 513.0666\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1392100.1250 - mae: 946.5583 - val_loss: 459242.7188 - val_mae: 493.4320\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1377292.2500 - mae: 940.1886 - val_loss: 482172.8750 - val_mae: 514.2455\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1401032.6250 - mae: 953.6560 - val_loss: 466624.6562 - val_mae: 501.8697\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1365471.8750 - mae: 935.7367 - val_loss: 466479.6562 - val_mae: 501.7252\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1380318.7500 - mae: 942.6823 - val_loss: 454770.0938 - val_mae: 490.5396\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 1396764.8750 - mae: 948.9240 - val_loss: 458527.7500 - val_mae: 492.7071\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1380494.2500 - mae: 944.8429 - val_loss: 458383.0312 - val_mae: 492.5603\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1376840.3750 - mae: 942.7024 - val_loss: 454378.0312 - val_mae: 490.6644\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1386643.3750 - mae: 943.7302 - val_loss: 481165.0312 - val_mae: 513.7911\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1371779.2500 - mae: 940.3343 - val_loss: 461834.0938 - val_mae: 494.1570\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1376410.0000 - mae: 942.4324 - val_loss: 476954.8438 - val_mae: 510.9099\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1371850.8750 - mae: 940.4014 - val_loss: 446147.8438 - val_mae: 481.0778\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1394779.0000 - mae: 950.4754 - val_loss: 480536.9062 - val_mae: 512.6524\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1379490.8750 - mae: 941.6458 - val_loss: 461244.5000 - val_mae: 493.2972\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1395141.2500 - mae: 949.5246 - val_loss: 461121.2812 - val_mae: 493.4352\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1397768.3750 - mae: 950.2308 - val_loss: 480108.3750 - val_mae: 512.4980\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1387351.3750 - mae: 944.3453 - val_loss: 460852.5312 - val_mae: 493.4260\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1383817.7500 - mae: 944.9194 - val_loss: 464467.4062 - val_mae: 499.9794\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1363976.8750 - mae: 935.0674 - val_loss: 468141.7188 - val_mae: 501.0208\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1387017.1250 - mae: 946.8680 - val_loss: 456548.7500 - val_mae: 490.9579\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1380141.7500 - mae: 942.4956 - val_loss: 448767.0938 - val_mae: 481.8048\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1387181.6250 - mae: 946.7842 - val_loss: 463854.2188 - val_mae: 498.8383\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1389381.0000 - mae: 945.4543 - val_loss: 479052.6250 - val_mae: 511.2027\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1370629.0000 - mae: 940.2938 - val_loss: 467452.2500 - val_mae: 500.8601\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1371569.8750 - mae: 937.9649 - val_loss: 467289.3438 - val_mae: 500.4333\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1381569.2500 - mae: 944.1617 - val_loss: 451318.5312 - val_mae: 483.0269\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1382349.7500 - mae: 944.6129 - val_loss: 466999.4062 - val_mae: 500.1436\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1382165.6250 - mae: 942.5698 - val_loss: 466870.1562 - val_mae: 500.2787\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1376818.5000 - mae: 942.0972 - val_loss: 455238.9062 - val_mae: 489.0941\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1386372.3750 - mae: 946.3224 - val_loss: 458980.9062 - val_mae: 491.2616\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1372426.8750 - mae: 938.0336 - val_loss: 466436.7812 - val_mae: 499.8453\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1377941.8750 - mae: 940.5936 - val_loss: 450967.1250 - val_mae: 486.9120\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1380953.3750 - mae: 944.9750 - val_loss: 458556.9062 - val_mae: 490.8299\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1393219.1250 - mae: 947.1060 - val_loss: 443108.2812 - val_mae: 478.1723\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 1377145.6250 - mae: 942.0284 - val_loss: 465859.1562 - val_mae: 499.2672\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1384983.6250 - mae: 945.9257 - val_loss: 461974.2812 - val_mae: 492.1418\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1362340.0000 - mae: 933.3624 - val_loss: 454106.3438 - val_mae: 487.9350\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1372772.2500 - mae: 940.1329 - val_loss: 465409.3750 - val_mae: 498.5515\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1378685.7500 - mae: 942.0240 - val_loss: 464630.8438 - val_mae: 498.0427\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1398352.5000 - mae: 950.7133 - val_loss: 442263.0938 - val_mae: 477.0232\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1382512.0000 - mae: 943.4676 - val_loss: 468836.0312 - val_mae: 500.1474\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1380366.3750 - mae: 943.4580 - val_loss: 464833.2500 - val_mae: 497.9734\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1371353.6250 - mae: 940.0129 - val_loss: 434293.2500 - val_mae: 468.1378\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1384822.1250 - mae: 943.7586 - val_loss: 453133.9688 - val_mae: 487.2027\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 1382153.2500 - mae: 944.4259 - val_loss: 464413.2500 - val_mae: 497.8170\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1379513.8750 - mae: 941.7407 - val_loss: 452835.5938 - val_mae: 486.6311\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1367372.0000 - mae: 936.4166 - val_loss: 445152.5000 - val_mae: 478.0392\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1380437.0000 - mae: 941.4092 - val_loss: 456425.7812 - val_mae: 488.6541\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 1360950.5000 - mae: 935.0564 - val_loss: 471376.9062 - val_mae: 505.6877\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1358258.8750 - mae: 933.8226 - val_loss: 433348.1562 - val_mae: 467.1273\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1392182.7500 - mae: 948.4349 - val_loss: 440744.4062 - val_mae: 475.4288\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1367162.7500 - mae: 936.3865 - val_loss: 463376.1250 - val_mae: 496.2421\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1389115.7500 - mae: 947.9753 - val_loss: 463248.2812 - val_mae: 496.3794\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1376116.8750 - mae: 940.6728 - val_loss: 455565.0000 - val_mae: 487.5066\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1388698.0000 - mae: 946.5694 - val_loss: 470498.5000 - val_mae: 504.8185\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1359802.0000 - mae: 934.2441 - val_loss: 447125.2500 - val_mae: 478.4007\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1370093.3750 - mae: 938.8388 - val_loss: 451304.5938 - val_mae: 485.3216\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1349446.2500 - mae: 929.9279 - val_loss: 466396.6562 - val_mae: 497.9693\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1378388.0000 - mae: 940.5060 - val_loss: 466238.7812 - val_mae: 497.5441\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1381893.8750 - mae: 941.9867 - val_loss: 469783.8750 - val_mae: 504.3772\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1375610.8750 - mae: 941.8683 - val_loss: 450742.2500 - val_mae: 484.7420\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1367881.6250 - mae: 937.3091 - val_loss: 458285.4688 - val_mae: 488.3797\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1369968.0000 - mae: 937.8692 - val_loss: 435442.9688 - val_mae: 467.2786\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1355062.2500 - mae: 935.4764 - val_loss: 454169.0000 - val_mae: 486.3394\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1396527.3750 - mae: 949.9374 - val_loss: 457860.9062 - val_mae: 487.9449\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1371164.7500 - mae: 939.4523 - val_loss: 457538.0938 - val_mae: 492.4667\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1350244.7500 - mae: 928.0875 - val_loss: 472604.6562 - val_mae: 505.3918\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1380469.3750 - mae: 945.8030 - val_loss: 445901.0938 - val_mae: 481.4148\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1368314.3750 - mae: 934.7758 - val_loss: 449622.4062 - val_mae: 483.5855\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1377830.6250 - mae: 941.8323 - val_loss: 460822.7188 - val_mae: 494.1975\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1369062.8750 - mae: 936.8000 - val_loss: 449352.8438 - val_mae: 483.5741\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1390142.5000 - mae: 947.6322 - val_loss: 479368.8438 - val_mae: 513.3973\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1388579.2500 - mae: 945.9583 - val_loss: 460405.8438 - val_mae: 494.0435\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1363512.8750 - mae: 935.6468 - val_loss: 452745.7188 - val_mae: 484.6064\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1361641.7500 - mae: 932.4537 - val_loss: 478909.7500 - val_mae: 512.6816\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1364444.6250 - mae: 934.8511 - val_loss: 459952.5938 - val_mae: 493.0484\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1358259.2500 - mae: 931.7222 - val_loss: 448698.2188 - val_mae: 477.7589\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 1343038.2500 - mae: 924.6739 - val_loss: 459696.5000 - val_mae: 493.3250\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1374394.7500 - mae: 938.8662 - val_loss: 459529.7188 - val_mae: 492.6195\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1380274.6250 - mae: 942.6756 - val_loss: 459386.2812 - val_mae: 492.4739\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1355690.8750 - mae: 931.3323 - val_loss: 455606.4688 - val_mae: 485.6292\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1368120.6250 - mae: 935.2142 - val_loss: 470418.2812 - val_mae: 503.2240\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1363499.3750 - mae: 934.4928 - val_loss: 466423.7500 - val_mae: 500.7664\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1382871.8750 - mae: 943.2079 - val_loss: 458802.1562 - val_mae: 491.6118\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1361022.2500 - mae: 934.1826 - val_loss: 443542.9062 - val_mae: 478.9593\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1367281.7500 - mae: 935.9067 - val_loss: 450465.0000 - val_mae: 483.0726\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1367845.0000 - mae: 937.9016 - val_loss: 469704.3750 - val_mae: 502.7834\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1377431.8750 - mae: 940.2112 - val_loss: 469510.1562 - val_mae: 501.5133\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1368272.0000 - mae: 935.7975 - val_loss: 450654.5000 - val_mae: 482.4439\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1357193.8750 - mae: 931.4617 - val_loss: 457975.8438 - val_mae: 491.3088\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 1371729.1250 - mae: 937.7083 - val_loss: 446542.0000 - val_mae: 480.1210\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 1363308.5000 - mae: 935.0929 - val_loss: 457668.4062 - val_mae: 490.4574\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1378694.7500 - mae: 940.4606 - val_loss: 457533.9688 - val_mae: 490.5896\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 1351410.6250 - mae: 929.9045 - val_loss: 446148.1562 - val_mae: 480.2490\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1373219.2500 - mae: 938.5121 - val_loss: 457264.0938 - val_mae: 490.5840\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1376411.0000 - mae: 941.5150 - val_loss: 442261.1562 - val_mae: 473.2587\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1363749.3750 - mae: 936.4547 - val_loss: 464407.8750 - val_mae: 499.0194\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1368915.7500 - mae: 938.2896 - val_loss: 449394.4062 - val_mae: 481.1361\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1380825.1250 - mae: 944.8126 - val_loss: 456689.2500 - val_mae: 489.9978\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1348916.0000 - mae: 930.2985 - val_loss: 463960.7500 - val_mae: 498.3010\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1346809.0000 - mae: 925.6832 - val_loss: 437749.7500 - val_mae: 470.5021\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1364747.6250 - mae: 933.6528 - val_loss: 452456.5000 - val_mae: 487.8201\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1355624.0000 - mae: 930.6182 - val_loss: 430065.8438 - val_mae: 461.4890\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1364946.0000 - mae: 936.2252 - val_loss: 455996.4688 - val_mae: 489.5606\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1347221.6250 - mae: 927.3320 - val_loss: 437200.6562 - val_mae: 469.6483\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1330133.8750 - mae: 923.3122 - val_loss: 451869.5000 - val_mae: 486.6773\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1352260.1250 - mae: 930.0601 - val_loss: 448155.7188 - val_mae: 480.1175\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 1351001.5000 - mae: 928.9010 - val_loss: 474028.6562 - val_mae: 507.6279\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1350411.6250 - mae: 928.8520 - val_loss: 436038.0000 - val_mae: 468.9837\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1359905.0000 - mae: 931.5120 - val_loss: 458937.1562 - val_mae: 489.8808\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1357664.5000 - mae: 932.8896 - val_loss: 435755.5312 - val_mae: 468.4122\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1371968.5000 - mae: 937.5225 - val_loss: 443641.5000 - val_mae: 477.3616\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1353265.2500 - mae: 929.4824 - val_loss: 458522.5938 - val_mae: 489.7284\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1352844.6250 - mae: 929.8090 - val_loss: 450754.8750 - val_mae: 485.8018\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1338410.1250 - mae: 926.1497 - val_loss: 443229.3750 - val_mae: 476.9296\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1341819.2500 - mae: 924.0500 - val_loss: 458099.2500 - val_mae: 489.2960\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1357027.0000 - mae: 930.9199 - val_loss: 461514.1562 - val_mae: 495.5686\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1373081.0000 - mae: 938.6735 - val_loss: 457813.5000 - val_mae: 489.0039\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1353724.8750 - mae: 929.3600 - val_loss: 427678.5938 - val_mae: 463.8376\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 8s 225ms/step - loss: 1326537.8750 - mae: 919.7730 - val_loss: 423988.4688 - val_mae: 456.9991\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1337693.2500 - mae: 921.2278 - val_loss: 442403.8750 - val_mae: 476.0634\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1358279.1250 - mae: 932.0506 - val_loss: 438720.0938 - val_mae: 469.5026\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1362823.7500 - mae: 933.8846 - val_loss: 434760.7188 - val_mae: 467.0435\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1324539.3750 - mae: 919.3473 - val_loss: 438447.7812 - val_mae: 469.2126\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1346417.0000 - mae: 929.3856 - val_loss: 445656.6250 - val_mae: 477.2361\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1362516.5000 - mae: 932.9942 - val_loss: 445528.0312 - val_mae: 477.3731\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1354478.7500 - mae: 929.8506 - val_loss: 456555.9688 - val_mae: 487.9885\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1331968.5000 - mae: 920.3645 - val_loss: 459941.6562 - val_mae: 493.9796\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1348753.0000 - mae: 927.1453 - val_loss: 448648.8438 - val_mae: 483.3572\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1352442.2500 - mae: 932.2662 - val_loss: 452314.2188 - val_mae: 485.2406\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1351564.0000 - mae: 928.5834 - val_loss: 441037.0312 - val_mae: 474.8979\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1359151.0000 - mae: 931.7828 - val_loss: 448226.6250 - val_mae: 482.9203\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1359962.7500 - mae: 935.3087 - val_loss: 436944.4688 - val_mae: 472.2931\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1358906.3750 - mae: 933.6439 - val_loss: 429476.6562 - val_mae: 463.6994\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1344037.7500 - mae: 928.5977 - val_loss: 436945.9688 - val_mae: 467.3371\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1345350.8750 - mae: 925.1411 - val_loss: 451476.5000 - val_mae: 484.6494\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1360712.6250 - mae: 931.9359 - val_loss: 462454.2500 - val_mae: 494.7018\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1350213.1250 - mae: 928.3524 - val_loss: 451193.7812 - val_mae: 484.3576\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1337384.0000 - mae: 922.3781 - val_loss: 451056.4062 - val_mae: 484.2158\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1363493.7500 - mae: 935.0216 - val_loss: 436266.0312 - val_mae: 466.6090\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1370472.8750 - mae: 936.7853 - val_loss: 461888.7188 - val_mae: 494.4032\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1351705.2500 - mae: 927.2644 - val_loss: 428400.8438 - val_mae: 462.5378\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1359059.2500 - mae: 932.9351 - val_loss: 446682.0000 - val_mae: 481.3184\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 1355500.2500 - mae: 930.2617 - val_loss: 446550.9062 - val_mae: 481.5067\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1341662.8750 - mae: 923.0443 - val_loss: 446404.7500 - val_mae: 481.1558\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1363217.0000 - mae: 932.7628 - val_loss: 438957.0938 - val_mae: 472.3623\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1340958.8750 - mae: 924.1120 - val_loss: 431524.0938 - val_mae: 463.8460\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1366487.7500 - mae: 935.5176 - val_loss: 427589.7812 - val_mae: 461.7441\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1332300.6250 - mae: 923.3735 - val_loss: 460736.5938 - val_mae: 493.3772\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1375201.3750 - mae: 939.3634 - val_loss: 445999.7812 - val_mae: 476.1454\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1352772.3750 - mae: 929.5406 - val_loss: 438268.7812 - val_mae: 472.0128\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1345098.5000 - mae: 925.2218 - val_loss: 456523.0312 - val_mae: 491.6364\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1343968.1250 - mae: 924.7534 - val_loss: 449082.0938 - val_mae: 482.6056\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1360716.3750 - mae: 934.4235 - val_loss: 460018.5938 - val_mae: 493.0127\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1322311.3750 - mae: 919.3900 - val_loss: 423167.7812 - val_mae: 454.8405\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1348351.7500 - mae: 926.5279 - val_loss: 444883.3750 - val_mae: 480.8424\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1340892.3750 - mae: 924.8292 - val_loss: 455818.6250 - val_mae: 491.4838\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1322602.7500 - mae: 920.4575 - val_loss: 433842.2500 - val_mae: 465.3477\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1331724.3750 - mae: 920.8384 - val_loss: 437181.5938 - val_mae: 471.4596\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1328684.2500 - mae: 919.9641 - val_loss: 448103.1250 - val_mae: 481.8696\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1340966.8750 - mae: 922.2656 - val_loss: 433125.7188 - val_mae: 469.5174\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1333779.3750 - mae: 921.5615 - val_loss: 436773.4688 - val_mae: 471.2516\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1356283.8750 - mae: 930.3669 - val_loss: 458745.6562 - val_mae: 492.5841\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1349935.0000 - mae: 927.0365 - val_loss: 436502.8438 - val_mae: 471.1135\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1353273.1250 - mae: 930.2716 - val_loss: 429112.7500 - val_mae: 462.5963\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1324471.0000 - mae: 916.4905 - val_loss: 450436.2500 - val_mae: 483.1209\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1346845.7500 - mae: 926.8159 - val_loss: 447132.0000 - val_mae: 481.3850\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1354776.8750 - mae: 929.9651 - val_loss: 454250.6562 - val_mae: 490.1766\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1337790.7500 - mae: 922.6378 - val_loss: 421008.6562 - val_mae: 458.2563\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1334154.2500 - mae: 922.0632 - val_loss: 439481.0938 - val_mae: 472.9308\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1348882.8750 - mae: 929.0728 - val_loss: 450377.6250 - val_mae: 483.7320\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1334687.0000 - mae: 921.2444 - val_loss: 446442.1562 - val_mae: 481.2321\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1323245.3750 - mae: 913.5086 - val_loss: 450091.4688 - val_mae: 483.3825\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1319986.7500 - mae: 915.7217 - val_loss: 449949.2188 - val_mae: 483.1196\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 1337951.6250 - mae: 919.5543 - val_loss: 435011.9062 - val_mae: 470.3515\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1349305.8750 - mae: 926.8807 - val_loss: 427650.2500 - val_mae: 461.8348\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1322961.6250 - mae: 915.1245 - val_loss: 438520.2500 - val_mae: 472.2433\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1326229.1250 - mae: 918.1708 - val_loss: 438399.7812 - val_mae: 472.5302\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1342036.3750 - mae: 926.3907 - val_loss: 449249.3438 - val_mae: 482.5845\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1336103.3750 - mae: 921.5363 - val_loss: 427120.5938 - val_mae: 461.5584\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1338688.2500 - mae: 923.6478 - val_loss: 434209.5938 - val_mae: 470.1056\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1353896.8750 - mae: 929.7131 - val_loss: 441289.3438 - val_mae: 478.6445\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1349767.3750 - mae: 927.3080 - val_loss: 455918.9062 - val_mae: 491.2401\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1336095.8750 - mae: 920.3614 - val_loss: 444788.8438 - val_mae: 480.5241\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1345914.2500 - mae: 926.2454 - val_loss: 444651.4062 - val_mae: 480.4492\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1352447.8750 - mae: 928.3365 - val_loss: 426326.3438 - val_mae: 461.2935\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1336831.5000 - mae: 920.2045 - val_loss: 415212.2812 - val_mae: 450.5915\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1333085.5000 - mae: 919.0431 - val_loss: 444235.6250 - val_mae: 480.2224\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1336199.0000 - mae: 921.6623 - val_loss: 433128.6562 - val_mae: 469.6681\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1314350.1250 - mae: 914.9908 - val_loss: 432994.1562 - val_mae: 469.5929\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1341192.6250 - mae: 925.1444 - val_loss: 436627.7812 - val_mae: 471.4114\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1329944.8750 - mae: 918.3630 - val_loss: 443687.7500 - val_mae: 480.0556\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1330049.1250 - mae: 916.5082 - val_loss: 450728.7500 - val_mae: 488.1625\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 1327895.8750 - mae: 917.9954 - val_loss: 439639.5938 - val_mae: 477.7402\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1334787.6250 - mae: 920.7498 - val_loss: 439500.6562 - val_mae: 477.6639\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1351786.5000 - mae: 930.0295 - val_loss: 443131.2812 - val_mae: 479.6189\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1334836.6250 - mae: 919.5999 - val_loss: 421098.4062 - val_mae: 458.3473\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1339963.0000 - mae: 922.7265 - val_loss: 435681.3438 - val_mae: 470.9052\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1323202.3750 - mae: 918.0037 - val_loss: 431779.8438 - val_mae: 468.9139\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1350230.2500 - mae: 929.7029 - val_loss: 432005.7500 - val_mae: 464.3418\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1335756.7500 - mae: 921.7324 - val_loss: 435270.7188 - val_mae: 470.5796\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1348771.2500 - mae: 926.2364 - val_loss: 442299.0312 - val_mae: 478.9579\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1332545.0000 - mae: 918.7586 - val_loss: 435005.6250 - val_mae: 470.6428\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 1344725.8750 - mae: 923.7845 - val_loss: 434867.2812 - val_mae: 470.4690\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1323508.0000 - mae: 916.4332 - val_loss: 430974.1562 - val_mae: 468.4622\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1319664.2500 - mae: 912.6628 - val_loss: 441758.0000 - val_mae: 478.8664\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 1339055.8750 - mae: 921.2274 - val_loss: 427313.2500 - val_mae: 461.7173\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1342750.1250 - mae: 924.4343 - val_loss: 434329.7500 - val_mae: 470.0956\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1327244.6250 - mae: 917.4177 - val_loss: 441343.5938 - val_mae: 478.5570\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1318582.6250 - mae: 913.7932 - val_loss: 437449.4688 - val_mae: 476.5346\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1338196.2500 - mae: 920.5441 - val_loss: 426407.4688 - val_mae: 465.9784\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1340237.2500 - mae: 922.1565 - val_loss: 451836.2812 - val_mae: 488.8944\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1317492.3750 - mae: 914.1340 - val_loss: 447934.1250 - val_mae: 486.7864\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1320266.2500 - mae: 915.0490 - val_loss: 426391.1562 - val_mae: 461.2332\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1320839.7500 - mae: 913.1764 - val_loss: 429633.0312 - val_mae: 467.6447\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1304398.1250 - mae: 909.8346 - val_loss: 422373.9062 - val_mae: 459.1268\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1335968.6250 - mae: 922.6880 - val_loss: 429366.8438 - val_mae: 467.4425\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1341936.7500 - mae: 922.9792 - val_loss: 451003.9688 - val_mae: 488.5569\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1303488.6250 - mae: 911.4690 - val_loss: 432857.3750 - val_mae: 469.3360\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1337324.8750 - mae: 923.2867 - val_loss: 421848.3750 - val_mae: 458.8371\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1323124.2500 - mae: 914.5802 - val_loss: 439709.7812 - val_mae: 477.6934\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1326628.0000 - mae: 917.3883 - val_loss: 428702.5938 - val_mae: 467.1845\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1338422.3750 - mae: 921.9495 - val_loss: 421458.0000 - val_mae: 458.6217\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1336412.6250 - mae: 920.2907 - val_loss: 446411.8438 - val_mae: 485.9625\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1317610.0000 - mae: 912.2059 - val_loss: 432056.9062 - val_mae: 468.9911\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1317033.2500 - mae: 914.4319 - val_loss: 435672.8750 - val_mae: 470.9145\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1321001.0000 - mae: 913.4442 - val_loss: 442644.4688 - val_mae: 479.3490\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1326889.8750 - mae: 915.6215 - val_loss: 431654.7500 - val_mae: 468.7393\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1330837.5000 - mae: 919.5960 - val_loss: 442370.0000 - val_mae: 479.1920\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1330637.3750 - mae: 919.9376 - val_loss: 449329.0312 - val_mae: 487.5421\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1330584.8750 - mae: 916.6978 - val_loss: 449188.3438 - val_mae: 487.4995\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1330474.2500 - mae: 917.6663 - val_loss: 456138.9688 - val_mae: 495.8244\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1327234.3750 - mae: 918.5703 - val_loss: 438070.0938 - val_mae: 476.8127\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1327015.1250 - mae: 915.5994 - val_loss: 437935.4062 - val_mae: 476.7440\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1330488.7500 - mae: 918.2189 - val_loss: 430714.8750 - val_mae: 468.2309\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1323766.3750 - mae: 914.3593 - val_loss: 448488.4062 - val_mae: 487.0853\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1332826.0000 - mae: 919.7623 - val_loss: 426704.2188 - val_mae: 466.0548\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1332458.7500 - mae: 916.8835 - val_loss: 455288.1562 - val_mae: 495.3812\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1332748.1250 - mae: 919.8334 - val_loss: 419364.3750 - val_mae: 457.4701\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1309866.2500 - mae: 907.4641 - val_loss: 447932.5938 - val_mae: 486.7858\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1332113.0000 - mae: 918.5096 - val_loss: 436982.5938 - val_mae: 476.2316\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1322626.2500 - mae: 917.7310 - val_loss: 418972.2500 - val_mae: 457.2453\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1318889.2500 - mae: 912.0234 - val_loss: 422584.2500 - val_mae: 459.2234\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1330209.7500 - mae: 917.2302 - val_loss: 432836.6562 - val_mae: 474.0196\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1292608.3750 - mae: 901.1913 - val_loss: 425642.4062 - val_mae: 465.4773\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1316427.6250 - mae: 910.6578 - val_loss: 425511.2500 - val_mae: 465.4056\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1316017.7500 - mae: 912.0546 - val_loss: 436173.1562 - val_mae: 475.7515\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1311440.6250 - mae: 911.6066 - val_loss: 443091.5000 - val_mae: 484.1897\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1317767.5000 - mae: 913.7391 - val_loss: 435904.8438 - val_mae: 475.6696\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1320450.0000 - mae: 912.6237 - val_loss: 424984.4688 - val_mae: 465.1179\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1326172.5000 - mae: 917.3575 - val_loss: 428589.4688 - val_mae: 467.0769\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1309089.7500 - mae: 907.7557 - val_loss: 417677.8750 - val_mae: 456.5708\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1307622.3750 - mae: 906.7321 - val_loss: 435361.6562 - val_mae: 475.2823\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1312948.3750 - mae: 906.9971 - val_loss: 431489.9688 - val_mae: 473.1721\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1332985.6250 - mae: 918.7578 - val_loss: 452897.6562 - val_mae: 494.1086\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1328616.0000 - mae: 919.0016 - val_loss: 427927.5938 - val_mae: 466.7738\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1301936.1250 - mae: 907.6251 - val_loss: 445585.9062 - val_mae: 485.5096\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1337962.8750 - mae: 921.3924 - val_loss: 420633.2500 - val_mae: 458.1861\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1310638.1250 - mae: 911.0544 - val_loss: 420063.9062 - val_mae: 462.4364\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1330511.1250 - mae: 917.2256 - val_loss: 438144.7500 - val_mae: 476.7638\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1340646.0000 - mae: 922.1854 - val_loss: 434278.2500 - val_mae: 474.7268\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 1319030.3750 - mae: 914.1770 - val_loss: 419667.0000 - val_mae: 462.2178\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1307568.3750 - mae: 907.5560 - val_loss: 434007.5938 - val_mae: 474.5757\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1330929.7500 - mae: 920.2142 - val_loss: 430139.8750 - val_mae: 472.3860\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1322803.7500 - mae: 913.2330 - val_loss: 433734.8750 - val_mae: 474.3391\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1316866.2500 - mae: 910.6226 - val_loss: 426598.6250 - val_mae: 466.0779\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1320838.5000 - mae: 914.4363 - val_loss: 444196.1562 - val_mae: 484.6605\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1318859.1250 - mae: 912.2154 - val_loss: 422604.5938 - val_mae: 463.8134\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1304791.0000 - mae: 908.7688 - val_loss: 429472.6562 - val_mae: 472.1887\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1300913.6250 - mae: 904.9487 - val_loss: 443787.5312 - val_mae: 484.5271\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1309687.8750 - mae: 908.4386 - val_loss: 418334.8438 - val_mae: 456.9200\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1324097.3750 - mae: 915.9886 - val_loss: 411361.7188 - val_mae: 453.0454\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1326523.0000 - mae: 916.6998 - val_loss: 425669.6250 - val_mae: 465.4820\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1310008.1250 - mae: 912.0707 - val_loss: 414827.3438 - val_mae: 454.9312\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1308203.1250 - mae: 906.3695 - val_loss: 425402.4062 - val_mae: 465.2215\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1302859.6250 - mae: 906.7576 - val_loss: 432258.4062 - val_mae: 473.7137\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1318532.3750 - mae: 911.3777 - val_loss: 421423.5000 - val_mae: 463.1632\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1324464.1250 - mae: 914.8929 - val_loss: 431990.1562 - val_mae: 473.4463\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1321683.2500 - mae: 912.2609 - val_loss: 421160.8438 - val_mae: 463.0184\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1310379.8750 - mae: 908.2023 - val_loss: 428000.2500 - val_mae: 471.2637\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1303375.7500 - mae: 905.9691 - val_loss: 417176.4688 - val_mae: 460.7093\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1319482.0000 - mae: 913.7163 - val_loss: 424486.0938 - val_mae: 464.8324\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1324308.6250 - mae: 914.4416 - val_loss: 416778.4688 - val_mae: 455.9469\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1316397.7500 - mae: 909.8779 - val_loss: 424223.3750 - val_mae: 464.6879\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1319093.6250 - mae: 912.0725 - val_loss: 420375.0938 - val_mae: 462.5846\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1323064.2500 - mae: 913.4318 - val_loss: 417003.2500 - val_mae: 456.2428\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1303250.0000 - mae: 903.8397 - val_loss: 427063.0000 - val_mae: 470.7346\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1316668.6250 - mae: 912.3606 - val_loss: 423697.3750 - val_mae: 464.3984\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1307604.8750 - mae: 907.5522 - val_loss: 427276.2812 - val_mae: 466.1992\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1308111.2500 - mae: 905.7069 - val_loss: 409057.7188 - val_mae: 451.7435\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1309831.2500 - mae: 909.5903 - val_loss: 423299.4062 - val_mae: 464.0177\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1296422.0000 - mae: 900.6618 - val_loss: 447708.7500 - val_mae: 491.1505\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1306918.0000 - mae: 907.4741 - val_loss: 416103.9062 - val_mae: 455.7585\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1318963.1250 - mae: 911.8679 - val_loss: 429844.0312 - val_mae: 472.2391\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1299457.2500 - mae: 905.0309 - val_loss: 415350.0000 - val_mae: 459.6527\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1310108.8750 - mae: 908.3976 - val_loss: 433292.7500 - val_mae: 474.2987\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1311692.5000 - mae: 909.4592 - val_loss: 440090.8438 - val_mae: 482.6746\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1300100.0000 - mae: 907.0628 - val_loss: 439948.0938 - val_mae: 482.4165\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1320645.6250 - mae: 913.2773 - val_loss: 439807.4062 - val_mae: 482.1536\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1315648.8750 - mae: 910.0828 - val_loss: 418420.9062 - val_mae: 461.6900\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1305065.2500 - mae: 907.7596 - val_loss: 421998.7188 - val_mae: 463.6521\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1320928.3750 - mae: 913.3712 - val_loss: 425568.4062 - val_mae: 465.4161\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1311455.8750 - mae: 909.1042 - val_loss: 439257.9062 - val_mae: 481.8380\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1304789.5000 - mae: 905.1459 - val_loss: 400362.1250 - val_mae: 442.4840\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1304764.6250 - mae: 907.2203 - val_loss: 428373.0312 - val_mae: 471.4082\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1306869.5000 - mae: 906.0474 - val_loss: 413930.9688 - val_mae: 459.2403\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1295277.3750 - mae: 902.1226 - val_loss: 406891.3750 - val_mae: 450.5124\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1298302.0000 - mae: 900.5028 - val_loss: 399865.0938 - val_mae: 442.2071\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1305120.5000 - mae: 903.7346 - val_loss: 438449.3750 - val_mae: 481.5876\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1291524.3750 - mae: 899.4657 - val_loss: 417121.5938 - val_mae: 460.9970\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1317390.5000 - mae: 912.5391 - val_loss: 438170.2500 - val_mae: 481.2121\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1301067.7500 - mae: 904.3885 - val_loss: 431157.7812 - val_mae: 473.3676\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1328890.2500 - mae: 917.7994 - val_loss: 409834.7500 - val_mae: 452.3383\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1314366.8750 - mae: 908.9302 - val_loss: 427176.6562 - val_mae: 470.7301\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1309516.6250 - mae: 907.8005 - val_loss: 430751.0000 - val_mae: 472.9230\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1308918.1250 - mae: 907.4266 - val_loss: 416333.5938 - val_mae: 460.3397\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1300604.5000 - mae: 904.3921 - val_loss: 437352.9062 - val_mae: 480.7408\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1293723.0000 - mae: 901.0393 - val_loss: 433529.6562 - val_mae: 478.8785\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1281120.0000 - mae: 894.7974 - val_loss: 426529.6562 - val_mae: 470.8517\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 1287480.2500 - mae: 899.4316 - val_loss: 412647.8438 - val_mae: 453.8874\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 1297984.7500 - mae: 901.1262 - val_loss: 426255.9062 - val_mae: 470.2071\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1297138.0000 - mae: 904.3815 - val_loss: 436688.2500 - val_mae: 480.6101\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1277670.2500 - mae: 896.7565 - val_loss: 436554.2812 - val_mae: 480.5356\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1290973.1250 - mae: 899.4751 - val_loss: 425852.1562 - val_mae: 469.7182\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1309723.8750 - mae: 910.6650 - val_loss: 425727.4062 - val_mae: 469.9063\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1309317.2500 - mae: 908.4083 - val_loss: 418735.3438 - val_mae: 461.3838\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1296799.5000 - mae: 901.8819 - val_loss: 425478.0938 - val_mae: 470.3004\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 1288103.0000 - mae: 898.6229 - val_loss: 435878.7500 - val_mae: 480.1595\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1316568.0000 - mae: 911.6585 - val_loss: 425209.9062 - val_mae: 469.8854\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 1284956.7500 - mae: 895.4792 - val_loss: 435619.5000 - val_mae: 480.2920\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1292998.7500 - mae: 901.0130 - val_loss: 424938.0938 - val_mae: 469.5282\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1293286.6250 - mae: 899.2851 - val_loss: 414277.8438 - val_mae: 459.3333\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1288152.0000 - mae: 896.0089 - val_loss: 396781.1562 - val_mae: 440.7055\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1312955.2500 - mae: 909.6479 - val_loss: 424543.1562 - val_mae: 469.5192\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1299866.8750 - mae: 901.7122 - val_loss: 420732.4062 - val_mae: 467.7663\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1281949.3750 - mae: 897.0958 - val_loss: 413766.0000 - val_mae: 459.3333\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1291527.0000 - mae: 900.7964 - val_loss: 430984.0000 - val_mae: 477.9372\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1300465.5000 - mae: 903.0703 - val_loss: 409820.3750 - val_mae: 457.3020\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1285734.1250 - mae: 897.2962 - val_loss: 420760.2500 - val_mae: 463.6771\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1311458.3750 - mae: 909.1425 - val_loss: 420067.0312 - val_mae: 467.4699\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1279213.3750 - mae: 896.4807 - val_loss: 416194.0000 - val_mae: 460.7188\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1296064.6250 - mae: 901.4976 - val_loss: 423493.7812 - val_mae: 469.4952\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1289061.0000 - mae: 899.8164 - val_loss: 402364.4062 - val_mae: 448.8932\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1280366.8750 - mae: 894.8943 - val_loss: 412739.9062 - val_mae: 459.3333\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1300279.3750 - mae: 905.6445 - val_loss: 412612.7812 - val_mae: 459.3333\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1291000.3750 - mae: 898.4374 - val_loss: 419847.7188 - val_mae: 463.3958\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1301055.5000 - mae: 904.0741 - val_loss: 422839.2500 - val_mae: 469.4803\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 10s 237ms/step - loss: 1305263.7500 - mae: 905.8652 - val_loss: 422716.2500 - val_mae: 469.7585\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1295132.2500 - mae: 900.1471 - val_loss: 425671.4062 - val_mae: 471.7034\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 1294217.8750 - mae: 898.5468 - val_loss: 422446.3438 - val_mae: 469.4712\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1309920.3750 - mae: 908.7876 - val_loss: 408730.3750 - val_mae: 453.2610\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 1303241.7500 - mae: 904.4029 - val_loss: 411725.5938 - val_mae: 459.6146\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1297773.1250 - mae: 901.8839 - val_loss: 418365.8438 - val_mae: 467.1497\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1303083.3750 - mae: 908.6291 - val_loss: 415147.8438 - val_mae: 461.6458\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1285855.6250 - mae: 896.3802 - val_loss: 428578.4062 - val_mae: 477.8289\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1304080.8750 - mae: 904.4919 - val_loss: 417990.0312 - val_mae: 467.7030\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1290426.6250 - mae: 900.5541 - val_loss: 414751.0938 - val_mae: 461.3646\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1281249.1250 - mae: 895.1319 - val_loss: 407273.2500 - val_mae: 457.3021\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1293433.8750 - mae: 902.8658 - val_loss: 431710.6562 - val_mae: 479.5548\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 1291806.5000 - mae: 898.4938 - val_loss: 410684.1250 - val_mae: 459.0521\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1288341.1250 - mae: 898.6561 - val_loss: 417331.0938 - val_mae: 467.4068\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1263183.6250 - mae: 890.6682 - val_loss: 417782.6250 - val_mae: 463.3958\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1271048.0000 - mae: 890.7395 - val_loss: 410301.6562 - val_mae: 459.0521\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1280517.0000 - mae: 894.6535 - val_loss: 431076.1250 - val_mae: 480.3687\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1282980.6250 - mae: 894.6695 - val_loss: 410059.6250 - val_mae: 459.3333\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1287640.1250 - mae: 897.1700 - val_loss: 420364.8750 - val_mae: 469.7043\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1304034.8750 - mae: 906.0781 - val_loss: 426961.2812 - val_mae: 477.1942\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1280734.3750 - mae: 897.7615 - val_loss: 395593.0938 - val_mae: 447.2184\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1272241.2500 - mae: 893.2236 - val_loss: 399134.8438 - val_mae: 449.2527\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1303855.1250 - mae: 904.2515 - val_loss: 423502.0938 - val_mae: 471.4423\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1305953.3750 - mae: 906.4659 - val_loss: 416056.4062 - val_mae: 467.9392\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1282587.6250 - mae: 895.0411 - val_loss: 415926.7812 - val_mae: 467.9362\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1268901.6250 - mae: 891.6294 - val_loss: 409040.7500 - val_mae: 459.3333\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1303667.7500 - mae: 905.4500 - val_loss: 398516.9688 - val_mae: 449.2677\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1285729.5000 - mae: 897.2379 - val_loss: 419193.5312 - val_mae: 469.6772\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1278777.1250 - mae: 894.4252 - val_loss: 419054.7188 - val_mae: 469.3930\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1310919.0000 - mae: 909.5258 - val_loss: 425658.5000 - val_mae: 477.6966\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1285621.3750 - mae: 898.5515 - val_loss: 422477.5312 - val_mae: 471.9807\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1272425.1250 - mae: 891.9565 - val_loss: 411954.6562 - val_mae: 461.6458\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1304531.2500 - mae: 904.4306 - val_loss: 408746.0000 - val_mae: 455.0982\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1303327.5000 - mae: 903.8693 - val_loss: 422063.1250 - val_mae: 471.4091\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1275015.2500 - mae: 890.1538 - val_loss: 418273.5312 - val_mae: 469.3750\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1282408.8750 - mae: 893.5618 - val_loss: 387022.2500 - val_mae: 438.9749\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1295831.0000 - mae: 904.3292 - val_loss: 403973.1250 - val_mae: 457.0208\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1292026.1250 - mae: 899.1833 - val_loss: 428258.2500 - val_mae: 479.6796\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1291408.5000 - mae: 898.9485 - val_loss: 411049.3438 - val_mae: 461.3646\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1270057.6250 - mae: 890.1624 - val_loss: 400552.6562 - val_mae: 451.0569\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1291842.0000 - mae: 898.9744 - val_loss: 417492.3438 - val_mae: 469.3568\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1277562.6250 - mae: 891.8155 - val_loss: 403343.0000 - val_mae: 457.0209\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1292234.7500 - mae: 898.6913 - val_loss: 413590.4688 - val_mae: 467.6008\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1284488.6250 - mae: 895.0823 - val_loss: 417106.9062 - val_mae: 469.3479\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1279777.8750 - mae: 893.6055 - val_loss: 427333.8750 - val_mae: 479.6377\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1300254.3750 - mae: 903.9464 - val_loss: 413817.2500 - val_mae: 463.3958\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1285743.5000 - mae: 896.5097 - val_loss: 406381.8750 - val_mae: 459.3333\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1297329.8750 - mae: 901.1456 - val_loss: 420267.7812 - val_mae: 471.9295\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1271546.0000 - mae: 890.0566 - val_loss: 433480.2812 - val_mae: 487.5817\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1282520.0000 - mae: 894.8616 - val_loss: 402980.0000 - val_mae: 453.3994\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1292698.5000 - mae: 899.6013 - val_loss: 409531.7500 - val_mae: 461.3646\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1279054.7500 - mae: 894.0973 - val_loss: 409402.7188 - val_mae: 461.3646\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1272135.1250 - mae: 890.9672 - val_loss: 398958.0000 - val_mae: 451.3772\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 1277553.3750 - mae: 893.1076 - val_loss: 409150.5938 - val_mae: 461.3646\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1282583.6250 - mae: 895.8623 - val_loss: 419335.5312 - val_mae: 471.3460\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1285377.3750 - mae: 895.9100 - val_loss: 394927.5000 - val_mae: 449.0737\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1298356.1250 - mae: 902.9041 - val_loss: 419088.0000 - val_mae: 471.6212\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1267480.6250 - mae: 887.6323 - val_loss: 411651.6250 - val_mae: 467.2744\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1280767.2500 - mae: 894.2489 - val_loss: 415183.5938 - val_mae: 469.5839\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1272465.7500 - mae: 890.4405 - val_loss: 415053.6250 - val_mae: 469.5809\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1286518.1250 - mae: 896.9234 - val_loss: 431897.3750 - val_mae: 488.0360\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1288532.5000 - mae: 897.3400 - val_loss: 404509.3438 - val_mae: 459.6145\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1292609.5000 - mae: 899.9564 - val_loss: 421298.8438 - val_mae: 477.2168\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1288672.8750 - mae: 898.4735 - val_loss: 407886.0000 - val_mae: 461.3646\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1280873.1250 - mae: 893.7363 - val_loss: 393838.5000 - val_mae: 449.3821\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1294719.3750 - mae: 900.8513 - val_loss: 410654.8750 - val_mae: 467.8128\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1292534.3750 - mae: 900.0071 - val_loss: 403865.0938 - val_mae: 459.3333\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1275090.7500 - mae: 893.6774 - val_loss: 410382.7500 - val_mae: 467.5255\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1268790.2500 - mae: 888.7301 - val_loss: 413879.5000 - val_mae: 469.2725\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1290155.5000 - mae: 897.8734 - val_loss: 404126.6562 - val_mae: 455.2097\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1259559.0000 - mae: 885.6274 - val_loss: 423896.8750 - val_mae: 479.4807\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1280302.8750 - mae: 893.8708 - val_loss: 403223.2188 - val_mae: 459.0521\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1280615.0000 - mae: 893.3937 - val_loss: 416989.7500 - val_mae: 471.0104\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1280468.6250 - mae: 893.1277 - val_loss: 423490.0312 - val_mae: 479.1814\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1287373.1250 - mae: 896.8583 - val_loss: 406484.5938 - val_mae: 461.0833\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 1285261.5000 - mae: 895.2850 - val_loss: 412968.4062 - val_mae: 468.9702\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1258882.0000 - mae: 884.5013 - val_loss: 412869.9062 - val_mae: 469.5297\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1279545.0000 - mae: 892.9907 - val_loss: 398853.5000 - val_mae: 457.3021\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1274707.7500 - mae: 891.1047 - val_loss: 416233.5312 - val_mae: 471.2737\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1276240.0000 - mae: 891.0611 - val_loss: 408853.0938 - val_mae: 467.4894\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1277053.1250 - mae: 894.1016 - val_loss: 418956.0312 - val_mae: 477.3894\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1277502.2500 - mae: 891.4662 - val_loss: 422487.0938 - val_mae: 479.9771\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1288482.2500 - mae: 896.3211 - val_loss: 387991.7500 - val_mae: 447.1239\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1292403.8750 - mae: 901.3382 - val_loss: 415603.7812 - val_mae: 471.5397\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1285904.1250 - mae: 896.0632 - val_loss: 395008.3438 - val_mae: 451.1925\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1279523.0000 - mae: 892.3210 - val_loss: 411716.0000 - val_mae: 469.5024\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1274796.2500 - mae: 889.6512 - val_loss: 411555.6562 - val_mae: 468.9369\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1272090.0000 - mae: 891.7390 - val_loss: 404850.1250 - val_mae: 461.0833\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1268997.8750 - mae: 888.3562 - val_loss: 421557.4062 - val_mae: 479.6537\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1285936.0000 - mae: 895.2077 - val_loss: 404600.9062 - val_mae: 461.0833\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1284173.3750 - mae: 893.5500 - val_loss: 394275.5000 - val_mae: 451.2105\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 1270470.7500 - mae: 888.2662 - val_loss: 421130.9062 - val_mae: 479.0729\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1277288.2500 - mae: 891.6178 - val_loss: 400614.6562 - val_mae: 459.3333\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1272107.0000 - mae: 889.5116 - val_loss: 380095.0000 - val_mae: 439.3246\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1248832.6250 - mae: 880.5762 - val_loss: 400352.3750 - val_mae: 459.0521\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 1288669.5000 - mae: 898.9882 - val_loss: 400243.7500 - val_mae: 459.3333\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1284438.2500 - mae: 896.1655 - val_loss: 397169.3438 - val_mae: 453.2599\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1277593.3750 - mae: 891.6469 - val_loss: 397062.8750 - val_mae: 453.5441\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1283982.0000 - mae: 893.0837 - val_loss: 413665.3750 - val_mae: 471.2133\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1267434.2500 - mae: 887.0774 - val_loss: 409933.7188 - val_mae: 469.4603\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1251472.5000 - mae: 881.3038 - val_loss: 393057.9062 - val_mae: 451.2406\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1284940.3750 - mae: 894.8981 - val_loss: 403102.9062 - val_mae: 461.0833\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1280659.2500 - mae: 893.8287 - val_loss: 416078.8750 - val_mae: 476.9755\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1282118.3750 - mae: 893.2511 - val_loss: 409409.5000 - val_mae: 469.1670\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1274510.8750 - mae: 891.1251 - val_loss: 402764.5312 - val_mae: 461.6458\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1281877.7500 - mae: 892.8782 - val_loss: 388849.3750 - val_mae: 449.5057\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1251259.3750 - mae: 879.7176 - val_loss: 388714.5938 - val_mae: 449.2274\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1256626.8750 - mae: 883.8760 - val_loss: 412557.2500 - val_mae: 471.7488\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1248709.0000 - mae: 879.5507 - val_loss: 408798.0938 - val_mae: 469.4333\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1237982.0000 - mae: 877.4531 - val_loss: 418794.7812 - val_mae: 478.9649\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1255703.1250 - mae: 882.0145 - val_loss: 408513.1562 - val_mae: 468.8649\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1254275.3750 - mae: 881.0871 - val_loss: 412000.7188 - val_mae: 470.8931\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1274467.3750 - mae: 891.3005 - val_loss: 411910.0938 - val_mae: 471.4526\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1275830.7500 - mae: 888.7780 - val_loss: 408150.0938 - val_mae: 469.1371\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1269511.3750 - mae: 889.1469 - val_loss: 397899.0312 - val_mae: 459.3334\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 1268276.2500 - mae: 887.9940 - val_loss: 414406.2500 - val_mae: 476.8976\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1250507.5000 - mae: 879.6447 - val_loss: 397672.8438 - val_mae: 459.6146\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1265620.8750 - mae: 888.0701 - val_loss: 411258.9062 - val_mae: 471.1564\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1260536.6250 - mae: 883.7154 - val_loss: 407538.2188 - val_mae: 469.4033\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1258536.7500 - mae: 884.1643 - val_loss: 390788.7500 - val_mae: 451.5788\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1248033.8750 - mae: 878.9344 - val_loss: 400775.6562 - val_mae: 461.3646\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1271358.1250 - mae: 888.9639 - val_loss: 410790.9062 - val_mae: 471.7069\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1275840.5000 - mae: 892.1356 - val_loss: 417117.2500 - val_mae: 478.8869\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1286184.1250 - mae: 896.6571 - val_loss: 389697.4688 - val_mae: 450.9450\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1265650.1250 - mae: 887.0967 - val_loss: 403866.5938 - val_mae: 463.1146\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1258930.7500 - mae: 882.7814 - val_loss: 413137.6562 - val_mae: 477.1186\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1280555.0000 - mae: 894.2180 - val_loss: 406491.1562 - val_mae: 468.8167\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1251787.3750 - mae: 880.6731 - val_loss: 393409.6562 - val_mae: 453.3529\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1271641.1250 - mae: 887.9960 - val_loss: 392573.6250 - val_mae: 457.3021\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1267131.8750 - mae: 886.6115 - val_loss: 402528.7500 - val_mae: 467.0577\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1259830.6250 - mae: 884.2551 - val_loss: 409633.4688 - val_mae: 471.3985\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1278387.8750 - mae: 893.5345 - val_loss: 389323.1250 - val_mae: 451.3337\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 1251025.1250 - mae: 881.9863 - val_loss: 409378.1562 - val_mae: 471.3925\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 1255006.6250 - mae: 884.0919 - val_loss: 415716.4062 - val_mae: 479.1018\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1269279.3750 - mae: 887.4836 - val_loss: 388981.0000 - val_mae: 451.6240\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1264417.3750 - mae: 886.5707 - val_loss: 382382.8750 - val_mae: 443.6395\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1274075.3750 - mae: 890.5720 - val_loss: 382265.3438 - val_mae: 443.6455\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1258349.1250 - mae: 883.4065 - val_loss: 384999.9688 - val_mae: 449.3206\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1251358.0000 - mae: 879.9316 - val_loss: 398549.6562 - val_mae: 461.3646\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1265230.5000 - mae: 887.0771 - val_loss: 408494.6562 - val_mae: 471.3713\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1262063.2500 - mae: 885.3447 - val_loss: 404750.6250 - val_mae: 469.0558\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1251294.6250 - mae: 881.8933 - val_loss: 404624.9062 - val_mae: 469.0528\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1268602.1250 - mae: 892.5483 - val_loss: 388000.7812 - val_mae: 451.3669\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1262728.3750 - mae: 888.4582 - val_loss: 393751.0312 - val_mae: 458.9688\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1238522.1250 - mae: 876.5078 - val_loss: 397812.7812 - val_mae: 461.3646\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1268004.8750 - mae: 888.7867 - val_loss: 394094.0938 - val_mae: 459.3333\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1246870.2500 - mae: 878.3285 - val_loss: 407591.5312 - val_mae: 471.0689\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1255745.0000 - mae: 882.1149 - val_loss: 397423.2500 - val_mae: 461.0833\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1261085.0000 - mae: 884.9775 - val_loss: 400174.3438 - val_mae: 467.2816\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1258411.7500 - mae: 884.6785 - val_loss: 403621.5000 - val_mae: 469.0287\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1268534.6250 - mae: 886.5204 - val_loss: 390656.7500 - val_mae: 453.7035\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 1267968.2500 - mae: 887.8423 - val_loss: 403389.3438 - val_mae: 469.3038\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1271753.2500 - mae: 890.3778 - val_loss: 403268.1562 - val_mae: 469.3009\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1249348.1250 - mae: 878.1870 - val_loss: 393118.9062 - val_mae: 459.3333\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1234571.1250 - mae: 873.2242 - val_loss: 406607.9062 - val_mae: 471.3261\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1261812.5000 - mae: 884.6013 - val_loss: 400031.8438 - val_mae: 463.1146\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1251708.8750 - mae: 880.8528 - val_loss: 399138.0938 - val_mae: 466.6951\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 6s 131ms/step - loss: 1252026.7500 - mae: 883.4592 - val_loss: 382641.1250 - val_mae: 449.6621\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1247992.2500 - mae: 877.9662 - val_loss: 418954.7188 - val_mae: 487.1508\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1275750.1250 - mae: 891.8937 - val_loss: 375985.2500 - val_mae: 441.7529\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1255499.0000 - mae: 880.9150 - val_loss: 395855.5938 - val_mae: 461.3646\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1235863.6250 - mae: 874.5521 - val_loss: 398710.8438 - val_mae: 462.7500\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1250178.2500 - mae: 878.3077 - val_loss: 411974.8438 - val_mae: 478.6457\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1266870.7500 - mae: 887.2748 - val_loss: 401897.1562 - val_mae: 469.2677\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1268263.0000 - mae: 887.2695 - val_loss: 401775.7812 - val_mae: 469.2648\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1251214.3750 - mae: 880.1188 - val_loss: 418065.7188 - val_mae: 487.3686\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1263039.2500 - mae: 885.3347 - val_loss: 391540.7500 - val_mae: 459.3333\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1273205.6250 - mae: 890.8974 - val_loss: 411385.0312 - val_mae: 479.1782\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1251205.2500 - mae: 879.6537 - val_loss: 391277.5938 - val_mae: 459.0521\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1251622.6250 - mae: 879.9663 - val_loss: 391156.5000 - val_mae: 459.0520\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1257267.0000 - mae: 882.3973 - val_loss: 397424.8750 - val_mae: 466.9342\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1263098.0000 - mae: 885.4800 - val_loss: 404508.3438 - val_mae: 471.5562\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1252607.6250 - mae: 879.8306 - val_loss: 397994.5938 - val_mae: 463.6771\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1251235.0000 - mae: 878.7804 - val_loss: 400679.3438 - val_mae: 469.5189\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1242216.2500 - mae: 876.1809 - val_loss: 397122.5312 - val_mae: 462.7500\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1247027.5000 - mae: 875.9556 - val_loss: 394005.0312 - val_mae: 461.0833\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1264498.0000 - mae: 886.7516 - val_loss: 406616.0000 - val_mae: 476.5302\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1255759.7500 - mae: 881.9565 - val_loss: 383834.5000 - val_mae: 451.4723\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1245002.8750 - mae: 877.1803 - val_loss: 387292.3750 - val_mae: 453.5066\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1243341.2500 - mae: 879.0027 - val_loss: 392939.0312 - val_mae: 460.7188\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1243259.2500 - mae: 874.1288 - val_loss: 409687.3438 - val_mae: 478.5374\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1262515.1250 - mae: 884.5105 - val_loss: 386963.4062 - val_mae: 453.7967\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1263267.5000 - mae: 884.2389 - val_loss: 393158.1250 - val_mae: 461.0833\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1250699.2500 - mae: 878.7591 - val_loss: 393060.3438 - val_mae: 461.3646\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1249186.2500 - mae: 876.8940 - val_loss: 405632.7500 - val_mae: 476.7634\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1252431.8750 - mae: 878.2345 - val_loss: 409080.0938 - val_mae: 478.7887\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1238766.7500 - mae: 875.9398 - val_loss: 405359.4688 - val_mae: 476.4702\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1249287.7500 - mae: 877.5792 - val_loss: 398928.1562 - val_mae: 469.1956\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1252615.1250 - mae: 880.0514 - val_loss: 378969.5938 - val_mae: 449.4741\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1235824.8750 - mae: 872.1129 - val_loss: 398685.0000 - val_mae: 469.1896\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1248927.3750 - mae: 878.8243 - val_loss: 388647.6250 - val_mae: 459.3333\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1253089.0000 - mae: 880.3392 - val_loss: 414664.2812 - val_mae: 486.5716\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1251269.3750 - mae: 877.9378 - val_loss: 385635.8750 - val_mae: 453.5486\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1255472.0000 - mae: 882.1093 - val_loss: 388286.6562 - val_mae: 459.3333\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1239324.7500 - mae: 875.8959 - val_loss: 394526.8438 - val_mae: 467.4245\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1252775.0000 - mae: 880.9161 - val_loss: 401511.7188 - val_mae: 471.2028\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 1247798.5000 - mae: 877.9559 - val_loss: 387927.0000 - val_mae: 459.3333\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1241755.7500 - mae: 874.7374 - val_loss: 387783.9062 - val_mae: 459.0521\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1248410.3750 - mae: 878.5811 - val_loss: 384122.9688 - val_mae: 457.3021\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1256127.1250 - mae: 882.7037 - val_loss: 377706.4688 - val_mae: 449.7885\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1237176.5000 - mae: 875.4949 - val_loss: 378398.7500 - val_mae: 446.0620\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1245401.3750 - mae: 876.1208 - val_loss: 377474.4062 - val_mae: 449.7945\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1239947.3750 - mae: 874.3572 - val_loss: 406910.2188 - val_mae: 478.4051\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1243586.6250 - mae: 875.0784 - val_loss: 387089.3750 - val_mae: 459.3333\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1236215.5000 - mae: 870.4533 - val_loss: 380660.3438 - val_mae: 451.5535\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1252815.2500 - mae: 881.6065 - val_loss: 396666.0312 - val_mae: 468.5789\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1251028.2500 - mae: 879.3694 - val_loss: 386703.8750 - val_mae: 459.0521\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1234951.7500 - mae: 870.6288 - val_loss: 402747.9062 - val_mae: 476.6249\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1242937.5000 - mae: 877.8986 - val_loss: 392790.7188 - val_mae: 467.1011\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1254558.7500 - mae: 881.4197 - val_loss: 383629.2500 - val_mae: 453.5999\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1241831.8750 - mae: 875.0526 - val_loss: 396102.6562 - val_mae: 469.1263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "1cad9ca4-923e-4866-a51d-593667dfd0e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1504759.75,\n",
              "  1514978.0,\n",
              "  1516002.25,\n",
              "  1523348.875,\n",
              "  1504105.875,\n",
              "  1523431.625,\n",
              "  1496448.5,\n",
              "  1518940.5,\n",
              "  1511732.0,\n",
              "  1513034.75,\n",
              "  1517300.625,\n",
              "  1513803.625,\n",
              "  1514151.75,\n",
              "  1519794.0,\n",
              "  1492495.0,\n",
              "  1509161.625,\n",
              "  1519262.75,\n",
              "  1478128.75,\n",
              "  1510751.875,\n",
              "  1497683.375,\n",
              "  1513505.375,\n",
              "  1504683.75,\n",
              "  1509060.5,\n",
              "  1510066.25,\n",
              "  1503549.625,\n",
              "  1499279.375,\n",
              "  1495386.375,\n",
              "  1503450.0,\n",
              "  1523733.75,\n",
              "  1499161.625,\n",
              "  1510540.25,\n",
              "  1515602.75,\n",
              "  1519522.25,\n",
              "  1506906.0,\n",
              "  1479048.75,\n",
              "  1505752.25,\n",
              "  1493679.125,\n",
              "  1516991.375,\n",
              "  1515983.25,\n",
              "  1486610.75,\n",
              "  1466570.875,\n",
              "  1501031.25,\n",
              "  1475969.25,\n",
              "  1488559.5,\n",
              "  1507924.5,\n",
              "  1456835.125,\n",
              "  1499205.5,\n",
              "  1492459.0,\n",
              "  1500114.25,\n",
              "  1493243.0,\n",
              "  1489096.5,\n",
              "  1492167.75,\n",
              "  1474113.75,\n",
              "  1499736.25,\n",
              "  1498299.25,\n",
              "  1493005.125,\n",
              "  1503428.5,\n",
              "  1512313.0,\n",
              "  1496497.375,\n",
              "  1462004.625,\n",
              "  1498811.625,\n",
              "  1490927.5,\n",
              "  1502711.625,\n",
              "  1498271.875,\n",
              "  1503808.875,\n",
              "  1481025.125,\n",
              "  1493495.75,\n",
              "  1474670.125,\n",
              "  1489930.125,\n",
              "  1506178.875,\n",
              "  1490796.875,\n",
              "  1477348.125,\n",
              "  1478682.875,\n",
              "  1480254.0,\n",
              "  1492507.625,\n",
              "  1483100.5,\n",
              "  1472417.375,\n",
              "  1510450.0,\n",
              "  1480344.625,\n",
              "  1491564.375,\n",
              "  1481382.0,\n",
              "  1494616.25,\n",
              "  1498536.125,\n",
              "  1468420.375,\n",
              "  1504214.125,\n",
              "  1506925.0,\n",
              "  1466476.0,\n",
              "  1485363.375,\n",
              "  1497099.0,\n",
              "  1490540.375,\n",
              "  1479438.875,\n",
              "  1497782.875,\n",
              "  1479834.0,\n",
              "  1493431.375,\n",
              "  1470969.5,\n",
              "  1489084.625,\n",
              "  1499114.5,\n",
              "  1490212.375,\n",
              "  1488277.875,\n",
              "  1465497.75,\n",
              "  1485886.625,\n",
              "  1455664.25,\n",
              "  1481698.625,\n",
              "  1493814.5,\n",
              "  1491745.625,\n",
              "  1473016.875,\n",
              "  1478154.375,\n",
              "  1481398.875,\n",
              "  1488162.875,\n",
              "  1482247.25,\n",
              "  1491415.625,\n",
              "  1475119.875,\n",
              "  1491550.75,\n",
              "  1474224.0,\n",
              "  1483356.75,\n",
              "  1480934.0,\n",
              "  1463666.75,\n",
              "  1496105.375,\n",
              "  1490134.625,\n",
              "  1466297.125,\n",
              "  1474502.75,\n",
              "  1482404.25,\n",
              "  1464349.25,\n",
              "  1467593.875,\n",
              "  1478747.0,\n",
              "  1446594.125,\n",
              "  1465648.375,\n",
              "  1476200.625,\n",
              "  1474668.75,\n",
              "  1459851.125,\n",
              "  1484223.5,\n",
              "  1468360.375,\n",
              "  1466618.5,\n",
              "  1475974.5,\n",
              "  1446934.125,\n",
              "  1474059.125,\n",
              "  1452668.125,\n",
              "  1458555.625,\n",
              "  1455901.0,\n",
              "  1481769.0,\n",
              "  1462558.0,\n",
              "  1491010.25,\n",
              "  1499231.375,\n",
              "  1500397.25,\n",
              "  1467309.0,\n",
              "  1466630.5,\n",
              "  1476897.125,\n",
              "  1480542.625,\n",
              "  1475983.625,\n",
              "  1485118.625,\n",
              "  1445423.5,\n",
              "  1477920.5,\n",
              "  1479053.125,\n",
              "  1465289.625,\n",
              "  1485207.25,\n",
              "  1479572.625,\n",
              "  1473596.5,\n",
              "  1449365.25,\n",
              "  1462123.375,\n",
              "  1442844.375,\n",
              "  1466513.5,\n",
              "  1443473.875,\n",
              "  1477556.875,\n",
              "  1473501.375,\n",
              "  1466656.5,\n",
              "  1463888.375,\n",
              "  1470370.375,\n",
              "  1483041.375,\n",
              "  1459905.875,\n",
              "  1444908.375,\n",
              "  1459532.375,\n",
              "  1452172.625,\n",
              "  1459609.125,\n",
              "  1466471.75,\n",
              "  1462355.25,\n",
              "  1444032.375,\n",
              "  1472448.25,\n",
              "  1478636.125,\n",
              "  1458933.875,\n",
              "  1462015.625,\n",
              "  1469688.375,\n",
              "  1461403.0,\n",
              "  1472173.0,\n",
              "  1471506.25,\n",
              "  1463110.75,\n",
              "  1461637.0,\n",
              "  1430938.625,\n",
              "  1453571.25,\n",
              "  1448330.375,\n",
              "  1450537.625,\n",
              "  1466755.25,\n",
              "  1439859.625,\n",
              "  1468355.375,\n",
              "  1442256.75,\n",
              "  1452780.125,\n",
              "  1452379.5,\n",
              "  1460781.5,\n",
              "  1429370.375,\n",
              "  1466465.25,\n",
              "  1446088.75,\n",
              "  1454437.5,\n",
              "  1413706.75,\n",
              "  1467379.625,\n",
              "  1457158.75,\n",
              "  1468621.375,\n",
              "  1453011.625,\n",
              "  1442303.5,\n",
              "  1458129.625,\n",
              "  1459286.0,\n",
              "  1458996.625,\n",
              "  1438978.25,\n",
              "  1453003.125,\n",
              "  1453839.125,\n",
              "  1453454.125,\n",
              "  1437104.375,\n",
              "  1458036.375,\n",
              "  1439633.5,\n",
              "  1465135.375,\n",
              "  1439219.0,\n",
              "  1433299.75,\n",
              "  1448701.0,\n",
              "  1432186.75,\n",
              "  1461302.25,\n",
              "  1465520.0,\n",
              "  1456699.875,\n",
              "  1438625.75,\n",
              "  1446807.375,\n",
              "  1441115.75,\n",
              "  1449070.625,\n",
              "  1436953.625,\n",
              "  1433323.625,\n",
              "  1426940.5,\n",
              "  1450622.125,\n",
              "  1460130.625,\n",
              "  1444761.125,\n",
              "  1442342.375,\n",
              "  1442076.75,\n",
              "  1447823.125,\n",
              "  1448706.125,\n",
              "  1447342.125,\n",
              "  1451622.5,\n",
              "  1446259.25,\n",
              "  1443348.125,\n",
              "  1436313.25,\n",
              "  1424367.625,\n",
              "  1447124.125,\n",
              "  1416425.125,\n",
              "  1452709.125,\n",
              "  1452120.25,\n",
              "  1443150.125,\n",
              "  1427983.125,\n",
              "  1437580.375,\n",
              "  1444317.375,\n",
              "  1445754.375,\n",
              "  1434038.875,\n",
              "  1440551.375,\n",
              "  1439478.875,\n",
              "  1427171.375,\n",
              "  1442106.0,\n",
              "  1431822.375,\n",
              "  1440386.75,\n",
              "  1440943.125,\n",
              "  1433205.25,\n",
              "  1444815.125,\n",
              "  1439030.375,\n",
              "  1421017.25,\n",
              "  1438505.875,\n",
              "  1445943.5,\n",
              "  1434143.25,\n",
              "  1431840.25,\n",
              "  1428383.0,\n",
              "  1436761.75,\n",
              "  1420972.375,\n",
              "  1419049.875,\n",
              "  1415062.375,\n",
              "  1411994.0,\n",
              "  1454710.875,\n",
              "  1429500.75,\n",
              "  1433939.5,\n",
              "  1428326.375,\n",
              "  1425361.5,\n",
              "  1425440.125,\n",
              "  1428469.375,\n",
              "  1446758.125,\n",
              "  1425476.0,\n",
              "  1425586.5,\n",
              "  1429070.625,\n",
              "  1428762.625,\n",
              "  1430387.625,\n",
              "  1415363.125,\n",
              "  1445334.125,\n",
              "  1422365.875,\n",
              "  1418049.75,\n",
              "  1429901.25,\n",
              "  1438341.875,\n",
              "  1452380.125,\n",
              "  1431544.25,\n",
              "  1423204.625,\n",
              "  1423904.125,\n",
              "  1427932.375,\n",
              "  1437545.0,\n",
              "  1434597.125,\n",
              "  1433315.0,\n",
              "  1417215.5,\n",
              "  1436855.625,\n",
              "  1432977.0,\n",
              "  1415208.625,\n",
              "  1418740.875,\n",
              "  1418543.125,\n",
              "  1423775.125,\n",
              "  1420337.125,\n",
              "  1439556.125,\n",
              "  1423188.5,\n",
              "  1405922.5,\n",
              "  1415765.125,\n",
              "  1424006.0,\n",
              "  1409494.625,\n",
              "  1434424.125,\n",
              "  1408589.125,\n",
              "  1412739.25,\n",
              "  1408231.625,\n",
              "  1411323.75,\n",
              "  1413499.625,\n",
              "  1414932.75,\n",
              "  1402643.875,\n",
              "  1422676.125,\n",
              "  1427759.125,\n",
              "  1417095.25,\n",
              "  1429411.0,\n",
              "  1413798.0,\n",
              "  1414661.0,\n",
              "  1399775.0,\n",
              "  1410900.75,\n",
              "  1423935.125,\n",
              "  1415743.5,\n",
              "  1433083.0,\n",
              "  1412919.375,\n",
              "  1415777.0,\n",
              "  1421306.0,\n",
              "  1410573.25,\n",
              "  1421322.375,\n",
              "  1416772.125,\n",
              "  1401615.625,\n",
              "  1410553.375,\n",
              "  1436584.625,\n",
              "  1392240.0,\n",
              "  1409163.875,\n",
              "  1416909.375,\n",
              "  1432463.75,\n",
              "  1409985.875,\n",
              "  1421104.75,\n",
              "  1420907.25,\n",
              "  1407408.75,\n",
              "  1416089.375,\n",
              "  1408614.875,\n",
              "  1413788.75,\n",
              "  1406373.75,\n",
              "  1400663.375,\n",
              "  1420914.375,\n",
              "  1408004.375,\n",
              "  1409843.125,\n",
              "  1412244.75,\n",
              "  1418941.5,\n",
              "  1425659.625,\n",
              "  1411667.5,\n",
              "  1410733.0,\n",
              "  1383343.625,\n",
              "  1407601.5,\n",
              "  1407433.375,\n",
              "  1398425.25,\n",
              "  1414694.375,\n",
              "  1406289.625,\n",
              "  1402598.75,\n",
              "  1394189.125,\n",
              "  1390265.125,\n",
              "  1413626.25,\n",
              "  1410635.75,\n",
              "  1423991.5,\n",
              "  1382968.5,\n",
              "  1408043.125,\n",
              "  1421501.625,\n",
              "  1413028.625,\n",
              "  1414503.0,\n",
              "  1375160.5,\n",
              "  1414268.625,\n",
              "  1404957.0,\n",
              "  1410770.75,\n",
              "  1426166.875,\n",
              "  1418017.75,\n",
              "  1386136.25,\n",
              "  1407145.875,\n",
              "  1401889.0,\n",
              "  1404780.125,\n",
              "  1396570.875,\n",
              "  1396921.125,\n",
              "  1406908.75,\n",
              "  1386404.75,\n",
              "  1382578.75,\n",
              "  1384159.75,\n",
              "  1404689.75,\n",
              "  1406197.25,\n",
              "  1392810.75,\n",
              "  1394015.0,\n",
              "  1399996.0,\n",
              "  1402890.125,\n",
              "  1386279.75,\n",
              "  1374008.0,\n",
              "  1404036.375,\n",
              "  1398317.0,\n",
              "  1396209.875,\n",
              "  1406467.5,\n",
              "  1387625.125,\n",
              "  1383416.25,\n",
              "  1392830.625,\n",
              "  1384133.625,\n",
              "  1398933.25,\n",
              "  1378535.0,\n",
              "  1408781.875,\n",
              "  1387680.875,\n",
              "  1365112.0,\n",
              "  1398581.875,\n",
              "  1407848.625,\n",
              "  1394144.875,\n",
              "  1386497.125,\n",
              "  1388987.5,\n",
              "  1400631.0,\n",
              "  1392782.25,\n",
              "  1370874.5,\n",
              "  1399772.75,\n",
              "  1384002.375,\n",
              "  1401080.375,\n",
              "  1373835.875,\n",
              "  1390028.5,\n",
              "  1382227.5,\n",
              "  1390653.25,\n",
              "  1387115.375,\n",
              "  1392828.5,\n",
              "  1356187.5,\n",
              "  1382949.0,\n",
              "  1397299.375,\n",
              "  1378838.25,\n",
              "  1370680.125,\n",
              "  1392100.125,\n",
              "  1377292.25,\n",
              "  1401032.625,\n",
              "  1365471.875,\n",
              "  1380318.75,\n",
              "  1396764.875,\n",
              "  1380494.25,\n",
              "  1376840.375,\n",
              "  1386643.375,\n",
              "  1371779.25,\n",
              "  1376410.0,\n",
              "  1371850.875,\n",
              "  1394779.0,\n",
              "  1379490.875,\n",
              "  1395141.25,\n",
              "  1397768.375,\n",
              "  1387351.375,\n",
              "  1383817.75,\n",
              "  1363976.875,\n",
              "  1387017.125,\n",
              "  1380141.75,\n",
              "  1387181.625,\n",
              "  1389381.0,\n",
              "  1370629.0,\n",
              "  1371569.875,\n",
              "  1381569.25,\n",
              "  1382349.75,\n",
              "  1382165.625,\n",
              "  1376818.5,\n",
              "  1386372.375,\n",
              "  1372426.875,\n",
              "  1377941.875,\n",
              "  1380953.375,\n",
              "  1393219.125,\n",
              "  1377145.625,\n",
              "  1384983.625,\n",
              "  1362340.0,\n",
              "  1372772.25,\n",
              "  1378685.75,\n",
              "  1398352.5,\n",
              "  1382512.0,\n",
              "  1380366.375,\n",
              "  1371353.625,\n",
              "  1384822.125,\n",
              "  1382153.25,\n",
              "  1379513.875,\n",
              "  1367372.0,\n",
              "  1380437.0,\n",
              "  1360950.5,\n",
              "  1358258.875,\n",
              "  1392182.75,\n",
              "  1367162.75,\n",
              "  1389115.75,\n",
              "  1376116.875,\n",
              "  1388698.0,\n",
              "  1359802.0,\n",
              "  1370093.375,\n",
              "  1349446.25,\n",
              "  1378388.0,\n",
              "  1381893.875,\n",
              "  1375610.875,\n",
              "  1367881.625,\n",
              "  1369968.0,\n",
              "  1355062.25,\n",
              "  1396527.375,\n",
              "  1371164.75,\n",
              "  1350244.75,\n",
              "  1380469.375,\n",
              "  1368314.375,\n",
              "  1377830.625,\n",
              "  1369062.875,\n",
              "  1390142.5,\n",
              "  1388579.25,\n",
              "  1363512.875,\n",
              "  1361641.75,\n",
              "  1364444.625,\n",
              "  1358259.25,\n",
              "  1343038.25,\n",
              "  1374394.75,\n",
              "  1380274.625,\n",
              "  1355690.875,\n",
              "  1368120.625,\n",
              "  1363499.375,\n",
              "  1382871.875,\n",
              "  1361022.25,\n",
              "  1367281.75,\n",
              "  1367845.0,\n",
              "  1377431.875,\n",
              "  1368272.0,\n",
              "  1357193.875,\n",
              "  1371729.125,\n",
              "  1363308.5,\n",
              "  1378694.75,\n",
              "  1351410.625,\n",
              "  1373219.25,\n",
              "  1376411.0,\n",
              "  1363749.375,\n",
              "  1368915.75,\n",
              "  1380825.125,\n",
              "  1348916.0,\n",
              "  1346809.0,\n",
              "  1364747.625,\n",
              "  1355624.0,\n",
              "  1364946.0,\n",
              "  1347221.625,\n",
              "  1330133.875,\n",
              "  1352260.125,\n",
              "  1351001.5,\n",
              "  1350411.625,\n",
              "  1359905.0,\n",
              "  1357664.5,\n",
              "  1371968.5,\n",
              "  1353265.25,\n",
              "  1352844.625,\n",
              "  1338410.125,\n",
              "  1341819.25,\n",
              "  1357027.0,\n",
              "  1373081.0,\n",
              "  1353724.875,\n",
              "  1326537.875,\n",
              "  1337693.25,\n",
              "  1358279.125,\n",
              "  1362823.75,\n",
              "  1324539.375,\n",
              "  1346417.0,\n",
              "  1362516.5,\n",
              "  1354478.75,\n",
              "  1331968.5,\n",
              "  1348753.0,\n",
              "  1352442.25,\n",
              "  1351564.0,\n",
              "  1359151.0,\n",
              "  1359962.75,\n",
              "  1358906.375,\n",
              "  1344037.75,\n",
              "  1345350.875,\n",
              "  1360712.625,\n",
              "  1350213.125,\n",
              "  1337384.0,\n",
              "  1363493.75,\n",
              "  1370472.875,\n",
              "  1351705.25,\n",
              "  1359059.25,\n",
              "  1355500.25,\n",
              "  1341662.875,\n",
              "  1363217.0,\n",
              "  1340958.875,\n",
              "  1366487.75,\n",
              "  1332300.625,\n",
              "  1375201.375,\n",
              "  1352772.375,\n",
              "  1345098.5,\n",
              "  1343968.125,\n",
              "  1360716.375,\n",
              "  1322311.375,\n",
              "  1348351.75,\n",
              "  1340892.375,\n",
              "  1322602.75,\n",
              "  1331724.375,\n",
              "  1328684.25,\n",
              "  1340966.875,\n",
              "  1333779.375,\n",
              "  1356283.875,\n",
              "  1349935.0,\n",
              "  1353273.125,\n",
              "  1324471.0,\n",
              "  1346845.75,\n",
              "  1354776.875,\n",
              "  1337790.75,\n",
              "  1334154.25,\n",
              "  1348882.875,\n",
              "  1334687.0,\n",
              "  1323245.375,\n",
              "  1319986.75,\n",
              "  1337951.625,\n",
              "  1349305.875,\n",
              "  1322961.625,\n",
              "  1326229.125,\n",
              "  1342036.375,\n",
              "  1336103.375,\n",
              "  1338688.25,\n",
              "  1353896.875,\n",
              "  1349767.375,\n",
              "  1336095.875,\n",
              "  1345914.25,\n",
              "  1352447.875,\n",
              "  1336831.5,\n",
              "  1333085.5,\n",
              "  1336199.0,\n",
              "  1314350.125,\n",
              "  1341192.625,\n",
              "  1329944.875,\n",
              "  1330049.125,\n",
              "  1327895.875,\n",
              "  1334787.625,\n",
              "  1351786.5,\n",
              "  1334836.625,\n",
              "  1339963.0,\n",
              "  1323202.375,\n",
              "  1350230.25,\n",
              "  1335756.75,\n",
              "  1348771.25,\n",
              "  1332545.0,\n",
              "  1344725.875,\n",
              "  1323508.0,\n",
              "  1319664.25,\n",
              "  1339055.875,\n",
              "  1342750.125,\n",
              "  1327244.625,\n",
              "  1318582.625,\n",
              "  1338196.25,\n",
              "  1340237.25,\n",
              "  1317492.375,\n",
              "  1320266.25,\n",
              "  1320839.75,\n",
              "  1304398.125,\n",
              "  1335968.625,\n",
              "  1341936.75,\n",
              "  1303488.625,\n",
              "  1337324.875,\n",
              "  1323124.25,\n",
              "  1326628.0,\n",
              "  1338422.375,\n",
              "  1336412.625,\n",
              "  1317610.0,\n",
              "  1317033.25,\n",
              "  1321001.0,\n",
              "  1326889.875,\n",
              "  1330837.5,\n",
              "  1330637.375,\n",
              "  1330584.875,\n",
              "  1330474.25,\n",
              "  1327234.375,\n",
              "  1327015.125,\n",
              "  1330488.75,\n",
              "  1323766.375,\n",
              "  1332826.0,\n",
              "  1332458.75,\n",
              "  1332748.125,\n",
              "  1309866.25,\n",
              "  1332113.0,\n",
              "  1322626.25,\n",
              "  1318889.25,\n",
              "  1330209.75,\n",
              "  1292608.375,\n",
              "  1316427.625,\n",
              "  1316017.75,\n",
              "  1311440.625,\n",
              "  1317767.5,\n",
              "  1320450.0,\n",
              "  1326172.5,\n",
              "  1309089.75,\n",
              "  1307622.375,\n",
              "  1312948.375,\n",
              "  1332985.625,\n",
              "  1328616.0,\n",
              "  1301936.125,\n",
              "  1337962.875,\n",
              "  1310638.125,\n",
              "  1330511.125,\n",
              "  1340646.0,\n",
              "  1319030.375,\n",
              "  1307568.375,\n",
              "  1330929.75,\n",
              "  1322803.75,\n",
              "  1316866.25,\n",
              "  1320838.5,\n",
              "  1318859.125,\n",
              "  1304791.0,\n",
              "  1300913.625,\n",
              "  1309687.875,\n",
              "  1324097.375,\n",
              "  1326523.0,\n",
              "  1310008.125,\n",
              "  1308203.125,\n",
              "  1302859.625,\n",
              "  1318532.375,\n",
              "  1324464.125,\n",
              "  1321683.25,\n",
              "  1310379.875,\n",
              "  1303375.75,\n",
              "  1319482.0,\n",
              "  1324308.625,\n",
              "  1316397.75,\n",
              "  1319093.625,\n",
              "  1323064.25,\n",
              "  1303250.0,\n",
              "  1316668.625,\n",
              "  1307604.875,\n",
              "  1308111.25,\n",
              "  1309831.25,\n",
              "  1296422.0,\n",
              "  1306918.0,\n",
              "  1318963.125,\n",
              "  1299457.25,\n",
              "  1310108.875,\n",
              "  1311692.5,\n",
              "  1300100.0,\n",
              "  1320645.625,\n",
              "  1315648.875,\n",
              "  1305065.25,\n",
              "  1320928.375,\n",
              "  1311455.875,\n",
              "  1304789.5,\n",
              "  1304764.625,\n",
              "  1306869.5,\n",
              "  1295277.375,\n",
              "  1298302.0,\n",
              "  1305120.5,\n",
              "  1291524.375,\n",
              "  1317390.5,\n",
              "  1301067.75,\n",
              "  1328890.25,\n",
              "  1314366.875,\n",
              "  1309516.625,\n",
              "  1308918.125,\n",
              "  1300604.5,\n",
              "  1293723.0,\n",
              "  1281120.0,\n",
              "  1287480.25,\n",
              "  1297984.75,\n",
              "  1297138.0,\n",
              "  1277670.25,\n",
              "  1290973.125,\n",
              "  1309723.875,\n",
              "  1309317.25,\n",
              "  1296799.5,\n",
              "  1288103.0,\n",
              "  1316568.0,\n",
              "  1284956.75,\n",
              "  1292998.75,\n",
              "  1293286.625,\n",
              "  1288152.0,\n",
              "  1312955.25,\n",
              "  1299866.875,\n",
              "  1281949.375,\n",
              "  1291527.0,\n",
              "  1300465.5,\n",
              "  1285734.125,\n",
              "  1311458.375,\n",
              "  1279213.375,\n",
              "  1296064.625,\n",
              "  1289061.0,\n",
              "  1280366.875,\n",
              "  1300279.375,\n",
              "  1291000.375,\n",
              "  1301055.5,\n",
              "  1305263.75,\n",
              "  1295132.25,\n",
              "  1294217.875,\n",
              "  1309920.375,\n",
              "  1303241.75,\n",
              "  1297773.125,\n",
              "  1303083.375,\n",
              "  1285855.625,\n",
              "  1304080.875,\n",
              "  1290426.625,\n",
              "  1281249.125,\n",
              "  1293433.875,\n",
              "  1291806.5,\n",
              "  1288341.125,\n",
              "  1263183.625,\n",
              "  1271048.0,\n",
              "  1280517.0,\n",
              "  1282980.625,\n",
              "  1287640.125,\n",
              "  1304034.875,\n",
              "  1280734.375,\n",
              "  1272241.25,\n",
              "  1303855.125,\n",
              "  1305953.375,\n",
              "  1282587.625,\n",
              "  1268901.625,\n",
              "  1303667.75,\n",
              "  1285729.5,\n",
              "  1278777.125,\n",
              "  1310919.0,\n",
              "  1285621.375,\n",
              "  1272425.125,\n",
              "  1304531.25,\n",
              "  1303327.5,\n",
              "  1275015.25,\n",
              "  1282408.875,\n",
              "  1295831.0,\n",
              "  1292026.125,\n",
              "  1291408.5,\n",
              "  1270057.625,\n",
              "  1291842.0,\n",
              "  1277562.625,\n",
              "  1292234.75,\n",
              "  1284488.625,\n",
              "  1279777.875,\n",
              "  1300254.375,\n",
              "  1285743.5,\n",
              "  1297329.875,\n",
              "  1271546.0,\n",
              "  1282520.0,\n",
              "  1292698.5,\n",
              "  1279054.75,\n",
              "  1272135.125,\n",
              "  1277553.375,\n",
              "  1282583.625,\n",
              "  1285377.375,\n",
              "  1298356.125,\n",
              "  1267480.625,\n",
              "  1280767.25,\n",
              "  1272465.75,\n",
              "  1286518.125,\n",
              "  1288532.5,\n",
              "  1292609.5,\n",
              "  1288672.875,\n",
              "  1280873.125,\n",
              "  1294719.375,\n",
              "  1292534.375,\n",
              "  1275090.75,\n",
              "  1268790.25,\n",
              "  1290155.5,\n",
              "  1259559.0,\n",
              "  1280302.875,\n",
              "  1280615.0,\n",
              "  1280468.625,\n",
              "  1287373.125,\n",
              "  1285261.5,\n",
              "  1258882.0,\n",
              "  1279545.0,\n",
              "  1274707.75,\n",
              "  1276240.0,\n",
              "  1277053.125,\n",
              "  1277502.25,\n",
              "  1288482.25,\n",
              "  1292403.875,\n",
              "  1285904.125,\n",
              "  1279523.0,\n",
              "  1274796.25,\n",
              "  1272090.0,\n",
              "  1268997.875,\n",
              "  1285936.0,\n",
              "  1284173.375,\n",
              "  1270470.75,\n",
              "  1277288.25,\n",
              "  1272107.0,\n",
              "  1248832.625,\n",
              "  1288669.5,\n",
              "  1284438.25,\n",
              "  1277593.375,\n",
              "  1283982.0,\n",
              "  1267434.25,\n",
              "  1251472.5,\n",
              "  1284940.375,\n",
              "  1280659.25,\n",
              "  1282118.375,\n",
              "  1274510.875,\n",
              "  1281877.75,\n",
              "  1251259.375,\n",
              "  1256626.875,\n",
              "  1248709.0,\n",
              "  1237982.0,\n",
              "  1255703.125,\n",
              "  1254275.375,\n",
              "  1274467.375,\n",
              "  1275830.75,\n",
              "  1269511.375,\n",
              "  1268276.25,\n",
              "  1250507.5,\n",
              "  1265620.875,\n",
              "  1260536.625,\n",
              "  1258536.75,\n",
              "  1248033.875,\n",
              "  1271358.125,\n",
              "  1275840.5,\n",
              "  1286184.125,\n",
              "  1265650.125,\n",
              "  1258930.75,\n",
              "  1280555.0,\n",
              "  1251787.375,\n",
              "  1271641.125,\n",
              "  1267131.875,\n",
              "  1259830.625,\n",
              "  1278387.875,\n",
              "  1251025.125,\n",
              "  1255006.625,\n",
              "  1269279.375,\n",
              "  1264417.375,\n",
              "  1274075.375,\n",
              "  1258349.125,\n",
              "  1251358.0,\n",
              "  1265230.5,\n",
              "  1262063.25,\n",
              "  1251294.625,\n",
              "  1268602.125,\n",
              "  1262728.375,\n",
              "  1238522.125,\n",
              "  1268004.875,\n",
              "  1246870.25,\n",
              "  1255745.0,\n",
              "  1261085.0,\n",
              "  1258411.75,\n",
              "  1268534.625,\n",
              "  1267968.25,\n",
              "  1271753.25,\n",
              "  1249348.125,\n",
              "  1234571.125,\n",
              "  1261812.5,\n",
              "  1251708.875,\n",
              "  1252026.75,\n",
              "  1247992.25,\n",
              "  1275750.125,\n",
              "  1255499.0,\n",
              "  1235863.625,\n",
              "  1250178.25,\n",
              "  1266870.75,\n",
              "  1268263.0,\n",
              "  1251214.375,\n",
              "  1263039.25,\n",
              "  1273205.625,\n",
              "  1251205.25,\n",
              "  1251622.625,\n",
              "  1257267.0,\n",
              "  1263098.0,\n",
              "  1252607.625,\n",
              "  1251235.0,\n",
              "  1242216.25,\n",
              "  1247027.5,\n",
              "  1264498.0,\n",
              "  1255759.75,\n",
              "  1245002.875,\n",
              "  1243341.25,\n",
              "  1243259.25,\n",
              "  1262515.125,\n",
              "  1263267.5,\n",
              "  1250699.25,\n",
              "  1249186.25,\n",
              "  1252431.875,\n",
              "  1238766.75,\n",
              "  1249287.75,\n",
              "  1252615.125,\n",
              "  1235824.875,\n",
              "  1248927.375,\n",
              "  1253089.0,\n",
              "  1251269.375,\n",
              "  1255472.0,\n",
              "  1239324.75,\n",
              "  1252775.0,\n",
              "  1247798.5,\n",
              "  1241755.75,\n",
              "  1248410.375,\n",
              "  1256127.125,\n",
              "  1237176.5,\n",
              "  1245401.375,\n",
              "  1239947.375,\n",
              "  1243586.625,\n",
              "  1236215.5,\n",
              "  1252815.25,\n",
              "  1251028.25,\n",
              "  1234951.75,\n",
              "  1242937.5,\n",
              "  1254558.75,\n",
              "  1241831.875],\n",
              " 'mae': [1002.1371459960938,\n",
              "  1007.1339111328125,\n",
              "  1006.320556640625,\n",
              "  1009.6442260742188,\n",
              "  1003.5938720703125,\n",
              "  1011.6112670898438,\n",
              "  1002.8279418945312,\n",
              "  1008.10107421875,\n",
              "  1004.5677490234375,\n",
              "  1007.252685546875,\n",
              "  1008.5203857421875,\n",
              "  1006.3929443359375,\n",
              "  1005.5890502929688,\n",
              "  1009.0394287109375,\n",
              "  997.6647338867188,\n",
              "  1005.6388549804688,\n",
              "  1010.220703125,\n",
              "  993.3704833984375,\n",
              "  1004.8783569335938,\n",
              "  998.9362182617188,\n",
              "  1005.5015869140625,\n",
              "  1001.6332397460938,\n",
              "  1003.7163696289062,\n",
              "  1004.2576293945312,\n",
              "  1001.218017578125,\n",
              "  999.8073120117188,\n",
              "  997.032958984375,\n",
              "  1001.0654907226562,\n",
              "  1011.8953857421875,\n",
              "  1002.59912109375,\n",
              "  1006.1577758789062,\n",
              "  1007.8543701171875,\n",
              "  1008.5448608398438,\n",
              "  1002.5283203125,\n",
              "  990.5555419921875,\n",
              "  1002.6576538085938,\n",
              "  996.1514282226562,\n",
              "  1006.7986450195312,\n",
              "  1006.4182739257812,\n",
              "  994.1029052734375,\n",
              "  990.9124755859375,\n",
              "  999.6009521484375,\n",
              "  994.4168701171875,\n",
              "  995.4154663085938,\n",
              "  1002.5167236328125,\n",
              "  984.9431762695312,\n",
              "  999.2416381835938,\n",
              "  997.9146118164062,\n",
              "  1000.8815307617188,\n",
              "  996.2525634765625,\n",
              "  992.7111206054688,\n",
              "  999.1578369140625,\n",
              "  989.6392822265625,\n",
              "  999.1032104492188,\n",
              "  998.3753051757812,\n",
              "  998.3787841796875,\n",
              "  1001.2685546875,\n",
              "  1004.5506591796875,\n",
              "  998.9069213867188,\n",
              "  983.0818481445312,\n",
              "  996.9366455078125,\n",
              "  996.9613647460938,\n",
              "  1000.6384887695312,\n",
              "  997.5300903320312,\n",
              "  999.3277587890625,\n",
              "  991.8279418945312,\n",
              "  996.2109375,\n",
              "  988.3720703125,\n",
              "  994.91357421875,\n",
              "  999.7977905273438,\n",
              "  994.1726684570312,\n",
              "  994.040283203125,\n",
              "  991.1373291015625,\n",
              "  994.88818359375,\n",
              "  993.8529052734375,\n",
              "  991.0707397460938,\n",
              "  986.6051635742188,\n",
              "  1002.6546630859375,\n",
              "  989.2316284179688,\n",
              "  995.0693969726562,\n",
              "  992.9962158203125,\n",
              "  994.8013305664062,\n",
              "  999.5335083007812,\n",
              "  984.4432983398438,\n",
              "  1002.5810546875,\n",
              "  1001.7734375,\n",
              "  983.9979858398438,\n",
              "  993.6469116210938,\n",
              "  997.4699096679688,\n",
              "  993.6104125976562,\n",
              "  990.2098388671875,\n",
              "  998.514404296875,\n",
              "  992.0685424804688,\n",
              "  996.5166625976562,\n",
              "  991.1307983398438,\n",
              "  994.5233154296875,\n",
              "  997.3695068359375,\n",
              "  994.3482666015625,\n",
              "  993.5048217773438,\n",
              "  985.538330078125,\n",
              "  991.3663940429688,\n",
              "  983.6426391601562,\n",
              "  990.6074829101562,\n",
              "  996.2985229492188,\n",
              "  993.1084594726562,\n",
              "  986.8421630859375,\n",
              "  990.2911376953125,\n",
              "  991.0740966796875,\n",
              "  993.1087036132812,\n",
              "  990.9497680664062,\n",
              "  993.5965576171875,\n",
              "  989.8522338867188,\n",
              "  993.939453125,\n",
              "  986.2757568359375,\n",
              "  993.45068359375,\n",
              "  992.0443725585938,\n",
              "  984.3837280273438,\n",
              "  998.6087646484375,\n",
              "  992.9703369140625,\n",
              "  985.7781372070312,\n",
              "  987.40966796875,\n",
              "  990.0680541992188,\n",
              "  985.2449340820312,\n",
              "  982.227783203125,\n",
              "  988.442626953125,\n",
              "  977.2897338867188,\n",
              "  984.6866455078125,\n",
              "  987.3848876953125,\n",
              "  986.1810302734375,\n",
              "  980.4332885742188,\n",
              "  992.1954345703125,\n",
              "  983.56005859375,\n",
              "  982.9514770507812,\n",
              "  986.2803955078125,\n",
              "  975.376953125,\n",
              "  989.00732421875,\n",
              "  980.6099853515625,\n",
              "  986.5897827148438,\n",
              "  979.5950317382812,\n",
              "  990.6065063476562,\n",
              "  983.0095825195312,\n",
              "  994.7679443359375,\n",
              "  998.221435546875,\n",
              "  999.1160888671875,\n",
              "  985.3446044921875,\n",
              "  983.3903198242188,\n",
              "  987.534912109375,\n",
              "  990.85986328125,\n",
              "  987.5762329101562,\n",
              "  991.1477661132812,\n",
              "  975.8072509765625,\n",
              "  988.9459838867188,\n",
              "  987.9567260742188,\n",
              "  983.2410888671875,\n",
              "  993.1317749023438,\n",
              "  988.19921875,\n",
              "  986.0931396484375,\n",
              "  976.7854614257812,\n",
              "  980.7276611328125,\n",
              "  976.058837890625,\n",
              "  981.8931274414062,\n",
              "  974.4979858398438,\n",
              "  987.1084594726562,\n",
              "  986.7203369140625,\n",
              "  983.906494140625,\n",
              "  982.20703125,\n",
              "  984.1969604492188,\n",
              "  991.2888793945312,\n",
              "  977.8898315429688,\n",
              "  978.012451171875,\n",
              "  978.9715576171875,\n",
              "  977.292724609375,\n",
              "  981.3399047851562,\n",
              "  981.885009765625,\n",
              "  981.4051513671875,\n",
              "  975.6463623046875,\n",
              "  984.6942749023438,\n",
              "  988.3984375,\n",
              "  981.1058959960938,\n",
              "  980.8516845703125,\n",
              "  984.587646484375,\n",
              "  983.4229736328125,\n",
              "  986.8225708007812,\n",
              "  984.8250732421875,\n",
              "  980.3150634765625,\n",
              "  980.1950073242188,\n",
              "  969.51806640625,\n",
              "  977.86181640625,\n",
              "  974.134765625,\n",
              "  975.968994140625,\n",
              "  983.125244140625,\n",
              "  973.1552734375,\n",
              "  983.2238159179688,\n",
              "  976.2955322265625,\n",
              "  977.13134765625,\n",
              "  979.4970092773438,\n",
              "  981.6177978515625,\n",
              "  969.4559936523438,\n",
              "  981.5135498046875,\n",
              "  972.18212890625,\n",
              "  976.9677734375,\n",
              "  961.34912109375,\n",
              "  982.7286376953125,\n",
              "  976.583984375,\n",
              "  984.5104370117188,\n",
              "  977.5321044921875,\n",
              "  970.76513671875,\n",
              "  978.5872802734375,\n",
              "  980.4559936523438,\n",
              "  980.2619018554688,\n",
              "  969.6099853515625,\n",
              "  976.24462890625,\n",
              "  976.6168212890625,\n",
              "  978.4185180664062,\n",
              "  968.8582763671875,\n",
              "  978.2078247070312,\n",
              "  973.2239990234375,\n",
              "  979.833984375,\n",
              "  971.3611450195312,\n",
              "  970.882080078125,\n",
              "  974.2112426757812,\n",
              "  968.5899047851562,\n",
              "  980.2014770507812,\n",
              "  981.5382080078125,\n",
              "  977.228515625,\n",
              "  971.24365234375,\n",
              "  973.6849365234375,\n",
              "  970.1962890625,\n",
              "  975.0433959960938,\n",
              "  968.1820068359375,\n",
              "  971.3934936523438,\n",
              "  965.8182983398438,\n",
              "  975.4222412109375,\n",
              "  979.0462036132812,\n",
              "  973.1261596679688,\n",
              "  971.5549926757812,\n",
              "  972.3240966796875,\n",
              "  974.0462036132812,\n",
              "  975.8364868164062,\n",
              "  975.3045654296875,\n",
              "  977.0440063476562,\n",
              "  971.9684448242188,\n",
              "  973.15283203125,\n",
              "  969.5924682617188,\n",
              "  965.7349243164062,\n",
              "  973.1102905273438,\n",
              "  962.5012817382812,\n",
              "  975.9943237304688,\n",
              "  976.2322998046875,\n",
              "  970.6121215820312,\n",
              "  967.39013671875,\n",
              "  968.71142578125,\n",
              "  972.2664184570312,\n",
              "  972.0967407226562,\n",
              "  967.8638305664062,\n",
              "  970.7656860351562,\n",
              "  971.9265747070312,\n",
              "  965.1306762695312,\n",
              "  973.6622924804688,\n",
              "  968.79296875,\n",
              "  970.2965087890625,\n",
              "  971.4697265625,\n",
              "  966.1033325195312,\n",
              "  972.6990966796875,\n",
              "  970.877685546875,\n",
              "  962.061767578125,\n",
              "  969.2324829101562,\n",
              "  972.6244506835938,\n",
              "  967.3573608398438,\n",
              "  968.7642822265625,\n",
              "  965.0531616210938,\n",
              "  967.803466796875,\n",
              "  961.7442016601562,\n",
              "  964.3832397460938,\n",
              "  962.7262573242188,\n",
              "  957.8270874023438,\n",
              "  977.9000244140625,\n",
              "  965.7014770507812,\n",
              "  967.5567016601562,\n",
              "  964.2525634765625,\n",
              "  962.7569580078125,\n",
              "  964.9843139648438,\n",
              "  965.916748046875,\n",
              "  975.171630859375,\n",
              "  964.5552978515625,\n",
              "  963.6453247070312,\n",
              "  966.0211791992188,\n",
              "  964.9672241210938,\n",
              "  965.21142578125,\n",
              "  962.2575073242188,\n",
              "  973.5445556640625,\n",
              "  962.7132568359375,\n",
              "  961.06591796875,\n",
              "  965.0121459960938,\n",
              "  970.9202880859375,\n",
              "  977.6207885742188,\n",
              "  967.5524291992188,\n",
              "  963.601806640625,\n",
              "  963.9756469726562,\n",
              "  964.876708984375,\n",
              "  968.932373046875,\n",
              "  967.3624267578125,\n",
              "  968.2247924804688,\n",
              "  960.420654296875,\n",
              "  969.1221923828125,\n",
              "  967.2528076171875,\n",
              "  962.2586059570312,\n",
              "  963.8390502929688,\n",
              "  961.7051391601562,\n",
              "  964.5491943359375,\n",
              "  963.6025390625,\n",
              "  968.8350219726562,\n",
              "  963.2767333984375,\n",
              "  955.2545166015625,\n",
              "  958.9398803710938,\n",
              "  962.105224609375,\n",
              "  960.442138671875,\n",
              "  967.0858154296875,\n",
              "  956.6573486328125,\n",
              "  958.1935424804688,\n",
              "  957.8798217773438,\n",
              "  956.14208984375,\n",
              "  958.142822265625,\n",
              "  958.9291381835938,\n",
              "  956.41796875,\n",
              "  962.713623046875,\n",
              "  964.5027465820312,\n",
              "  960.1958618164062,\n",
              "  964.6725463867188,\n",
              "  958.1050415039062,\n",
              "  958.3802490234375,\n",
              "  951.5892333984375,\n",
              "  956.971435546875,\n",
              "  962.4591674804688,\n",
              "  960.1080322265625,\n",
              "  967.2376708984375,\n",
              "  961.916015625,\n",
              "  958.44189453125,\n",
              "  962.8704833984375,\n",
              "  956.693603515625,\n",
              "  960.4594116210938,\n",
              "  960.9366455078125,\n",
              "  951.7774658203125,\n",
              "  957.1779174804688,\n",
              "  969.7958374023438,\n",
              "  950.2153930664062,\n",
              "  958.59130859375,\n",
              "  961.15869140625,\n",
              "  966.666748046875,\n",
              "  955.916748046875,\n",
              "  960.6654052734375,\n",
              "  959.7347412109375,\n",
              "  955.8405151367188,\n",
              "  960.8388061523438,\n",
              "  957.0037841796875,\n",
              "  957.9163208007812,\n",
              "  953.2366943359375,\n",
              "  950.916748046875,\n",
              "  960.8331298828125,\n",
              "  956.9111938476562,\n",
              "  956.564453125,\n",
              "  956.574951171875,\n",
              "  961.0210571289062,\n",
              "  963.5709838867188,\n",
              "  956.8287963867188,\n",
              "  958.2935791015625,\n",
              "  948.490234375,\n",
              "  955.3967895507812,\n",
              "  953.474853515625,\n",
              "  953.0639038085938,\n",
              "  961.7366943359375,\n",
              "  953.625,\n",
              "  954.6026611328125,\n",
              "  952.2666625976562,\n",
              "  950.060791015625,\n",
              "  958.6146850585938,\n",
              "  958.3314819335938,\n",
              "  963.4584350585938,\n",
              "  949.233154296875,\n",
              "  955.9996948242188,\n",
              "  959.8345947265625,\n",
              "  956.7412109375,\n",
              "  957.4468383789062,\n",
              "  942.8056030273438,\n",
              "  958.9310302734375,\n",
              "  954.1460571289062,\n",
              "  957.08056640625,\n",
              "  963.5966796875,\n",
              "  958.2988891601562,\n",
              "  947.3211059570312,\n",
              "  955.1336059570312,\n",
              "  952.94482421875,\n",
              "  954.1050415039062,\n",
              "  949.947265625,\n",
              "  951.9369506835938,\n",
              "  954.6166381835938,\n",
              "  945.6720581054688,\n",
              "  944.95361328125,\n",
              "  945.9765625,\n",
              "  953.5394897460938,\n",
              "  953.4667358398438,\n",
              "  947.19921875,\n",
              "  953.7568969726562,\n",
              "  949.420654296875,\n",
              "  953.6387329101562,\n",
              "  943.2211303710938,\n",
              "  940.5537109375,\n",
              "  954.401611328125,\n",
              "  953.1235961914062,\n",
              "  948.7582397460938,\n",
              "  952.6090087890625,\n",
              "  946.9420166015625,\n",
              "  944.6637573242188,\n",
              "  948.3177490234375,\n",
              "  947.942626953125,\n",
              "  951.18017578125,\n",
              "  941.2243041992188,\n",
              "  955.20556640625,\n",
              "  946.6201782226562,\n",
              "  936.3560791015625,\n",
              "  950.9800415039062,\n",
              "  954.1582641601562,\n",
              "  950.2249755859375,\n",
              "  945.5547485351562,\n",
              "  948.0689697265625,\n",
              "  951.3314208984375,\n",
              "  949.63818359375,\n",
              "  940.02587890625,\n",
              "  953.5172729492188,\n",
              "  944.9364624023438,\n",
              "  951.9166870117188,\n",
              "  939.1484375,\n",
              "  945.9833374023438,\n",
              "  943.5890502929688,\n",
              "  945.9903564453125,\n",
              "  946.30078125,\n",
              "  946.6279907226562,\n",
              "  933.9287109375,\n",
              "  943.3895874023438,\n",
              "  949.97021484375,\n",
              "  941.976806640625,\n",
              "  937.5292358398438,\n",
              "  946.5582885742188,\n",
              "  940.1885986328125,\n",
              "  953.656005859375,\n",
              "  935.7366943359375,\n",
              "  942.6822509765625,\n",
              "  948.9239501953125,\n",
              "  944.8428955078125,\n",
              "  942.702392578125,\n",
              "  943.730224609375,\n",
              "  940.3342895507812,\n",
              "  942.4324340820312,\n",
              "  940.4014282226562,\n",
              "  950.4754028320312,\n",
              "  941.6458129882812,\n",
              "  949.5245971679688,\n",
              "  950.2307739257812,\n",
              "  944.3453369140625,\n",
              "  944.91943359375,\n",
              "  935.0674438476562,\n",
              "  946.8680419921875,\n",
              "  942.49560546875,\n",
              "  946.7841796875,\n",
              "  945.454345703125,\n",
              "  940.2938232421875,\n",
              "  937.9649047851562,\n",
              "  944.1617431640625,\n",
              "  944.6128540039062,\n",
              "  942.56982421875,\n",
              "  942.09716796875,\n",
              "  946.3223876953125,\n",
              "  938.0336303710938,\n",
              "  940.5936279296875,\n",
              "  944.9749755859375,\n",
              "  947.1060180664062,\n",
              "  942.0284423828125,\n",
              "  945.9256591796875,\n",
              "  933.3624267578125,\n",
              "  940.1329345703125,\n",
              "  942.0240478515625,\n",
              "  950.7132568359375,\n",
              "  943.4675903320312,\n",
              "  943.4580078125,\n",
              "  940.0128784179688,\n",
              "  943.7586059570312,\n",
              "  944.4259033203125,\n",
              "  941.7406616210938,\n",
              "  936.4165649414062,\n",
              "  941.4091796875,\n",
              "  935.056396484375,\n",
              "  933.8225708007812,\n",
              "  948.4349365234375,\n",
              "  936.386474609375,\n",
              "  947.9752807617188,\n",
              "  940.6727905273438,\n",
              "  946.5693969726562,\n",
              "  934.2440795898438,\n",
              "  938.8388061523438,\n",
              "  929.9279174804688,\n",
              "  940.5060424804688,\n",
              "  941.9866943359375,\n",
              "  941.8683471679688,\n",
              "  937.30908203125,\n",
              "  937.8692016601562,\n",
              "  935.4763793945312,\n",
              "  949.9373779296875,\n",
              "  939.4523315429688,\n",
              "  928.0875244140625,\n",
              "  945.8030395507812,\n",
              "  934.7757568359375,\n",
              "  941.832275390625,\n",
              "  936.800048828125,\n",
              "  947.6322021484375,\n",
              "  945.9583129882812,\n",
              "  935.6467895507812,\n",
              "  932.4537353515625,\n",
              "  934.8511352539062,\n",
              "  931.72216796875,\n",
              "  924.6738891601562,\n",
              "  938.8662109375,\n",
              "  942.6755981445312,\n",
              "  931.332275390625,\n",
              "  935.2142333984375,\n",
              "  934.4927978515625,\n",
              "  943.2078857421875,\n",
              "  934.1825561523438,\n",
              "  935.9066772460938,\n",
              "  937.9015502929688,\n",
              "  940.2112426757812,\n",
              "  935.7975463867188,\n",
              "  931.461669921875,\n",
              "  937.7083129882812,\n",
              "  935.0928955078125,\n",
              "  940.4605712890625,\n",
              "  929.904541015625,\n",
              "  938.5121459960938,\n",
              "  941.5150146484375,\n",
              "  936.4547119140625,\n",
              "  938.2896118164062,\n",
              "  944.8126220703125,\n",
              "  930.2984619140625,\n",
              "  925.6832275390625,\n",
              "  933.65283203125,\n",
              "  930.6182250976562,\n",
              "  936.2252197265625,\n",
              "  927.33203125,\n",
              "  923.3121948242188,\n",
              "  930.06005859375,\n",
              "  928.9010009765625,\n",
              "  928.8519897460938,\n",
              "  931.5120239257812,\n",
              "  932.8895874023438,\n",
              "  937.5224609375,\n",
              "  929.4823608398438,\n",
              "  929.8089599609375,\n",
              "  926.1497192382812,\n",
              "  924.050048828125,\n",
              "  930.9198608398438,\n",
              "  938.6735229492188,\n",
              "  929.3599853515625,\n",
              "  919.7730102539062,\n",
              "  921.227783203125,\n",
              "  932.0505981445312,\n",
              "  933.8846435546875,\n",
              "  919.3472900390625,\n",
              "  929.3856201171875,\n",
              "  932.9942016601562,\n",
              "  929.8506469726562,\n",
              "  920.364501953125,\n",
              "  927.1453247070312,\n",
              "  932.2662353515625,\n",
              "  928.5834350585938,\n",
              "  931.7828369140625,\n",
              "  935.3086547851562,\n",
              "  933.6439208984375,\n",
              "  928.5977172851562,\n",
              "  925.14111328125,\n",
              "  931.9359130859375,\n",
              "  928.3523559570312,\n",
              "  922.3781127929688,\n",
              "  935.0216064453125,\n",
              "  936.7853393554688,\n",
              "  927.264404296875,\n",
              "  932.9351196289062,\n",
              "  930.26171875,\n",
              "  923.0442504882812,\n",
              "  932.7627563476562,\n",
              "  924.1119995117188,\n",
              "  935.5176391601562,\n",
              "  923.3734741210938,\n",
              "  939.3634033203125,\n",
              "  929.5405883789062,\n",
              "  925.2218017578125,\n",
              "  924.7533569335938,\n",
              "  934.4235229492188,\n",
              "  919.3900146484375,\n",
              "  926.5278930664062,\n",
              "  924.8291625976562,\n",
              "  920.4574584960938,\n",
              "  920.83837890625,\n",
              "  919.9640502929688,\n",
              "  922.2655639648438,\n",
              "  921.5615234375,\n",
              "  930.366943359375,\n",
              "  927.0364990234375,\n",
              "  930.2716064453125,\n",
              "  916.4905395507812,\n",
              "  926.8158569335938,\n",
              "  929.965087890625,\n",
              "  922.6377563476562,\n",
              "  922.0631713867188,\n",
              "  929.0728149414062,\n",
              "  921.2444458007812,\n",
              "  913.5086059570312,\n",
              "  915.7216796875,\n",
              "  919.5543212890625,\n",
              "  926.8806762695312,\n",
              "  915.1244506835938,\n",
              "  918.1707763671875,\n",
              "  926.3906860351562,\n",
              "  921.5363159179688,\n",
              "  923.6477661132812,\n",
              "  929.7130737304688,\n",
              "  927.3079833984375,\n",
              "  920.3613891601562,\n",
              "  926.245361328125,\n",
              "  928.3365478515625,\n",
              "  920.2044677734375,\n",
              "  919.0430908203125,\n",
              "  921.6622924804688,\n",
              "  914.9908447265625,\n",
              "  925.1444091796875,\n",
              "  918.3629760742188,\n",
              "  916.5082397460938,\n",
              "  917.995361328125,\n",
              "  920.749755859375,\n",
              "  930.029541015625,\n",
              "  919.599853515625,\n",
              "  922.7265014648438,\n",
              "  918.003662109375,\n",
              "  929.702880859375,\n",
              "  921.7323608398438,\n",
              "  926.2363891601562,\n",
              "  918.7586059570312,\n",
              "  923.7844848632812,\n",
              "  916.4332275390625,\n",
              "  912.6627807617188,\n",
              "  921.2273559570312,\n",
              "  924.4342651367188,\n",
              "  917.417724609375,\n",
              "  913.7931518554688,\n",
              "  920.5441284179688,\n",
              "  922.156494140625,\n",
              "  914.134033203125,\n",
              "  915.0489501953125,\n",
              "  913.1763916015625,\n",
              "  909.8345947265625,\n",
              "  922.6880493164062,\n",
              "  922.9791870117188,\n",
              "  911.468994140625,\n",
              "  923.2867431640625,\n",
              "  914.5802001953125,\n",
              "  917.3883056640625,\n",
              "  921.9495239257812,\n",
              "  920.2907104492188,\n",
              "  912.2058715820312,\n",
              "  914.4319458007812,\n",
              "  913.4441528320312,\n",
              "  915.6214599609375,\n",
              "  919.5960083007812,\n",
              "  919.9376220703125,\n",
              "  916.6978149414062,\n",
              "  917.666259765625,\n",
              "  918.5703125,\n",
              "  915.5994262695312,\n",
              "  918.2188720703125,\n",
              "  914.3593139648438,\n",
              "  919.7623291015625,\n",
              "  916.8834838867188,\n",
              "  919.8334350585938,\n",
              "  907.464111328125,\n",
              "  918.5095825195312,\n",
              "  917.7310180664062,\n",
              "  912.0234375,\n",
              "  917.2301635742188,\n",
              "  901.1913452148438,\n",
              "  910.6578369140625,\n",
              "  912.0545654296875,\n",
              "  911.6066284179688,\n",
              "  913.7390747070312,\n",
              "  912.6237182617188,\n",
              "  917.3574829101562,\n",
              "  907.7556762695312,\n",
              "  906.7320556640625,\n",
              "  906.9970703125,\n",
              "  918.7578125,\n",
              "  919.0015869140625,\n",
              "  907.6251220703125,\n",
              "  921.3923950195312,\n",
              "  911.054443359375,\n",
              "  917.2256469726562,\n",
              "  922.1853637695312,\n",
              "  914.177001953125,\n",
              "  907.5559692382812,\n",
              "  920.2142333984375,\n",
              "  913.2330322265625,\n",
              "  910.6226196289062,\n",
              "  914.4363403320312,\n",
              "  912.2153930664062,\n",
              "  908.768798828125,\n",
              "  904.9486694335938,\n",
              "  908.4385986328125,\n",
              "  915.9885864257812,\n",
              "  916.6997680664062,\n",
              "  912.0707397460938,\n",
              "  906.3695068359375,\n",
              "  906.7576293945312,\n",
              "  911.377685546875,\n",
              "  914.8929443359375,\n",
              "  912.2609252929688,\n",
              "  908.2023315429688,\n",
              "  905.9690551757812,\n",
              "  913.71630859375,\n",
              "  914.4415893554688,\n",
              "  909.8779296875,\n",
              "  912.072509765625,\n",
              "  913.4318237304688,\n",
              "  903.8396606445312,\n",
              "  912.360595703125,\n",
              "  907.5521850585938,\n",
              "  905.7069091796875,\n",
              "  909.59033203125,\n",
              "  900.6618041992188,\n",
              "  907.47412109375,\n",
              "  911.867919921875,\n",
              "  905.0309448242188,\n",
              "  908.3976440429688,\n",
              "  909.4591674804688,\n",
              "  907.0628051757812,\n",
              "  913.2772827148438,\n",
              "  910.0828247070312,\n",
              "  907.7595825195312,\n",
              "  913.3711547851562,\n",
              "  909.1041870117188,\n",
              "  905.1459350585938,\n",
              "  907.2202758789062,\n",
              "  906.04736328125,\n",
              "  902.1226196289062,\n",
              "  900.5028076171875,\n",
              "  903.734619140625,\n",
              "  899.4656982421875,\n",
              "  912.5390625,\n",
              "  904.3885498046875,\n",
              "  917.7993774414062,\n",
              "  908.93017578125,\n",
              "  907.8004760742188,\n",
              "  907.4265747070312,\n",
              "  904.39208984375,\n",
              "  901.039306640625,\n",
              "  894.79736328125,\n",
              "  899.4315795898438,\n",
              "  901.1261596679688,\n",
              "  904.3814697265625,\n",
              "  896.7565307617188,\n",
              "  899.47509765625,\n",
              "  910.6650390625,\n",
              "  908.4082641601562,\n",
              "  901.8818969726562,\n",
              "  898.6229248046875,\n",
              "  911.6585083007812,\n",
              "  895.479248046875,\n",
              "  901.0130004882812,\n",
              "  899.2850952148438,\n",
              "  896.0089111328125,\n",
              "  909.6478881835938,\n",
              "  901.7122192382812,\n",
              "  897.0958251953125,\n",
              "  900.79638671875,\n",
              "  903.0703125,\n",
              "  897.2962036132812,\n",
              "  909.1425170898438,\n",
              "  896.4806518554688,\n",
              "  901.49755859375,\n",
              "  899.81640625,\n",
              "  894.8943481445312,\n",
              "  905.64453125,\n",
              "  898.4373779296875,\n",
              "  904.0740966796875,\n",
              "  905.865234375,\n",
              "  900.1470947265625,\n",
              "  898.5467529296875,\n",
              "  908.78759765625,\n",
              "  904.4028930664062,\n",
              "  901.8839111328125,\n",
              "  908.6290893554688,\n",
              "  896.3802490234375,\n",
              "  904.491943359375,\n",
              "  900.5540771484375,\n",
              "  895.1318969726562,\n",
              "  902.8658447265625,\n",
              "  898.4938354492188,\n",
              "  898.6560668945312,\n",
              "  890.6681518554688,\n",
              "  890.739501953125,\n",
              "  894.6535034179688,\n",
              "  894.6694946289062,\n",
              "  897.1700439453125,\n",
              "  906.0780639648438,\n",
              "  897.7615356445312,\n",
              "  893.2235717773438,\n",
              "  904.2515258789062,\n",
              "  906.4659423828125,\n",
              "  895.0411376953125,\n",
              "  891.62939453125,\n",
              "  905.449951171875,\n",
              "  897.2379150390625,\n",
              "  894.4252319335938,\n",
              "  909.5257568359375,\n",
              "  898.551513671875,\n",
              "  891.95654296875,\n",
              "  904.4306030273438,\n",
              "  903.8693237304688,\n",
              "  890.15380859375,\n",
              "  893.5618286132812,\n",
              "  904.3291625976562,\n",
              "  899.183349609375,\n",
              "  898.948486328125,\n",
              "  890.1624145507812,\n",
              "  898.9744262695312,\n",
              "  891.8154907226562,\n",
              "  898.6913452148438,\n",
              "  895.0823364257812,\n",
              "  893.60546875,\n",
              "  903.9464111328125,\n",
              "  896.5097045898438,\n",
              "  901.1456298828125,\n",
              "  890.0565795898438,\n",
              "  894.8616333007812,\n",
              "  899.601318359375,\n",
              "  894.0972900390625,\n",
              "  890.9671630859375,\n",
              "  893.1076049804688,\n",
              "  895.8623046875,\n",
              "  895.9099731445312,\n",
              "  902.9041137695312,\n",
              "  887.6322631835938,\n",
              "  894.2489013671875,\n",
              "  890.4404907226562,\n",
              "  896.9234008789062,\n",
              "  897.3399658203125,\n",
              "  899.9563598632812,\n",
              "  898.4735107421875,\n",
              "  893.736328125,\n",
              "  900.8512573242188,\n",
              "  900.007080078125,\n",
              "  893.6774291992188,\n",
              "  888.7301025390625,\n",
              "  897.8733520507812,\n",
              "  885.6273803710938,\n",
              "  893.8707885742188,\n",
              "  893.3936767578125,\n",
              "  893.1277465820312,\n",
              "  896.8583374023438,\n",
              "  895.2849731445312,\n",
              "  884.5012817382812,\n",
              "  892.99072265625,\n",
              "  891.1046752929688,\n",
              "  891.0610961914062,\n",
              "  894.1016235351562,\n",
              "  891.4662475585938,\n",
              "  896.3211059570312,\n",
              "  901.3381958007812,\n",
              "  896.063232421875,\n",
              "  892.3209838867188,\n",
              "  889.6511840820312,\n",
              "  891.7389526367188,\n",
              "  888.356201171875,\n",
              "  895.2077026367188,\n",
              "  893.550048828125,\n",
              "  888.2662353515625,\n",
              "  891.6177978515625,\n",
              "  889.5115966796875,\n",
              "  880.5762329101562,\n",
              "  898.9881591796875,\n",
              "  896.1654663085938,\n",
              "  891.6469116210938,\n",
              "  893.0836791992188,\n",
              "  887.077392578125,\n",
              "  881.3037719726562,\n",
              "  894.8980712890625,\n",
              "  893.8286743164062,\n",
              "  893.2510986328125,\n",
              "  891.1251220703125,\n",
              "  892.8782348632812,\n",
              "  879.7175903320312,\n",
              "  883.8759765625,\n",
              "  879.5507202148438,\n",
              "  877.453125,\n",
              "  882.0144653320312,\n",
              "  881.0870971679688,\n",
              "  891.3004760742188,\n",
              "  888.7780151367188,\n",
              "  889.1469116210938,\n",
              "  887.9940185546875,\n",
              "  879.6446533203125,\n",
              "  888.0701293945312,\n",
              "  883.7153930664062,\n",
              "  884.164306640625,\n",
              "  878.9343872070312,\n",
              "  888.9639282226562,\n",
              "  892.1355590820312,\n",
              "  896.6571044921875,\n",
              "  887.0966796875,\n",
              "  882.7813720703125,\n",
              "  894.218017578125,\n",
              "  880.673095703125,\n",
              "  887.9959716796875,\n",
              "  886.6115112304688,\n",
              "  884.255126953125,\n",
              "  893.5344848632812,\n",
              "  881.9862670898438,\n",
              "  884.0918579101562,\n",
              "  887.483642578125,\n",
              "  886.5707397460938,\n",
              "  890.5719604492188,\n",
              "  883.406494140625,\n",
              "  879.9315795898438,\n",
              "  887.0771484375,\n",
              "  885.3447265625,\n",
              "  881.893310546875,\n",
              "  892.54833984375,\n",
              "  888.4581909179688,\n",
              "  876.5077514648438,\n",
              "  888.7866821289062,\n",
              "  878.3284912109375,\n",
              "  882.1148681640625,\n",
              "  884.9774780273438,\n",
              "  884.678466796875,\n",
              "  886.5203857421875,\n",
              "  887.8423461914062,\n",
              "  890.3778076171875,\n",
              "  878.1869506835938,\n",
              "  873.2241821289062,\n",
              "  884.601318359375,\n",
              "  880.8528442382812,\n",
              "  883.4591674804688,\n",
              "  877.9662475585938,\n",
              "  891.8936767578125,\n",
              "  880.9149780273438,\n",
              "  874.5520629882812,\n",
              "  878.3077392578125,\n",
              "  887.2747802734375,\n",
              "  887.2694702148438,\n",
              "  880.1187744140625,\n",
              "  885.3346557617188,\n",
              "  890.8973999023438,\n",
              "  879.6536865234375,\n",
              "  879.96630859375,\n",
              "  882.3972778320312,\n",
              "  885.4800415039062,\n",
              "  879.8306274414062,\n",
              "  878.7803955078125,\n",
              "  876.180908203125,\n",
              "  875.95556640625,\n",
              "  886.7515869140625,\n",
              "  881.95654296875,\n",
              "  877.1802978515625,\n",
              "  879.0027465820312,\n",
              "  874.1288452148438,\n",
              "  884.510498046875,\n",
              "  884.2388916015625,\n",
              "  878.7590942382812,\n",
              "  876.89404296875,\n",
              "  878.2344970703125,\n",
              "  875.9398193359375,\n",
              "  877.5792236328125,\n",
              "  880.0513916015625,\n",
              "  872.1128540039062,\n",
              "  878.8243408203125,\n",
              "  880.3391723632812,\n",
              "  877.9378051757812,\n",
              "  882.1093139648438,\n",
              "  875.8959350585938,\n",
              "  880.9160766601562,\n",
              "  877.9559326171875,\n",
              "  874.7374267578125,\n",
              "  878.5810546875,\n",
              "  882.7037353515625,\n",
              "  875.4949340820312,\n",
              "  876.1207885742188,\n",
              "  874.357177734375,\n",
              "  875.0784301757812,\n",
              "  870.4533081054688,\n",
              "  881.6065063476562,\n",
              "  879.3694458007812,\n",
              "  870.6287841796875,\n",
              "  877.8986206054688,\n",
              "  881.419677734375,\n",
              "  875.0526123046875],\n",
              " 'val_loss': [539610.375,\n",
              "  534795.5625,\n",
              "  539222.625,\n",
              "  534411.9375,\n",
              "  543033.25,\n",
              "  530001.625,\n",
              "  560205.5625,\n",
              "  516074.125,\n",
              "  538229.5,\n",
              "  559637.25,\n",
              "  528394.5625,\n",
              "  550638.375,\n",
              "  540973.0625,\n",
              "  524512.6875,\n",
              "  524297.0,\n",
              "  545730.9375,\n",
              "  536970.0,\n",
              "  536751.5,\n",
              "  527841.5625,\n",
              "  514801.59375,\n",
              "  518775.34375,\n",
              "  518614.125,\n",
              "  531691.9375,\n",
              "  557315.0625,\n",
              "  557143.25,\n",
              "  522628.15625,\n",
              "  522467.28125,\n",
              "  547856.8125,\n",
              "  556411.75,\n",
              "  513209.0,\n",
              "  521821.90625,\n",
              "  521607.46875,\n",
              "  534230.875,\n",
              "  542777.8125,\n",
              "  538030.5625,\n",
              "  546623.8125,\n",
              "  507980.0,\n",
              "  533469.8125,\n",
              "  524655.6875,\n",
              "  498861.75,\n",
              "  528856.5,\n",
              "  528243.1875,\n",
              "  536718.6875,\n",
              "  532489.4375,\n",
              "  545075.25,\n",
              "  519370.09375,\n",
              "  553371.625,\n",
              "  523110.25,\n",
              "  518887.15625,\n",
              "  531510.875,\n",
              "  522733.59375,\n",
              "  518405.375,\n",
              "  509587.09375,\n",
              "  518140.03125,\n",
              "  551969.3125,\n",
              "  530481.5625,\n",
              "  521664.21875,\n",
              "  521603.90625,\n",
              "  530038.1875,\n",
              "  529828.1875,\n",
              "  529614.5625,\n",
              "  529551.75,\n",
              "  512115.34375,\n",
              "  525119.0,\n",
              "  532490.1875,\n",
              "  516115.03125,\n",
              "  528738.125,\n",
              "  519903.25,\n",
              "  532518.5625,\n",
              "  528151.375,\n",
              "  528088.6875,\n",
              "  518585.625,\n",
              "  518474.59375,\n",
              "  531756.375,\n",
              "  527392.25,\n",
              "  535884.0625,\n",
              "  527069.75,\n",
              "  497109.21875,\n",
              "  526843.4375,\n",
              "  513940.46875,\n",
              "  517875.75,\n",
              "  526307.0625,\n",
              "  513461.46875,\n",
              "  538663.4375,\n",
              "  500470.46875,\n",
              "  538334.8125,\n",
              "  525450.8125,\n",
              "  538003.375,\n",
              "  508422.09375,\n",
              "  533539.0,\n",
              "  516338.78125,\n",
              "  507952.5,\n",
              "  503323.84375,\n",
              "  528463.9375,\n",
              "  524213.40625,\n",
              "  528092.6875,\n",
              "  523793.84375,\n",
              "  485868.75,\n",
              "  523563.84375,\n",
              "  536030.5625,\n",
              "  513979.0,\n",
              "  522406.375,\n",
              "  526959.0,\n",
              "  522712.40625,\n",
              "  500781.46875,\n",
              "  530918.125,\n",
              "  530758.4375,\n",
              "  517631.03125,\n",
              "  505233.53125,\n",
              "  521796.25,\n",
              "  492061.28125,\n",
              "  513013.125,\n",
              "  521225.34375,\n",
              "  483449.78125,\n",
              "  495879.40625,\n",
              "  533462.125,\n",
              "  504025.71875,\n",
              "  512018.53125,\n",
              "  516325.09375,\n",
              "  528685.6875,\n",
              "  507427.59375,\n",
              "  528361.5,\n",
              "  528157.0625,\n",
              "  507003.0,\n",
              "  494295.03125,\n",
              "  523265.71875,\n",
              "  531534.8125,\n",
              "  493879.5,\n",
              "  510247.53125,\n",
              "  489178.0,\n",
              "  497490.125,\n",
              "  505711.75,\n",
              "  505554.875,\n",
              "  496986.53125,\n",
              "  517763.21875,\n",
              "  501067.03125,\n",
              "  525848.4375,\n",
              "  508885.53125,\n",
              "  512791.25,\n",
              "  512905.90625,\n",
              "  508415.40625,\n",
              "  529192.3125,\n",
              "  511490.15625,\n",
              "  507984.25,\n",
              "  495341.59375,\n",
              "  524482.3125,\n",
              "  519953.09375,\n",
              "  511635.46875,\n",
              "  519677.09375,\n",
              "  507043.0,\n",
              "  490363.0,\n",
              "  506689.96875,\n",
              "  510928.25,\n",
              "  510429.71875,\n",
              "  502124.65625,\n",
              "  506108.375,\n",
              "  501810.96875,\n",
              "  509847.15625,\n",
              "  509982.75,\n",
              "  517884.21875,\n",
              "  484748.90625,\n",
              "  525913.5,\n",
              "  505019.09375,\n",
              "  491766.5,\n",
              "  513000.40625,\n",
              "  508840.40625,\n",
              "  500265.03125,\n",
              "  504237.34375,\n",
              "  499994.875,\n",
              "  491478.34375,\n",
              "  487321.84375,\n",
              "  515939.59375,\n",
              "  511779.90625,\n",
              "  507498.71875,\n",
              "  507467.25,\n",
              "  511269.625,\n",
              "  498761.59375,\n",
              "  523337.125,\n",
              "  494413.25,\n",
              "  514715.875,\n",
              "  485766.59375,\n",
              "  502066.375,\n",
              "  510241.90625,\n",
              "  497678.65625,\n",
              "  501603.15625,\n",
              "  501408.21875,\n",
              "  509533.40625,\n",
              "  488787.09375,\n",
              "  496911.84375,\n",
              "  517302.03125,\n",
              "  492612.59375,\n",
              "  504721.875,\n",
              "  508596.34375,\n",
              "  496102.90625,\n",
              "  504049.03125,\n",
              "  499905.53125,\n",
              "  491453.5,\n",
              "  483244.5,\n",
              "  511721.375,\n",
              "  503435.0,\n",
              "  503315.625,\n",
              "  490932.53125,\n",
              "  498829.09375,\n",
              "  498637.375,\n",
              "  510744.34375,\n",
              "  506601.875,\n",
              "  518740.0,\n",
              "  506293.25,\n",
              "  514327.75,\n",
              "  497714.96875,\n",
              "  501841.28125,\n",
              "  497410.75,\n",
              "  493238.65625,\n",
              "  488888.625,\n",
              "  496250.96875,\n",
              "  496793.96875,\n",
              "  500870.28125,\n",
              "  525240.6875,\n",
              "  496337.46875,\n",
              "  508436.40625,\n",
              "  512499.34375,\n",
              "  491864.90625,\n",
              "  508005.15625,\n",
              "  499821.625,\n",
              "  491403.5,\n",
              "  491249.28125,\n",
              "  498460.78125,\n",
              "  490946.75,\n",
              "  503019.40625,\n",
              "  486472.40625,\n",
              "  498664.75,\n",
              "  502484.15625,\n",
              "  498202.90625,\n",
              "  514490.75,\n",
              "  489919.0,\n",
              "  514103.0,\n",
              "  489576.03125,\n",
              "  493466.40625,\n",
              "  509623.09375,\n",
              "  505282.75,\n",
              "  509378.09375,\n",
              "  496859.15625,\n",
              "  488702.28125,\n",
              "  496655.65625,\n",
              "  508642.46875,\n",
              "  512521.375,\n",
              "  500232.5,\n",
              "  467612.28125,\n",
              "  495854.96875,\n",
              "  511962.65625,\n",
              "  507741.21875,\n",
              "  495430.625,\n",
              "  491114.875,\n",
              "  487037.84375,\n",
              "  478579.34375,\n",
              "  470568.09375,\n",
              "  486588.625,\n",
              "  482409.90625,\n",
              "  490279.15625,\n",
              "  498229.59375,\n",
              "  481957.40625,\n",
              "  501913.40625,\n",
              "  481542.53125,\n",
              "  488796.78125,\n",
              "  493471.40625,\n",
              "  517471.53125,\n",
              "  489033.75,\n",
              "  472681.28125,\n",
              "  504860.09375,\n",
              "  496658.71875,\n",
              "  484443.25,\n",
              "  484261.59375,\n",
              "  488129.71875,\n",
              "  491310.65625,\n",
              "  499880.5,\n",
              "  487713.90625,\n",
              "  479534.65625,\n",
              "  499417.15625,\n",
              "  495283.03125,\n",
              "  487044.15625,\n",
              "  486927.46875,\n",
              "  494859.75,\n",
              "  494705.65625,\n",
              "  478396.46875,\n",
              "  494433.78125,\n",
              "  466207.59375,\n",
              "  494097.53125,\n",
              "  505920.34375,\n",
              "  485726.90625,\n",
              "  461537.96875,\n",
              "  493451.71875,\n",
              "  488596.03125,\n",
              "  481157.78125,\n",
              "  501080.15625,\n",
              "  476812.21875,\n",
              "  484645.90625,\n",
              "  492569.09375,\n",
              "  476467.5,\n",
              "  492264.125,\n",
              "  492083.46875,\n",
              "  467959.90625,\n",
              "  491809.375,\n",
              "  483627.625,\n",
              "  511461.96875,\n",
              "  467402.0,\n",
              "  495131.375,\n",
              "  478971.71875,\n",
              "  478913.75,\n",
              "  502686.90625,\n",
              "  486568.03125,\n",
              "  478497.34375,\n",
              "  470343.0,\n",
              "  462242.34375,\n",
              "  478051.84375,\n",
              "  489802.875,\n",
              "  485688.90625,\n",
              "  485569.28125,\n",
              "  485372.46875,\n",
              "  477281.15625,\n",
              "  485043.59375,\n",
              "  492881.59375,\n",
              "  500711.75,\n",
              "  476692.59375,\n",
              "  475920.59375,\n",
              "  492247.21875,\n",
              "  476216.25,\n",
              "  491969.25,\n",
              "  495804.09375,\n",
              "  471792.0,\n",
              "  471704.40625,\n",
              "  471529.15625,\n",
              "  499113.71875,\n",
              "  498961.15625,\n",
              "  475096.21875,\n",
              "  478869.15625,\n",
              "  466174.96875,\n",
              "  474622.78125,\n",
              "  482339.78125,\n",
              "  494122.90625,\n",
              "  466273.625,\n",
              "  489801.90625,\n",
              "  481790.65625,\n",
              "  489502.03125,\n",
              "  477511.78125,\n",
              "  485339.40625,\n",
              "  469321.65625,\n",
              "  461285.84375,\n",
              "  496737.75,\n",
              "  476765.78125,\n",
              "  472733.65625,\n",
              "  488343.09375,\n",
              "  468450.21875,\n",
              "  464392.65625,\n",
              "  491805.84375,\n",
              "  479846.59375,\n",
              "  475763.0,\n",
              "  483507.59375,\n",
              "  463646.96875,\n",
              "  491046.59375,\n",
              "  475177.53125,\n",
              "  471122.84375,\n",
              "  482709.875,\n",
              "  482610.5,\n",
              "  474586.65625,\n",
              "  482283.03125,\n",
              "  474313.46875,\n",
              "  470212.71875,\n",
              "  466131.78125,\n",
              "  485613.53125,\n",
              "  473729.78125,\n",
              "  485314.53125,\n",
              "  485191.375,\n",
              "  488967.53125,\n",
              "  492667.15625,\n",
              "  465126.96875,\n",
              "  484545.625,\n",
              "  480521.625,\n",
              "  468609.53125,\n",
              "  468438.78125,\n",
              "  480098.59375,\n",
              "  464249.78125,\n",
              "  487531.21875,\n",
              "  487424.46875,\n",
              "  455991.59375,\n",
              "  475433.125,\n",
              "  471360.59375,\n",
              "  455564.90625,\n",
              "  482822.59375,\n",
              "  463089.53125,\n",
              "  474671.53125,\n",
              "  482329.71875,\n",
              "  481514.15625,\n",
              "  458628.03125,\n",
              "  474081.90625,\n",
              "  462205.0,\n",
              "  485493.96875,\n",
              "  469781.0,\n",
              "  481291.65625,\n",
              "  473405.15625,\n",
              "  480994.59375,\n",
              "  461386.25,\n",
              "  457376.53125,\n",
              "  472800.5,\n",
              "  464237.59375,\n",
              "  464716.5,\n",
              "  480128.46875,\n",
              "  468315.96875,\n",
              "  487577.59375,\n",
              "  452455.34375,\n",
              "  475615.125,\n",
              "  459977.0,\n",
              "  475321.625,\n",
              "  467441.40625,\n",
              "  486652.09375,\n",
              "  463272.96875,\n",
              "  466365.25,\n",
              "  443604.96875,\n",
              "  478353.625,\n",
              "  454960.5,\n",
              "  474163.25,\n",
              "  470204.0,\n",
              "  473845.84375,\n",
              "  465994.90625,\n",
              "  473573.84375,\n",
              "  461835.21875,\n",
              "  465543.28125,\n",
              "  473134.53125,\n",
              "  465276.65625,\n",
              "  468384.21875,\n",
              "  464966.34375,\n",
              "  476438.09375,\n",
              "  480042.0,\n",
              "  472233.875,\n",
              "  463773.84375,\n",
              "  460399.59375,\n",
              "  471838.09375,\n",
              "  452407.65625,\n",
              "  467642.40625,\n",
              "  475269.84375,\n",
              "  463574.34375,\n",
              "  478753.78125,\n",
              "  478624.84375,\n",
              "  459242.71875,\n",
              "  482172.875,\n",
              "  466624.65625,\n",
              "  466479.65625,\n",
              "  454770.09375,\n",
              "  458527.75,\n",
              "  458383.03125,\n",
              "  454378.03125,\n",
              "  481165.03125,\n",
              "  461834.09375,\n",
              "  476954.84375,\n",
              "  446147.84375,\n",
              "  480536.90625,\n",
              "  461244.5,\n",
              "  461121.28125,\n",
              "  480108.375,\n",
              "  460852.53125,\n",
              "  464467.40625,\n",
              "  468141.71875,\n",
              "  456548.75,\n",
              "  448767.09375,\n",
              "  463854.21875,\n",
              "  479052.625,\n",
              "  467452.25,\n",
              "  467289.34375,\n",
              "  451318.53125,\n",
              "  466999.40625,\n",
              "  466870.15625,\n",
              "  455238.90625,\n",
              "  458980.90625,\n",
              "  466436.78125,\n",
              "  450967.125,\n",
              "  458556.90625,\n",
              "  443108.28125,\n",
              "  465859.15625,\n",
              "  461974.28125,\n",
              "  454106.34375,\n",
              "  465409.375,\n",
              "  464630.84375,\n",
              "  442263.09375,\n",
              "  468836.03125,\n",
              "  464833.25,\n",
              "  434293.25,\n",
              "  453133.96875,\n",
              "  464413.25,\n",
              "  452835.59375,\n",
              "  445152.5,\n",
              "  456425.78125,\n",
              "  471376.90625,\n",
              "  433348.15625,\n",
              "  440744.40625,\n",
              "  463376.125,\n",
              "  463248.28125,\n",
              "  455565.0,\n",
              "  470498.5,\n",
              "  447125.25,\n",
              "  451304.59375,\n",
              "  466396.65625,\n",
              "  466238.78125,\n",
              "  469783.875,\n",
              "  450742.25,\n",
              "  458285.46875,\n",
              "  435442.96875,\n",
              "  454169.0,\n",
              "  457860.90625,\n",
              "  457538.09375,\n",
              "  472604.65625,\n",
              "  445901.09375,\n",
              "  449622.40625,\n",
              "  460822.71875,\n",
              "  449352.84375,\n",
              "  479368.84375,\n",
              "  460405.84375,\n",
              "  452745.71875,\n",
              "  478909.75,\n",
              "  459952.59375,\n",
              "  448698.21875,\n",
              "  459696.5,\n",
              "  459529.71875,\n",
              "  459386.28125,\n",
              "  455606.46875,\n",
              "  470418.28125,\n",
              "  466423.75,\n",
              "  458802.15625,\n",
              "  443542.90625,\n",
              "  450465.0,\n",
              "  469704.375,\n",
              "  469510.15625,\n",
              "  450654.5,\n",
              "  457975.84375,\n",
              "  446542.0,\n",
              "  457668.40625,\n",
              "  457533.96875,\n",
              "  446148.15625,\n",
              "  457264.09375,\n",
              "  442261.15625,\n",
              "  464407.875,\n",
              "  449394.40625,\n",
              "  456689.25,\n",
              "  463960.75,\n",
              "  437749.75,\n",
              "  452456.5,\n",
              "  430065.84375,\n",
              "  455996.46875,\n",
              "  437200.65625,\n",
              "  451869.5,\n",
              "  448155.71875,\n",
              "  474028.65625,\n",
              "  436038.0,\n",
              "  458937.15625,\n",
              "  435755.53125,\n",
              "  443641.5,\n",
              "  458522.59375,\n",
              "  450754.875,\n",
              "  443229.375,\n",
              "  458099.25,\n",
              "  461514.15625,\n",
              "  457813.5,\n",
              "  427678.59375,\n",
              "  423988.46875,\n",
              "  442403.875,\n",
              "  438720.09375,\n",
              "  434760.71875,\n",
              "  438447.78125,\n",
              "  445656.625,\n",
              "  445528.03125,\n",
              "  456555.96875,\n",
              "  459941.65625,\n",
              "  448648.84375,\n",
              "  452314.21875,\n",
              "  441037.03125,\n",
              "  448226.625,\n",
              "  436944.46875,\n",
              "  429476.65625,\n",
              "  436945.96875,\n",
              "  451476.5,\n",
              "  462454.25,\n",
              "  451193.78125,\n",
              "  451056.40625,\n",
              "  436266.03125,\n",
              "  461888.71875,\n",
              "  428400.84375,\n",
              "  446682.0,\n",
              "  446550.90625,\n",
              "  446404.75,\n",
              "  438957.09375,\n",
              "  431524.09375,\n",
              "  427589.78125,\n",
              "  460736.59375,\n",
              "  445999.78125,\n",
              "  438268.78125,\n",
              "  456523.03125,\n",
              "  449082.09375,\n",
              "  460018.59375,\n",
              "  423167.78125,\n",
              "  444883.375,\n",
              "  455818.625,\n",
              "  433842.25,\n",
              "  437181.59375,\n",
              "  448103.125,\n",
              "  433125.71875,\n",
              "  436773.46875,\n",
              "  458745.65625,\n",
              "  436502.84375,\n",
              "  429112.75,\n",
              "  450436.25,\n",
              "  447132.0,\n",
              "  454250.65625,\n",
              "  421008.65625,\n",
              "  439481.09375,\n",
              "  450377.625,\n",
              "  446442.15625,\n",
              "  450091.46875,\n",
              "  449949.21875,\n",
              "  435011.90625,\n",
              "  427650.25,\n",
              "  438520.25,\n",
              "  438399.78125,\n",
              "  449249.34375,\n",
              "  427120.59375,\n",
              "  434209.59375,\n",
              "  441289.34375,\n",
              "  455918.90625,\n",
              "  444788.84375,\n",
              "  444651.40625,\n",
              "  426326.34375,\n",
              "  415212.28125,\n",
              "  444235.625,\n",
              "  433128.65625,\n",
              "  432994.15625,\n",
              "  436627.78125,\n",
              "  443687.75,\n",
              "  450728.75,\n",
              "  439639.59375,\n",
              "  439500.65625,\n",
              "  443131.28125,\n",
              "  421098.40625,\n",
              "  435681.34375,\n",
              "  431779.84375,\n",
              "  432005.75,\n",
              "  435270.71875,\n",
              "  442299.03125,\n",
              "  435005.625,\n",
              "  434867.28125,\n",
              "  430974.15625,\n",
              "  441758.0,\n",
              "  427313.25,\n",
              "  434329.75,\n",
              "  441343.59375,\n",
              "  437449.46875,\n",
              "  426407.46875,\n",
              "  451836.28125,\n",
              "  447934.125,\n",
              "  426391.15625,\n",
              "  429633.03125,\n",
              "  422373.90625,\n",
              "  429366.84375,\n",
              "  451003.96875,\n",
              "  432857.375,\n",
              "  421848.375,\n",
              "  439709.78125,\n",
              "  428702.59375,\n",
              "  421458.0,\n",
              "  446411.84375,\n",
              "  432056.90625,\n",
              "  435672.875,\n",
              "  442644.46875,\n",
              "  431654.75,\n",
              "  442370.0,\n",
              "  449329.03125,\n",
              "  449188.34375,\n",
              "  456138.96875,\n",
              "  438070.09375,\n",
              "  437935.40625,\n",
              "  430714.875,\n",
              "  448488.40625,\n",
              "  426704.21875,\n",
              "  455288.15625,\n",
              "  419364.375,\n",
              "  447932.59375,\n",
              "  436982.59375,\n",
              "  418972.25,\n",
              "  422584.25,\n",
              "  432836.65625,\n",
              "  425642.40625,\n",
              "  425511.25,\n",
              "  436173.15625,\n",
              "  443091.5,\n",
              "  435904.84375,\n",
              "  424984.46875,\n",
              "  428589.46875,\n",
              "  417677.875,\n",
              "  435361.65625,\n",
              "  431489.96875,\n",
              "  452897.65625,\n",
              "  427927.59375,\n",
              "  445585.90625,\n",
              "  420633.25,\n",
              "  420063.90625,\n",
              "  438144.75,\n",
              "  434278.25,\n",
              "  419667.0,\n",
              "  434007.59375,\n",
              "  430139.875,\n",
              "  433734.875,\n",
              "  426598.625,\n",
              "  444196.15625,\n",
              "  422604.59375,\n",
              "  429472.65625,\n",
              "  443787.53125,\n",
              "  418334.84375,\n",
              "  411361.71875,\n",
              "  425669.625,\n",
              "  414827.34375,\n",
              "  425402.40625,\n",
              "  432258.40625,\n",
              "  421423.5,\n",
              "  431990.15625,\n",
              "  421160.84375,\n",
              "  428000.25,\n",
              "  417176.46875,\n",
              "  424486.09375,\n",
              "  416778.46875,\n",
              "  424223.375,\n",
              "  420375.09375,\n",
              "  417003.25,\n",
              "  427063.0,\n",
              "  423697.375,\n",
              "  427276.28125,\n",
              "  409057.71875,\n",
              "  423299.40625,\n",
              "  447708.75,\n",
              "  416103.90625,\n",
              "  429844.03125,\n",
              "  415350.0,\n",
              "  433292.75,\n",
              "  440090.84375,\n",
              "  439948.09375,\n",
              "  439807.40625,\n",
              "  418420.90625,\n",
              "  421998.71875,\n",
              "  425568.40625,\n",
              "  439257.90625,\n",
              "  400362.125,\n",
              "  428373.03125,\n",
              "  413930.96875,\n",
              "  406891.375,\n",
              "  399865.09375,\n",
              "  438449.375,\n",
              "  417121.59375,\n",
              "  438170.25,\n",
              "  431157.78125,\n",
              "  409834.75,\n",
              "  427176.65625,\n",
              "  430751.0,\n",
              "  416333.59375,\n",
              "  437352.90625,\n",
              "  433529.65625,\n",
              "  426529.65625,\n",
              "  412647.84375,\n",
              "  426255.90625,\n",
              "  436688.25,\n",
              "  436554.28125,\n",
              "  425852.15625,\n",
              "  425727.40625,\n",
              "  418735.34375,\n",
              "  425478.09375,\n",
              "  435878.75,\n",
              "  425209.90625,\n",
              "  435619.5,\n",
              "  424938.09375,\n",
              "  414277.84375,\n",
              "  396781.15625,\n",
              "  424543.15625,\n",
              "  420732.40625,\n",
              "  413766.0,\n",
              "  430984.0,\n",
              "  409820.375,\n",
              "  420760.25,\n",
              "  420067.03125,\n",
              "  416194.0,\n",
              "  423493.78125,\n",
              "  402364.40625,\n",
              "  412739.90625,\n",
              "  412612.78125,\n",
              "  419847.71875,\n",
              "  422839.25,\n",
              "  422716.25,\n",
              "  425671.40625,\n",
              "  422446.34375,\n",
              "  408730.375,\n",
              "  411725.59375,\n",
              "  418365.84375,\n",
              "  415147.84375,\n",
              "  428578.40625,\n",
              "  417990.03125,\n",
              "  414751.09375,\n",
              "  407273.25,\n",
              "  431710.65625,\n",
              "  410684.125,\n",
              "  417331.09375,\n",
              "  417782.625,\n",
              "  410301.65625,\n",
              "  431076.125,\n",
              "  410059.625,\n",
              "  420364.875,\n",
              "  426961.28125,\n",
              "  395593.09375,\n",
              "  399134.84375,\n",
              "  423502.09375,\n",
              "  416056.40625,\n",
              "  415926.78125,\n",
              "  409040.75,\n",
              "  398516.96875,\n",
              "  419193.53125,\n",
              "  419054.71875,\n",
              "  425658.5,\n",
              "  422477.53125,\n",
              "  411954.65625,\n",
              "  408746.0,\n",
              "  422063.125,\n",
              "  418273.53125,\n",
              "  387022.25,\n",
              "  403973.125,\n",
              "  428258.25,\n",
              "  411049.34375,\n",
              "  400552.65625,\n",
              "  417492.34375,\n",
              "  403343.0,\n",
              "  413590.46875,\n",
              "  417106.90625,\n",
              "  427333.875,\n",
              "  413817.25,\n",
              "  406381.875,\n",
              "  420267.78125,\n",
              "  433480.28125,\n",
              "  402980.0,\n",
              "  409531.75,\n",
              "  409402.71875,\n",
              "  398958.0,\n",
              "  409150.59375,\n",
              "  419335.53125,\n",
              "  394927.5,\n",
              "  419088.0,\n",
              "  411651.625,\n",
              "  415183.59375,\n",
              "  415053.625,\n",
              "  431897.375,\n",
              "  404509.34375,\n",
              "  421298.84375,\n",
              "  407886.0,\n",
              "  393838.5,\n",
              "  410654.875,\n",
              "  403865.09375,\n",
              "  410382.75,\n",
              "  413879.5,\n",
              "  404126.65625,\n",
              "  423896.875,\n",
              "  403223.21875,\n",
              "  416989.75,\n",
              "  423490.03125,\n",
              "  406484.59375,\n",
              "  412968.40625,\n",
              "  412869.90625,\n",
              "  398853.5,\n",
              "  416233.53125,\n",
              "  408853.09375,\n",
              "  418956.03125,\n",
              "  422487.09375,\n",
              "  387991.75,\n",
              "  415603.78125,\n",
              "  395008.34375,\n",
              "  411716.0,\n",
              "  411555.65625,\n",
              "  404850.125,\n",
              "  421557.40625,\n",
              "  404600.90625,\n",
              "  394275.5,\n",
              "  421130.90625,\n",
              "  400614.65625,\n",
              "  380095.0,\n",
              "  400352.375,\n",
              "  400243.75,\n",
              "  397169.34375,\n",
              "  397062.875,\n",
              "  413665.375,\n",
              "  409933.71875,\n",
              "  393057.90625,\n",
              "  403102.90625,\n",
              "  416078.875,\n",
              "  409409.5,\n",
              "  402764.53125,\n",
              "  388849.375,\n",
              "  388714.59375,\n",
              "  412557.25,\n",
              "  408798.09375,\n",
              "  418794.78125,\n",
              "  408513.15625,\n",
              "  412000.71875,\n",
              "  411910.09375,\n",
              "  408150.09375,\n",
              "  397899.03125,\n",
              "  414406.25,\n",
              "  397672.84375,\n",
              "  411258.90625,\n",
              "  407538.21875,\n",
              "  390788.75,\n",
              "  400775.65625,\n",
              "  410790.90625,\n",
              "  417117.25,\n",
              "  389697.46875,\n",
              "  403866.59375,\n",
              "  413137.65625,\n",
              "  406491.15625,\n",
              "  393409.65625,\n",
              "  392573.625,\n",
              "  402528.75,\n",
              "  409633.46875,\n",
              "  389323.125,\n",
              "  409378.15625,\n",
              "  415716.40625,\n",
              "  388981.0,\n",
              "  382382.875,\n",
              "  382265.34375,\n",
              "  384999.96875,\n",
              "  398549.65625,\n",
              "  408494.65625,\n",
              "  404750.625,\n",
              "  404624.90625,\n",
              "  388000.78125,\n",
              "  393751.03125,\n",
              "  397812.78125,\n",
              "  394094.09375,\n",
              "  407591.53125,\n",
              "  397423.25,\n",
              "  400174.34375,\n",
              "  403621.5,\n",
              "  390656.75,\n",
              "  403389.34375,\n",
              "  403268.15625,\n",
              "  393118.90625,\n",
              "  406607.90625,\n",
              "  400031.84375,\n",
              "  399138.09375,\n",
              "  382641.125,\n",
              "  418954.71875,\n",
              "  375985.25,\n",
              "  395855.59375,\n",
              "  398710.84375,\n",
              "  411974.84375,\n",
              "  401897.15625,\n",
              "  401775.78125,\n",
              "  418065.71875,\n",
              "  391540.75,\n",
              "  411385.03125,\n",
              "  391277.59375,\n",
              "  391156.5,\n",
              "  397424.875,\n",
              "  404508.34375,\n",
              "  397994.59375,\n",
              "  400679.34375,\n",
              "  397122.53125,\n",
              "  394005.03125,\n",
              "  406616.0,\n",
              "  383834.5,\n",
              "  387292.375,\n",
              "  392939.03125,\n",
              "  409687.34375,\n",
              "  386963.40625,\n",
              "  393158.125,\n",
              "  393060.34375,\n",
              "  405632.75,\n",
              "  409080.09375,\n",
              "  405359.46875,\n",
              "  398928.15625,\n",
              "  378969.59375,\n",
              "  398685.0,\n",
              "  388647.625,\n",
              "  414664.28125,\n",
              "  385635.875,\n",
              "  388286.65625,\n",
              "  394526.84375,\n",
              "  401511.71875,\n",
              "  387927.0,\n",
              "  387783.90625,\n",
              "  384122.96875,\n",
              "  377706.46875,\n",
              "  378398.75,\n",
              "  377474.40625,\n",
              "  406910.21875,\n",
              "  387089.375,\n",
              "  380660.34375,\n",
              "  396666.03125,\n",
              "  386703.875,\n",
              "  402747.90625,\n",
              "  392790.71875,\n",
              "  383629.25,\n",
              "  396102.65625],\n",
              " 'val_mae': [568.5801391601562,\n",
              "  561.7349243164062,\n",
              "  568.00634765625,\n",
              "  561.1610717773438,\n",
              "  569.74560546875,\n",
              "  559.122802734375,\n",
              "  586.633056640625,\n",
              "  547.7059936523438,\n",
              "  567.1314086914062,\n",
              "  585.9135131835938,\n",
              "  557.467529296875,\n",
              "  577.4552612304688,\n",
              "  567.938720703125,\n",
              "  555.6484375,\n",
              "  555.2217407226562,\n",
              "  574.2865600585938,\n",
              "  566.2534790039062,\n",
              "  565.826904296875,\n",
              "  556.9547729492188,\n",
              "  546.0498657226562,\n",
              "  547.9354858398438,\n",
              "  547.788330078125,\n",
              "  562.507080078125,\n",
              "  584.1641235351562,\n",
              "  584.0170288085938,\n",
              "  554.183349609375,\n",
              "  554.0381469726562,\n",
              "  574.5714721679688,\n",
              "  583.154541015625,\n",
              "  544.589599609375,\n",
              "  553.4553833007812,\n",
              "  553.028076171875,\n",
              "  563.360595703125,\n",
              "  571.9450073242188,\n",
              "  565.1034545898438,\n",
              "  573.9686889648438,\n",
              "  541.542236328125,\n",
              "  562.9194946289062,\n",
              "  554.321533203125,\n",
              "  532.6552124023438,\n",
              "  560.4507446289062,\n",
              "  555.6375122070312,\n",
              "  563.94140625,\n",
              "  562.048095703125,\n",
              "  572.3818969726562,\n",
              "  551.0014038085938,\n",
              "  580.5420532226562,\n",
              "  552.456298828125,\n",
              "  550.5630493164062,\n",
              "  561.1768188476562,\n",
              "  552.5850219726562,\n",
              "  550.1253051757812,\n",
              "  541.2540893554688,\n",
              "  550.1190795898438,\n",
              "  579.0956420898438,\n",
              "  560.0230102539062,\n",
              "  551.1459350585938,\n",
              "  551.5619506835938,\n",
              "  559.8632202148438,\n",
              "  559.4392700195312,\n",
              "  559.01220703125,\n",
              "  559.4286499023438,\n",
              "  541.8240356445312,\n",
              "  557.1064453125,\n",
              "  560.657470703125,\n",
              "  547.8040161132812,\n",
              "  558.700927734375,\n",
              "  549.546142578125,\n",
              "  560.442138671875,\n",
              "  557.701904296875,\n",
              "  558.119384765625,\n",
              "  548.5991821289062,\n",
              "  548.7343139648438,\n",
              "  559.9990234375,\n",
              "  557.258056640625,\n",
              "  566.1242065429688,\n",
              "  556.968505859375,\n",
              "  529.1649780273438,\n",
              "  557.2401733398438,\n",
              "  546.0521850585938,\n",
              "  547.9349975585938,\n",
              "  556.52099609375,\n",
              "  545.6133422851562,\n",
              "  566.9917602539062,\n",
              "  534.5639038085938,\n",
              "  566.7018432617188,\n",
              "  555.5132446289062,\n",
              "  566.4093627929688,\n",
              "  542.7138061523438,\n",
              "  563.8104858398438,\n",
              "  546.7681274414062,\n",
              "  542.2810668945312,\n",
              "  535.4369506835938,\n",
              "  556.8130493164062,\n",
              "  554.6367797851562,\n",
              "  556.240966796875,\n",
              "  553.7813110351562,\n",
              "  522.197509765625,\n",
              "  554.0508422851562,\n",
              "  564.6652221679688,\n",
              "  544.384521484375,\n",
              "  553.2506713867188,\n",
              "  555.220947265625,\n",
              "  553.0430297851562,\n",
              "  533.3252563476562,\n",
              "  561.4813842773438,\n",
              "  561.339111328125,\n",
              "  545.765869140625,\n",
              "  539.5299682617188,\n",
              "  552.4535522460938,\n",
              "  524.37353515625,\n",
              "  543.718505859375,\n",
              "  551.45751953125,\n",
              "  519.876220703125,\n",
              "  530.4901733398438,\n",
              "  562.626708984375,\n",
              "  538.6482543945312,\n",
              "  542.5639038085938,\n",
              "  549.39697265625,\n",
              "  559.7304077148438,\n",
              "  539.8164672851562,\n",
              "  559.440673828125,\n",
              "  559.0172729492188,\n",
              "  539.6624145507812,\n",
              "  528.756103515625,\n",
              "  551.885009765625,\n",
              "  560.187744140625,\n",
              "  528.601806640625,\n",
              "  540.6895141601562,\n",
              "  521.616943359375,\n",
              "  530.2005004882812,\n",
              "  538.224853515625,\n",
              "  538.0790405273438,\n",
              "  529.4860229492188,\n",
              "  548.55029296875,\n",
              "  535.8948364257812,\n",
              "  556.706787109375,\n",
              "  539.6690063476562,\n",
              "  541.5542602539062,\n",
              "  545.794189453125,\n",
              "  539.2332153320312,\n",
              "  558.5772094726562,\n",
              "  540.6072998046875,\n",
              "  539.0741577148438,\n",
              "  528.1704711914062,\n",
              "  555.9629516601562,\n",
              "  549.117431640625,\n",
              "  544.6289672851562,\n",
              "  549.10791015625,\n",
              "  538.200439453125,\n",
              "  525.2651977539062,\n",
              "  537.6309204101562,\n",
              "  544.4633178710938,\n",
              "  539.3695678710938,\n",
              "  534.8821411132812,\n",
              "  537.3313598632812,\n",
              "  534.5888061523438,\n",
              "  539.0712280273438,\n",
              "  543.59423828125,\n",
              "  547.2304077148438,\n",
              "  521.5021362304688,\n",
              "  555.3911743164062,\n",
              "  536.3168334960938,\n",
              "  525.0469360351562,\n",
              "  544.1917114257812,\n",
              "  542.2997436523438,\n",
              "  533.1409301757812,\n",
              "  535.587646484375,\n",
              "  533.1298217773438,\n",
              "  524.2587280273438,\n",
              "  522.3622436523438,\n",
              "  545.207275390625,\n",
              "  543.3125,\n",
              "  540.57470703125,\n",
              "  541.2754516601562,\n",
              "  542.59912109375,\n",
              "  531.9719848632812,\n",
              "  553.0668334960938,\n",
              "  529.6503295898438,\n",
              "  544.3277587890625,\n",
              "  520.6290893554688,\n",
              "  533.5569458007812,\n",
              "  542.1392211914062,\n",
              "  530.9530639648438,\n",
              "  533.1227416992188,\n",
              "  532.6962280273438,\n",
              "  540.9968872070312,\n",
              "  521.9286499023438,\n",
              "  530.23046875,\n",
              "  548.7323608398438,\n",
              "  528.1913452148438,\n",
              "  538.24462890625,\n",
              "  540.130126953125,\n",
              "  529.2230834960938,\n",
              "  533.4218139648438,\n",
              "  531.5281372070312,\n",
              "  522.6530151367188,\n",
              "  518.4447631835938,\n",
              "  541.570068359375,\n",
              "  536.8027954101562,\n",
              "  536.9365844726562,\n",
              "  526.5985717773438,\n",
              "  530.5145874023438,\n",
              "  530.089111328125,\n",
              "  540.4218139648438,\n",
              "  538.526123046875,\n",
              "  549.1412963867188,\n",
              "  538.2396850585938,\n",
              "  546.2616577148438,\n",
              "  529.2183227539062,\n",
              "  536.0532836914062,\n",
              "  528.930908203125,\n",
              "  526.7553100585938,\n",
              "  520.1935424804688,\n",
              "  527.850830078125,\n",
              "  528.3474731445312,\n",
              "  534.9008178710938,\n",
              "  556.2810668945312,\n",
              "  527.915283203125,\n",
              "  538.52880859375,\n",
              "  545.0796508789062,\n",
              "  525.4497680664062,\n",
              "  538.375,\n",
              "  534.166259765625,\n",
              "  525.0105590820312,\n",
              "  524.8635864257812,\n",
              "  528.419189453125,\n",
              "  524.5752563476562,\n",
              "  535.189697265625,\n",
              "  517.8659057617188,\n",
              "  532.588134765625,\n",
              "  534.1948852539062,\n",
              "  527.9136352539062,\n",
              "  545.506591796875,\n",
              "  523.841552734375,\n",
              "  544.6547241210938,\n",
              "  523.2671508789062,\n",
              "  525.4360961914062,\n",
              "  542.1865844726562,\n",
              "  535.3447875976562,\n",
              "  542.457275390625,\n",
              "  526.8867797851562,\n",
              "  522.678955078125,\n",
              "  530.6986694335938,\n",
              "  541.0328979492188,\n",
              "  543.2008666992188,\n",
              "  532.57958984375,\n",
              "  502.4659729003906,\n",
              "  529.6953735351562,\n",
              "  543.1845092773438,\n",
              "  540.4483032226562,\n",
              "  529.543212890625,\n",
              "  522.6973266601562,\n",
              "  521.0842895507812,\n",
              "  507.54638671875,\n",
              "  507.723876953125,\n",
              "  520.653076171875,\n",
              "  518.1951293945312,\n",
              "  522.3943481445312,\n",
              "  530.6958618164062,\n",
              "  517.7582397460938,\n",
              "  532.43798828125,\n",
              "  513.083984375,\n",
              "  520.7420043945312,\n",
              "  527.9393920898438,\n",
              "  548.7522583007812,\n",
              "  520.9518432617188,\n",
              "  503.629150390625,\n",
              "  537.2758178710938,\n",
              "  528.9639282226562,\n",
              "  518.3394165039062,\n",
              "  517.9148559570312,\n",
              "  520.0834350585938,\n",
              "  521.6098022460938,\n",
              "  530.2753295898438,\n",
              "  519.9331665039062,\n",
              "  515.163818359375,\n",
              "  529.8381958007812,\n",
              "  527.662109375,\n",
              "  518.788818359375,\n",
              "  518.9263305664062,\n",
              "  527.511474609375,\n",
              "  527.3652954101562,\n",
              "  509.7591857910156,\n",
              "  527.3583984375,\n",
              "  503.6609802246094,\n",
              "  526.78857421875,\n",
              "  536.8388061523438,\n",
              "  517.768310546875,\n",
              "  496.3846740722656,\n",
              "  525.923828125,\n",
              "  519.0010375976562,\n",
              "  515.1604614257812,\n",
              "  534.502197265625,\n",
              "  508.453369140625,\n",
              "  516.4722290039062,\n",
              "  525.3357543945312,\n",
              "  512.6802368164062,\n",
              "  525.04541015625,\n",
              "  524.6215209960938,\n",
              "  503.2374572753906,\n",
              "  524.6121215820312,\n",
              "  515.737060546875,\n",
              "  543.2489624023438,\n",
              "  502.933837890625,\n",
              "  525.7782592773438,\n",
              "  508.4554748535156,\n",
              "  512.9778442382812,\n",
              "  534.0718383789062,\n",
              "  520.8554077148438,\n",
              "  512.8237915039062,\n",
              "  503.9498596191406,\n",
              "  499.45751953125,\n",
              "  512.3892211914062,\n",
              "  522.443359375,\n",
              "  520.263671875,\n",
              "  520.401611328125,\n",
              "  515.8711547851562,\n",
              "  511.3841247558594,\n",
              "  515.2993774414062,\n",
              "  523.8877563476562,\n",
              "  532.473388671875,\n",
              "  510.8082580566406,\n",
              "  510.57861328125,\n",
              "  523.0282592773438,\n",
              "  510.0887145996094,\n",
              "  523.016357421875,\n",
              "  529.2894897460938,\n",
              "  503.2394104003906,\n",
              "  507.760498046875,\n",
              "  503.2313537597656,\n",
              "  530.4613647460938,\n",
              "  530.3175659179688,\n",
              "  509.4969787597656,\n",
              "  511.1029357910156,\n",
              "  499.829345703125,\n",
              "  508.7782287597656,\n",
              "  512.4149780273438,\n",
              "  527.6989135742188,\n",
              "  499.8960876464844,\n",
              "  520.4306030273438,\n",
              "  516.503662109375,\n",
              "  520.142333984375,\n",
              "  509.518798828125,\n",
              "  518.6641235351562,\n",
              "  500.779052734375,\n",
              "  492.1857604980469,\n",
              "  528.9844360351562,\n",
              "  508.7862243652344,\n",
              "  507.1731872558594,\n",
              "  519.5381469726562,\n",
              "  504.2908630371094,\n",
              "  498.0111083984375,\n",
              "  525.2427368164062,\n",
              "  514.6182250976562,\n",
              "  508.0552673339844,\n",
              "  516.6392211914062,\n",
              "  497.0069274902344,\n",
              "  524.5194702148438,\n",
              "  507.4786376953125,\n",
              "  505.5826110839844,\n",
              "  515.3540649414062,\n",
              "  515.7703247070312,\n",
              "  506.896240234375,\n",
              "  515.1962890625,\n",
              "  506.8827209472656,\n",
              "  504.4256286621094,\n",
              "  497.5838928222656,\n",
              "  516.647705078125,\n",
              "  506.3067321777344,\n",
              "  516.3583374023438,\n",
              "  516.49609375,\n",
              "  523.0487670898438,\n",
              "  524.0922241210938,\n",
              "  496.5731506347656,\n",
              "  515.3558349609375,\n",
              "  513.7413330078125,\n",
              "  502.8340759277344,\n",
              "  502.4072265625,\n",
              "  513.5869750976562,\n",
              "  500.365966796875,\n",
              "  520.8995971679688,\n",
              "  521.3134155273438,\n",
              "  491.2022399902344,\n",
              "  510.8277893066406,\n",
              "  503.7039489746094,\n",
              "  490.7677307128906,\n",
              "  514.4560546875,\n",
              "  499.2051696777344,\n",
              "  509.8236389160156,\n",
              "  513.4598999023438,\n",
              "  512.6698608398438,\n",
              "  491.9317932128906,\n",
              "  509.2450256347656,\n",
              "  498.0603942871094,\n",
              "  519.7177124023438,\n",
              "  502.3919372558594,\n",
              "  512.4481811523438,\n",
              "  504.1385803222656,\n",
              "  512.1581420898438,\n",
              "  497.7547302246094,\n",
              "  490.9164123535156,\n",
              "  503.2796325683594,\n",
              "  494.0398864746094,\n",
              "  499.2068786621094,\n",
              "  511.5713806152344,\n",
              "  500.672607421875,\n",
              "  519.7339477539062,\n",
              "  487.5894470214844,\n",
              "  508.6839904785156,\n",
              "  491.3602600097656,\n",
              "  508.3954162597656,\n",
              "  499.7984313964844,\n",
              "  518.582275390625,\n",
              "  497.7588806152344,\n",
              "  498.9990234375,\n",
              "  477.9798889160156,\n",
              "  509.833740234375,\n",
              "  488.4495544433594,\n",
              "  507.515380859375,\n",
              "  500.9537048339844,\n",
              "  506.94189453125,\n",
              "  498.3492431640625,\n",
              "  506.934326171875,\n",
              "  496.3125915527344,\n",
              "  497.6353454589844,\n",
              "  506.5008850097656,\n",
              "  497.628173828125,\n",
              "  499.1495056152344,\n",
              "  497.0552673339844,\n",
              "  507.95166015625,\n",
              "  513.6605834960938,\n",
              "  505.3495178222656,\n",
              "  496.3955993652344,\n",
              "  494.8641662597656,\n",
              "  505.4810791015625,\n",
              "  485.5682678222656,\n",
              "  502.8827209472656,\n",
              "  506.8003845214844,\n",
              "  496.1763610839844,\n",
              "  512.9298706054688,\n",
              "  513.0665893554688,\n",
              "  493.4320373535156,\n",
              "  514.2455444335938,\n",
              "  501.8697204589844,\n",
              "  501.7252197265625,\n",
              "  490.5396423339844,\n",
              "  492.7071228027344,\n",
              "  492.560302734375,\n",
              "  490.6644287109375,\n",
              "  513.7910766601562,\n",
              "  494.156982421875,\n",
              "  510.909912109375,\n",
              "  481.0777893066406,\n",
              "  512.6524047851562,\n",
              "  493.2972106933594,\n",
              "  493.4352111816406,\n",
              "  512.4979858398438,\n",
              "  493.426025390625,\n",
              "  499.9794006347656,\n",
              "  501.020751953125,\n",
              "  490.9579162597656,\n",
              "  481.8048400878906,\n",
              "  498.8382873535156,\n",
              "  511.2027282714844,\n",
              "  500.860107421875,\n",
              "  500.433349609375,\n",
              "  483.0269470214844,\n",
              "  500.1435546875,\n",
              "  500.2787170410156,\n",
              "  489.0941467285156,\n",
              "  491.2616271972656,\n",
              "  499.8453063964844,\n",
              "  486.9120178222656,\n",
              "  490.8299255371094,\n",
              "  478.1722717285156,\n",
              "  499.2671813964844,\n",
              "  492.141845703125,\n",
              "  487.9349670410156,\n",
              "  498.551513671875,\n",
              "  498.042724609375,\n",
              "  477.023193359375,\n",
              "  500.1473693847656,\n",
              "  497.973388671875,\n",
              "  468.1378479003906,\n",
              "  487.2027282714844,\n",
              "  497.8169860839844,\n",
              "  486.631103515625,\n",
              "  478.0391540527344,\n",
              "  488.6541442871094,\n",
              "  505.687744140625,\n",
              "  467.1273498535156,\n",
              "  475.4288024902344,\n",
              "  496.2420654296875,\n",
              "  496.37939453125,\n",
              "  487.506591796875,\n",
              "  504.8184814453125,\n",
              "  478.4007263183594,\n",
              "  485.3216247558594,\n",
              "  497.9693298339844,\n",
              "  497.5440979003906,\n",
              "  504.377197265625,\n",
              "  484.7420349121094,\n",
              "  488.3797302246094,\n",
              "  467.278564453125,\n",
              "  486.3394470214844,\n",
              "  487.9449157714844,\n",
              "  492.4667053222656,\n",
              "  505.3917541503906,\n",
              "  481.414794921875,\n",
              "  483.5855407714844,\n",
              "  494.1975402832031,\n",
              "  483.5740661621094,\n",
              "  513.3972778320312,\n",
              "  494.04345703125,\n",
              "  484.6063537597656,\n",
              "  512.6815795898438,\n",
              "  493.0484313964844,\n",
              "  477.7588806152344,\n",
              "  493.3250427246094,\n",
              "  492.6195373535156,\n",
              "  492.473876953125,\n",
              "  485.6292419433594,\n",
              "  503.2240295410156,\n",
              "  500.766357421875,\n",
              "  491.61181640625,\n",
              "  478.9593200683594,\n",
              "  483.0726013183594,\n",
              "  502.7833557128906,\n",
              "  501.5133361816406,\n",
              "  482.4439392089844,\n",
              "  491.308837890625,\n",
              "  480.1210021972656,\n",
              "  490.4574279785156,\n",
              "  490.589599609375,\n",
              "  480.2490234375,\n",
              "  490.583984375,\n",
              "  473.2586975097656,\n",
              "  499.0194396972656,\n",
              "  481.1361389160156,\n",
              "  489.997802734375,\n",
              "  498.301025390625,\n",
              "  470.5021057128906,\n",
              "  487.820068359375,\n",
              "  461.489013671875,\n",
              "  489.5605773925781,\n",
              "  469.6483459472656,\n",
              "  486.6773376464844,\n",
              "  480.1175231933594,\n",
              "  507.6279296875,\n",
              "  468.9837341308594,\n",
              "  489.8807678222656,\n",
              "  468.4122009277344,\n",
              "  477.361572265625,\n",
              "  489.7284240722656,\n",
              "  485.8018493652344,\n",
              "  476.9295959472656,\n",
              "  489.2959899902344,\n",
              "  495.568603515625,\n",
              "  489.00390625,\n",
              "  463.8375549316406,\n",
              "  456.9991455078125,\n",
              "  476.0633850097656,\n",
              "  469.5025939941406,\n",
              "  467.04345703125,\n",
              "  469.2125549316406,\n",
              "  477.236083984375,\n",
              "  477.3731384277344,\n",
              "  487.988525390625,\n",
              "  493.9795837402344,\n",
              "  483.357177734375,\n",
              "  485.2406311035156,\n",
              "  474.8978576660156,\n",
              "  482.9203186035156,\n",
              "  472.2930603027344,\n",
              "  463.6993713378906,\n",
              "  467.3370666503906,\n",
              "  484.6493835449219,\n",
              "  494.7017517089844,\n",
              "  484.3576354980469,\n",
              "  484.2158203125,\n",
              "  466.6090393066406,\n",
              "  494.4031677246094,\n",
              "  462.537841796875,\n",
              "  481.318359375,\n",
              "  481.5067443847656,\n",
              "  481.15576171875,\n",
              "  472.3623352050781,\n",
              "  463.8460388183594,\n",
              "  461.744140625,\n",
              "  493.377197265625,\n",
              "  476.1454162597656,\n",
              "  472.0128173828125,\n",
              "  491.6363830566406,\n",
              "  482.6055908203125,\n",
              "  493.0127258300781,\n",
              "  454.8404846191406,\n",
              "  480.8424072265625,\n",
              "  491.4837646484375,\n",
              "  465.3477478027344,\n",
              "  471.4595642089844,\n",
              "  481.86962890625,\n",
              "  469.5174255371094,\n",
              "  471.2516174316406,\n",
              "  492.5840759277344,\n",
              "  471.113525390625,\n",
              "  462.5963134765625,\n",
              "  483.1209411621094,\n",
              "  481.3850402832031,\n",
              "  490.1766052246094,\n",
              "  458.25634765625,\n",
              "  472.9307556152344,\n",
              "  483.7319641113281,\n",
              "  481.2321472167969,\n",
              "  483.3824768066406,\n",
              "  483.11962890625,\n",
              "  470.3515319824219,\n",
              "  461.8348388671875,\n",
              "  472.2432861328125,\n",
              "  472.5301818847656,\n",
              "  482.58447265625,\n",
              "  461.5584411621094,\n",
              "  470.1055908203125,\n",
              "  478.64453125,\n",
              "  491.2401428222656,\n",
              "  480.5241394042969,\n",
              "  480.4492492675781,\n",
              "  461.2935485839844,\n",
              "  450.5914611816406,\n",
              "  480.2223815917969,\n",
              "  469.6680603027344,\n",
              "  469.5929260253906,\n",
              "  471.4114074707031,\n",
              "  480.0555725097656,\n",
              "  488.1624755859375,\n",
              "  477.740234375,\n",
              "  477.6639404296875,\n",
              "  479.618896484375,\n",
              "  458.3472900390625,\n",
              "  470.9052429199219,\n",
              "  468.9139099121094,\n",
              "  464.3417663574219,\n",
              "  470.5796203613281,\n",
              "  478.9579162597656,\n",
              "  470.642822265625,\n",
              "  470.468994140625,\n",
              "  468.4622497558594,\n",
              "  478.8663635253906,\n",
              "  461.71728515625,\n",
              "  470.0956115722656,\n",
              "  478.5570068359375,\n",
              "  476.5345764160156,\n",
              "  465.9784240722656,\n",
              "  488.8944396972656,\n",
              "  486.7864074707031,\n",
              "  461.2332458496094,\n",
              "  467.6446838378906,\n",
              "  459.1268310546875,\n",
              "  467.4425048828125,\n",
              "  488.556884765625,\n",
              "  469.3360290527344,\n",
              "  458.8371276855469,\n",
              "  477.693359375,\n",
              "  467.1845397949219,\n",
              "  458.6217346191406,\n",
              "  485.9624938964844,\n",
              "  468.9911193847656,\n",
              "  470.9144592285156,\n",
              "  479.3489990234375,\n",
              "  468.7393493652344,\n",
              "  479.1920471191406,\n",
              "  487.5421447753906,\n",
              "  487.4994812011719,\n",
              "  495.8244323730469,\n",
              "  476.8126525878906,\n",
              "  476.7440185546875,\n",
              "  468.2308654785156,\n",
              "  487.0853271484375,\n",
              "  466.0547790527344,\n",
              "  495.3812255859375,\n",
              "  457.4700622558594,\n",
              "  486.7857666015625,\n",
              "  476.2315979003906,\n",
              "  457.2452697753906,\n",
              "  459.223388671875,\n",
              "  474.0195617675781,\n",
              "  465.4772644042969,\n",
              "  465.4056091308594,\n",
              "  475.75146484375,\n",
              "  484.1896667480469,\n",
              "  475.6696472167969,\n",
              "  465.117919921875,\n",
              "  467.076904296875,\n",
              "  456.57080078125,\n",
              "  475.2822570800781,\n",
              "  473.172119140625,\n",
              "  494.1086120605469,\n",
              "  466.7737731933594,\n",
              "  485.5096435546875,\n",
              "  458.1861267089844,\n",
              "  462.4363708496094,\n",
              "  476.7638244628906,\n",
              "  474.726806640625,\n",
              "  462.2177734375,\n",
              "  474.57568359375,\n",
              "  472.385986328125,\n",
              "  474.339111328125,\n",
              "  466.0779113769531,\n",
              "  484.6604919433594,\n",
              "  463.8134460449219,\n",
              "  472.188720703125,\n",
              "  484.5271301269531,\n",
              "  456.9200134277344,\n",
              "  453.04541015625,\n",
              "  465.4819641113281,\n",
              "  454.9311828613281,\n",
              "  465.2214660644531,\n",
              "  473.7137145996094,\n",
              "  463.1632385253906,\n",
              "  473.4463195800781,\n",
              "  463.0184020996094,\n",
              "  471.263671875,\n",
              "  460.7093200683594,\n",
              "  464.8323669433594,\n",
              "  455.9469299316406,\n",
              "  464.6878967285156,\n",
              "  462.5845642089844,\n",
              "  456.2427673339844,\n",
              "  470.7345886230469,\n",
              "  464.3984069824219,\n",
              "  466.1992492675781,\n",
              "  451.7434997558594,\n",
              "  464.0177001953125,\n",
              "  491.1504821777344,\n",
              "  455.758544921875,\n",
              "  472.2391052246094,\n",
              "  459.6527099609375,\n",
              "  474.2987365722656,\n",
              "  482.674560546875,\n",
              "  482.4164733886719,\n",
              "  482.1535949707031,\n",
              "  461.6900329589844,\n",
              "  463.652099609375,\n",
              "  465.4161071777344,\n",
              "  481.8380126953125,\n",
              "  442.4840087890625,\n",
              "  471.4081726074219,\n",
              "  459.2403259277344,\n",
              "  450.5123596191406,\n",
              "  442.2070617675781,\n",
              "  481.587646484375,\n",
              "  460.9969787597656,\n",
              "  481.2121276855469,\n",
              "  473.3676452636719,\n",
              "  452.3382873535156,\n",
              "  470.7301330566406,\n",
              "  472.9230041503906,\n",
              "  460.3396911621094,\n",
              "  480.7408142089844,\n",
              "  478.8785400390625,\n",
              "  470.8517150878906,\n",
              "  453.8874206542969,\n",
              "  470.2071228027344,\n",
              "  480.610107421875,\n",
              "  480.5356140136719,\n",
              "  469.7181701660156,\n",
              "  469.9063415527344,\n",
              "  461.3837890625,\n",
              "  470.3004455566406,\n",
              "  480.1594543457031,\n",
              "  469.8853759765625,\n",
              "  480.2919616699219,\n",
              "  469.5282287597656,\n",
              "  459.3333435058594,\n",
              "  440.7054748535156,\n",
              "  469.5191955566406,\n",
              "  467.7662658691406,\n",
              "  459.3333435058594,\n",
              "  477.9371643066406,\n",
              "  457.3020324707031,\n",
              "  463.6770935058594,\n",
              "  467.4699401855469,\n",
              "  460.71875,\n",
              "  469.4952392578125,\n",
              "  448.8932189941406,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  463.3958435058594,\n",
              "  469.4802551269531,\n",
              "  469.7584533691406,\n",
              "  471.703369140625,\n",
              "  469.4712219238281,\n",
              "  453.2610168457031,\n",
              "  459.6146240234375,\n",
              "  467.1496887207031,\n",
              "  461.6458435058594,\n",
              "  477.828857421875,\n",
              "  467.7030334472656,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  479.5548400878906,\n",
              "  459.0520935058594,\n",
              "  467.4067687988281,\n",
              "  463.3958435058594,\n",
              "  459.0520935058594,\n",
              "  480.36865234375,\n",
              "  459.3333435058594,\n",
              "  469.7043151855469,\n",
              "  477.1941833496094,\n",
              "  447.2183532714844,\n",
              "  449.2526550292969,\n",
              "  471.4422912597656,\n",
              "  467.939208984375,\n",
              "  467.9361877441406,\n",
              "  459.3333435058594,\n",
              "  449.2676696777344,\n",
              "  469.67724609375,\n",
              "  469.3930358886719,\n",
              "  477.6966247558594,\n",
              "  471.980712890625,\n",
              "  461.6458435058594,\n",
              "  455.0982360839844,\n",
              "  471.4091491699219,\n",
              "  469.3749694824219,\n",
              "  438.9749450683594,\n",
              "  457.0208435058594,\n",
              "  479.6795959472656,\n",
              "  461.3646240234375,\n",
              "  451.0568542480469,\n",
              "  469.3568115234375,\n",
              "  457.0208740234375,\n",
              "  467.6007995605469,\n",
              "  469.3478698730469,\n",
              "  479.6376647949219,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  471.9295349121094,\n",
              "  487.5816955566406,\n",
              "  453.3994445800781,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  451.3772277832031,\n",
              "  461.3645935058594,\n",
              "  471.3459777832031,\n",
              "  449.07373046875,\n",
              "  471.6211853027344,\n",
              "  467.2744140625,\n",
              "  469.5838928222656,\n",
              "  469.5809020996094,\n",
              "  488.0360412597656,\n",
              "  459.6145324707031,\n",
              "  477.2167663574219,\n",
              "  461.3645935058594,\n",
              "  449.382080078125,\n",
              "  467.8127746582031,\n",
              "  459.3333435058594,\n",
              "  467.5255126953125,\n",
              "  469.2724609375,\n",
              "  455.2097473144531,\n",
              "  479.4806823730469,\n",
              "  459.0520935058594,\n",
              "  471.0104064941406,\n",
              "  479.1814270019531,\n",
              "  461.0833435058594,\n",
              "  468.9701843261719,\n",
              "  469.5296630859375,\n",
              "  457.3020935058594,\n",
              "  471.2736511230469,\n",
              "  467.4893798828125,\n",
              "  477.389404296875,\n",
              "  479.97705078125,\n",
              "  447.1238708496094,\n",
              "  471.5397033691406,\n",
              "  451.1924743652344,\n",
              "  469.50244140625,\n",
              "  468.9368896484375,\n",
              "  461.0833435058594,\n",
              "  479.6536560058594,\n",
              "  461.0833435058594,\n",
              "  451.2105407714844,\n",
              "  479.0729064941406,\n",
              "  459.3333435058594,\n",
              "  439.3245849609375,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  453.2598571777344,\n",
              "  453.5440979003906,\n",
              "  471.2132568359375,\n",
              "  469.4602966308594,\n",
              "  451.2406311035156,\n",
              "  461.0833435058594,\n",
              "  476.9754943847656,\n",
              "  469.1669921875,\n",
              "  461.6458435058594,\n",
              "  449.5057373046875,\n",
              "  449.2274169921875,\n",
              "  471.748779296875,\n",
              "  469.433349609375,\n",
              "  478.9648742675781,\n",
              "  468.8648986816406,\n",
              "  470.8930969238281,\n",
              "  471.4526062011719,\n",
              "  469.1370849609375,\n",
              "  459.3333740234375,\n",
              "  476.8975830078125,\n",
              "  459.6145935058594,\n",
              "  471.1564025878906,\n",
              "  469.4033203125,\n",
              "  451.5788269042969,\n",
              "  461.3646240234375,\n",
              "  471.7069091796875,\n",
              "  478.8868713378906,\n",
              "  450.9450378417969,\n",
              "  463.1145935058594,\n",
              "  477.1186218261719,\n",
              "  468.8167419433594,\n",
              "  453.3529052734375,\n",
              "  457.3020935058594,\n",
              "  467.0577392578125,\n",
              "  471.3985290527344,\n",
              "  451.3337097167969,\n",
              "  471.3924560546875,\n",
              "  479.101806640625,\n",
              "  451.6240234375,\n",
              "  443.6395263671875,\n",
              "  443.6455383300781,\n",
              "  449.3205871582031,\n",
              "  461.3645935058594,\n",
              "  471.3713073730469,\n",
              "  469.0558166503906,\n",
              "  469.0528259277344,\n",
              "  451.3669128417969,\n",
              "  458.96875,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  471.0689392089844,\n",
              "  461.0833435058594,\n",
              "  467.2816162109375,\n",
              "  469.0286865234375,\n",
              "  453.7035217285156,\n",
              "  469.3038024902344,\n",
              "  469.3008728027344,\n",
              "  459.3333435058594,\n",
              "  471.3260803222656,\n",
              "  463.1145935058594,\n",
              "  466.6950988769531,\n",
              "  449.662109375,\n",
              "  487.1507873535156,\n",
              "  441.7529296875,\n",
              "  461.3645935058594,\n",
              "  462.7500305175781,\n",
              "  478.6457214355469,\n",
              "  469.2677307128906,\n",
              "  469.2648010253906,\n",
              "  487.3686218261719,\n",
              "  459.3333435058594,\n",
              "  479.17822265625,\n",
              "  459.0520935058594,\n",
              "  459.0520324707031,\n",
              "  466.9341735839844,\n",
              "  471.5562438964844,\n",
              "  463.6770935058594,\n",
              "  469.5188903808594,\n",
              "  462.75,\n",
              "  461.0833435058594,\n",
              "  476.5301513671875,\n",
              "  451.4722900390625,\n",
              "  453.5065612792969,\n",
              "  460.71875,\n",
              "  478.5374450683594,\n",
              "  453.7967224121094,\n",
              "  461.0833435058594,\n",
              "  461.3646240234375,\n",
              "  476.7633972167969,\n",
              "  478.7886657714844,\n",
              "  476.47021484375,\n",
              "  469.195556640625,\n",
              "  449.4740905761719,\n",
              "  469.1896057128906,\n",
              "  459.3333435058594,\n",
              "  486.5716247558594,\n",
              "  453.5486145019531,\n",
              "  459.3333435058594,\n",
              "  467.4245300292969,\n",
              "  471.2027893066406,\n",
              "  459.3333435058594,\n",
              "  459.0520935058594,\n",
              "  457.3020935058594,\n",
              "  449.7884521484375,\n",
              "  446.06201171875,\n",
              "  449.7945251464844,\n",
              "  478.4050598144531,\n",
              "  459.3333435058594,\n",
              "  451.5534973144531,\n",
              "  468.5789489746094,\n",
              "  459.0520935058594,\n",
              "  476.6249084472656,\n",
              "  467.1011047363281,\n",
              "  453.5999450683594,\n",
              "  469.1263122558594]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "2ac6170f-493d-40f3-ac12-dde30508205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression_unfreeze_1000.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression_unfreeze_1000.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "03b2ae4b-0a08-46d5-e148-53b869735214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1dc69c8-0a9b-4fd2-be3e-3edb9acd0baf\", \"2Class_regression_unfreeze_1000.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}