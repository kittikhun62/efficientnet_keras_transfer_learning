{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuwOSwGddLsg1LVTYw9riZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a918d88-2e67-484b-9d20-df494f64877a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "0109c152-f97c-4ffb-d563-bacdc206b76c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-827ce19c-fdac-47f8-a146-6d21226191b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-827ce19c-fdac-47f8-a146-6d21226191b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-827ce19c-fdac-47f8-a146-6d21226191b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-827ce19c-fdac-47f8-a146-6d21226191b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "9296344a-cd03-4637-d5e8-f18d370fea8e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "38e88850-60ae-479f-88a3-e6ae1f69eed1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a52cbe2-feee-4747-97eb-6563bd9a14d3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "id": "XyxDPsEi6yYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaeea674-988c-4dde-b62d-ee92ec6f1edc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dde9870-27ac-4cc8-81a1-324b12a7e58f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e96bce-bc5e-4cb6-b455-fc48666da767"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)            (None, 75, 75, 32)   864         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 75, 75, 32)  128         ['conv2d_130[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_98 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_32 (Depthwise  (None, 75, 75, 32)  288         ['swish_98[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 75, 75, 32)  128         ['depthwise_conv2d_32[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_99 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_32 (Lambda)             (None, 1, 1, 32)     0           ['swish_99[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)            (None, 1, 1, 8)      264         ['lambda_32[0][0]']              \n",
            "                                                                                                  \n",
            " swish_100 (Swish)              (None, 1, 1, 8)      0           ['conv2d_131[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)            (None, 1, 1, 32)     288         ['swish_100[0][0]']              \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 1, 1, 32)     0           ['conv2d_132[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_32 (Multiply)         (None, 75, 75, 32)   0           ['activation_32[0][0]',          \n",
            "                                                                  'swish_99[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)            (None, 75, 75, 16)   512         ['multiply_32[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_100 (Batch  (None, 75, 75, 16)  64          ['conv2d_133[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)            (None, 75, 75, 96)   1536        ['batch_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_101 (Batch  (None, 75, 75, 96)  384         ['conv2d_134[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_101 (Swish)              (None, 75, 75, 96)   0           ['batch_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_33 (Depthwise  (None, 38, 38, 96)  864         ['swish_101[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_102 (Batch  (None, 38, 38, 96)  384         ['depthwise_conv2d_33[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_102 (Swish)              (None, 38, 38, 96)   0           ['batch_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " lambda_33 (Lambda)             (None, 1, 1, 96)     0           ['swish_102[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)            (None, 1, 1, 4)      388         ['lambda_33[0][0]']              \n",
            "                                                                                                  \n",
            " swish_103 (Swish)              (None, 1, 1, 4)      0           ['conv2d_135[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)            (None, 1, 1, 96)     480         ['swish_103[0][0]']              \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 1, 1, 96)     0           ['conv2d_136[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_33 (Multiply)         (None, 38, 38, 96)   0           ['activation_33[0][0]',          \n",
            "                                                                  'swish_102[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)            (None, 38, 38, 24)   2304        ['multiply_33[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_103 (Batch  (None, 38, 38, 24)  96          ['conv2d_137[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)            (None, 38, 38, 144)  3456        ['batch_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_104 (Batch  (None, 38, 38, 144)  576        ['conv2d_138[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_104 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_34 (Depthwise  (None, 38, 38, 144)  1296       ['swish_104[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_105 (Batch  (None, 38, 38, 144)  576        ['depthwise_conv2d_34[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_105 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " lambda_34 (Lambda)             (None, 1, 1, 144)    0           ['swish_105[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_34[0][0]']              \n",
            "                                                                                                  \n",
            " swish_106 (Swish)              (None, 1, 1, 6)      0           ['conv2d_139[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_106[0][0]']              \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 1, 1, 144)    0           ['conv2d_140[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_34 (Multiply)         (None, 38, 38, 144)  0           ['activation_34[0][0]',          \n",
            "                                                                  'swish_105[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)            (None, 38, 38, 24)   3456        ['multiply_34[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_106 (Batch  (None, 38, 38, 24)  96          ['conv2d_141[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_18 (DropConnect)  (None, 38, 38, 24)   0           ['batch_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 38, 38, 24)   0           ['drop_connect_18[0][0]',        \n",
            "                                                                  'batch_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)            (None, 38, 38, 144)  3456        ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 38, 38, 144)  576        ['conv2d_142[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_107 (Swish)              (None, 38, 38, 144)  0           ['batch_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_35 (Depthwise  (None, 19, 19, 144)  3600       ['swish_107[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 19, 19, 144)  576        ['depthwise_conv2d_35[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_108 (Swish)              (None, 19, 19, 144)  0           ['batch_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " lambda_35 (Lambda)             (None, 1, 1, 144)    0           ['swish_108[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)            (None, 1, 1, 6)      870         ['lambda_35[0][0]']              \n",
            "                                                                                                  \n",
            " swish_109 (Swish)              (None, 1, 1, 6)      0           ['conv2d_143[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)            (None, 1, 1, 144)    1008        ['swish_109[0][0]']              \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 1, 1, 144)    0           ['conv2d_144[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_35 (Multiply)         (None, 19, 19, 144)  0           ['activation_35[0][0]',          \n",
            "                                                                  'swish_108[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 19, 19, 40)   5760        ['multiply_35[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 19, 19, 40)  160         ['conv2d_145[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 19, 19, 240)  9600        ['batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 19, 19, 240)  960        ['conv2d_146[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_110 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_36 (Depthwise  (None, 19, 19, 240)  6000       ['swish_110[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 19, 19, 240)  960        ['depthwise_conv2d_36[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_111 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " lambda_36 (Lambda)             (None, 1, 1, 240)    0           ['swish_111[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_36[0][0]']              \n",
            "                                                                                                  \n",
            " swish_112 (Swish)              (None, 1, 1, 10)     0           ['conv2d_147[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_112[0][0]']              \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 1, 1, 240)    0           ['conv2d_148[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_36 (Multiply)         (None, 19, 19, 240)  0           ['activation_36[0][0]',          \n",
            "                                                                  'swish_111[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 19, 19, 40)   9600        ['multiply_36[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_112 (Batch  (None, 19, 19, 40)  160         ['conv2d_149[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_19 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_19[0][0]',        \n",
            "                                                                  'batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 19, 19, 240)  9600        ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_113 (Batch  (None, 19, 19, 240)  960        ['conv2d_150[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_113 (Swish)              (None, 19, 19, 240)  0           ['batch_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_37 (Depthwise  (None, 10, 10, 240)  2160       ['swish_113[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_114 (Batch  (None, 10, 10, 240)  960        ['depthwise_conv2d_37[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_114 (Swish)              (None, 10, 10, 240)  0           ['batch_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " lambda_37 (Lambda)             (None, 1, 1, 240)    0           ['swish_114[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 1, 1, 10)     2410        ['lambda_37[0][0]']              \n",
            "                                                                                                  \n",
            " swish_115 (Swish)              (None, 1, 1, 10)     0           ['conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 1, 1, 240)    2640        ['swish_115[0][0]']              \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 1, 1, 240)    0           ['conv2d_152[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_37 (Multiply)         (None, 10, 10, 240)  0           ['activation_37[0][0]',          \n",
            "                                                                  'swish_114[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 10, 10, 80)   19200       ['multiply_37[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_115 (Batch  (None, 10, 10, 80)  320         ['conv2d_153[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 10, 10, 480)  38400       ['batch_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_116 (Batch  (None, 10, 10, 480)  1920       ['conv2d_154[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_116 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_38 (Depthwise  (None, 10, 10, 480)  4320       ['swish_116[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_117 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_38[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_117 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " lambda_38 (Lambda)             (None, 1, 1, 480)    0           ['swish_117[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_38[0][0]']              \n",
            "                                                                                                  \n",
            " swish_118 (Swish)              (None, 1, 1, 20)     0           ['conv2d_155[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_118[0][0]']              \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 1, 1, 480)    0           ['conv2d_156[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_38 (Multiply)         (None, 10, 10, 480)  0           ['activation_38[0][0]',          \n",
            "                                                                  'swish_117[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_38[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_118 (Batch  (None, 10, 10, 80)  320         ['conv2d_157[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_20 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_20[0][0]',        \n",
            "                                                                  'batch_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 10, 10, 480)  38400       ['add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_119 (Batch  (None, 10, 10, 480)  1920       ['conv2d_158[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_119 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_39 (Depthwise  (None, 10, 10, 480)  4320       ['swish_119[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_120 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_39[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_120 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " lambda_39 (Lambda)             (None, 1, 1, 480)    0           ['swish_120[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_39[0][0]']              \n",
            "                                                                                                  \n",
            " swish_121 (Swish)              (None, 1, 1, 20)     0           ['conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_121[0][0]']              \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 1, 1, 480)    0           ['conv2d_160[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_39 (Multiply)         (None, 10, 10, 480)  0           ['activation_39[0][0]',          \n",
            "                                                                  'swish_120[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_161 (Conv2D)            (None, 10, 10, 80)   38400       ['multiply_39[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_121 (Batch  (None, 10, 10, 80)  320         ['conv2d_161[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_21 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_21[0][0]',        \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)            (None, 10, 10, 480)  38400       ['add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_122 (Batch  (None, 10, 10, 480)  1920       ['conv2d_162[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_122 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_40 (Depthwise  (None, 10, 10, 480)  12000      ['swish_122[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_123 (Batch  (None, 10, 10, 480)  1920       ['depthwise_conv2d_40[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_123 (Swish)              (None, 10, 10, 480)  0           ['batch_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " lambda_40 (Lambda)             (None, 1, 1, 480)    0           ['swish_123[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)            (None, 1, 1, 20)     9620        ['lambda_40[0][0]']              \n",
            "                                                                                                  \n",
            " swish_124 (Swish)              (None, 1, 1, 20)     0           ['conv2d_163[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)            (None, 1, 1, 480)    10080       ['swish_124[0][0]']              \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 1, 1, 480)    0           ['conv2d_164[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_40 (Multiply)         (None, 10, 10, 480)  0           ['activation_40[0][0]',          \n",
            "                                                                  'swish_123[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_40[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_124 (Batch  (None, 10, 10, 112)  448        ['conv2d_165[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_125 (Batch  (None, 10, 10, 672)  2688       ['conv2d_166[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_125 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_41 (Depthwise  (None, 10, 10, 672)  16800      ['swish_125[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_126 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_41[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_126 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_126[0][0]']\n",
            "                                                                                                  \n",
            " lambda_41 (Lambda)             (None, 1, 1, 672)    0           ['swish_126[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_41[0][0]']              \n",
            "                                                                                                  \n",
            " swish_127 (Swish)              (None, 1, 1, 28)     0           ['conv2d_167[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_168 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_127[0][0]']              \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 1, 1, 672)    0           ['conv2d_168[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_41 (Multiply)         (None, 10, 10, 672)  0           ['activation_41[0][0]',          \n",
            "                                                                  'swish_126[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_169 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_41[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_127 (Batch  (None, 10, 10, 112)  448        ['conv2d_169[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_22 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_22[0][0]',        \n",
            "                                                                  'batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_170 (Conv2D)            (None, 10, 10, 672)  75264       ['add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_128 (Batch  (None, 10, 10, 672)  2688       ['conv2d_170[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_128 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_128[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_42 (Depthwise  (None, 10, 10, 672)  16800      ['swish_128[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_129 (Batch  (None, 10, 10, 672)  2688       ['depthwise_conv2d_42[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_129 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " lambda_42 (Lambda)             (None, 1, 1, 672)    0           ['swish_129[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_171 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_42[0][0]']              \n",
            "                                                                                                  \n",
            " swish_130 (Swish)              (None, 1, 1, 28)     0           ['conv2d_171[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_172 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_130[0][0]']              \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 1, 1, 672)    0           ['conv2d_172[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_42 (Multiply)         (None, 10, 10, 672)  0           ['activation_42[0][0]',          \n",
            "                                                                  'swish_129[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_173 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_42[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_130 (Batch  (None, 10, 10, 112)  448        ['conv2d_173[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_23 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_23[0][0]',        \n",
            "                                                                  'add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_174 (Conv2D)            (None, 10, 10, 672)  75264       ['add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_131 (Batch  (None, 10, 10, 672)  2688       ['conv2d_174[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_131 (Swish)              (None, 10, 10, 672)  0           ['batch_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_43 (Depthwise  (None, 5, 5, 672)   16800       ['swish_131[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_132 (Batch  (None, 5, 5, 672)   2688        ['depthwise_conv2d_43[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_132 (Swish)              (None, 5, 5, 672)    0           ['batch_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " lambda_43 (Lambda)             (None, 1, 1, 672)    0           ['swish_132[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_175 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_43[0][0]']              \n",
            "                                                                                                  \n",
            " swish_133 (Swish)              (None, 1, 1, 28)     0           ['conv2d_175[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_176 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_133[0][0]']              \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 1, 1, 672)    0           ['conv2d_176[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_43 (Multiply)         (None, 5, 5, 672)    0           ['activation_43[0][0]',          \n",
            "                                                                  'swish_132[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_177 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_43[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_133 (Batch  (None, 5, 5, 192)   768         ['conv2d_177[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_178 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_134 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_178[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_134 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_44 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_134[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_44[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_135 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " lambda_44 (Lambda)             (None, 1, 1, 1152)   0           ['swish_135[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_179 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_44[0][0]']              \n",
            "                                                                                                  \n",
            " swish_136 (Swish)              (None, 1, 1, 48)     0           ['conv2d_179[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_136[0][0]']              \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_180[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_44 (Multiply)         (None, 5, 5, 1152)   0           ['activation_44[0][0]',          \n",
            "                                                                  'swish_135[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_44[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_136 (Batch  (None, 5, 5, 192)   768         ['conv2d_181[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_24 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_24[0][0]',        \n",
            "                                                                  'batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_137 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_182[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_137 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_45 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_137[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_138 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_45[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_138 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " lambda_45 (Lambda)             (None, 1, 1, 1152)   0           ['swish_138[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_45[0][0]']              \n",
            "                                                                                                  \n",
            " swish_139 (Swish)              (None, 1, 1, 48)     0           ['conv2d_183[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_139[0][0]']              \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_184[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_45 (Multiply)         (None, 5, 5, 1152)   0           ['activation_45[0][0]',          \n",
            "                                                                  'swish_138[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_45[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_139 (Batch  (None, 5, 5, 192)   768         ['conv2d_185[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_25 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_25[0][0]',        \n",
            "                                                                  'add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_140 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_186[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_140 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_46 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_140[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_141 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_46[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_141 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " lambda_46 (Lambda)             (None, 1, 1, 1152)   0           ['swish_141[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_46[0][0]']              \n",
            "                                                                                                  \n",
            " swish_142 (Swish)              (None, 1, 1, 48)     0           ['conv2d_187[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_188 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_142[0][0]']              \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_188[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_46 (Multiply)         (None, 5, 5, 1152)   0           ['activation_46[0][0]',          \n",
            "                                                                  'swish_141[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_189 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_46[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_142 (Batch  (None, 5, 5, 192)   768         ['conv2d_189[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " drop_connect_26 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_26[0][0]',        \n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_190 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_143 (Batch  (None, 5, 5, 1152)  4608        ['conv2d_190[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_143 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " depthwise_conv2d_47 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_143[0][0]']              \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_144 (Batch  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_47[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_144 (Swish)              (None, 5, 5, 1152)   0           ['batch_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " lambda_47 (Lambda)             (None, 1, 1, 1152)   0           ['swish_144[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_191 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_47[0][0]']              \n",
            "                                                                                                  \n",
            " swish_145 (Swish)              (None, 1, 1, 48)     0           ['conv2d_191[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_192 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_145[0][0]']              \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_192[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_47 (Multiply)         (None, 5, 5, 1152)   0           ['activation_47[0][0]',          \n",
            "                                                                  'swish_144[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_193 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_47[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_145 (Batch  (None, 5, 5, 320)   1280        ['conv2d_193[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_194 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_146 (Batch  (None, 5, 5, 1280)  5120        ['conv2d_194[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " swish_146 (Swish)              (None, 5, 5, 1280)   0           ['batch_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc1a309-a45a-4cc1-bbe1-58488a6f52f1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c16c33-25c5-4f2a-a39b-a949ac97fe4a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667dabfe-1280-4758-8d2b-2ae636839d74"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=2e-2),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8078f17f-ecc2-4605-8ff6-4d6d77994242"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-103-28c75c78f62a>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 19s 291ms/step - loss: 1505312.2500 - mae: 1005.0330 - val_loss: 532883.6250 - val_mae: 559.5649\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 1510168.2500 - mae: 1002.4022 - val_loss: 518962.4688 - val_mae: 552.5722\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1503516.7500 - mae: 1001.3934 - val_loss: 542877.6875 - val_mae: 571.7971\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 1500651.3750 - mae: 999.2476 - val_loss: 541332.0625 - val_mae: 570.9155\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1511018.2500 - mae: 1004.6882 - val_loss: 522400.5312 - val_mae: 552.2836\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1482492.5000 - mae: 992.4402 - val_loss: 525256.6250 - val_mae: 556.9937\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1501300.2500 - mae: 1001.6033 - val_loss: 531823.6875 - val_mae: 559.5847\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1494626.6250 - mae: 996.6467 - val_loss: 534656.3125 - val_mae: 564.5622\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 1500101.8750 - mae: 996.9652 - val_loss: 520450.5000 - val_mae: 552.9006\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1498736.6250 - mae: 999.0601 - val_loss: 526909.0625 - val_mae: 554.9370\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1487057.6250 - mae: 992.4120 - val_loss: 512773.5000 - val_mae: 543.0203\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1489359.0000 - mae: 994.9283 - val_loss: 515613.1562 - val_mae: 548.2686\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 1481176.2500 - mae: 990.2836 - val_loss: 501222.3438 - val_mae: 531.6769\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 1475426.7500 - mae: 986.7631 - val_loss: 504054.8438 - val_mae: 536.9240\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1473220.1250 - mae: 985.2732 - val_loss: 519039.9062 - val_mae: 548.2853\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1488308.2500 - mae: 992.9949 - val_loss: 500969.2812 - val_mae: 534.0429\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 1476409.7500 - mae: 985.9741 - val_loss: 523504.1250 - val_mae: 553.4560\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1466132.2500 - mae: 983.4252 - val_loss: 522633.8438 - val_mae: 552.9200\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1463168.6250 - mae: 980.3456 - val_loss: 496125.8438 - val_mae: 525.2971\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 1465470.3750 - mae: 981.8970 - val_loss: 503052.0312 - val_mae: 536.6912\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1464872.1250 - mae: 981.4393 - val_loss: 485071.9688 - val_mae: 518.3574\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1462915.8750 - mae: 980.1395 - val_loss: 499766.6562 - val_mae: 529.1464\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 1453020.2500 - mae: 974.1675 - val_loss: 498418.9062 - val_mae: 532.1101\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 1430711.8750 - mae: 965.8190 - val_loss: 513125.3438 - val_mae: 543.7565\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1419974.0000 - mae: 960.7411 - val_loss: 479191.6250 - val_mae: 512.6540\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1427411.6250 - mae: 967.0936 - val_loss: 497913.1562 - val_mae: 530.1484\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1445370.0000 - mae: 972.2697 - val_loss: 496440.7500 - val_mae: 529.0078\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 1434345.1250 - mae: 968.1832 - val_loss: 490941.4062 - val_mae: 525.7883\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 1408123.6250 - mae: 958.4754 - val_loss: 469376.3750 - val_mae: 504.8928\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 1419046.0000 - mae: 960.4123 - val_loss: 487810.7188 - val_mae: 517.7258\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 1434467.2500 - mae: 966.0712 - val_loss: 498429.2188 - val_mae: 532.0167\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 1425813.5000 - mae: 963.8229 - val_loss: 496896.5938 - val_mae: 530.5743\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 1410357.5000 - mae: 959.1044 - val_loss: 475520.9062 - val_mae: 509.6601\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 1416951.0000 - mae: 960.1256 - val_loss: 489871.7812 - val_mae: 520.7524\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1407166.1250 - mae: 957.4189 - val_loss: 488386.7500 - val_mae: 519.3246\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 1410229.3750 - mae: 956.1189 - val_loss: 490887.4688 - val_mae: 524.6243\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1413075.6250 - mae: 957.8558 - val_loss: 457975.6562 - val_mae: 493.2177\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1392280.6250 - mae: 951.3333 - val_loss: 480048.1562 - val_mae: 513.0228\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 1412769.7500 - mae: 956.8446 - val_loss: 459102.0312 - val_mae: 492.4133\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 1394249.8750 - mae: 950.1148 - val_loss: 446020.0000 - val_mae: 480.4995\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 1407951.8750 - mae: 957.0659 - val_loss: 475694.8750 - val_mae: 508.7623\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 1405834.5000 - mae: 955.6740 - val_loss: 474260.5938 - val_mae: 507.6112\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1407005.5000 - mae: 957.8825 - val_loss: 476676.2188 - val_mae: 508.1860\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1375351.7500 - mae: 941.2107 - val_loss: 467459.6562 - val_mae: 502.4391\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1389312.7500 - mae: 946.5208 - val_loss: 446875.4688 - val_mae: 482.0954\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 1393528.6250 - mae: 946.9070 - val_loss: 464618.3438 - val_mae: 499.3402\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1384254.6250 - mae: 944.6357 - val_loss: 467090.7812 - val_mae: 500.2350\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 1379823.0000 - mae: 942.0460 - val_loss: 473218.5000 - val_mae: 507.2402\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 1376777.2500 - mae: 940.4966 - val_loss: 456711.5312 - val_mae: 488.9464\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 1383625.8750 - mae: 944.3867 - val_loss: 470411.6250 - val_mae: 505.2658\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 1345389.6250 - mae: 925.8698 - val_loss: 472776.1562 - val_mae: 505.0264\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1355096.0000 - mae: 930.4738 - val_loss: 452558.7188 - val_mae: 484.4134\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 11s 264ms/step - loss: 1364960.3750 - mae: 935.9359 - val_loss: 454822.1562 - val_mae: 489.9701\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 1353322.0000 - mae: 930.6039 - val_loss: 446028.4688 - val_mae: 480.1244\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1373436.0000 - mae: 940.1384 - val_loss: 452081.6562 - val_mae: 487.4358\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 1347988.0000 - mae: 928.3145 - val_loss: 447112.6562 - val_mae: 478.7591\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1362117.0000 - mae: 933.7224 - val_loss: 449569.4062 - val_mae: 479.3733\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 1372315.0000 - mae: 939.3077 - val_loss: 448220.3438 - val_mae: 478.2366\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1349112.2500 - mae: 928.2779 - val_loss: 443044.8438 - val_mae: 474.8148\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 1346561.0000 - mae: 927.5253 - val_loss: 426838.7812 - val_mae: 461.3539\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1342589.5000 - mae: 925.1645 - val_loss: 440405.3438 - val_mae: 473.6386\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1344398.2500 - mae: 924.8702 - val_loss: 446295.8750 - val_mae: 480.9670\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 1350836.3750 - mae: 927.5804 - val_loss: 444952.8438 - val_mae: 480.4538\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 1326959.6250 - mae: 917.2882 - val_loss: 425456.0312 - val_mae: 460.8182\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 1325268.8750 - mae: 916.0653 - val_loss: 438871.7188 - val_mae: 472.5261\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1327004.7500 - mae: 917.0140 - val_loss: 448085.0000 - val_mae: 486.8679\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1327586.5000 - mae: 919.6465 - val_loss: 428718.9062 - val_mae: 467.1500\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 1303769.8750 - mae: 906.0530 - val_loss: 456222.4062 - val_mae: 495.9289\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1314101.7500 - mae: 911.3712 - val_loss: 436980.1250 - val_mae: 476.2171\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 1342241.3750 - mae: 925.7107 - val_loss: 414100.0938 - val_mae: 454.5829\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1322244.0000 - mae: 914.0765 - val_loss: 452104.0000 - val_mae: 493.6833\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1317841.1250 - mae: 912.2917 - val_loss: 432990.2188 - val_mae: 474.0068\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 1327795.0000 - mae: 916.1988 - val_loss: 410314.0312 - val_mae: 452.5836\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 1314826.8750 - mae: 912.7751 - val_loss: 423454.5938 - val_mae: 464.2646\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1286365.1250 - mae: 898.1499 - val_loss: 436029.8750 - val_mae: 480.4564\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 1309136.6250 - mae: 910.1581 - val_loss: 427795.3438 - val_mae: 470.8652\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1296444.3750 - mae: 902.9026 - val_loss: 426523.6562 - val_mae: 470.3593\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1300750.1250 - mae: 901.3975 - val_loss: 428939.2188 - val_mae: 471.6642\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 1306858.0000 - mae: 907.7179 - val_loss: 441326.5938 - val_mae: 488.6643\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1296217.3750 - mae: 900.2034 - val_loss: 409090.7188 - val_mae: 453.2524\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1282889.5000 - mae: 898.1266 - val_loss: 404169.9062 - val_mae: 450.9695\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 1284193.8750 - mae: 896.6951 - val_loss: 413402.9062 - val_mae: 461.6458\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 1294339.0000 - mae: 898.4158 - val_loss: 412151.2500 - val_mae: 461.3646\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 1296793.8750 - mae: 901.8229 - val_loss: 414576.8438 - val_mae: 463.3958\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 1275954.2500 - mae: 894.9213 - val_loss: 426709.6250 - val_mae: 479.8899\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1275616.8750 - mae: 889.3834 - val_loss: 414513.6562 - val_mae: 469.2176\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 7s 153ms/step - loss: 1270757.0000 - mae: 888.3387 - val_loss: 399967.9688 - val_mae: 457.3021\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 1270800.0000 - mae: 888.8809 - val_loss: 412650.8438 - val_mae: 469.5245\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1272282.7500 - mae: 890.2771 - val_loss: 421598.7812 - val_mae: 479.0945\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 1289153.8750 - mae: 897.8134 - val_loss: 409561.7812 - val_mae: 469.1009\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1264132.2500 - mae: 885.3085 - val_loss: 382087.7812 - val_mae: 441.4388\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 1269079.3750 - mae: 888.3144 - val_loss: 417828.2812 - val_mae: 479.2003\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1269569.1250 - mae: 887.4467 - val_loss: 406475.7812 - val_mae: 469.0972\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 1268111.2500 - mae: 887.0212 - val_loss: 408870.5938 - val_mae: 471.3803\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1260688.0000 - mae: 884.2863 - val_loss: 404038.0938 - val_mae: 469.3195\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 1258301.7500 - mae: 882.5447 - val_loss: 386420.2812 - val_mae: 451.6885\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 1245228.2500 - mae: 878.6492 - val_loss: 378853.9062 - val_mae: 443.8214\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 1260680.1250 - mae: 885.1552 - val_loss: 380511.4062 - val_mae: 449.7164\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 1260795.3750 - mae: 885.6235 - val_loss: 392894.7500 - val_mae: 461.6458\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1237365.5000 - mae: 873.3210 - val_loss: 394477.2500 - val_mae: 467.1427\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1244421.8750 - mae: 876.5818 - val_loss: 403191.5938 - val_mae: 477.2063\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1247699.7500 - mae: 876.8252 - val_loss: 378975.6562 - val_mae: 451.2173\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 1250143.0000 - mae: 879.0143 - val_loss: 404258.4688 - val_mae: 478.2778\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1250428.2500 - mae: 878.9905 - val_loss: 396238.1562 - val_mae: 470.4425\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1240052.7500 - mae: 873.4471 - val_loss: 376197.0938 - val_mae: 451.9510\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1230278.3750 - mae: 869.7118 - val_loss: 387475.3438 - val_mae: 466.9688\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1246034.0000 - mae: 879.5781 - val_loss: 383630.4062 - val_mae: 461.0834\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1221102.3750 - mae: 865.4356 - val_loss: 382526.3750 - val_mae: 461.3646\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 1214895.8750 - mae: 865.6246 - val_loss: 403259.1250 - val_mae: 485.7594\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1229868.0000 - mae: 868.6387 - val_loss: 380264.6562 - val_mae: 461.3646\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 1223033.2500 - mae: 865.2952 - val_loss: 385248.2188 - val_mae: 469.1355\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 1233086.6250 - mae: 870.4059 - val_loss: 387614.6250 - val_mae: 471.1379\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1234259.2500 - mae: 873.2208 - val_loss: 360828.6562 - val_mae: 444.3868\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 1222569.7500 - mae: 864.7794 - val_loss: 387724.3750 - val_mae: 475.6067\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1215289.1250 - mae: 861.6460 - val_loss: 374689.8750 - val_mae: 461.3646\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 1209468.7500 - mae: 857.8741 - val_loss: 379539.1562 - val_mae: 468.7089\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 1201264.8750 - mae: 855.6520 - val_loss: 374883.6562 - val_mae: 466.0865\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 1219942.5000 - mae: 862.9831 - val_loss: 358600.4688 - val_mae: 450.2968\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1198865.7500 - mae: 854.6739 - val_loss: 347630.3750 - val_mae: 440.6729\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 1184219.8750 - mae: 849.0800 - val_loss: 384372.1562 - val_mae: 477.5722\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 1193173.2500 - mae: 852.4136 - val_loss: 373997.9688 - val_mae: 468.5646\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 1194968.3750 - mae: 851.1398 - val_loss: 363632.9688 - val_mae: 459.3333\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1210785.7500 - mae: 861.5062 - val_loss: 362561.9062 - val_mae: 459.3333\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1191256.2500 - mae: 852.3552 - val_loss: 355782.3438 - val_mae: 452.5016\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 1201499.8750 - mae: 857.1047 - val_loss: 384546.9688 - val_mae: 484.6491\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 1191163.8750 - mae: 850.6002 - val_loss: 359375.0000 - val_mae: 459.3333\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1189365.8750 - mae: 849.9246 - val_loss: 358352.0000 - val_mae: 459.3333\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 1176835.7500 - mae: 844.4688 - val_loss: 366328.0938 - val_mae: 468.0805\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1159811.5000 - mae: 836.4211 - val_loss: 356209.1562 - val_mae: 459.0520\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1179792.1250 - mae: 843.7745 - val_loss: 369822.2812 - val_mae: 474.9624\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 1177290.0000 - mae: 844.6008 - val_loss: 357521.7188 - val_mae: 461.0834\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1158156.3750 - mae: 836.9835 - val_loss: 344179.1562 - val_mae: 450.4206\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 7s 156ms/step - loss: 1166715.3750 - mae: 839.0705 - val_loss: 361089.4062 - val_mae: 468.2180\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1174660.0000 - mae: 842.4315 - val_loss: 365519.9062 - val_mae: 474.7325\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 1168341.1250 - mae: 838.9977 - val_loss: 350047.2188 - val_mae: 459.0521\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1141914.2500 - mae: 829.7388 - val_loss: 349128.8438 - val_mae: 459.6146\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1164376.2500 - mae: 837.6963 - val_loss: 356833.5000 - val_mae: 467.8207\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 1150868.1250 - mae: 831.2004 - val_loss: 361280.5938 - val_mae: 474.7810\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 1171103.5000 - mae: 841.5695 - val_loss: 333953.6562 - val_mae: 448.5910\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 1136209.8750 - mae: 826.3779 - val_loss: 350445.1250 - val_mae: 465.9839\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1161180.6250 - mae: 835.7245 - val_loss: 356027.2812 - val_mae: 469.7360\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1153269.5000 - mae: 832.8520 - val_loss: 343068.9062 - val_mae: 459.3333\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 1149106.0000 - mae: 830.6719 - val_loss: 342101.2188 - val_mae: 459.3333\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 1147823.7500 - mae: 829.9859 - val_loss: 349715.9062 - val_mae: 467.8997\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 1146431.1250 - mae: 828.7177 - val_loss: 340110.9062 - val_mae: 459.3333\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1150594.2500 - mae: 829.7322 - val_loss: 347655.7812 - val_mae: 467.5611\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 1118455.3750 - mae: 815.5865 - val_loss: 346657.4062 - val_mae: 467.5323\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 1131197.2500 - mae: 821.3099 - val_loss: 345705.1250 - val_mae: 467.7842\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1128983.2500 - mae: 818.2646 - val_loss: 331131.0938 - val_mae: 453.2235\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 1134666.0000 - mae: 822.8673 - val_loss: 340517.3438 - val_mae: 465.9769\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1123476.5000 - mae: 819.7767 - val_loss: 329261.3750 - val_mae: 453.2812\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1133775.1250 - mae: 823.4951 - val_loss: 346801.9688 - val_mae: 473.6916\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1133082.1250 - mae: 822.5820 - val_loss: 335688.0312 - val_mae: 461.3646\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 1124163.5000 - mae: 818.2069 - val_loss: 336563.4688 - val_mae: 465.5804\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 1117904.0000 - mae: 815.4539 - val_loss: 342089.6562 - val_mae: 469.6134\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1116920.2500 - mae: 815.4965 - val_loss: 324661.1562 - val_mae: 453.4255\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 1125793.6250 - mae: 817.6100 - val_loss: 328662.0625 - val_mae: 459.3333\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 1116514.1250 - mae: 812.8260 - val_loss: 332558.6562 - val_mae: 464.9015\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1105072.7500 - mae: 808.4481 - val_loss: 326817.1562 - val_mae: 459.3333\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 10s 238ms/step - loss: 1103454.6250 - mae: 809.0211 - val_loss: 325905.6250 - val_mae: 459.3333\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1105610.7500 - mae: 809.2203 - val_loss: 336333.2188 - val_mae: 469.4406\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 1113023.8750 - mae: 813.3078 - val_loss: 327293.0938 - val_mae: 461.3646\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1113972.6250 - mae: 812.8400 - val_loss: 319931.0938 - val_mae: 457.3021\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1090497.1250 - mae: 801.7484 - val_loss: 312647.5312 - val_mae: 447.6996\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 1097444.3750 - mae: 803.8859 - val_loss: 321428.4688 - val_mae: 459.6146\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1104948.0000 - mae: 809.4869 - val_loss: 336330.7188 - val_mae: 474.9150\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 1099392.2500 - mae: 804.8868 - val_loss: 332105.7500 - val_mae: 472.5444\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1108390.3750 - mae: 810.4158 - val_loss: 313929.7500 - val_mae: 453.4907\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 1072332.6250 - mae: 794.1158 - val_loss: 317790.1250 - val_mae: 459.3334\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1094651.1250 - mae: 800.8275 - val_loss: 316833.0312 - val_mae: 459.0521\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 1062730.6250 - mae: 789.2008 - val_loss: 321872.2500 - val_mae: 463.0312\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 1075468.2500 - mae: 794.7212 - val_loss: 318272.7812 - val_mae: 461.0833\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 1080638.7500 - mae: 795.6996 - val_loss: 309704.3438 - val_mae: 453.6331\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1080504.2500 - mae: 797.3453 - val_loss: 308350.2812 - val_mae: 453.2975\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 1078465.6250 - mae: 794.2747 - val_loss: 312686.4688 - val_mae: 459.6146\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1055941.7500 - mae: 784.4611 - val_loss: 308615.1875 - val_mae: 457.3021\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1080036.1250 - mae: 797.0355 - val_loss: 310912.5938 - val_mae: 459.3333\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 1073945.2500 - mae: 792.8437 - val_loss: 299389.5938 - val_mae: 449.9949\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 1069254.7500 - mae: 791.9533 - val_loss: 313710.1562 - val_mae: 464.8620\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1058667.0000 - mae: 785.2458 - val_loss: 311599.6250 - val_mae: 461.6458\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 1065689.2500 - mae: 787.9959 - val_loss: 306307.4062 - val_mae: 456.1747\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 1074607.0000 - mae: 794.4376 - val_loss: 305407.7188 - val_mae: 455.9220\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1055183.5000 - mae: 782.8425 - val_loss: 316471.5312 - val_mae: 468.8095\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1058518.3750 - mae: 784.1735 - val_loss: 316984.8438 - val_mae: 472.4165\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 1060304.6250 - mae: 785.3304 - val_loss: 308583.9375 - val_mae: 464.6898\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1058379.3750 - mae: 785.6212 - val_loss: 310736.6875 - val_mae: 466.4112\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 1069153.6250 - mae: 790.4141 - val_loss: 317265.2812 - val_mae: 473.7127\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 1051979.7500 - mae: 781.2234 - val_loss: 304864.5312 - val_mae: 461.3646\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 1051272.5000 - mae: 783.3674 - val_loss: 303561.5312 - val_mae: 461.0000\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1044994.1875 - mae: 777.5594 - val_loss: 292907.5312 - val_mae: 452.0889\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 1029385.8125 - mae: 770.9956 - val_loss: 313631.6562 - val_mae: 472.9213\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1041069.9375 - mae: 775.6984 - val_loss: 302700.9062 - val_mae: 464.2083\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1037880.4375 - mae: 773.3937 - val_loss: 300777.2188 - val_mae: 461.0833\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 1048509.0000 - mae: 780.0198 - val_loss: 304136.5625 - val_mae: 466.1831\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 1030744.4375 - mae: 769.9427 - val_loss: 295279.5625 - val_mae: 456.5754\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1039904.1250 - mae: 774.8560 - val_loss: 292569.2188 - val_mae: 457.5834\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 1044023.0000 - mae: 777.2673 - val_loss: 298785.0312 - val_mae: 464.3469\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 1026243.4375 - mae: 769.5916 - val_loss: 304944.5000 - val_mae: 471.0529\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1018535.5000 - mae: 765.3560 - val_loss: 293283.0312 - val_mae: 459.6146\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 1035459.3125 - mae: 774.6077 - val_loss: 306411.5312 - val_mae: 473.2505\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1028576.6875 - mae: 769.1599 - val_loss: 294763.3750 - val_mae: 461.6458\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 1029971.8125 - mae: 769.6187 - val_loss: 297835.0625 - val_mae: 466.2345\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1017929.6875 - mae: 765.9653 - val_loss: 293142.5312 - val_mae: 461.3646\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 1015437.8125 - mae: 762.1982 - val_loss: 282523.5938 - val_mae: 452.4888\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1007498.3125 - mae: 760.0431 - val_loss: 278036.3125 - val_mae: 448.0139\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1015629.3125 - mae: 762.2015 - val_loss: 294669.3750 - val_mae: 465.8402\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 1000432.9375 - mae: 757.3032 - val_loss: 296970.2188 - val_mae: 468.1238\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1004981.2500 - mae: 758.0753 - val_loss: 293153.0938 - val_mae: 465.7834\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 11s 269ms/step - loss: 1011590.9375 - mae: 763.2501 - val_loss: 288613.7188 - val_mae: 461.0833\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1002665.0000 - mae: 755.6346 - val_loss: 278435.5625 - val_mae: 452.9403\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 998341.0625 - mae: 754.4791 - val_loss: 294532.7812 - val_mae: 470.0315\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 985197.9375 - mae: 746.0346 - val_loss: 296819.4062 - val_mae: 472.2873\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 1003157.7500 - mae: 756.0665 - val_loss: 279958.3438 - val_mae: 457.3021\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 986094.9375 - mae: 749.3707 - val_loss: 282199.5625 - val_mae: 459.3333\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 998207.0000 - mae: 752.5185 - val_loss: 290857.7188 - val_mae: 467.6148\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 996552.3750 - mae: 754.4717 - val_loss: 277279.4688 - val_mae: 455.1424\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1007621.6875 - mae: 760.4134 - val_loss: 290191.0000 - val_mae: 470.2502\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 967438.1875 - mae: 739.4896 - val_loss: 288787.0938 - val_mae: 467.8108\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 995193.6250 - mae: 751.8529 - val_loss: 288621.0938 - val_mae: 469.8574\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 994211.5000 - mae: 751.1092 - val_loss: 278032.1250 - val_mae: 459.3333\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 978405.3750 - mae: 745.2681 - val_loss: 290031.7500 - val_mae: 471.7741\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 980855.3125 - mae: 742.0984 - val_loss: 279920.1562 - val_mae: 463.0719\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 983215.5000 - mae: 746.2025 - val_loss: 279358.2500 - val_mae: 463.3253\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 985387.6875 - mae: 746.3380 - val_loss: 278668.2188 - val_mae: 463.2970\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 970881.4375 - mae: 737.9875 - val_loss: 274805.2500 - val_mae: 459.6146\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 976541.5000 - mae: 743.8700 - val_loss: 267882.6875 - val_mae: 453.3955\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 979625.1250 - mae: 744.4098 - val_loss: 276215.9688 - val_mae: 461.3646\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 975947.0000 - mae: 741.4260 - val_loss: 282008.0312 - val_mae: 469.0641\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 957747.5625 - mae: 733.0474 - val_loss: 278231.2188 - val_mae: 465.4673\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 972155.3750 - mae: 739.9384 - val_loss: 268110.1562 - val_mae: 455.2591\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 969226.7500 - mae: 739.0430 - val_loss: 283016.0000 - val_mae: 471.4888\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 958426.0625 - mae: 733.4889 - val_loss: 276082.1562 - val_mae: 465.1003\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 963727.9375 - mae: 734.6487 - val_loss: 284454.7812 - val_mae: 474.5179\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 953979.5000 - mae: 730.9016 - val_loss: 271688.7188 - val_mae: 461.3646\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 966421.9375 - mae: 737.1254 - val_loss: 255923.7969 - val_mae: 447.3237\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 954508.1875 - mae: 728.7384 - val_loss: 264599.2500 - val_mae: 455.7102\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 962457.7500 - mae: 734.2900 - val_loss: 275760.5312 - val_mae: 467.2720\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 964740.3750 - mae: 735.7294 - val_loss: 272305.6875 - val_mae: 465.2118\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 945951.0000 - mae: 725.1035 - val_loss: 271371.3438 - val_mae: 463.3958\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 948801.0000 - mae: 727.9846 - val_loss: 270900.9062 - val_mae: 464.8740\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 957109.5625 - mae: 731.3885 - val_loss: 267497.4062 - val_mae: 462.8146\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 945313.4375 - mae: 727.8069 - val_loss: 261226.2344 - val_mae: 457.3021\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 943478.1250 - mae: 726.1600 - val_loss: 271784.5938 - val_mae: 466.8203\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 7s 150ms/step - loss: 938158.0625 - mae: 724.9733 - val_loss: 271383.2188 - val_mae: 468.4384\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 948860.8125 - mae: 728.8835 - val_loss: 264763.4062 - val_mae: 459.7462\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 949164.1250 - mae: 727.7203 - val_loss: 261616.0000 - val_mae: 459.3333\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 941917.5625 - mae: 724.1032 - val_loss: 261181.7500 - val_mae: 459.6146\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 924884.0000 - mae: 719.6270 - val_loss: 254540.7031 - val_mae: 453.6545\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 924545.1875 - mae: 714.7223 - val_loss: 259759.6250 - val_mae: 459.0521\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 930429.8125 - mae: 720.0017 - val_loss: 267616.0312 - val_mae: 468.1002\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 933308.1250 - mae: 723.0792 - val_loss: 261498.4375 - val_mae: 461.3646\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 928935.5625 - mae: 719.0645 - val_loss: 260354.9375 - val_mae: 460.7188\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 922040.8750 - mae: 715.5503 - val_loss: 255095.1250 - val_mae: 457.5833\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 922374.6875 - mae: 715.1584 - val_loss: 262274.3438 - val_mae: 464.1975\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 930189.0625 - mae: 718.5475 - val_loss: 267407.0312 - val_mae: 470.1294\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 920888.5625 - mae: 714.4138 - val_loss: 258841.7031 - val_mae: 461.6458\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 916353.1250 - mae: 715.5753 - val_loss: 258140.2031 - val_mae: 462.6435\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 926336.8750 - mae: 719.5685 - val_loss: 252405.6875 - val_mae: 456.3326\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 917775.6875 - mae: 712.1553 - val_loss: 265007.5938 - val_mae: 469.9023\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 910370.9375 - mae: 712.9905 - val_loss: 261888.3906 - val_mae: 468.0951\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 911960.7500 - mae: 710.0118 - val_loss: 261396.8281 - val_mae: 466.8741\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 906428.3750 - mae: 706.7449 - val_loss: 255458.0781 - val_mae: 461.3646\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 914318.3750 - mae: 712.3115 - val_loss: 257316.1719 - val_mae: 464.2231\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 904944.5000 - mae: 705.3832 - val_loss: 254545.6719 - val_mae: 461.6458\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 897669.6875 - mae: 705.0269 - val_loss: 253878.2969 - val_mae: 461.3646\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 899207.5000 - mae: 705.2493 - val_loss: 245923.4375 - val_mae: 453.5047\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 882014.3750 - mae: 695.2101 - val_loss: 247927.1094 - val_mae: 456.5874\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 881623.9375 - mae: 697.0067 - val_loss: 254800.9375 - val_mae: 464.3643\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 899883.1250 - mae: 705.6376 - val_loss: 256546.2656 - val_mae: 467.0279\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 909223.0625 - mae: 710.1754 - val_loss: 243758.0469 - val_mae: 454.3581\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 895628.1250 - mae: 702.0178 - val_loss: 250884.6250 - val_mae: 461.3646\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 895541.7500 - mae: 702.4235 - val_loss: 252735.3750 - val_mae: 464.2522\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 887245.2500 - mae: 698.4010 - val_loss: 252502.7656 - val_mae: 463.3958\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 894762.0625 - mae: 700.8125 - val_loss: 239348.0625 - val_mae: 452.4390\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 885169.8750 - mae: 699.6782 - val_loss: 253832.0625 - val_mae: 466.2004\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 885210.6250 - mae: 696.2154 - val_loss: 250894.9531 - val_mae: 463.1146\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 889264.8125 - mae: 699.0561 - val_loss: 252969.6250 - val_mae: 466.4244\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 881873.4375 - mae: 696.1860 - val_loss: 251905.1719 - val_mae: 465.7503\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 877458.8750 - mae: 694.8366 - val_loss: 249246.8750 - val_mae: 464.0555\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 866082.3125 - mae: 687.9779 - val_loss: 244218.8594 - val_mae: 459.6146\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 873387.9375 - mae: 696.4464 - val_loss: 244178.8281 - val_mae: 459.0108\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 887152.4375 - mae: 698.7416 - val_loss: 243314.0469 - val_mae: 459.6146\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 6s 131ms/step - loss: 868465.5000 - mae: 689.2360 - val_loss: 240773.7656 - val_mae: 457.0359\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 867924.4375 - mae: 689.0783 - val_loss: 239728.1875 - val_mae: 457.3021\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 887411.5625 - mae: 700.9901 - val_loss: 241823.4531 - val_mae: 459.3333\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 870315.2500 - mae: 690.7767 - val_loss: 243406.8594 - val_mae: 461.8275\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 864954.3750 - mae: 687.2448 - val_loss: 246186.0469 - val_mae: 463.6771\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 862986.8750 - mae: 688.2914 - val_loss: 238718.2969 - val_mae: 457.1768\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 865824.3750 - mae: 688.4124 - val_loss: 242450.7656 - val_mae: 461.0833\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 861782.3125 - mae: 687.5206 - val_loss: 248590.3906 - val_mae: 468.1623\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 841051.5000 - mae: 677.0203 - val_loss: 243708.9844 - val_mae: 463.7199\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 852125.8125 - mae: 683.1171 - val_loss: 244994.1250 - val_mae: 465.7392\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 863058.5625 - mae: 691.5520 - val_loss: 244364.8594 - val_mae: 465.4023\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 852769.5000 - mae: 684.8301 - val_loss: 242418.4531 - val_mae: 463.6369\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 852293.4375 - mae: 682.2361 - val_loss: 242832.5469 - val_mae: 463.6771\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 857591.1250 - mae: 687.5831 - val_loss: 239766.3594 - val_mae: 461.3646\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 862093.0625 - mae: 689.0256 - val_loss: 241630.8906 - val_mae: 463.3125\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 838722.7500 - mae: 678.1953 - val_loss: 238804.6719 - val_mae: 461.0833\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 845331.8125 - mae: 682.4091 - val_loss: 239956.0625 - val_mae: 462.9342\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 7s 147ms/step - loss: 846191.3750 - mae: 683.0132 - val_loss: 244075.9531 - val_mae: 467.6058\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 833075.5625 - mae: 673.7711 - val_loss: 240832.0469 - val_mae: 464.9558\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 852061.0625 - mae: 684.5314 - val_loss: 238920.5781 - val_mae: 463.1320\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 842337.4375 - mae: 679.1387 - val_loss: 236080.9219 - val_mae: 461.0725\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 835261.5000 - mae: 675.2689 - val_loss: 232832.9844 - val_mae: 457.6212\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 853569.1250 - mae: 686.9412 - val_loss: 236323.1719 - val_mae: 461.3646\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 837737.8750 - mae: 677.1818 - val_loss: 238560.2031 - val_mae: 463.6771\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 847598.4375 - mae: 684.5306 - val_loss: 238510.4219 - val_mae: 464.9014\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 836612.6250 - mae: 677.3384 - val_loss: 240694.6719 - val_mae: 467.1570\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 836288.1250 - mae: 675.7384 - val_loss: 236384.3125 - val_mae: 463.2173\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 829262.8750 - mae: 672.3754 - val_loss: 235831.6875 - val_mae: 462.9087\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 836434.3750 - mae: 676.4243 - val_loss: 237102.2031 - val_mae: 464.9588\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 820192.3750 - mae: 670.5003 - val_loss: 226391.3281 - val_mae: 454.0439\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 841145.6250 - mae: 682.1191 - val_loss: 236066.9219 - val_mae: 463.6771\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 833844.8125 - mae: 678.2402 - val_loss: 230796.0000 - val_mae: 459.3333\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 808794.4375 - mae: 664.9312 - val_loss: 235572.2500 - val_mae: 464.7356\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 821977.4375 - mae: 672.0806 - val_loss: 234822.0156 - val_mae: 464.1181\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 838690.5000 - mae: 681.9686 - val_loss: 231060.2500 - val_mae: 460.0156\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 819275.6250 - mae: 671.4663 - val_loss: 234283.1094 - val_mae: 464.2890\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 826465.4375 - mae: 673.3127 - val_loss: 232832.1875 - val_mae: 462.9393\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 821568.5625 - mae: 670.3007 - val_loss: 235903.3906 - val_mae: 466.2086\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 818067.6875 - mae: 668.9485 - val_loss: 235559.9844 - val_mae: 466.3492\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 812742.6250 - mae: 666.7709 - val_loss: 225954.1406 - val_mae: 457.3021\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 810640.6875 - mae: 665.4326 - val_loss: 233647.1094 - val_mae: 464.5784\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 805814.3750 - mae: 662.6862 - val_loss: 227708.6250 - val_mae: 459.3333\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 806914.3125 - mae: 666.9460 - val_loss: 227407.8281 - val_mae: 459.3333\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 814435.8125 - mae: 669.2938 - val_loss: 230753.1250 - val_mae: 463.0256\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 801804.9375 - mae: 665.1230 - val_loss: 230037.3125 - val_mae: 462.4346\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 800507.3750 - mae: 659.2774 - val_loss: 224616.0781 - val_mae: 456.9657\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 10s 269ms/step - loss: 806043.1250 - mae: 663.3036 - val_loss: 226089.8594 - val_mae: 459.0521\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 802521.5000 - mae: 664.2176 - val_loss: 226013.5781 - val_mae: 459.3333\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 802019.8750 - mae: 663.1522 - val_loss: 226221.0781 - val_mae: 459.1642\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 796957.8750 - mae: 659.8745 - val_loss: 224710.4844 - val_mae: 458.4004\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 795859.0000 - mae: 662.6489 - val_loss: 227300.3125 - val_mae: 461.0833\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 799859.0625 - mae: 664.8029 - val_loss: 222505.4375 - val_mae: 457.0208\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 796242.5000 - mae: 659.7715 - val_loss: 229019.4375 - val_mae: 463.9069\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 797854.3750 - mae: 664.0337 - val_loss: 227835.6719 - val_mae: 462.7497\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 787244.3125 - mae: 654.8270 - val_loss: 227361.9531 - val_mae: 462.4414\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 6s 124ms/step - loss: 786784.1250 - mae: 658.2803 - val_loss: 224635.0781 - val_mae: 460.1011\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 791726.7500 - mae: 660.8544 - val_loss: 223753.0781 - val_mae: 459.3333\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 11s 269ms/step - loss: 794599.8125 - mae: 664.1317 - val_loss: 225954.6250 - val_mae: 461.6458\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 787486.5625 - mae: 659.9730 - val_loss: 223066.4844 - val_mae: 459.0521\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 794232.0000 - mae: 665.2885 - val_loss: 226374.7031 - val_mae: 462.6770\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 7s 169ms/step - loss: 781170.0625 - mae: 655.1375 - val_loss: 227272.8750 - val_mae: 463.3959\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 785556.1250 - mae: 658.9584 - val_loss: 219674.0000 - val_mae: 456.4197\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 784282.9375 - mae: 657.2918 - val_loss: 222385.9531 - val_mae: 459.3334\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 785085.2500 - mae: 659.4238 - val_loss: 227460.1875 - val_mae: 464.5034\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 774915.2500 - mae: 655.0601 - val_loss: 226797.8594 - val_mae: 463.9143\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 10s 269ms/step - loss: 785869.3750 - mae: 661.3134 - val_loss: 221539.2500 - val_mae: 459.0521\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 770176.3750 - mae: 653.1257 - val_loss: 224564.0156 - val_mae: 462.3906\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 11s 269ms/step - loss: 782433.3125 - mae: 659.1959 - val_loss: 224120.0469 - val_mae: 462.0816\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 787764.9375 - mae: 664.9133 - val_loss: 221156.4219 - val_mae: 459.3333\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 764859.3125 - mae: 649.6064 - val_loss: 221060.8125 - val_mae: 459.4319\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 769146.1250 - mae: 651.9286 - val_loss: 218097.2344 - val_mae: 456.6675\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 758791.3125 - mae: 648.1025 - val_loss: 222208.0781 - val_mae: 460.7188\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 767254.1250 - mae: 653.9651 - val_loss: 219943.3594 - val_mae: 458.7536\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 766103.8125 - mae: 652.2772 - val_loss: 220230.9219 - val_mae: 459.3333\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 749307.9375 - mae: 646.4615 - val_loss: 220284.3750 - val_mae: 459.6146\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 764065.6250 - mae: 651.8484 - val_loss: 224432.4375 - val_mae: 463.7455\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 763026.5000 - mae: 653.0128 - val_loss: 223836.0625 - val_mae: 463.2069\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 753991.4375 - mae: 651.8153 - val_loss: 222269.0469 - val_mae: 462.0880\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 757332.5000 - mae: 648.7121 - val_loss: 223988.8125 - val_mae: 463.8116\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 756232.5000 - mae: 649.7126 - val_loss: 221682.9844 - val_mae: 461.7535\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 760034.4375 - mae: 651.3055 - val_loss: 221790.2656 - val_mae: 462.0862\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 765165.1250 - mae: 654.7751 - val_loss: 223958.0469 - val_mae: 464.3919\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 757972.6875 - mae: 651.8909 - val_loss: 218563.1719 - val_mae: 459.0521\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 757804.3125 - mae: 652.6060 - val_loss: 218643.4844 - val_mae: 459.3333\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 746316.1875 - mae: 646.3597 - val_loss: 218710.3281 - val_mae: 459.5835\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 748946.1250 - mae: 650.9667 - val_loss: 222365.0000 - val_mae: 463.1729\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 756049.0000 - mae: 653.3149 - val_loss: 222252.7656 - val_mae: 463.2226\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 747973.2500 - mae: 648.4982 - val_loss: 220091.1250 - val_mae: 461.2513\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 754669.5000 - mae: 652.8105 - val_loss: 222127.4844 - val_mae: 463.3958\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 758222.0625 - mae: 656.6801 - val_loss: 219875.5469 - val_mae: 461.3087\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 744388.9375 - mae: 648.1577 - val_loss: 215686.5000 - val_mae: 457.3021\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 747924.6250 - mae: 649.5449 - val_loss: 217628.8281 - val_mae: 459.3333\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 730359.4375 - mae: 641.1959 - val_loss: 221639.8125 - val_mae: 463.4267\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 746831.9375 - mae: 652.7403 - val_loss: 219224.0781 - val_mae: 461.0911\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 728254.1250 - mae: 641.9530 - val_loss: 217236.7969 - val_mae: 459.2500\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 740679.6875 - mae: 650.1125 - val_loss: 219242.4531 - val_mae: 461.3646\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 735813.6875 - mae: 645.8707 - val_loss: 218533.4844 - val_mae: 460.6420\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 738927.6875 - mae: 648.1227 - val_loss: 217101.0625 - val_mae: 459.4373\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 729889.7500 - mae: 647.3998 - val_loss: 217273.8125 - val_mae: 459.7449\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 728444.2500 - mae: 643.6931 - val_loss: 218118.8750 - val_mae: 460.4871\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 737785.1250 - mae: 648.7887 - val_loss: 216214.8281 - val_mae: 458.6875\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 720658.5000 - mae: 636.8187 - val_loss: 216917.7500 - val_mae: 459.6146\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 737220.3750 - mae: 650.2913 - val_loss: 219033.6250 - val_mae: 461.8846\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 732659.9375 - mae: 647.8149 - val_loss: 216985.0781 - val_mae: 459.8810\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 741406.2500 - mae: 653.4800 - val_loss: 216214.3125 - val_mae: 459.0387\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 728404.2500 - mae: 646.5267 - val_loss: 217357.8125 - val_mae: 460.1589\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 705529.5625 - mae: 636.8760 - val_loss: 218015.8750 - val_mae: 461.0156\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 721777.6250 - mae: 641.5771 - val_loss: 218480.6094 - val_mae: 461.6458\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 731369.6250 - mae: 648.6583 - val_loss: 219565.5625 - val_mae: 462.7119\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 7s 154ms/step - loss: 730537.8750 - mae: 650.7321 - val_loss: 218710.3906 - val_mae: 462.0763\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 714014.8125 - mae: 641.2786 - val_loss: 218057.6250 - val_mae: 461.3645\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 715660.8125 - mae: 642.4011 - val_loss: 216307.9844 - val_mae: 459.6146\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 714817.0625 - mae: 639.8259 - val_loss: 211264.3125 - val_mae: 454.5431\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 714696.3750 - mae: 640.7626 - val_loss: 219169.6250 - val_mae: 462.5770\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 713413.1250 - mae: 642.6529 - val_loss: 217666.6875 - val_mae: 461.0802\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 703748.3125 - mae: 635.4547 - val_loss: 217611.0469 - val_mae: 461.0548\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 10s 273ms/step - loss: 707307.5625 - mae: 638.1896 - val_loss: 218056.3750 - val_mae: 461.6458\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 709237.8750 - mae: 640.7177 - val_loss: 216631.5625 - val_mae: 460.2593\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 712791.5000 - mae: 642.5096 - val_loss: 216947.0156 - val_mae: 460.4117\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 685732.1875 - mae: 628.1525 - val_loss: 214161.5469 - val_mae: 457.5833\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 708848.3750 - mae: 642.7343 - val_loss: 214739.2969 - val_mae: 458.3084\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 711505.4375 - mae: 641.9913 - val_loss: 216719.2500 - val_mae: 460.1425\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 714936.8750 - mae: 646.3689 - val_loss: 216147.8281 - val_mae: 459.5257\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 701384.0625 - mae: 639.2876 - val_loss: 217719.9219 - val_mae: 461.2208\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 707075.6875 - mae: 643.6229 - val_loss: 216972.9219 - val_mae: 460.5315\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 692399.5000 - mae: 633.0145 - val_loss: 218564.8281 - val_mae: 462.2558\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 699672.3750 - mae: 638.0388 - val_loss: 216032.0781 - val_mae: 459.6146\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 687705.8125 - mae: 631.6865 - val_loss: 218769.5156 - val_mae: 462.4823\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 684813.8125 - mae: 632.6396 - val_loss: 219510.5625 - val_mae: 463.3958\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 7s 168ms/step - loss: 691397.9375 - mae: 634.3038 - val_loss: 218702.8594 - val_mae: 462.6107\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 690080.8750 - mae: 637.5162 - val_loss: 215278.4844 - val_mae: 458.6226\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 697273.3125 - mae: 638.9632 - val_loss: 219530.6875 - val_mae: 463.3958\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 6s 126ms/step - loss: 687742.1875 - mae: 634.2286 - val_loss: 215480.5156 - val_mae: 458.7113\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 692733.8125 - mae: 638.9291 - val_loss: 217556.2969 - val_mae: 460.9700\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 691519.3750 - mae: 637.8770 - val_loss: 217532.5000 - val_mae: 460.9177\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 11s 271ms/step - loss: 683602.2500 - mae: 633.3937 - val_loss: 217131.5000 - val_mae: 460.7390\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 690299.3750 - mae: 637.6202 - val_loss: 216875.6406 - val_mae: 460.2131\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 684773.9375 - mae: 634.8133 - val_loss: 217576.3281 - val_mae: 461.0000\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 694880.2500 - mae: 640.9634 - val_loss: 216636.7969 - val_mae: 459.8785\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 689139.5625 - mae: 638.9438 - val_loss: 216924.4219 - val_mae: 460.1334\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 687497.5625 - mae: 638.2283 - val_loss: 217457.1250 - val_mae: 460.8717\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 675085.8125 - mae: 635.8363 - val_loss: 218764.8906 - val_mae: 462.1130\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 677304.0625 - mae: 635.0843 - val_loss: 217272.1406 - val_mae: 460.3361\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 674159.4375 - mae: 630.2904 - val_loss: 216605.7656 - val_mae: 459.6146\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 660624.1875 - mae: 626.3930 - val_loss: 216796.9844 - val_mae: 459.7202\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 675548.0000 - mae: 635.5903 - val_loss: 216627.3750 - val_mae: 459.2336\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 675659.3750 - mae: 632.3729 - val_loss: 215727.3594 - val_mae: 458.2524\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 667485.0625 - mae: 631.5067 - val_loss: 216874.3594 - val_mae: 459.6146\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 676803.0000 - mae: 634.0619 - val_loss: 216673.7500 - val_mae: 459.3333\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 10s 238ms/step - loss: 677626.5000 - mae: 635.3038 - val_loss: 218505.9219 - val_mae: 461.3646\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 675094.3750 - mae: 633.5676 - val_loss: 217861.7500 - val_mae: 460.3539\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 651513.9375 - mae: 625.7111 - val_loss: 215666.7656 - val_mae: 457.7851\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 673509.6250 - mae: 632.8632 - val_loss: 216936.3125 - val_mae: 459.1447\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 661885.9375 - mae: 627.2278 - val_loss: 215953.2969 - val_mae: 457.8849\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 668659.0000 - mae: 632.3974 - val_loss: 218917.5625 - val_mae: 461.3646\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 669976.7500 - mae: 632.7215 - val_loss: 216147.6875 - val_mae: 457.8793\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 668413.3125 - mae: 632.6141 - val_loss: 217740.1719 - val_mae: 459.6864\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 671613.6250 - mae: 635.8766 - val_loss: 218965.9844 - val_mae: 461.1283\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 663246.0000 - mae: 631.5685 - val_loss: 219583.8906 - val_mae: 461.6458\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 666578.8750 - mae: 633.1204 - val_loss: 218257.6719 - val_mae: 459.8799\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 666822.6250 - mae: 632.6534 - val_loss: 221212.5781 - val_mae: 463.3958\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 659449.6250 - mae: 628.2531 - val_loss: 216689.4375 - val_mae: 457.7440\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 670095.9375 - mae: 637.6274 - val_loss: 216564.8281 - val_mae: 457.4968\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 651334.6875 - mae: 630.0507 - val_loss: 221366.2656 - val_mae: 463.2272\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 658023.1875 - mae: 629.3042 - val_loss: 216908.1875 - val_mae: 457.5833\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 656402.0000 - mae: 629.5762 - val_loss: 216702.1719 - val_mae: 457.2552\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 662164.3125 - mae: 632.6739 - val_loss: 220224.9219 - val_mae: 461.3646\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 638972.3750 - mae: 620.0210 - val_loss: 220347.6250 - val_mae: 461.3646\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 654957.6875 - mae: 630.4275 - val_loss: 218864.9531 - val_mae: 459.3722\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 661250.0000 - mae: 634.9578 - val_loss: 217339.7969 - val_mae: 457.3286\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 661168.8750 - mae: 635.3869 - val_loss: 217161.9219 - val_mae: 457.0071\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 651304.5000 - mae: 630.4631 - val_loss: 220854.6406 - val_mae: 461.3253\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 662649.6250 - mae: 636.7633 - val_loss: 221047.2969 - val_mae: 461.3646\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 656785.1875 - mae: 633.8605 - val_loss: 218795.8125 - val_mae: 458.5832\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 652045.0000 - mae: 631.1349 - val_loss: 217393.3594 - val_mae: 456.6199\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 7s 149ms/step - loss: 640125.8125 - mae: 625.4714 - val_loss: 221946.1875 - val_mae: 461.7919\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 648232.2500 - mae: 629.2527 - val_loss: 219274.8125 - val_mae: 458.6006\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 645259.4375 - mae: 628.1980 - val_loss: 218094.4219 - val_mae: 456.8254\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 643777.7500 - mae: 627.5542 - val_loss: 219921.2969 - val_mae: 458.8891\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 639635.4375 - mae: 624.3615 - val_loss: 221912.9219 - val_mae: 461.0000\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 642219.4375 - mae: 626.7266 - val_loss: 222304.5625 - val_mae: 461.3646\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 644696.4375 - mae: 628.6847 - val_loss: 224372.1094 - val_mae: 463.6963\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 640725.8750 - mae: 625.6079 - val_loss: 220745.8125 - val_mae: 459.0066\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 6s 131ms/step - loss: 632936.1875 - mae: 622.9931 - val_loss: 218312.2500 - val_mae: 455.9644\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 636823.3125 - mae: 625.5097 - val_loss: 222118.3906 - val_mae: 459.9923\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 635914.4375 - mae: 625.6434 - val_loss: 221524.9531 - val_mae: 459.2104\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 634786.6875 - mae: 625.0471 - val_loss: 226935.0156 - val_mae: 465.8559\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 646840.8750 - mae: 632.8046 - val_loss: 218631.7500 - val_mae: 455.5466\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 636945.8750 - mae: 627.6534 - val_loss: 220934.5156 - val_mae: 458.0910\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 636404.8750 - mae: 627.2116 - val_loss: 222406.8906 - val_mae: 459.3333\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 633889.1250 - mae: 627.4846 - val_loss: 220224.6250 - val_mae: 456.4887\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 633822.8750 - mae: 625.5474 - val_loss: 221473.5781 - val_mae: 457.8531\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 626096.5000 - mae: 622.1824 - val_loss: 219114.3125 - val_mae: 454.9895\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 634015.0625 - mae: 625.9728 - val_loss: 222005.7500 - val_mae: 458.1618\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 632897.2500 - mae: 626.2522 - val_loss: 225275.1719 - val_mae: 461.6458\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 625439.6250 - mae: 623.1190 - val_loss: 223661.3281 - val_mae: 459.3333\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 632520.3750 - mae: 629.1553 - val_loss: 228610.1719 - val_mae: 465.0509\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 628439.9375 - mae: 623.8848 - val_loss: 224287.5000 - val_mae: 460.0896\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 624975.9375 - mae: 623.6877 - val_loss: 222531.4531 - val_mae: 457.0208\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 624369.2500 - mae: 623.4965 - val_loss: 223798.4531 - val_mae: 458.5701\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 620841.8125 - mae: 621.4529 - val_loss: 221091.8125 - val_mae: 455.1625\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 634830.6875 - mae: 630.8809 - val_loss: 226308.9375 - val_mae: 461.0000\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 622989.6875 - mae: 621.8533 - val_loss: 225380.3750 - val_mae: 459.8797\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 620275.2500 - mae: 621.9933 - val_loss: 222842.0469 - val_mae: 456.1560\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 633324.3750 - mae: 631.2866 - val_loss: 229334.2969 - val_mae: 463.4276\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 625152.6875 - mae: 626.5154 - val_loss: 223839.5000 - val_mae: 457.2196\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 621699.1875 - mae: 622.9344 - val_loss: 224994.2500 - val_mae: 458.1091\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 613261.5000 - mae: 621.9288 - val_loss: 229423.4375 - val_mae: 463.3958\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 617503.3125 - mae: 620.6242 - val_loss: 227197.6094 - val_mae: 460.3732\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 617986.5000 - mae: 621.9153 - val_loss: 224950.8125 - val_mae: 457.3000\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 617153.7500 - mae: 621.1856 - val_loss: 224529.4531 - val_mae: 456.6849\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 621346.5625 - mae: 624.9657 - val_loss: 227160.7344 - val_mae: 458.3701\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 615323.7500 - mae: 620.0221 - val_loss: 228896.5156 - val_mae: 461.0833\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 613681.1875 - mae: 621.0162 - val_loss: 229151.5781 - val_mae: 460.4492\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 619445.5625 - mae: 624.1168 - val_loss: 227159.5469 - val_mae: 458.1913\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 614494.4375 - mae: 622.1647 - val_loss: 224495.0781 - val_mae: 454.6853\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 7s 160ms/step - loss: 615535.6875 - mae: 622.9075 - val_loss: 225817.1875 - val_mae: 456.3846\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 611365.1875 - mae: 621.1667 - val_loss: 229004.5469 - val_mae: 459.8648\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 616930.5000 - mae: 625.3971 - val_loss: 227846.1094 - val_mae: 457.8087\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 612515.0000 - mae: 620.8391 - val_loss: 232390.8906 - val_mae: 462.6339\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 603746.7500 - mae: 620.1777 - val_loss: 229989.5156 - val_mae: 459.3333\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 10s 272ms/step - loss: 608310.5625 - mae: 620.5931 - val_loss: 229930.7031 - val_mae: 459.0521\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 614226.0625 - mae: 624.3333 - val_loss: 235016.3750 - val_mae: 465.0206\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 600073.3750 - mae: 614.9612 - val_loss: 230857.0781 - val_mae: 459.9968\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 607141.3125 - mae: 620.3460 - val_loss: 231119.2500 - val_mae: 459.9721\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 604258.1875 - mae: 617.5762 - val_loss: 231063.5469 - val_mae: 459.6655\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 613952.7500 - mae: 625.1730 - val_loss: 233523.6094 - val_mae: 461.0569\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 610016.5000 - mae: 623.8178 - val_loss: 228444.5469 - val_mae: 455.8346\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 603020.5000 - mae: 619.9716 - val_loss: 229514.7031 - val_mae: 455.8084\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 597397.8125 - mae: 616.7668 - val_loss: 238595.7344 - val_mae: 467.2269\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 596238.9375 - mae: 616.9072 - val_loss: 234329.6875 - val_mae: 461.3646\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 11s 279ms/step - loss: 605163.0000 - mae: 621.3467 - val_loss: 229739.3281 - val_mae: 455.1700\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 594553.6875 - mae: 616.5734 - val_loss: 233176.9531 - val_mae: 459.4070\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 576575.0625 - mae: 606.9683 - val_loss: 235254.9375 - val_mae: 461.3646\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 598940.5000 - mae: 617.7952 - val_loss: 232910.7031 - val_mae: 457.9751\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 603353.5000 - mae: 621.4467 - val_loss: 234909.3125 - val_mae: 459.6146\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 599288.1250 - mae: 619.8416 - val_loss: 229845.0781 - val_mae: 453.6393\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 596526.5000 - mae: 618.3141 - val_loss: 236200.6719 - val_mae: 461.0833\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 582430.1875 - mae: 612.9745 - val_loss: 239917.2969 - val_mae: 465.1351\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 593187.3750 - mae: 617.7480 - val_loss: 234556.1094 - val_mae: 457.3021\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 591836.1250 - mae: 616.8233 - val_loss: 234746.7344 - val_mae: 457.8271\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 596488.3125 - mae: 619.4479 - val_loss: 237835.6875 - val_mae: 461.3646\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 593325.3125 - mae: 618.2922 - val_loss: 233192.0000 - val_mae: 455.6600\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 11s 271ms/step - loss: 593247.2500 - mae: 619.8978 - val_loss: 240725.0156 - val_mae: 463.5068\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 594589.8750 - mae: 619.2100 - val_loss: 235678.7969 - val_mae: 457.4477\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 590904.2500 - mae: 618.7181 - val_loss: 235988.7500 - val_mae: 457.4243\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 593204.3125 - mae: 619.6320 - val_loss: 236324.9844 - val_mae: 457.3991\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 588291.8750 - mae: 617.6379 - val_loss: 236655.0625 - val_mae: 457.3745\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 595294.4375 - mae: 621.9050 - val_loss: 240575.7656 - val_mae: 461.6458\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 584988.0000 - mae: 616.9949 - val_loss: 244205.9375 - val_mae: 465.6841\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 11s 268ms/step - loss: 596696.0625 - mae: 624.6318 - val_loss: 238779.0469 - val_mae: 457.5833\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 583768.1875 - mae: 615.3885 - val_loss: 243362.1719 - val_mae: 463.4199\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 591488.1875 - mae: 619.3975 - val_loss: 239732.1250 - val_mae: 459.2025\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 586741.0000 - mae: 618.2296 - val_loss: 240445.8750 - val_mae: 459.0520\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 583155.0000 - mae: 616.2289 - val_loss: 241132.3125 - val_mae: 459.3333\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 579725.3125 - mae: 616.5932 - val_loss: 236149.1875 - val_mae: 454.1842\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 581891.3125 - mae: 616.8624 - val_loss: 237782.4844 - val_mae: 454.5631\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 577307.5625 - mae: 612.7520 - val_loss: 239186.0156 - val_mae: 455.3828\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 588974.9375 - mae: 620.8638 - val_loss: 237823.3594 - val_mae: 454.6024\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 576127.2500 - mae: 613.5469 - val_loss: 237305.1406 - val_mae: 452.8030\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 581306.3125 - mae: 617.2262 - val_loss: 238143.2500 - val_mae: 454.2239\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 587110.0000 - mae: 621.8691 - val_loss: 241147.9844 - val_mae: 456.7542\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 585186.1875 - mae: 620.9534 - val_loss: 238345.2500 - val_mae: 452.6566\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 570653.2500 - mae: 616.1066 - val_loss: 248146.8281 - val_mae: 463.7109\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 579455.8750 - mae: 617.4864 - val_loss: 244963.1094 - val_mae: 459.3333\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 578356.3125 - mae: 617.8855 - val_loss: 245732.1719 - val_mae: 459.6146\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 573203.4375 - mae: 613.7561 - val_loss: 241780.5781 - val_mae: 454.6049\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 577450.8750 - mae: 618.1749 - val_loss: 246514.7969 - val_mae: 459.6146\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 575152.7500 - mae: 615.9890 - val_loss: 246555.2031 - val_mae: 459.3333\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 579019.5000 - mae: 619.3471 - val_loss: 244121.6094 - val_mae: 456.5621\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 574732.6250 - mae: 617.5663 - val_loss: 248542.0000 - val_mae: 461.3646\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 579150.8750 - mae: 620.9471 - val_loss: 241177.7031 - val_mae: 451.9461\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 571797.3125 - mae: 616.2268 - val_loss: 242318.2500 - val_mae: 453.6503\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 567031.5625 - mae: 612.4815 - val_loss: 249731.1406 - val_mae: 461.3646\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 10s 269ms/step - loss: 570523.6875 - mae: 616.5842 - val_loss: 247888.2656 - val_mae: 459.0382\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 565073.4375 - mae: 612.7752 - val_loss: 252415.5625 - val_mae: 462.2447\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 573672.0625 - mae: 618.5453 - val_loss: 245667.3594 - val_mae: 454.3669\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 573677.2500 - mae: 618.6948 - val_loss: 247204.0000 - val_mae: 456.3738\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 572146.5625 - mae: 618.3587 - val_loss: 246478.6875 - val_mae: 454.3191\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 568371.0000 - mae: 616.2128 - val_loss: 251488.8906 - val_mae: 459.6146\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 565332.0625 - mae: 615.7155 - val_loss: 251515.1250 - val_mae: 459.3333\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 567828.8125 - mae: 616.6077 - val_loss: 251959.6250 - val_mae: 459.3333\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 560042.2500 - mae: 613.1572 - val_loss: 253488.7969 - val_mae: 461.3646\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 572557.6875 - mae: 621.2652 - val_loss: 248148.8281 - val_mae: 453.9206\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 568559.5625 - mae: 617.3376 - val_loss: 256130.5156 - val_mae: 462.1768\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 11s 283ms/step - loss: 573124.4375 - mae: 623.3353 - val_loss: 250437.1719 - val_mae: 456.1861\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 569174.9375 - mae: 619.4174 - val_loss: 247563.5156 - val_mae: 452.9937\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 564333.3125 - mae: 615.1841 - val_loss: 258572.3125 - val_mae: 464.2771\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 561472.4375 - mae: 615.0533 - val_loss: 254649.4531 - val_mae: 459.0521\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 562966.6250 - mae: 615.6771 - val_loss: 254387.0625 - val_mae: 457.3021\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 556146.4375 - mae: 611.2902 - val_loss: 256961.9531 - val_mae: 461.3646\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 552666.8125 - mae: 609.8382 - val_loss: 253964.1406 - val_mae: 458.0802\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 11s 264ms/step - loss: 566406.8750 - mae: 620.2057 - val_loss: 254391.2031 - val_mae: 458.0567\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 561026.0625 - mae: 618.1288 - val_loss: 254154.2344 - val_mae: 456.2825\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 560469.5625 - mae: 615.7056 - val_loss: 255236.0781 - val_mae: 458.0106\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 7s 159ms/step - loss: 566069.1875 - mae: 620.3646 - val_loss: 266765.2812 - val_mae: 469.8657\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 562965.7500 - mae: 619.0439 - val_loss: 254694.6719 - val_mae: 455.6510\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 11s 280ms/step - loss: 558397.6875 - mae: 615.8685 - val_loss: 254808.6406 - val_mae: 454.1611\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 554294.0000 - mae: 614.6428 - val_loss: 260086.3125 - val_mae: 461.0833\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 552829.3750 - mae: 611.0327 - val_loss: 263479.1250 - val_mae: 462.8001\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 555505.4375 - mae: 614.1191 - val_loss: 256080.2969 - val_mae: 454.0952\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 551479.7500 - mae: 613.7208 - val_loss: 261181.5781 - val_mae: 459.6146\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 557643.1250 - mae: 616.0180 - val_loss: 257994.1719 - val_mae: 456.0805\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 554617.2500 - mae: 615.2950 - val_loss: 263126.6250 - val_mae: 461.6458\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 558166.1250 - mae: 618.0531 - val_loss: 258514.8281 - val_mae: 455.7532\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 552092.5625 - mae: 614.8447 - val_loss: 255606.0469 - val_mae: 452.4105\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 546630.6250 - mae: 611.7923 - val_loss: 268882.9062 - val_mae: 467.0209\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 555658.3125 - mae: 618.1534 - val_loss: 257256.7969 - val_mae: 452.8788\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 555621.8125 - mae: 618.2636 - val_loss: 266861.8750 - val_mae: 460.9706\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 547212.8750 - mae: 612.1601 - val_loss: 262474.4375 - val_mae: 458.2359\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 543146.7500 - mae: 612.8175 - val_loss: 257337.0469 - val_mae: 451.9069\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 546828.3750 - mae: 615.8237 - val_loss: 266480.4688 - val_mae: 461.3646\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 551457.2500 - mae: 615.1239 - val_loss: 266346.2188 - val_mae: 459.6146\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 548704.7500 - mae: 614.1006 - val_loss: 263314.2188 - val_mae: 456.1154\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 546522.3750 - mae: 615.6353 - val_loss: 261709.3281 - val_mae: 453.2157\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 554945.6875 - mae: 619.3528 - val_loss: 267439.5312 - val_mae: 459.3333\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 544171.7500 - mae: 613.1968 - val_loss: 272467.7500 - val_mae: 464.9316\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 551503.2500 - mae: 619.6410 - val_loss: 269378.0938 - val_mae: 461.3646\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 549884.4375 - mae: 617.7449 - val_loss: 273895.5000 - val_mae: 465.2585\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 547676.5000 - mae: 618.5132 - val_loss: 265749.4688 - val_mae: 455.6993\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 537973.2500 - mae: 612.4767 - val_loss: 270312.8125 - val_mae: 459.6146\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 550707.8125 - mae: 617.9100 - val_loss: 270045.9375 - val_mae: 459.0521\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 543323.1875 - mae: 615.2986 - val_loss: 275021.6250 - val_mae: 463.3157\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 544197.3750 - mae: 617.3832 - val_loss: 263124.0000 - val_mae: 451.3253\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 548994.2500 - mae: 619.4561 - val_loss: 268265.5938 - val_mae: 457.0572\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 543014.7500 - mae: 614.2212 - val_loss: 272355.3438 - val_mae: 459.3333\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 551313.0000 - mae: 620.4012 - val_loss: 267338.0312 - val_mae: 452.9535\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 548585.3125 - mae: 618.3253 - val_loss: 268240.2500 - val_mae: 453.2117\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 545259.8750 - mae: 616.9212 - val_loss: 265730.9688 - val_mae: 451.3940\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 545625.0625 - mae: 617.6431 - val_loss: 270075.3750 - val_mae: 455.2008\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 540888.6875 - mae: 613.7921 - val_loss: 269653.9062 - val_mae: 453.1484\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 544459.4375 - mae: 616.0699 - val_loss: 274497.2812 - val_mae: 457.3021\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 532328.0000 - mae: 610.2853 - val_loss: 267143.2812 - val_mae: 449.4657\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 539503.0000 - mae: 613.9660 - val_loss: 268466.1562 - val_mae: 451.4562\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 539076.5625 - mae: 615.0897 - val_loss: 268544.6562 - val_mae: 451.1311\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 543411.8750 - mae: 617.1184 - val_loss: 272903.2188 - val_mae: 454.7062\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 542312.8750 - mae: 617.2754 - val_loss: 269045.9688 - val_mae: 450.7672\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 539650.2500 - mae: 616.4600 - val_loss: 278841.9062 - val_mae: 459.6146\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 544160.7500 - mae: 620.2509 - val_loss: 281061.2500 - val_mae: 463.6771\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 539114.5625 - mae: 616.7021 - val_loss: 270907.9375 - val_mae: 450.9157\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 538647.0625 - mae: 616.3712 - val_loss: 284041.2812 - val_mae: 463.0586\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 541651.7500 - mae: 619.2985 - val_loss: 285079.7500 - val_mae: 463.7259\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 527819.3125 - mae: 611.0966 - val_loss: 277313.8125 - val_mae: 455.4817\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 537316.5000 - mae: 617.0502 - val_loss: 281976.4062 - val_mae: 459.6146\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 543046.1875 - mae: 621.4150 - val_loss: 277904.6250 - val_mae: 456.6284\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 538478.6875 - mae: 618.9418 - val_loss: 272561.7812 - val_mae: 448.3465\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 536710.8750 - mae: 617.9919 - val_loss: 283536.6875 - val_mae: 461.0833\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 537553.7500 - mae: 618.0683 - val_loss: 279383.2500 - val_mae: 455.0965\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 536592.3750 - mae: 617.6622 - val_loss: 273574.4375 - val_mae: 447.9424\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 530697.1250 - mae: 616.5967 - val_loss: 285860.0938 - val_mae: 463.1146\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 526882.5000 - mae: 614.9230 - val_loss: 280008.2500 - val_mae: 454.4745\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 528972.2500 - mae: 615.7491 - val_loss: 281720.3750 - val_mae: 456.7664\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 529125.8750 - mae: 616.6133 - val_loss: 281809.7188 - val_mae: 454.9964\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 522268.8750 - mae: 611.3917 - val_loss: 282326.5312 - val_mae: 454.9753\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 535154.8750 - mae: 619.7324 - val_loss: 282772.3438 - val_mae: 454.9572\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 533682.9375 - mae: 618.1127 - val_loss: 286962.3438 - val_mae: 457.3021\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 532697.0000 - mae: 619.6475 - val_loss: 284133.2188 - val_mae: 455.2000\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 532614.8750 - mae: 619.0281 - val_loss: 289112.8125 - val_mae: 461.0833\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 533200.0625 - mae: 619.5645 - val_loss: 285054.5312 - val_mae: 456.2622\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 534667.6875 - mae: 622.0355 - val_loss: 289753.0938 - val_mae: 459.3334\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 532040.5000 - mae: 620.0150 - val_loss: 280480.3438 - val_mae: 448.3089\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 531200.5625 - mae: 619.9490 - val_loss: 291580.0938 - val_mae: 461.3646\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 533347.4375 - mae: 620.3515 - val_loss: 297831.8438 - val_mae: 468.2129\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 530963.8750 - mae: 618.6819 - val_loss: 302996.6562 - val_mae: 472.7916\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 531321.1250 - mae: 620.2694 - val_loss: 293112.0312 - val_mae: 461.3646\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 532128.9375 - mae: 621.8767 - val_loss: 287510.2812 - val_mae: 452.7077\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 528419.1875 - mae: 618.7408 - val_loss: 288648.2812 - val_mae: 456.1895\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 531421.9375 - mae: 622.1548 - val_loss: 283008.7188 - val_mae: 447.4745\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 521400.9062 - mae: 614.9974 - val_loss: 285088.1875 - val_mae: 450.0276\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 523334.3750 - mae: 617.5373 - val_loss: 290244.2500 - val_mae: 454.6621\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 10s 269ms/step - loss: 534378.1875 - mae: 625.4247 - val_loss: 290773.4688 - val_mae: 454.6419\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 532480.0000 - mae: 623.9462 - val_loss: 296043.1562 - val_mae: 459.3333\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 523927.0312 - mae: 617.3319 - val_loss: 301748.3750 - val_mae: 464.3457\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 530733.9375 - mae: 622.1596 - val_loss: 295905.6562 - val_mae: 457.0208\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 519880.3438 - mae: 620.0560 - val_loss: 297150.0312 - val_mae: 459.0521\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 532061.9375 - mae: 626.8023 - val_loss: 292572.5000 - val_mae: 452.5151\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 526486.3750 - mae: 621.0627 - val_loss: 300091.7188 - val_mae: 463.3958\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 524156.6875 - mae: 618.5792 - val_loss: 298823.7812 - val_mae: 459.0521\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 7s 161ms/step - loss: 533416.9375 - mae: 626.8174 - val_loss: 299330.2500 - val_mae: 459.0521\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 527743.6250 - mae: 622.4860 - val_loss: 300549.9062 - val_mae: 461.0833\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 517677.6250 - mae: 616.8656 - val_loss: 295900.2188 - val_mae: 454.4487\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 523115.0625 - mae: 620.0414 - val_loss: 301953.7500 - val_mae: 461.0000\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 521472.1250 - mae: 618.4311 - val_loss: 285915.6875 - val_mae: 442.2528\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 11s 276ms/step - loss: 521836.2188 - mae: 620.3810 - val_loss: 302000.9688 - val_mae: 459.0521\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 522633.9062 - mae: 620.6894 - val_loss: 297897.3438 - val_mae: 454.3752\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 526246.4375 - mae: 623.9304 - val_loss: 304082.3438 - val_mae: 461.3646\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 526343.0625 - mae: 625.2084 - val_loss: 298990.5938 - val_mae: 454.3353\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 524056.0938 - mae: 622.4565 - val_loss: 293939.0625 - val_mae: 449.0258\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 527108.3125 - mae: 624.0681 - val_loss: 294467.0312 - val_mae: 448.9861\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 519690.6562 - mae: 620.4693 - val_loss: 305754.3438 - val_mae: 461.0833\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 520229.5000 - mae: 620.4177 - val_loss: 307397.7188 - val_mae: 463.3958\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 523244.0312 - mae: 622.9983 - val_loss: 296309.5938 - val_mae: 449.1616\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 527099.5000 - mae: 626.7817 - val_loss: 307576.3438 - val_mae: 459.6146\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 513086.9062 - mae: 619.4714 - val_loss: 302052.4062 - val_mae: 453.9286\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 6s 125ms/step - loss: 510198.7500 - mae: 618.6514 - val_loss: 302947.7500 - val_mae: 454.1932\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 514516.7188 - mae: 621.6038 - val_loss: 303241.9062 - val_mae: 452.4252\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 510839.6562 - mae: 617.3241 - val_loss: 314989.2188 - val_mae: 466.5398\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 523154.2500 - mae: 624.3901 - val_loss: 315885.9375 - val_mae: 466.8365\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 510222.5000 - mae: 619.9850 - val_loss: 315867.2188 - val_mae: 464.8254\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 524046.6250 - mae: 626.3442 - val_loss: 311536.6562 - val_mae: 463.1146\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 522673.8125 - mae: 623.8629 - val_loss: 299873.7500 - val_mae: 448.2745\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 523755.0625 - mae: 626.8088 - val_loss: 311829.4062 - val_mae: 459.3334\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 518590.7500 - mae: 624.0731 - val_loss: 306823.9062 - val_mae: 452.3007\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 520674.4688 - mae: 624.2216 - val_loss: 312440.4688 - val_mae: 459.0521\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 512722.4688 - mae: 620.2308 - val_loss: 302098.4375 - val_mae: 446.6641\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 511960.2500 - mae: 621.0760 - val_loss: 308045.6250 - val_mae: 453.7186\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 515474.0312 - mae: 621.6422 - val_loss: 320846.0000 - val_mae: 466.9985\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 514740.6875 - mae: 620.1735 - val_loss: 315396.5625 - val_mae: 459.6146\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 521320.8125 - mae: 626.2770 - val_loss: 309889.9375 - val_mae: 452.1960\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 516059.9375 - mae: 623.8704 - val_loss: 310513.7500 - val_mae: 453.9302\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 508349.1875 - mae: 620.8083 - val_loss: 310014.7812 - val_mae: 451.5998\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 517676.9062 - mae: 623.9559 - val_loss: 306042.7188 - val_mae: 448.4594\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 519605.5938 - mae: 626.9012 - val_loss: 318262.4688 - val_mae: 463.1146\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 516014.0625 - mae: 624.3028 - val_loss: 324640.1562 - val_mae: 467.1198\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 511051.4375 - mae: 621.6315 - val_loss: 306480.9062 - val_mae: 446.0453\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 515052.0000 - mae: 623.6475 - val_loss: 325218.0312 - val_mae: 466.8708\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 515561.6250 - mae: 623.2101 - val_loss: 313460.0625 - val_mae: 451.7804\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 513232.9375 - mae: 624.4500 - val_loss: 320190.6875 - val_mae: 461.0834\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 515590.7188 - mae: 624.6177 - val_loss: 316057.9375 - val_mae: 456.0901\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 513567.8438 - mae: 624.4118 - val_loss: 308471.3438 - val_mae: 445.5971\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 517840.5000 - mae: 626.8456 - val_loss: 322108.0938 - val_mae: 459.6146\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 516028.2500 - mae: 624.8869 - val_loss: 310420.5000 - val_mae: 447.8447\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 519880.0312 - mae: 629.5033 - val_loss: 322211.2812 - val_mae: 459.0521\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 11s 262ms/step - loss: 516094.4062 - mae: 625.3456 - val_loss: 316566.0312 - val_mae: 451.3824\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 517006.6875 - mae: 627.3458 - val_loss: 318211.2188 - val_mae: 455.4267\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 518884.1250 - mae: 628.5397 - val_loss: 311956.1562 - val_mae: 447.4290\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 517693.1875 - mae: 627.5903 - val_loss: 319993.7812 - val_mae: 455.9598\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 515763.7500 - mae: 626.5723 - val_loss: 319105.8438 - val_mae: 453.3485\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 519094.8750 - mae: 630.5173 - val_loss: 313428.1562 - val_mae: 447.3297\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 515600.5000 - mae: 626.7245 - val_loss: 320072.4688 - val_mae: 453.3171\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 519207.3750 - mae: 629.7045 - val_loss: 327910.8438 - val_mae: 463.3958\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 511904.5000 - mae: 624.4746 - val_loss: 320685.5000 - val_mae: 453.0017\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 513245.0938 - mae: 626.5975 - val_loss: 315133.0938 - val_mae: 445.4608\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 11s 277ms/step - loss: 515175.0312 - mae: 628.1061 - val_loss: 340911.8438 - val_mae: 473.2403\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 514330.6562 - mae: 627.1874 - val_loss: 329808.2188 - val_mae: 463.0312\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 517202.8125 - mae: 629.9744 - val_loss: 323036.5938 - val_mae: 453.2218\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 513053.8125 - mae: 626.2203 - val_loss: 335680.4688 - val_mae: 463.4293\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 511926.5938 - mae: 625.7892 - val_loss: 337250.5312 - val_mae: 467.5085\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 7s 175ms/step - loss: 513501.9688 - mae: 627.9255 - val_loss: 331843.6250 - val_mae: 461.6458\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 510395.7812 - mae: 625.3649 - val_loss: 325076.4688 - val_mae: 454.9085\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 515010.0312 - mae: 629.2064 - val_loss: 326032.8750 - val_mae: 455.1737\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 512056.8438 - mae: 628.2197 - val_loss: 333394.9062 - val_mae: 463.3958\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 507165.5938 - mae: 624.5886 - val_loss: 333306.6562 - val_mae: 459.6146\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 513364.6250 - mae: 630.1068 - val_loss: 320454.6250 - val_mae: 446.8661\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 510707.4688 - mae: 628.3467 - val_loss: 321350.9688 - val_mae: 447.1186\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 512065.0312 - mae: 628.5989 - val_loss: 335746.9688 - val_mae: 463.6771\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 506391.6562 - mae: 624.3812 - val_loss: 335219.2812 - val_mae: 461.3646\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 510810.7812 - mae: 627.0999 - val_loss: 336176.0938 - val_mae: 461.6458\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 511565.3750 - mae: 629.8306 - val_loss: 328298.7500 - val_mae: 450.7145\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 512487.1562 - mae: 628.3633 - val_loss: 330185.0625 - val_mae: 453.2926\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 504058.1562 - mae: 625.1153 - val_loss: 323129.9688 - val_mae: 444.6327\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 508096.4375 - mae: 626.5643 - val_loss: 338166.1562 - val_mae: 461.6458\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 511492.2188 - mae: 629.3871 - val_loss: 345166.6250 - val_mae: 469.7600\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 511231.6562 - mae: 629.0341 - val_loss: 332007.4062 - val_mae: 453.2364\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 513403.0625 - mae: 630.3869 - val_loss: 325414.2188 - val_mae: 444.7984\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 512482.9688 - mae: 630.4748 - val_loss: 339095.8438 - val_mae: 458.9688\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 509637.2812 - mae: 628.4348 - val_loss: 339629.1250 - val_mae: 459.3333\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 510973.4688 - mae: 629.6810 - val_loss: 327748.7188 - val_mae: 447.0212\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 506079.2812 - mae: 628.1582 - val_loss: 327773.3125 - val_mae: 444.9605\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 506171.2188 - mae: 626.4725 - val_loss: 341024.5000 - val_mae: 459.3333\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 513088.5625 - mae: 631.1181 - val_loss: 334792.2188 - val_mae: 452.8563\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 10s 266ms/step - loss: 510090.7812 - mae: 630.1801 - val_loss: 341840.7812 - val_mae: 461.0833\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 512998.9375 - mae: 631.5674 - val_loss: 336167.3438 - val_mae: 453.1099\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 505997.1562 - mae: 629.2393 - val_loss: 336515.1562 - val_mae: 453.0994\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 510197.8438 - mae: 628.8815 - val_loss: 329830.3125 - val_mae: 444.5234\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 11s 277ms/step - loss: 507776.1562 - mae: 628.4290 - val_loss: 336541.6250 - val_mae: 450.7594\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 505352.7500 - mae: 629.5781 - val_loss: 337380.8438 - val_mae: 452.7783\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 502492.8125 - mae: 626.6530 - val_loss: 324783.5625 - val_mae: 438.1568\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 505792.8125 - mae: 630.7330 - val_loss: 344227.4688 - val_mae: 457.0208\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 511351.9062 - mae: 631.0898 - val_loss: 338306.3438 - val_mae: 452.4556\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 511922.6250 - mae: 632.4495 - val_loss: 346576.3438 - val_mae: 459.6146\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 507542.9688 - mae: 628.5167 - val_loss: 353713.8438 - val_mae: 469.7379\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 508000.5938 - mae: 629.0083 - val_loss: 340203.7188 - val_mae: 450.9458\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 507569.5938 - mae: 629.6826 - val_loss: 353452.3438 - val_mae: 463.6711\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 11s 277ms/step - loss: 506039.3750 - mae: 630.7257 - val_loss: 355554.2188 - val_mae: 470.0581\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 510974.4062 - mae: 633.5382 - val_loss: 334560.0938 - val_mae: 445.9810\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 510805.8438 - mae: 632.2000 - val_loss: 355650.6562 - val_mae: 467.7725\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 509889.5312 - mae: 632.2567 - val_loss: 349166.2812 - val_mae: 461.1624\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 511344.3750 - mae: 633.0834 - val_loss: 350011.1562 - val_mae: 461.3529\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 505389.5938 - mae: 631.1858 - val_loss: 336260.5938 - val_mae: 446.5926\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 507339.5312 - mae: 629.9622 - val_loss: 349693.7812 - val_mae: 458.0398\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 505893.7188 - mae: 629.6594 - val_loss: 351058.9062 - val_mae: 460.6599\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 496710.1562 - mae: 626.1965 - val_loss: 351423.7500 - val_mae: 462.6459\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 512050.4688 - mae: 635.0855 - val_loss: 352738.1562 - val_mae: 465.1833\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 508922.8438 - mae: 633.2206 - val_loss: 352813.8750 - val_mae: 461.8150\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 507314.7500 - mae: 632.3490 - val_loss: 345745.2188 - val_mae: 455.0186\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 508910.0312 - mae: 633.0263 - val_loss: 346227.3750 - val_mae: 455.3397\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 511086.9062 - mae: 634.8261 - val_loss: 353713.0000 - val_mae: 462.4551\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 509614.3438 - mae: 632.6431 - val_loss: 340092.2188 - val_mae: 449.1170\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 505367.1562 - mae: 630.2476 - val_loss: 354998.2500 - val_mae: 464.9485\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 504302.4688 - mae: 630.1660 - val_loss: 339909.3125 - val_mae: 447.3923\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 507725.0312 - mae: 633.3422 - val_loss: 347793.5312 - val_mae: 456.4071\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 11s 280ms/step - loss: 505804.0312 - mae: 630.8845 - val_loss: 363263.6250 - val_mae: 472.5777\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 498382.0625 - mae: 628.4028 - val_loss: 342916.3750 - val_mae: 450.9075\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 503809.0000 - mae: 629.7185 - val_loss: 363246.0000 - val_mae: 471.0014\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 507865.5000 - mae: 632.3873 - val_loss: 343128.0938 - val_mae: 451.0670\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 503993.0312 - mae: 631.5507 - val_loss: 357769.1562 - val_mae: 466.7308\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 503625.1250 - mae: 631.0997 - val_loss: 351151.2188 - val_mae: 458.4429\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 502534.8750 - mae: 630.1957 - val_loss: 358147.5000 - val_mae: 465.3080\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 498499.4688 - mae: 629.9398 - val_loss: 365714.5938 - val_mae: 474.1947\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 507279.8750 - mae: 633.8006 - val_loss: 344607.9062 - val_mae: 450.4834\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 507638.6562 - mae: 634.2834 - val_loss: 352649.1562 - val_mae: 457.9850\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 504400.2812 - mae: 631.2342 - val_loss: 353488.7812 - val_mae: 460.0561\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 506340.8438 - mae: 633.0562 - val_loss: 366979.8750 - val_mae: 473.5066\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 7s 155ms/step - loss: 503109.5000 - mae: 631.3440 - val_loss: 346083.2188 - val_mae: 452.9866\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 503750.8750 - mae: 632.7130 - val_loss: 360388.0938 - val_mae: 466.9242\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 7s 170ms/step - loss: 500789.1562 - mae: 631.7270 - val_loss: 362036.9062 - val_mae: 469.4246\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 503613.9062 - mae: 631.4445 - val_loss: 354373.5938 - val_mae: 460.6777\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 505985.5000 - mae: 633.7007 - val_loss: 354723.7188 - val_mae: 460.9035\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 502582.8125 - mae: 631.8186 - val_loss: 376900.9062 - val_mae: 483.7603\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 503538.5938 - mae: 632.0032 - val_loss: 363182.5000 - val_mae: 470.1626\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 11s 277ms/step - loss: 508326.3750 - mae: 635.5297 - val_loss: 364265.2812 - val_mae: 472.2658\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 499055.5000 - mae: 631.5576 - val_loss: 348069.6562 - val_mae: 452.8391\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 505740.1562 - mae: 633.2278 - val_loss: 350193.4062 - val_mae: 455.5542\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 501777.9688 - mae: 631.9481 - val_loss: 357429.5938 - val_mae: 462.6170\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 499153.2500 - mae: 629.6277 - val_loss: 350411.7812 - val_mae: 455.7092\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 504168.0625 - mae: 633.3238 - val_loss: 373020.7500 - val_mae: 480.2393\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 506079.5625 - mae: 635.0598 - val_loss: 350636.8438 - val_mae: 455.8680\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 502722.6250 - mae: 632.8940 - val_loss: 366549.0312 - val_mae: 472.2501\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 502251.8750 - mae: 631.4212 - val_loss: 359184.1250 - val_mae: 463.7347\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 503458.1875 - mae: 633.4897 - val_loss: 352558.4688 - val_mae: 457.0367\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 507469.9062 - mae: 635.5797 - val_loss: 352201.0938 - val_mae: 455.4770\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 507314.3125 - mae: 636.6370 - val_loss: 359445.0000 - val_mae: 462.5785\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 7s 158ms/step - loss: 503482.1250 - mae: 634.4800 - val_loss: 360582.3750 - val_mae: 464.6205\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 491867.9375 - mae: 628.6750 - val_loss: 361091.6562 - val_mae: 463.5946\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 502406.2188 - mae: 633.0784 - val_loss: 375653.4062 - val_mae: 479.1813\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 505283.3438 - mae: 634.8047 - val_loss: 361608.5312 - val_mae: 465.2678\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 496106.7188 - mae: 628.6785 - val_loss: 354372.5000 - val_mae: 458.1802\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 495024.3438 - mae: 629.7931 - val_loss: 369724.0312 - val_mae: 475.5280\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 505885.3438 - mae: 635.6837 - val_loss: 376945.8438 - val_mae: 480.0185\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 499627.0938 - mae: 632.5818 - val_loss: 362340.3750 - val_mae: 465.7476\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 506133.2188 - mae: 635.2737 - val_loss: 370648.3438 - val_mae: 473.5010\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 506873.3438 - mae: 637.0428 - val_loss: 370826.9062 - val_mae: 474.9019\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 507462.3750 - mae: 636.8922 - val_loss: 371122.8750 - val_mae: 475.0830\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 11s 271ms/step - loss: 502359.1875 - mae: 633.3932 - val_loss: 378804.9062 - val_mae: 483.7386\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 500447.0625 - mae: 631.5568 - val_loss: 356582.0000 - val_mae: 458.2950\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 504427.2812 - mae: 635.8408 - val_loss: 349879.7812 - val_mae: 451.4972\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 502171.7188 - mae: 634.4490 - val_loss: 364624.1562 - val_mae: 467.1753\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 506292.9062 - mae: 636.1852 - val_loss: 380146.8438 - val_mae: 482.0780\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 506552.6875 - mae: 636.3973 - val_loss: 350319.7812 - val_mae: 451.7927\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 496077.3750 - mae: 631.5798 - val_loss: 365495.8750 - val_mae: 467.7173\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 500984.0312 - mae: 634.9011 - val_loss: 358781.0938 - val_mae: 460.8889\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 502775.0312 - mae: 634.3547 - val_loss: 367078.4688 - val_mae: 468.6635\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 502809.7188 - mae: 633.6263 - val_loss: 374063.8438 - val_mae: 475.6824\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 507166.7500 - mae: 637.4244 - val_loss: 351688.7500 - val_mae: 452.6467\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 11s 276ms/step - loss: 504299.8125 - mae: 635.5858 - val_loss: 374906.5938 - val_mae: 477.3828\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 507241.9062 - mae: 637.2565 - val_loss: 375090.4062 - val_mae: 476.3264\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 505107.5625 - mae: 636.3546 - val_loss: 376201.2812 - val_mae: 478.1537\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 498054.3125 - mae: 631.3757 - val_loss: 360704.2500 - val_mae: 462.0573\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 505667.5312 - mae: 636.1658 - val_loss: 367921.9062 - val_mae: 468.0704\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 6s 129ms/step - loss: 506341.3750 - mae: 637.5142 - val_loss: 368491.6250 - val_mae: 469.5750\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 11s 262ms/step - loss: 509366.9688 - mae: 640.3632 - val_loss: 369125.1250 - val_mae: 471.1053\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 11s 269ms/step - loss: 503265.3125 - mae: 635.0660 - val_loss: 384330.4062 - val_mae: 485.9093\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 11s 277ms/step - loss: 505908.2812 - mae: 637.6815 - val_loss: 361469.4062 - val_mae: 461.4419\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 507285.5000 - mae: 637.8859 - val_loss: 362547.2188 - val_mae: 463.2109\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 506020.7812 - mae: 637.0375 - val_loss: 385986.2812 - val_mae: 488.3878\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 503844.1875 - mae: 635.2703 - val_loss: 370590.9688 - val_mae: 469.7984\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 11s 267ms/step - loss: 509606.2188 - mae: 641.6382 - val_loss: 377846.5000 - val_mae: 478.1281\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 11s 276ms/step - loss: 494967.8125 - mae: 631.5521 - val_loss: 378903.9062 - val_mae: 479.8543\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 505482.2188 - mae: 637.6377 - val_loss: 378661.4688 - val_mae: 479.7243\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 494958.7500 - mae: 631.7097 - val_loss: 364383.6562 - val_mae: 463.2709\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 7s 166ms/step - loss: 500877.8438 - mae: 633.8091 - val_loss: 379903.5312 - val_mae: 479.3907\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 507713.3750 - mae: 639.7531 - val_loss: 379701.1562 - val_mae: 479.2920\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 503214.2500 - mae: 635.7815 - val_loss: 365717.8438 - val_mae: 464.1015\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 11s 280ms/step - loss: 494588.0000 - mae: 633.0014 - val_loss: 381066.7500 - val_mae: 481.1764\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 490143.5625 - mae: 628.9354 - val_loss: 388261.4062 - val_mae: 489.4312\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 497466.0625 - mae: 634.0316 - val_loss: 380961.8438 - val_mae: 481.1314\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 495997.0312 - mae: 633.3041 - val_loss: 381552.5000 - val_mae: 480.4489\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 496766.3438 - mae: 631.8615 - val_loss: 381366.5938 - val_mae: 481.3816\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 498110.0625 - mae: 633.0200 - val_loss: 365851.0000 - val_mae: 465.2988\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 502752.8125 - mae: 636.5590 - val_loss: 381266.2500 - val_mae: 480.3269\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 501289.0312 - mae: 635.2417 - val_loss: 374793.8438 - val_mae: 473.5509\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 505952.2812 - mae: 638.9096 - val_loss: 374785.3438 - val_mae: 472.5492\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 6s 124ms/step - loss: 499625.5000 - mae: 634.5323 - val_loss: 383097.9062 - val_mae: 481.4358\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 10s 267ms/step - loss: 495903.5000 - mae: 634.1218 - val_loss: 382898.9688 - val_mae: 481.3388\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 492158.4688 - mae: 631.2736 - val_loss: 390864.9688 - val_mae: 489.0156\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 499770.0000 - mae: 635.0364 - val_loss: 375767.7812 - val_mae: 474.1626\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 11s 280ms/step - loss: 498900.3438 - mae: 634.6714 - val_loss: 367467.3438 - val_mae: 465.3256\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 11s 276ms/step - loss: 499418.4375 - mae: 636.8294 - val_loss: 383397.8438 - val_mae: 480.6895\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 499799.4375 - mae: 635.7407 - val_loss: 383778.5312 - val_mae: 481.8983\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 497932.9062 - mae: 634.7241 - val_loss: 369176.9062 - val_mae: 467.3145\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 505046.7188 - mae: 638.1800 - val_loss: 375582.1250 - val_mae: 473.1401\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 502382.7500 - mae: 637.3471 - val_loss: 392047.7500 - val_mae: 490.7582\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 7s 162ms/step - loss: 493406.1250 - mae: 630.8392 - val_loss: 377217.9062 - val_mae: 475.0700\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 500169.9375 - mae: 634.6383 - val_loss: 400850.4688 - val_mae: 500.8531\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 492574.5000 - mae: 634.2435 - val_loss: 385209.5938 - val_mae: 483.7423\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 501336.9062 - mae: 636.0784 - val_loss: 377989.7500 - val_mae: 476.4852\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 502733.3125 - mae: 636.7733 - val_loss: 378578.6250 - val_mae: 475.8940\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 501973.8750 - mae: 636.8671 - val_loss: 386635.3438 - val_mae: 485.8711\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 7s 163ms/step - loss: 496813.0312 - mae: 635.1003 - val_loss: 370626.6562 - val_mae: 468.2238\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 491700.0625 - mae: 630.7936 - val_loss: 370274.4062 - val_mae: 468.0272\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 11s 284ms/step - loss: 503630.0625 - mae: 638.0066 - val_loss: 378466.8438 - val_mae: 475.8718\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 500502.0625 - mae: 637.1638 - val_loss: 394154.4688 - val_mae: 491.1609\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 11s 272ms/step - loss: 495965.2500 - mae: 633.7615 - val_loss: 386629.4688 - val_mae: 484.6248\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 11s 280ms/step - loss: 506150.5938 - mae: 640.1614 - val_loss: 379181.0938 - val_mae: 476.3152\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 497203.0312 - mae: 635.2542 - val_loss: 387381.5000 - val_mae: 484.1753\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 501652.2812 - mae: 636.6214 - val_loss: 387021.0000 - val_mae: 483.9764\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 504457.4062 - mae: 638.7288 - val_loss: 380172.1250 - val_mae: 476.0297\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 500720.1250 - mae: 637.5945 - val_loss: 388697.8438 - val_mae: 485.8454\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 505348.2188 - mae: 639.3199 - val_loss: 395986.5938 - val_mae: 493.1919\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 499487.4375 - mae: 636.6802 - val_loss: 380279.3438 - val_mae: 476.1321\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 503491.8438 - mae: 638.4502 - val_loss: 380781.8750 - val_mae: 477.3053\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 501576.1250 - mae: 636.8771 - val_loss: 397063.8750 - val_mae: 493.8323\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 498647.7500 - mae: 637.6865 - val_loss: 389306.5312 - val_mae: 486.2292\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 7s 154ms/step - loss: 504232.9688 - mae: 639.6916 - val_loss: 397258.7812 - val_mae: 493.1047\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 502708.7188 - mae: 637.4971 - val_loss: 405400.7812 - val_mae: 502.6562\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 6s 118ms/step - loss: 504175.7812 - mae: 640.2730 - val_loss: 381583.8750 - val_mae: 477.7994\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 497942.8125 - mae: 635.5618 - val_loss: 389791.3438 - val_mae: 485.6848\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 491950.3438 - mae: 632.9415 - val_loss: 382173.1250 - val_mae: 478.9908\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 502646.4375 - mae: 638.2481 - val_loss: 390515.6562 - val_mae: 486.1104\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 493066.5000 - mae: 633.5150 - val_loss: 390866.7188 - val_mae: 487.1533\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 505607.9062 - mae: 641.0782 - val_loss: 381904.6562 - val_mae: 478.0187\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 500322.8125 - mae: 637.5444 - val_loss: 375502.4062 - val_mae: 471.1688\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 501516.2500 - mae: 637.8115 - val_loss: 390687.7812 - val_mae: 487.8906\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 498442.7188 - mae: 634.7969 - val_loss: 382561.2188 - val_mae: 478.4217\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 504294.1562 - mae: 639.0351 - val_loss: 375523.5938 - val_mae: 471.1980\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 504104.7500 - mae: 639.6495 - val_loss: 383904.5000 - val_mae: 480.3523\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 502532.2188 - mae: 637.6096 - val_loss: 391815.0938 - val_mae: 487.7374\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 6s 127ms/step - loss: 503097.2812 - mae: 637.8514 - val_loss: 400071.0938 - val_mae: 495.6683\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 11s 288ms/step - loss: 503455.1875 - mae: 638.9877 - val_loss: 384307.6250 - val_mae: 478.6633\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 11s 269ms/step - loss: 503119.1250 - mae: 639.1044 - val_loss: 376330.3438 - val_mae: 470.8997\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 500556.2188 - mae: 638.9772 - val_loss: 384304.7812 - val_mae: 479.4673\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 489260.1562 - mae: 631.8049 - val_loss: 384629.2812 - val_mae: 480.4396\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 499000.5312 - mae: 637.8629 - val_loss: 392664.1562 - val_mae: 487.4709\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 7s 160ms/step - loss: 503632.3438 - mae: 639.2327 - val_loss: 385346.2812 - val_mae: 480.0817\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 506080.0625 - mae: 641.7010 - val_loss: 384818.2812 - val_mae: 479.0178\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 498932.7188 - mae: 635.8890 - val_loss: 385379.2188 - val_mae: 480.8799\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 7s 162ms/step - loss: 500590.8438 - mae: 636.5306 - val_loss: 378240.7500 - val_mae: 472.8133\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 504313.6875 - mae: 640.4532 - val_loss: 393470.4062 - val_mae: 487.2197\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 11s 274ms/step - loss: 499190.6562 - mae: 637.8797 - val_loss: 377328.4062 - val_mae: 471.5480\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 7s 164ms/step - loss: 501104.8750 - mae: 637.1773 - val_loss: 386073.8438 - val_mae: 479.7782\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 504296.6250 - mae: 640.8088 - val_loss: 393521.5938 - val_mae: 488.0260\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 497554.8125 - mae: 637.9599 - val_loss: 377897.5000 - val_mae: 472.6383\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 504132.9375 - mae: 639.9062 - val_loss: 378496.9062 - val_mae: 472.2464\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 500505.1250 - mae: 638.2773 - val_loss: 387190.7188 - val_mae: 481.9308\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 7s 158ms/step - loss: 494934.8125 - mae: 634.1277 - val_loss: 402879.4062 - val_mae: 498.1125\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 494327.8125 - mae: 636.4219 - val_loss: 387284.9688 - val_mae: 481.2591\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 500740.2188 - mae: 637.7570 - val_loss: 379344.3438 - val_mae: 473.8335\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 502357.6562 - mae: 638.5826 - val_loss: 387147.2500 - val_mae: 481.1952\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 499797.2812 - mae: 637.4130 - val_loss: 380194.7188 - val_mae: 473.9783\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 500980.2812 - mae: 638.8010 - val_loss: 387841.0000 - val_mae: 480.8881\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 503028.3438 - mae: 639.1183 - val_loss: 411779.0000 - val_mae: 505.7433\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 503020.1562 - mae: 639.8144 - val_loss: 396538.3750 - val_mae: 490.8817\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 499281.5938 - mae: 636.8990 - val_loss: 396090.2500 - val_mae: 489.5821\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 11s 273ms/step - loss: 496224.5625 - mae: 637.1799 - val_loss: 396405.6250 - val_mae: 490.4694\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 502155.0938 - mae: 639.0295 - val_loss: 380881.0000 - val_mae: 473.6931\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 494259.3125 - mae: 634.7419 - val_loss: 397034.2812 - val_mae: 490.1370\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 501377.3125 - mae: 640.8043 - val_loss: 396661.8750 - val_mae: 489.9324\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 504029.8125 - mae: 640.3273 - val_loss: 381341.5938 - val_mae: 473.9765\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 498013.5938 - mae: 636.4459 - val_loss: 397023.1562 - val_mae: 490.1535\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 505519.4062 - mae: 641.3824 - val_loss: 381188.5000 - val_mae: 474.5816\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 497249.0000 - mae: 637.6974 - val_loss: 389031.8438 - val_mae: 482.6785\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 11s 281ms/step - loss: 504560.8750 - mae: 640.9088 - val_loss: 397717.7500 - val_mae: 491.9160\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 499240.2500 - mae: 637.0208 - val_loss: 389833.5938 - val_mae: 482.7968\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 491778.3125 - mae: 633.7819 - val_loss: 406054.2812 - val_mae: 498.6145\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 503257.7188 - mae: 640.0073 - val_loss: 398451.2188 - val_mae: 491.0026\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 504225.8750 - mae: 640.7947 - val_loss: 406050.1250 - val_mae: 499.2914\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 11s 285ms/step - loss: 499376.1250 - mae: 636.3613 - val_loss: 406681.9062 - val_mae: 499.6523\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 498486.1562 - mae: 637.0077 - val_loss: 390144.7500 - val_mae: 483.0018\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 498007.2500 - mae: 637.3748 - val_loss: 389983.7500 - val_mae: 483.5685\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 499325.0938 - mae: 636.8890 - val_loss: 390426.9688 - val_mae: 483.1711\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 499586.4062 - mae: 637.0140 - val_loss: 382384.3750 - val_mae: 475.3014\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 501655.1875 - mae: 639.1241 - val_loss: 407071.8438 - val_mae: 500.5383\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 11s 256ms/step - loss: 504091.4375 - mae: 640.6148 - val_loss: 390462.5000 - val_mae: 483.8450\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 501371.2812 - mae: 638.8091 - val_loss: 374358.0000 - val_mae: 466.8041\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 502477.3750 - mae: 639.6228 - val_loss: 390584.8750 - val_mae: 482.6576\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 500531.3125 - mae: 638.6791 - val_loss: 399419.9062 - val_mae: 490.9910\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 11s 285ms/step - loss: 498647.6875 - mae: 636.6459 - val_loss: 400252.4688 - val_mae: 492.0981\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 497343.0625 - mae: 636.9037 - val_loss: 392017.9062 - val_mae: 484.1056\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 11s 268ms/step - loss: 502926.5625 - mae: 640.3889 - val_loss: 392039.6562 - val_mae: 483.5024\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 500121.8438 - mae: 638.0837 - val_loss: 407981.6250 - val_mae: 500.4435\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 498636.8750 - mae: 638.3220 - val_loss: 383593.2188 - val_mae: 476.0126\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 11s 260ms/step - loss: 498546.4062 - mae: 640.0173 - val_loss: 400901.0000 - val_mae: 493.1000\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 501276.1562 - mae: 639.1490 - val_loss: 391570.5000 - val_mae: 483.8730\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 503042.7500 - mae: 639.8220 - val_loss: 400607.6562 - val_mae: 492.3353\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 497397.5000 - mae: 636.9154 - val_loss: 408920.7500 - val_mae: 501.5995\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 11s 263ms/step - loss: 501654.5000 - mae: 639.4824 - val_loss: 400843.7188 - val_mae: 492.4783\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 11s 283ms/step - loss: 503670.7500 - mae: 642.1393 - val_loss: 391959.3750 - val_mae: 483.5089\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 11s 268ms/step - loss: 497788.4375 - mae: 637.3163 - val_loss: 393245.9062 - val_mae: 484.8379\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 499415.1562 - mae: 637.6607 - val_loss: 401292.4688 - val_mae: 493.3395\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 503478.9688 - mae: 640.7045 - val_loss: 392481.8438 - val_mae: 484.4166\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 11s 268ms/step - loss: 502210.8438 - mae: 640.3343 - val_loss: 409214.8750 - val_mae: 500.0082\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 11s 278ms/step - loss: 500202.1562 - mae: 638.8846 - val_loss: 394019.2500 - val_mae: 485.8767\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 498397.9062 - mae: 637.5512 - val_loss: 385068.2500 - val_mae: 476.3001\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 502371.9688 - mae: 640.4877 - val_loss: 394405.5000 - val_mae: 486.0975\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 502635.1562 - mae: 640.3210 - val_loss: 418795.7812 - val_mae: 511.1270\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 503563.7188 - mae: 641.0153 - val_loss: 403022.1562 - val_mae: 494.3361\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 500950.4375 - mae: 640.3596 - val_loss: 393675.8438 - val_mae: 485.1266\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 493708.9688 - mae: 636.8279 - val_loss: 394772.2188 - val_mae: 485.7445\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 502036.1250 - mae: 639.6917 - val_loss: 394478.7188 - val_mae: 486.1446\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 500942.2812 - mae: 638.9119 - val_loss: 394575.9688 - val_mae: 486.5439\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 11s 266ms/step - loss: 499929.2500 - mae: 639.4488 - val_loss: 386171.5000 - val_mae: 477.5211\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 11s 286ms/step - loss: 505056.5625 - mae: 642.1645 - val_loss: 411014.9062 - val_mae: 502.2410\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 498124.3750 - mae: 640.1877 - val_loss: 395237.1250 - val_mae: 486.0198\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 494686.0625 - mae: 637.8696 - val_loss: 394302.6250 - val_mae: 485.4984\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 501500.4375 - mae: 639.1160 - val_loss: 402630.3438 - val_mae: 493.0324\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 11s 255ms/step - loss: 505132.8750 - mae: 642.9189 - val_loss: 404072.7188 - val_mae: 494.9457\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 499655.7500 - mae: 639.0299 - val_loss: 404050.6250 - val_mae: 494.3908\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 11s 284ms/step - loss: 499562.7812 - mae: 639.1342 - val_loss: 378363.5312 - val_mae: 469.1991\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 10s 268ms/step - loss: 492324.2188 - mae: 635.2853 - val_loss: 396395.7812 - val_mae: 487.2275\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 503934.1250 - mae: 641.3755 - val_loss: 403860.4688 - val_mae: 495.3728\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 495211.3750 - mae: 635.1906 - val_loss: 395329.8438 - val_mae: 485.5561\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 500050.0312 - mae: 638.6158 - val_loss: 404629.8438 - val_mae: 495.8030\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 500530.4688 - mae: 639.5844 - val_loss: 404019.5000 - val_mae: 494.3925\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 499164.0000 - mae: 638.4091 - val_loss: 396363.1250 - val_mae: 486.6853\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 500488.5625 - mae: 638.5148 - val_loss: 412240.1562 - val_mae: 502.4389\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 11s 287ms/step - loss: 499160.3438 - mae: 638.1529 - val_loss: 405024.7188 - val_mae: 495.8394\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 500295.4688 - mae: 639.6446 - val_loss: 395564.3438 - val_mae: 486.2447\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 505219.9688 - mae: 643.5369 - val_loss: 404582.6562 - val_mae: 494.7302\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 502227.9688 - mae: 640.3971 - val_loss: 404757.4688 - val_mae: 494.8349\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 498578.1875 - mae: 638.2491 - val_loss: 413313.7188 - val_mae: 503.5798\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 11s 275ms/step - loss: 497853.1562 - mae: 637.2775 - val_loss: 404831.8750 - val_mae: 494.8795\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 501137.2188 - mae: 639.9896 - val_loss: 421502.7812 - val_mae: 511.6106\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 492030.9688 - mae: 635.8020 - val_loss: 396846.6250 - val_mae: 487.8350\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 498637.5938 - mae: 637.8048 - val_loss: 396126.2500 - val_mae: 486.0698\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 500487.0938 - mae: 639.2765 - val_loss: 413897.8438 - val_mae: 503.9226\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 502793.6250 - mae: 641.4702 - val_loss: 406011.8438 - val_mae: 496.0663\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 496269.1562 - mae: 636.3092 - val_loss: 381074.5938 - val_mae: 470.7716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e31792c-c1b3-4860-bcb2-ae13ea0c16db"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1505312.25,\n",
              "  1510168.25,\n",
              "  1503516.75,\n",
              "  1500651.375,\n",
              "  1511018.25,\n",
              "  1482492.5,\n",
              "  1501300.25,\n",
              "  1494626.625,\n",
              "  1500101.875,\n",
              "  1498736.625,\n",
              "  1487057.625,\n",
              "  1489359.0,\n",
              "  1481176.25,\n",
              "  1475426.75,\n",
              "  1473220.125,\n",
              "  1488308.25,\n",
              "  1476409.75,\n",
              "  1466132.25,\n",
              "  1463168.625,\n",
              "  1465470.375,\n",
              "  1464872.125,\n",
              "  1462915.875,\n",
              "  1453020.25,\n",
              "  1430711.875,\n",
              "  1419974.0,\n",
              "  1427411.625,\n",
              "  1445370.0,\n",
              "  1434345.125,\n",
              "  1408123.625,\n",
              "  1419046.0,\n",
              "  1434467.25,\n",
              "  1425813.5,\n",
              "  1410357.5,\n",
              "  1416951.0,\n",
              "  1407166.125,\n",
              "  1410229.375,\n",
              "  1413075.625,\n",
              "  1392280.625,\n",
              "  1412769.75,\n",
              "  1394249.875,\n",
              "  1407951.875,\n",
              "  1405834.5,\n",
              "  1407005.5,\n",
              "  1375351.75,\n",
              "  1389312.75,\n",
              "  1393528.625,\n",
              "  1384254.625,\n",
              "  1379823.0,\n",
              "  1376777.25,\n",
              "  1383625.875,\n",
              "  1345389.625,\n",
              "  1355096.0,\n",
              "  1364960.375,\n",
              "  1353322.0,\n",
              "  1373436.0,\n",
              "  1347988.0,\n",
              "  1362117.0,\n",
              "  1372315.0,\n",
              "  1349112.25,\n",
              "  1346561.0,\n",
              "  1342589.5,\n",
              "  1344398.25,\n",
              "  1350836.375,\n",
              "  1326959.625,\n",
              "  1325268.875,\n",
              "  1327004.75,\n",
              "  1327586.5,\n",
              "  1303769.875,\n",
              "  1314101.75,\n",
              "  1342241.375,\n",
              "  1322244.0,\n",
              "  1317841.125,\n",
              "  1327795.0,\n",
              "  1314826.875,\n",
              "  1286365.125,\n",
              "  1309136.625,\n",
              "  1296444.375,\n",
              "  1300750.125,\n",
              "  1306858.0,\n",
              "  1296217.375,\n",
              "  1282889.5,\n",
              "  1284193.875,\n",
              "  1294339.0,\n",
              "  1296793.875,\n",
              "  1275954.25,\n",
              "  1275616.875,\n",
              "  1270757.0,\n",
              "  1270800.0,\n",
              "  1272282.75,\n",
              "  1289153.875,\n",
              "  1264132.25,\n",
              "  1269079.375,\n",
              "  1269569.125,\n",
              "  1268111.25,\n",
              "  1260688.0,\n",
              "  1258301.75,\n",
              "  1245228.25,\n",
              "  1260680.125,\n",
              "  1260795.375,\n",
              "  1237365.5,\n",
              "  1244421.875,\n",
              "  1247699.75,\n",
              "  1250143.0,\n",
              "  1250428.25,\n",
              "  1240052.75,\n",
              "  1230278.375,\n",
              "  1246034.0,\n",
              "  1221102.375,\n",
              "  1214895.875,\n",
              "  1229868.0,\n",
              "  1223033.25,\n",
              "  1233086.625,\n",
              "  1234259.25,\n",
              "  1222569.75,\n",
              "  1215289.125,\n",
              "  1209468.75,\n",
              "  1201264.875,\n",
              "  1219942.5,\n",
              "  1198865.75,\n",
              "  1184219.875,\n",
              "  1193173.25,\n",
              "  1194968.375,\n",
              "  1210785.75,\n",
              "  1191256.25,\n",
              "  1201499.875,\n",
              "  1191163.875,\n",
              "  1189365.875,\n",
              "  1176835.75,\n",
              "  1159811.5,\n",
              "  1179792.125,\n",
              "  1177290.0,\n",
              "  1158156.375,\n",
              "  1166715.375,\n",
              "  1174660.0,\n",
              "  1168341.125,\n",
              "  1141914.25,\n",
              "  1164376.25,\n",
              "  1150868.125,\n",
              "  1171103.5,\n",
              "  1136209.875,\n",
              "  1161180.625,\n",
              "  1153269.5,\n",
              "  1149106.0,\n",
              "  1147823.75,\n",
              "  1146431.125,\n",
              "  1150594.25,\n",
              "  1118455.375,\n",
              "  1131197.25,\n",
              "  1128983.25,\n",
              "  1134666.0,\n",
              "  1123476.5,\n",
              "  1133775.125,\n",
              "  1133082.125,\n",
              "  1124163.5,\n",
              "  1117904.0,\n",
              "  1116920.25,\n",
              "  1125793.625,\n",
              "  1116514.125,\n",
              "  1105072.75,\n",
              "  1103454.625,\n",
              "  1105610.75,\n",
              "  1113023.875,\n",
              "  1113972.625,\n",
              "  1090497.125,\n",
              "  1097444.375,\n",
              "  1104948.0,\n",
              "  1099392.25,\n",
              "  1108390.375,\n",
              "  1072332.625,\n",
              "  1094651.125,\n",
              "  1062730.625,\n",
              "  1075468.25,\n",
              "  1080638.75,\n",
              "  1080504.25,\n",
              "  1078465.625,\n",
              "  1055941.75,\n",
              "  1080036.125,\n",
              "  1073945.25,\n",
              "  1069254.75,\n",
              "  1058667.0,\n",
              "  1065689.25,\n",
              "  1074607.0,\n",
              "  1055183.5,\n",
              "  1058518.375,\n",
              "  1060304.625,\n",
              "  1058379.375,\n",
              "  1069153.625,\n",
              "  1051979.75,\n",
              "  1051272.5,\n",
              "  1044994.1875,\n",
              "  1029385.8125,\n",
              "  1041069.9375,\n",
              "  1037880.4375,\n",
              "  1048509.0,\n",
              "  1030744.4375,\n",
              "  1039904.125,\n",
              "  1044023.0,\n",
              "  1026243.4375,\n",
              "  1018535.5,\n",
              "  1035459.3125,\n",
              "  1028576.6875,\n",
              "  1029971.8125,\n",
              "  1017929.6875,\n",
              "  1015437.8125,\n",
              "  1007498.3125,\n",
              "  1015629.3125,\n",
              "  1000432.9375,\n",
              "  1004981.25,\n",
              "  1011590.9375,\n",
              "  1002665.0,\n",
              "  998341.0625,\n",
              "  985197.9375,\n",
              "  1003157.75,\n",
              "  986094.9375,\n",
              "  998207.0,\n",
              "  996552.375,\n",
              "  1007621.6875,\n",
              "  967438.1875,\n",
              "  995193.625,\n",
              "  994211.5,\n",
              "  978405.375,\n",
              "  980855.3125,\n",
              "  983215.5,\n",
              "  985387.6875,\n",
              "  970881.4375,\n",
              "  976541.5,\n",
              "  979625.125,\n",
              "  975947.0,\n",
              "  957747.5625,\n",
              "  972155.375,\n",
              "  969226.75,\n",
              "  958426.0625,\n",
              "  963727.9375,\n",
              "  953979.5,\n",
              "  966421.9375,\n",
              "  954508.1875,\n",
              "  962457.75,\n",
              "  964740.375,\n",
              "  945951.0,\n",
              "  948801.0,\n",
              "  957109.5625,\n",
              "  945313.4375,\n",
              "  943478.125,\n",
              "  938158.0625,\n",
              "  948860.8125,\n",
              "  949164.125,\n",
              "  941917.5625,\n",
              "  924884.0,\n",
              "  924545.1875,\n",
              "  930429.8125,\n",
              "  933308.125,\n",
              "  928935.5625,\n",
              "  922040.875,\n",
              "  922374.6875,\n",
              "  930189.0625,\n",
              "  920888.5625,\n",
              "  916353.125,\n",
              "  926336.875,\n",
              "  917775.6875,\n",
              "  910370.9375,\n",
              "  911960.75,\n",
              "  906428.375,\n",
              "  914318.375,\n",
              "  904944.5,\n",
              "  897669.6875,\n",
              "  899207.5,\n",
              "  882014.375,\n",
              "  881623.9375,\n",
              "  899883.125,\n",
              "  909223.0625,\n",
              "  895628.125,\n",
              "  895541.75,\n",
              "  887245.25,\n",
              "  894762.0625,\n",
              "  885169.875,\n",
              "  885210.625,\n",
              "  889264.8125,\n",
              "  881873.4375,\n",
              "  877458.875,\n",
              "  866082.3125,\n",
              "  873387.9375,\n",
              "  887152.4375,\n",
              "  868465.5,\n",
              "  867924.4375,\n",
              "  887411.5625,\n",
              "  870315.25,\n",
              "  864954.375,\n",
              "  862986.875,\n",
              "  865824.375,\n",
              "  861782.3125,\n",
              "  841051.5,\n",
              "  852125.8125,\n",
              "  863058.5625,\n",
              "  852769.5,\n",
              "  852293.4375,\n",
              "  857591.125,\n",
              "  862093.0625,\n",
              "  838722.75,\n",
              "  845331.8125,\n",
              "  846191.375,\n",
              "  833075.5625,\n",
              "  852061.0625,\n",
              "  842337.4375,\n",
              "  835261.5,\n",
              "  853569.125,\n",
              "  837737.875,\n",
              "  847598.4375,\n",
              "  836612.625,\n",
              "  836288.125,\n",
              "  829262.875,\n",
              "  836434.375,\n",
              "  820192.375,\n",
              "  841145.625,\n",
              "  833844.8125,\n",
              "  808794.4375,\n",
              "  821977.4375,\n",
              "  838690.5,\n",
              "  819275.625,\n",
              "  826465.4375,\n",
              "  821568.5625,\n",
              "  818067.6875,\n",
              "  812742.625,\n",
              "  810640.6875,\n",
              "  805814.375,\n",
              "  806914.3125,\n",
              "  814435.8125,\n",
              "  801804.9375,\n",
              "  800507.375,\n",
              "  806043.125,\n",
              "  802521.5,\n",
              "  802019.875,\n",
              "  796957.875,\n",
              "  795859.0,\n",
              "  799859.0625,\n",
              "  796242.5,\n",
              "  797854.375,\n",
              "  787244.3125,\n",
              "  786784.125,\n",
              "  791726.75,\n",
              "  794599.8125,\n",
              "  787486.5625,\n",
              "  794232.0,\n",
              "  781170.0625,\n",
              "  785556.125,\n",
              "  784282.9375,\n",
              "  785085.25,\n",
              "  774915.25,\n",
              "  785869.375,\n",
              "  770176.375,\n",
              "  782433.3125,\n",
              "  787764.9375,\n",
              "  764859.3125,\n",
              "  769146.125,\n",
              "  758791.3125,\n",
              "  767254.125,\n",
              "  766103.8125,\n",
              "  749307.9375,\n",
              "  764065.625,\n",
              "  763026.5,\n",
              "  753991.4375,\n",
              "  757332.5,\n",
              "  756232.5,\n",
              "  760034.4375,\n",
              "  765165.125,\n",
              "  757972.6875,\n",
              "  757804.3125,\n",
              "  746316.1875,\n",
              "  748946.125,\n",
              "  756049.0,\n",
              "  747973.25,\n",
              "  754669.5,\n",
              "  758222.0625,\n",
              "  744388.9375,\n",
              "  747924.625,\n",
              "  730359.4375,\n",
              "  746831.9375,\n",
              "  728254.125,\n",
              "  740679.6875,\n",
              "  735813.6875,\n",
              "  738927.6875,\n",
              "  729889.75,\n",
              "  728444.25,\n",
              "  737785.125,\n",
              "  720658.5,\n",
              "  737220.375,\n",
              "  732659.9375,\n",
              "  741406.25,\n",
              "  728404.25,\n",
              "  705529.5625,\n",
              "  721777.625,\n",
              "  731369.625,\n",
              "  730537.875,\n",
              "  714014.8125,\n",
              "  715660.8125,\n",
              "  714817.0625,\n",
              "  714696.375,\n",
              "  713413.125,\n",
              "  703748.3125,\n",
              "  707307.5625,\n",
              "  709237.875,\n",
              "  712791.5,\n",
              "  685732.1875,\n",
              "  708848.375,\n",
              "  711505.4375,\n",
              "  714936.875,\n",
              "  701384.0625,\n",
              "  707075.6875,\n",
              "  692399.5,\n",
              "  699672.375,\n",
              "  687705.8125,\n",
              "  684813.8125,\n",
              "  691397.9375,\n",
              "  690080.875,\n",
              "  697273.3125,\n",
              "  687742.1875,\n",
              "  692733.8125,\n",
              "  691519.375,\n",
              "  683602.25,\n",
              "  690299.375,\n",
              "  684773.9375,\n",
              "  694880.25,\n",
              "  689139.5625,\n",
              "  687497.5625,\n",
              "  675085.8125,\n",
              "  677304.0625,\n",
              "  674159.4375,\n",
              "  660624.1875,\n",
              "  675548.0,\n",
              "  675659.375,\n",
              "  667485.0625,\n",
              "  676803.0,\n",
              "  677626.5,\n",
              "  675094.375,\n",
              "  651513.9375,\n",
              "  673509.625,\n",
              "  661885.9375,\n",
              "  668659.0,\n",
              "  669976.75,\n",
              "  668413.3125,\n",
              "  671613.625,\n",
              "  663246.0,\n",
              "  666578.875,\n",
              "  666822.625,\n",
              "  659449.625,\n",
              "  670095.9375,\n",
              "  651334.6875,\n",
              "  658023.1875,\n",
              "  656402.0,\n",
              "  662164.3125,\n",
              "  638972.375,\n",
              "  654957.6875,\n",
              "  661250.0,\n",
              "  661168.875,\n",
              "  651304.5,\n",
              "  662649.625,\n",
              "  656785.1875,\n",
              "  652045.0,\n",
              "  640125.8125,\n",
              "  648232.25,\n",
              "  645259.4375,\n",
              "  643777.75,\n",
              "  639635.4375,\n",
              "  642219.4375,\n",
              "  644696.4375,\n",
              "  640725.875,\n",
              "  632936.1875,\n",
              "  636823.3125,\n",
              "  635914.4375,\n",
              "  634786.6875,\n",
              "  646840.875,\n",
              "  636945.875,\n",
              "  636404.875,\n",
              "  633889.125,\n",
              "  633822.875,\n",
              "  626096.5,\n",
              "  634015.0625,\n",
              "  632897.25,\n",
              "  625439.625,\n",
              "  632520.375,\n",
              "  628439.9375,\n",
              "  624975.9375,\n",
              "  624369.25,\n",
              "  620841.8125,\n",
              "  634830.6875,\n",
              "  622989.6875,\n",
              "  620275.25,\n",
              "  633324.375,\n",
              "  625152.6875,\n",
              "  621699.1875,\n",
              "  613261.5,\n",
              "  617503.3125,\n",
              "  617986.5,\n",
              "  617153.75,\n",
              "  621346.5625,\n",
              "  615323.75,\n",
              "  613681.1875,\n",
              "  619445.5625,\n",
              "  614494.4375,\n",
              "  615535.6875,\n",
              "  611365.1875,\n",
              "  616930.5,\n",
              "  612515.0,\n",
              "  603746.75,\n",
              "  608310.5625,\n",
              "  614226.0625,\n",
              "  600073.375,\n",
              "  607141.3125,\n",
              "  604258.1875,\n",
              "  613952.75,\n",
              "  610016.5,\n",
              "  603020.5,\n",
              "  597397.8125,\n",
              "  596238.9375,\n",
              "  605163.0,\n",
              "  594553.6875,\n",
              "  576575.0625,\n",
              "  598940.5,\n",
              "  603353.5,\n",
              "  599288.125,\n",
              "  596526.5,\n",
              "  582430.1875,\n",
              "  593187.375,\n",
              "  591836.125,\n",
              "  596488.3125,\n",
              "  593325.3125,\n",
              "  593247.25,\n",
              "  594589.875,\n",
              "  590904.25,\n",
              "  593204.3125,\n",
              "  588291.875,\n",
              "  595294.4375,\n",
              "  584988.0,\n",
              "  596696.0625,\n",
              "  583768.1875,\n",
              "  591488.1875,\n",
              "  586741.0,\n",
              "  583155.0,\n",
              "  579725.3125,\n",
              "  581891.3125,\n",
              "  577307.5625,\n",
              "  588974.9375,\n",
              "  576127.25,\n",
              "  581306.3125,\n",
              "  587110.0,\n",
              "  585186.1875,\n",
              "  570653.25,\n",
              "  579455.875,\n",
              "  578356.3125,\n",
              "  573203.4375,\n",
              "  577450.875,\n",
              "  575152.75,\n",
              "  579019.5,\n",
              "  574732.625,\n",
              "  579150.875,\n",
              "  571797.3125,\n",
              "  567031.5625,\n",
              "  570523.6875,\n",
              "  565073.4375,\n",
              "  573672.0625,\n",
              "  573677.25,\n",
              "  572146.5625,\n",
              "  568371.0,\n",
              "  565332.0625,\n",
              "  567828.8125,\n",
              "  560042.25,\n",
              "  572557.6875,\n",
              "  568559.5625,\n",
              "  573124.4375,\n",
              "  569174.9375,\n",
              "  564333.3125,\n",
              "  561472.4375,\n",
              "  562966.625,\n",
              "  556146.4375,\n",
              "  552666.8125,\n",
              "  566406.875,\n",
              "  561026.0625,\n",
              "  560469.5625,\n",
              "  566069.1875,\n",
              "  562965.75,\n",
              "  558397.6875,\n",
              "  554294.0,\n",
              "  552829.375,\n",
              "  555505.4375,\n",
              "  551479.75,\n",
              "  557643.125,\n",
              "  554617.25,\n",
              "  558166.125,\n",
              "  552092.5625,\n",
              "  546630.625,\n",
              "  555658.3125,\n",
              "  555621.8125,\n",
              "  547212.875,\n",
              "  543146.75,\n",
              "  546828.375,\n",
              "  551457.25,\n",
              "  548704.75,\n",
              "  546522.375,\n",
              "  554945.6875,\n",
              "  544171.75,\n",
              "  551503.25,\n",
              "  549884.4375,\n",
              "  547676.5,\n",
              "  537973.25,\n",
              "  550707.8125,\n",
              "  543323.1875,\n",
              "  544197.375,\n",
              "  548994.25,\n",
              "  543014.75,\n",
              "  551313.0,\n",
              "  548585.3125,\n",
              "  545259.875,\n",
              "  545625.0625,\n",
              "  540888.6875,\n",
              "  544459.4375,\n",
              "  532328.0,\n",
              "  539503.0,\n",
              "  539076.5625,\n",
              "  543411.875,\n",
              "  542312.875,\n",
              "  539650.25,\n",
              "  544160.75,\n",
              "  539114.5625,\n",
              "  538647.0625,\n",
              "  541651.75,\n",
              "  527819.3125,\n",
              "  537316.5,\n",
              "  543046.1875,\n",
              "  538478.6875,\n",
              "  536710.875,\n",
              "  537553.75,\n",
              "  536592.375,\n",
              "  530697.125,\n",
              "  526882.5,\n",
              "  528972.25,\n",
              "  529125.875,\n",
              "  522268.875,\n",
              "  535154.875,\n",
              "  533682.9375,\n",
              "  532697.0,\n",
              "  532614.875,\n",
              "  533200.0625,\n",
              "  534667.6875,\n",
              "  532040.5,\n",
              "  531200.5625,\n",
              "  533347.4375,\n",
              "  530963.875,\n",
              "  531321.125,\n",
              "  532128.9375,\n",
              "  528419.1875,\n",
              "  531421.9375,\n",
              "  521400.90625,\n",
              "  523334.375,\n",
              "  534378.1875,\n",
              "  532480.0,\n",
              "  523927.03125,\n",
              "  530733.9375,\n",
              "  519880.34375,\n",
              "  532061.9375,\n",
              "  526486.375,\n",
              "  524156.6875,\n",
              "  533416.9375,\n",
              "  527743.625,\n",
              "  517677.625,\n",
              "  523115.0625,\n",
              "  521472.125,\n",
              "  521836.21875,\n",
              "  522633.90625,\n",
              "  526246.4375,\n",
              "  526343.0625,\n",
              "  524056.09375,\n",
              "  527108.3125,\n",
              "  519690.65625,\n",
              "  520229.5,\n",
              "  523244.03125,\n",
              "  527099.5,\n",
              "  513086.90625,\n",
              "  510198.75,\n",
              "  514516.71875,\n",
              "  510839.65625,\n",
              "  523154.25,\n",
              "  510222.5,\n",
              "  524046.625,\n",
              "  522673.8125,\n",
              "  523755.0625,\n",
              "  518590.75,\n",
              "  520674.46875,\n",
              "  512722.46875,\n",
              "  511960.25,\n",
              "  515474.03125,\n",
              "  514740.6875,\n",
              "  521320.8125,\n",
              "  516059.9375,\n",
              "  508349.1875,\n",
              "  517676.90625,\n",
              "  519605.59375,\n",
              "  516014.0625,\n",
              "  511051.4375,\n",
              "  515052.0,\n",
              "  515561.625,\n",
              "  513232.9375,\n",
              "  515590.71875,\n",
              "  513567.84375,\n",
              "  517840.5,\n",
              "  516028.25,\n",
              "  519880.03125,\n",
              "  516094.40625,\n",
              "  517006.6875,\n",
              "  518884.125,\n",
              "  517693.1875,\n",
              "  515763.75,\n",
              "  519094.875,\n",
              "  515600.5,\n",
              "  519207.375,\n",
              "  511904.5,\n",
              "  513245.09375,\n",
              "  515175.03125,\n",
              "  514330.65625,\n",
              "  517202.8125,\n",
              "  513053.8125,\n",
              "  511926.59375,\n",
              "  513501.96875,\n",
              "  510395.78125,\n",
              "  515010.03125,\n",
              "  512056.84375,\n",
              "  507165.59375,\n",
              "  513364.625,\n",
              "  510707.46875,\n",
              "  512065.03125,\n",
              "  506391.65625,\n",
              "  510810.78125,\n",
              "  511565.375,\n",
              "  512487.15625,\n",
              "  504058.15625,\n",
              "  508096.4375,\n",
              "  511492.21875,\n",
              "  511231.65625,\n",
              "  513403.0625,\n",
              "  512482.96875,\n",
              "  509637.28125,\n",
              "  510973.46875,\n",
              "  506079.28125,\n",
              "  506171.21875,\n",
              "  513088.5625,\n",
              "  510090.78125,\n",
              "  512998.9375,\n",
              "  505997.15625,\n",
              "  510197.84375,\n",
              "  507776.15625,\n",
              "  505352.75,\n",
              "  502492.8125,\n",
              "  505792.8125,\n",
              "  511351.90625,\n",
              "  511922.625,\n",
              "  507542.96875,\n",
              "  508000.59375,\n",
              "  507569.59375,\n",
              "  506039.375,\n",
              "  510974.40625,\n",
              "  510805.84375,\n",
              "  509889.53125,\n",
              "  511344.375,\n",
              "  505389.59375,\n",
              "  507339.53125,\n",
              "  505893.71875,\n",
              "  496710.15625,\n",
              "  512050.46875,\n",
              "  508922.84375,\n",
              "  507314.75,\n",
              "  508910.03125,\n",
              "  511086.90625,\n",
              "  509614.34375,\n",
              "  505367.15625,\n",
              "  504302.46875,\n",
              "  507725.03125,\n",
              "  505804.03125,\n",
              "  498382.0625,\n",
              "  503809.0,\n",
              "  507865.5,\n",
              "  503993.03125,\n",
              "  503625.125,\n",
              "  502534.875,\n",
              "  498499.46875,\n",
              "  507279.875,\n",
              "  507638.65625,\n",
              "  504400.28125,\n",
              "  506340.84375,\n",
              "  503109.5,\n",
              "  503750.875,\n",
              "  500789.15625,\n",
              "  503613.90625,\n",
              "  505985.5,\n",
              "  502582.8125,\n",
              "  503538.59375,\n",
              "  508326.375,\n",
              "  499055.5,\n",
              "  505740.15625,\n",
              "  501777.96875,\n",
              "  499153.25,\n",
              "  504168.0625,\n",
              "  506079.5625,\n",
              "  502722.625,\n",
              "  502251.875,\n",
              "  503458.1875,\n",
              "  507469.90625,\n",
              "  507314.3125,\n",
              "  503482.125,\n",
              "  491867.9375,\n",
              "  502406.21875,\n",
              "  505283.34375,\n",
              "  496106.71875,\n",
              "  495024.34375,\n",
              "  505885.34375,\n",
              "  499627.09375,\n",
              "  506133.21875,\n",
              "  506873.34375,\n",
              "  507462.375,\n",
              "  502359.1875,\n",
              "  500447.0625,\n",
              "  504427.28125,\n",
              "  502171.71875,\n",
              "  506292.90625,\n",
              "  506552.6875,\n",
              "  496077.375,\n",
              "  500984.03125,\n",
              "  502775.03125,\n",
              "  502809.71875,\n",
              "  507166.75,\n",
              "  504299.8125,\n",
              "  507241.90625,\n",
              "  505107.5625,\n",
              "  498054.3125,\n",
              "  505667.53125,\n",
              "  506341.375,\n",
              "  509366.96875,\n",
              "  503265.3125,\n",
              "  505908.28125,\n",
              "  507285.5,\n",
              "  506020.78125,\n",
              "  503844.1875,\n",
              "  509606.21875,\n",
              "  494967.8125,\n",
              "  505482.21875,\n",
              "  494958.75,\n",
              "  500877.84375,\n",
              "  507713.375,\n",
              "  503214.25,\n",
              "  494588.0,\n",
              "  490143.5625,\n",
              "  497466.0625,\n",
              "  495997.03125,\n",
              "  496766.34375,\n",
              "  498110.0625,\n",
              "  502752.8125,\n",
              "  501289.03125,\n",
              "  505952.28125,\n",
              "  499625.5,\n",
              "  495903.5,\n",
              "  492158.46875,\n",
              "  499770.0,\n",
              "  498900.34375,\n",
              "  499418.4375,\n",
              "  499799.4375,\n",
              "  497932.90625,\n",
              "  505046.71875,\n",
              "  502382.75,\n",
              "  493406.125,\n",
              "  500169.9375,\n",
              "  492574.5,\n",
              "  501336.90625,\n",
              "  502733.3125,\n",
              "  501973.875,\n",
              "  496813.03125,\n",
              "  491700.0625,\n",
              "  503630.0625,\n",
              "  500502.0625,\n",
              "  495965.25,\n",
              "  506150.59375,\n",
              "  497203.03125,\n",
              "  501652.28125,\n",
              "  504457.40625,\n",
              "  500720.125,\n",
              "  505348.21875,\n",
              "  499487.4375,\n",
              "  503491.84375,\n",
              "  501576.125,\n",
              "  498647.75,\n",
              "  504232.96875,\n",
              "  502708.71875,\n",
              "  504175.78125,\n",
              "  497942.8125,\n",
              "  491950.34375,\n",
              "  502646.4375,\n",
              "  493066.5,\n",
              "  505607.90625,\n",
              "  500322.8125,\n",
              "  501516.25,\n",
              "  498442.71875,\n",
              "  504294.15625,\n",
              "  504104.75,\n",
              "  502532.21875,\n",
              "  503097.28125,\n",
              "  503455.1875,\n",
              "  503119.125,\n",
              "  500556.21875,\n",
              "  489260.15625,\n",
              "  499000.53125,\n",
              "  503632.34375,\n",
              "  506080.0625,\n",
              "  498932.71875,\n",
              "  500590.84375,\n",
              "  504313.6875,\n",
              "  499190.65625,\n",
              "  501104.875,\n",
              "  504296.625,\n",
              "  497554.8125,\n",
              "  504132.9375,\n",
              "  500505.125,\n",
              "  494934.8125,\n",
              "  494327.8125,\n",
              "  500740.21875,\n",
              "  502357.65625,\n",
              "  499797.28125,\n",
              "  500980.28125,\n",
              "  503028.34375,\n",
              "  503020.15625,\n",
              "  499281.59375,\n",
              "  496224.5625,\n",
              "  502155.09375,\n",
              "  494259.3125,\n",
              "  501377.3125,\n",
              "  504029.8125,\n",
              "  498013.59375,\n",
              "  505519.40625,\n",
              "  497249.0,\n",
              "  504560.875,\n",
              "  499240.25,\n",
              "  491778.3125,\n",
              "  503257.71875,\n",
              "  504225.875,\n",
              "  499376.125,\n",
              "  498486.15625,\n",
              "  498007.25,\n",
              "  499325.09375,\n",
              "  499586.40625,\n",
              "  501655.1875,\n",
              "  504091.4375,\n",
              "  501371.28125,\n",
              "  502477.375,\n",
              "  500531.3125,\n",
              "  498647.6875,\n",
              "  497343.0625,\n",
              "  502926.5625,\n",
              "  500121.84375,\n",
              "  498636.875,\n",
              "  498546.40625,\n",
              "  501276.15625,\n",
              "  503042.75,\n",
              "  497397.5,\n",
              "  501654.5,\n",
              "  503670.75,\n",
              "  497788.4375,\n",
              "  499415.15625,\n",
              "  503478.96875,\n",
              "  502210.84375,\n",
              "  500202.15625,\n",
              "  498397.90625,\n",
              "  502371.96875,\n",
              "  502635.15625,\n",
              "  503563.71875,\n",
              "  500950.4375,\n",
              "  493708.96875,\n",
              "  502036.125,\n",
              "  500942.28125,\n",
              "  499929.25,\n",
              "  505056.5625,\n",
              "  498124.375,\n",
              "  494686.0625,\n",
              "  501500.4375,\n",
              "  505132.875,\n",
              "  499655.75,\n",
              "  499562.78125,\n",
              "  492324.21875,\n",
              "  503934.125,\n",
              "  495211.375,\n",
              "  500050.03125,\n",
              "  500530.46875,\n",
              "  499164.0,\n",
              "  500488.5625,\n",
              "  499160.34375,\n",
              "  500295.46875,\n",
              "  505219.96875,\n",
              "  502227.96875,\n",
              "  498578.1875,\n",
              "  497853.15625,\n",
              "  501137.21875,\n",
              "  492030.96875,\n",
              "  498637.59375,\n",
              "  500487.09375,\n",
              "  502793.625,\n",
              "  496269.15625],\n",
              " 'mae': [1005.032958984375,\n",
              "  1002.4021606445312,\n",
              "  1001.3933715820312,\n",
              "  999.24755859375,\n",
              "  1004.6881713867188,\n",
              "  992.440185546875,\n",
              "  1001.6033325195312,\n",
              "  996.6466674804688,\n",
              "  996.9652099609375,\n",
              "  999.0601196289062,\n",
              "  992.4120483398438,\n",
              "  994.9283447265625,\n",
              "  990.2836303710938,\n",
              "  986.7631225585938,\n",
              "  985.273193359375,\n",
              "  992.9949340820312,\n",
              "  985.97412109375,\n",
              "  983.4251708984375,\n",
              "  980.3455810546875,\n",
              "  981.8970336914062,\n",
              "  981.4393310546875,\n",
              "  980.1395263671875,\n",
              "  974.1675415039062,\n",
              "  965.8190307617188,\n",
              "  960.7410888671875,\n",
              "  967.0936279296875,\n",
              "  972.2696533203125,\n",
              "  968.1832275390625,\n",
              "  958.4754028320312,\n",
              "  960.4122924804688,\n",
              "  966.0712280273438,\n",
              "  963.8229370117188,\n",
              "  959.1043701171875,\n",
              "  960.1256103515625,\n",
              "  957.4189453125,\n",
              "  956.118896484375,\n",
              "  957.8557739257812,\n",
              "  951.3333129882812,\n",
              "  956.8446044921875,\n",
              "  950.1148071289062,\n",
              "  957.0658569335938,\n",
              "  955.6739501953125,\n",
              "  957.8825073242188,\n",
              "  941.210693359375,\n",
              "  946.5208129882812,\n",
              "  946.906982421875,\n",
              "  944.6357421875,\n",
              "  942.0459594726562,\n",
              "  940.4966430664062,\n",
              "  944.38671875,\n",
              "  925.8697509765625,\n",
              "  930.4738159179688,\n",
              "  935.9359130859375,\n",
              "  930.6039428710938,\n",
              "  940.138427734375,\n",
              "  928.3145141601562,\n",
              "  933.722412109375,\n",
              "  939.3076782226562,\n",
              "  928.2778930664062,\n",
              "  927.5253295898438,\n",
              "  925.1644897460938,\n",
              "  924.8701782226562,\n",
              "  927.5804443359375,\n",
              "  917.2882080078125,\n",
              "  916.0653076171875,\n",
              "  917.0140380859375,\n",
              "  919.646484375,\n",
              "  906.0530395507812,\n",
              "  911.3712158203125,\n",
              "  925.710693359375,\n",
              "  914.0764770507812,\n",
              "  912.291748046875,\n",
              "  916.1987915039062,\n",
              "  912.775146484375,\n",
              "  898.14990234375,\n",
              "  910.1580810546875,\n",
              "  902.902587890625,\n",
              "  901.3975219726562,\n",
              "  907.7178955078125,\n",
              "  900.2034301757812,\n",
              "  898.1265869140625,\n",
              "  896.6951293945312,\n",
              "  898.4158325195312,\n",
              "  901.8229370117188,\n",
              "  894.9212646484375,\n",
              "  889.3833618164062,\n",
              "  888.3387451171875,\n",
              "  888.8809204101562,\n",
              "  890.277099609375,\n",
              "  897.8133544921875,\n",
              "  885.3085327148438,\n",
              "  888.3143920898438,\n",
              "  887.4466552734375,\n",
              "  887.0211791992188,\n",
              "  884.2863159179688,\n",
              "  882.544677734375,\n",
              "  878.6492309570312,\n",
              "  885.1551513671875,\n",
              "  885.6234741210938,\n",
              "  873.3209838867188,\n",
              "  876.581787109375,\n",
              "  876.8251953125,\n",
              "  879.0143432617188,\n",
              "  878.990478515625,\n",
              "  873.4470825195312,\n",
              "  869.7117919921875,\n",
              "  879.578125,\n",
              "  865.4356079101562,\n",
              "  865.6245727539062,\n",
              "  868.638671875,\n",
              "  865.295166015625,\n",
              "  870.4058837890625,\n",
              "  873.2208251953125,\n",
              "  864.7793579101562,\n",
              "  861.64599609375,\n",
              "  857.8741455078125,\n",
              "  855.6520385742188,\n",
              "  862.9830932617188,\n",
              "  854.6738891601562,\n",
              "  849.0799560546875,\n",
              "  852.4136352539062,\n",
              "  851.1397705078125,\n",
              "  861.5062255859375,\n",
              "  852.3551635742188,\n",
              "  857.1046752929688,\n",
              "  850.6001586914062,\n",
              "  849.924560546875,\n",
              "  844.46875,\n",
              "  836.421142578125,\n",
              "  843.7745361328125,\n",
              "  844.6007690429688,\n",
              "  836.9834594726562,\n",
              "  839.0704956054688,\n",
              "  842.4315185546875,\n",
              "  838.9977416992188,\n",
              "  829.73876953125,\n",
              "  837.6962890625,\n",
              "  831.2003784179688,\n",
              "  841.5695190429688,\n",
              "  826.3779296875,\n",
              "  835.7245483398438,\n",
              "  832.8519897460938,\n",
              "  830.671875,\n",
              "  829.9859008789062,\n",
              "  828.7177124023438,\n",
              "  829.7322387695312,\n",
              "  815.5864868164062,\n",
              "  821.3099365234375,\n",
              "  818.2645874023438,\n",
              "  822.8673095703125,\n",
              "  819.7767333984375,\n",
              "  823.4951171875,\n",
              "  822.5819702148438,\n",
              "  818.2069091796875,\n",
              "  815.453857421875,\n",
              "  815.4965209960938,\n",
              "  817.6100463867188,\n",
              "  812.8260498046875,\n",
              "  808.4481201171875,\n",
              "  809.0210571289062,\n",
              "  809.2203369140625,\n",
              "  813.3078002929688,\n",
              "  812.8399658203125,\n",
              "  801.7484130859375,\n",
              "  803.8859252929688,\n",
              "  809.4868774414062,\n",
              "  804.8868408203125,\n",
              "  810.4158325195312,\n",
              "  794.1158447265625,\n",
              "  800.8275146484375,\n",
              "  789.2008056640625,\n",
              "  794.72119140625,\n",
              "  795.6996459960938,\n",
              "  797.3453369140625,\n",
              "  794.2747192382812,\n",
              "  784.4611206054688,\n",
              "  797.0355224609375,\n",
              "  792.8436889648438,\n",
              "  791.9533081054688,\n",
              "  785.245849609375,\n",
              "  787.9959106445312,\n",
              "  794.4375610351562,\n",
              "  782.842529296875,\n",
              "  784.1734619140625,\n",
              "  785.3304443359375,\n",
              "  785.6212158203125,\n",
              "  790.4140625,\n",
              "  781.2234497070312,\n",
              "  783.367431640625,\n",
              "  777.5593872070312,\n",
              "  770.99560546875,\n",
              "  775.6984252929688,\n",
              "  773.3937377929688,\n",
              "  780.0198364257812,\n",
              "  769.9426879882812,\n",
              "  774.85595703125,\n",
              "  777.2672729492188,\n",
              "  769.5916137695312,\n",
              "  765.3560180664062,\n",
              "  774.607666015625,\n",
              "  769.159912109375,\n",
              "  769.6187133789062,\n",
              "  765.96533203125,\n",
              "  762.1982421875,\n",
              "  760.0430908203125,\n",
              "  762.2015380859375,\n",
              "  757.30322265625,\n",
              "  758.0753173828125,\n",
              "  763.2501220703125,\n",
              "  755.6345825195312,\n",
              "  754.4791259765625,\n",
              "  746.0346069335938,\n",
              "  756.0664672851562,\n",
              "  749.3707275390625,\n",
              "  752.5184936523438,\n",
              "  754.4716796875,\n",
              "  760.4133911132812,\n",
              "  739.4896240234375,\n",
              "  751.8529052734375,\n",
              "  751.1091918945312,\n",
              "  745.2681274414062,\n",
              "  742.0984497070312,\n",
              "  746.2024536132812,\n",
              "  746.3380126953125,\n",
              "  737.9874877929688,\n",
              "  743.8699951171875,\n",
              "  744.4097900390625,\n",
              "  741.426025390625,\n",
              "  733.0474243164062,\n",
              "  739.9384155273438,\n",
              "  739.04296875,\n",
              "  733.4888916015625,\n",
              "  734.6487426757812,\n",
              "  730.901611328125,\n",
              "  737.1254272460938,\n",
              "  728.7384033203125,\n",
              "  734.2900390625,\n",
              "  735.7293701171875,\n",
              "  725.103515625,\n",
              "  727.984619140625,\n",
              "  731.3884887695312,\n",
              "  727.806884765625,\n",
              "  726.1600341796875,\n",
              "  724.9732666015625,\n",
              "  728.883544921875,\n",
              "  727.7202758789062,\n",
              "  724.1032104492188,\n",
              "  719.6270141601562,\n",
              "  714.7222900390625,\n",
              "  720.001708984375,\n",
              "  723.0791625976562,\n",
              "  719.0645141601562,\n",
              "  715.55029296875,\n",
              "  715.1583862304688,\n",
              "  718.5475463867188,\n",
              "  714.413818359375,\n",
              "  715.5752563476562,\n",
              "  719.5685424804688,\n",
              "  712.1553344726562,\n",
              "  712.9905395507812,\n",
              "  710.0118408203125,\n",
              "  706.744873046875,\n",
              "  712.3115234375,\n",
              "  705.3832397460938,\n",
              "  705.0269165039062,\n",
              "  705.2493286132812,\n",
              "  695.2101440429688,\n",
              "  697.0066528320312,\n",
              "  705.6375732421875,\n",
              "  710.1754150390625,\n",
              "  702.017822265625,\n",
              "  702.4234619140625,\n",
              "  698.4010009765625,\n",
              "  700.8125,\n",
              "  699.6781616210938,\n",
              "  696.2153930664062,\n",
              "  699.0560913085938,\n",
              "  696.1859741210938,\n",
              "  694.8366088867188,\n",
              "  687.9779052734375,\n",
              "  696.4463500976562,\n",
              "  698.7415771484375,\n",
              "  689.2360229492188,\n",
              "  689.0783081054688,\n",
              "  700.9900512695312,\n",
              "  690.7766723632812,\n",
              "  687.2448120117188,\n",
              "  688.2914428710938,\n",
              "  688.412353515625,\n",
              "  687.5206298828125,\n",
              "  677.0203247070312,\n",
              "  683.1170654296875,\n",
              "  691.552001953125,\n",
              "  684.830078125,\n",
              "  682.2361450195312,\n",
              "  687.5831298828125,\n",
              "  689.0255737304688,\n",
              "  678.1952514648438,\n",
              "  682.4091186523438,\n",
              "  683.01318359375,\n",
              "  673.7710571289062,\n",
              "  684.5313720703125,\n",
              "  679.1387329101562,\n",
              "  675.2689208984375,\n",
              "  686.9412231445312,\n",
              "  677.1818237304688,\n",
              "  684.5305786132812,\n",
              "  677.33837890625,\n",
              "  675.7384033203125,\n",
              "  672.3753662109375,\n",
              "  676.42431640625,\n",
              "  670.5003051757812,\n",
              "  682.119140625,\n",
              "  678.240234375,\n",
              "  664.9312133789062,\n",
              "  672.0806274414062,\n",
              "  681.9686279296875,\n",
              "  671.46630859375,\n",
              "  673.3126831054688,\n",
              "  670.3006591796875,\n",
              "  668.9485473632812,\n",
              "  666.7709350585938,\n",
              "  665.4326171875,\n",
              "  662.6862182617188,\n",
              "  666.946044921875,\n",
              "  669.2938232421875,\n",
              "  665.1229858398438,\n",
              "  659.2774047851562,\n",
              "  663.3036499023438,\n",
              "  664.2175903320312,\n",
              "  663.1521606445312,\n",
              "  659.87451171875,\n",
              "  662.6488647460938,\n",
              "  664.8029174804688,\n",
              "  659.771484375,\n",
              "  664.03369140625,\n",
              "  654.8270263671875,\n",
              "  658.2802734375,\n",
              "  660.8543701171875,\n",
              "  664.1317138671875,\n",
              "  659.9729614257812,\n",
              "  665.2885131835938,\n",
              "  655.1375122070312,\n",
              "  658.9584350585938,\n",
              "  657.2918090820312,\n",
              "  659.4237670898438,\n",
              "  655.0601196289062,\n",
              "  661.3134155273438,\n",
              "  653.1256713867188,\n",
              "  659.1958618164062,\n",
              "  664.913330078125,\n",
              "  649.6064453125,\n",
              "  651.9285888671875,\n",
              "  648.1025390625,\n",
              "  653.9651489257812,\n",
              "  652.2772216796875,\n",
              "  646.4614868164062,\n",
              "  651.8484497070312,\n",
              "  653.0127563476562,\n",
              "  651.8153076171875,\n",
              "  648.7120971679688,\n",
              "  649.712646484375,\n",
              "  651.3055419921875,\n",
              "  654.775146484375,\n",
              "  651.890869140625,\n",
              "  652.6060180664062,\n",
              "  646.3597412109375,\n",
              "  650.9667358398438,\n",
              "  653.3148803710938,\n",
              "  648.4981689453125,\n",
              "  652.8104858398438,\n",
              "  656.6800537109375,\n",
              "  648.1576538085938,\n",
              "  649.544921875,\n",
              "  641.1959228515625,\n",
              "  652.7402954101562,\n",
              "  641.9530029296875,\n",
              "  650.112548828125,\n",
              "  645.8706665039062,\n",
              "  648.1226806640625,\n",
              "  647.3997802734375,\n",
              "  643.6930541992188,\n",
              "  648.7886962890625,\n",
              "  636.8186645507812,\n",
              "  650.2913208007812,\n",
              "  647.81494140625,\n",
              "  653.47998046875,\n",
              "  646.5267333984375,\n",
              "  636.8760375976562,\n",
              "  641.5770874023438,\n",
              "  648.6582641601562,\n",
              "  650.7320556640625,\n",
              "  641.2786254882812,\n",
              "  642.401123046875,\n",
              "  639.8258666992188,\n",
              "  640.7626342773438,\n",
              "  642.6528930664062,\n",
              "  635.4547119140625,\n",
              "  638.1896362304688,\n",
              "  640.7177124023438,\n",
              "  642.5096435546875,\n",
              "  628.1524658203125,\n",
              "  642.7342529296875,\n",
              "  641.9912719726562,\n",
              "  646.368896484375,\n",
              "  639.28759765625,\n",
              "  643.6229248046875,\n",
              "  633.0145263671875,\n",
              "  638.038818359375,\n",
              "  631.6864624023438,\n",
              "  632.6396484375,\n",
              "  634.3038330078125,\n",
              "  637.5161743164062,\n",
              "  638.9631958007812,\n",
              "  634.2285766601562,\n",
              "  638.9291381835938,\n",
              "  637.8770141601562,\n",
              "  633.3936767578125,\n",
              "  637.6201782226562,\n",
              "  634.8132934570312,\n",
              "  640.9634399414062,\n",
              "  638.9437866210938,\n",
              "  638.2283325195312,\n",
              "  635.8363037109375,\n",
              "  635.0842895507812,\n",
              "  630.2904052734375,\n",
              "  626.3930053710938,\n",
              "  635.59033203125,\n",
              "  632.3729248046875,\n",
              "  631.5067138671875,\n",
              "  634.0618896484375,\n",
              "  635.3038330078125,\n",
              "  633.567626953125,\n",
              "  625.7110595703125,\n",
              "  632.8631591796875,\n",
              "  627.2278442382812,\n",
              "  632.3973999023438,\n",
              "  632.7214965820312,\n",
              "  632.6141357421875,\n",
              "  635.8765869140625,\n",
              "  631.5685424804688,\n",
              "  633.1204223632812,\n",
              "  632.6533813476562,\n",
              "  628.2531127929688,\n",
              "  637.62744140625,\n",
              "  630.0506591796875,\n",
              "  629.30419921875,\n",
              "  629.5762329101562,\n",
              "  632.6738891601562,\n",
              "  620.02099609375,\n",
              "  630.427490234375,\n",
              "  634.957763671875,\n",
              "  635.3869018554688,\n",
              "  630.463134765625,\n",
              "  636.7633056640625,\n",
              "  633.8604736328125,\n",
              "  631.1349487304688,\n",
              "  625.471435546875,\n",
              "  629.2527465820312,\n",
              "  628.197998046875,\n",
              "  627.55419921875,\n",
              "  624.3614501953125,\n",
              "  626.7265625,\n",
              "  628.6846923828125,\n",
              "  625.60791015625,\n",
              "  622.9931030273438,\n",
              "  625.5097045898438,\n",
              "  625.6433715820312,\n",
              "  625.0470581054688,\n",
              "  632.8046264648438,\n",
              "  627.6533813476562,\n",
              "  627.2116088867188,\n",
              "  627.484619140625,\n",
              "  625.54736328125,\n",
              "  622.1824340820312,\n",
              "  625.9727783203125,\n",
              "  626.252197265625,\n",
              "  623.1189575195312,\n",
              "  629.1553344726562,\n",
              "  623.8848266601562,\n",
              "  623.687744140625,\n",
              "  623.4965209960938,\n",
              "  621.4529418945312,\n",
              "  630.880859375,\n",
              "  621.8533325195312,\n",
              "  621.9932861328125,\n",
              "  631.2865600585938,\n",
              "  626.515380859375,\n",
              "  622.9344482421875,\n",
              "  621.9287719726562,\n",
              "  620.6242065429688,\n",
              "  621.915283203125,\n",
              "  621.1856079101562,\n",
              "  624.9656982421875,\n",
              "  620.0220947265625,\n",
              "  621.0161743164062,\n",
              "  624.1167602539062,\n",
              "  622.1646728515625,\n",
              "  622.907470703125,\n",
              "  621.166748046875,\n",
              "  625.3970947265625,\n",
              "  620.8390502929688,\n",
              "  620.177734375,\n",
              "  620.5930786132812,\n",
              "  624.3333129882812,\n",
              "  614.961181640625,\n",
              "  620.3460083007812,\n",
              "  617.576171875,\n",
              "  625.1729736328125,\n",
              "  623.8178100585938,\n",
              "  619.9716186523438,\n",
              "  616.7667846679688,\n",
              "  616.9072265625,\n",
              "  621.3467407226562,\n",
              "  616.5733642578125,\n",
              "  606.96826171875,\n",
              "  617.7952270507812,\n",
              "  621.4466552734375,\n",
              "  619.841552734375,\n",
              "  618.3140869140625,\n",
              "  612.9745483398438,\n",
              "  617.748046875,\n",
              "  616.8233032226562,\n",
              "  619.4478759765625,\n",
              "  618.2921752929688,\n",
              "  619.8977661132812,\n",
              "  619.2099609375,\n",
              "  618.7180786132812,\n",
              "  619.6320190429688,\n",
              "  617.6378784179688,\n",
              "  621.905029296875,\n",
              "  616.9949340820312,\n",
              "  624.6318359375,\n",
              "  615.3885498046875,\n",
              "  619.3974609375,\n",
              "  618.2295532226562,\n",
              "  616.2288818359375,\n",
              "  616.5932006835938,\n",
              "  616.8623657226562,\n",
              "  612.7520141601562,\n",
              "  620.86376953125,\n",
              "  613.5469360351562,\n",
              "  617.2261962890625,\n",
              "  621.8690795898438,\n",
              "  620.953369140625,\n",
              "  616.1066284179688,\n",
              "  617.4863891601562,\n",
              "  617.885498046875,\n",
              "  613.756103515625,\n",
              "  618.1748657226562,\n",
              "  615.989013671875,\n",
              "  619.3471069335938,\n",
              "  617.5662841796875,\n",
              "  620.9471435546875,\n",
              "  616.226806640625,\n",
              "  612.4815063476562,\n",
              "  616.5841674804688,\n",
              "  612.7752075195312,\n",
              "  618.5452880859375,\n",
              "  618.69482421875,\n",
              "  618.3587036132812,\n",
              "  616.2128295898438,\n",
              "  615.7155151367188,\n",
              "  616.6077270507812,\n",
              "  613.1572265625,\n",
              "  621.2651977539062,\n",
              "  617.337646484375,\n",
              "  623.3353271484375,\n",
              "  619.4173583984375,\n",
              "  615.18408203125,\n",
              "  615.0533447265625,\n",
              "  615.6770629882812,\n",
              "  611.2901611328125,\n",
              "  609.8381958007812,\n",
              "  620.2056884765625,\n",
              "  618.1287841796875,\n",
              "  615.70556640625,\n",
              "  620.3645629882812,\n",
              "  619.0438842773438,\n",
              "  615.8684692382812,\n",
              "  614.6427612304688,\n",
              "  611.0326538085938,\n",
              "  614.1190795898438,\n",
              "  613.7207641601562,\n",
              "  616.0180053710938,\n",
              "  615.2950439453125,\n",
              "  618.0531005859375,\n",
              "  614.8447265625,\n",
              "  611.7922973632812,\n",
              "  618.1534423828125,\n",
              "  618.2636108398438,\n",
              "  612.1600952148438,\n",
              "  612.8175048828125,\n",
              "  615.8236694335938,\n",
              "  615.1239013671875,\n",
              "  614.1005859375,\n",
              "  615.63525390625,\n",
              "  619.3528442382812,\n",
              "  613.1968383789062,\n",
              "  619.6409912109375,\n",
              "  617.7449340820312,\n",
              "  618.5132446289062,\n",
              "  612.4766845703125,\n",
              "  617.9100341796875,\n",
              "  615.298583984375,\n",
              "  617.3831787109375,\n",
              "  619.4560546875,\n",
              "  614.22119140625,\n",
              "  620.4012451171875,\n",
              "  618.3253173828125,\n",
              "  616.9212036132812,\n",
              "  617.64306640625,\n",
              "  613.7921142578125,\n",
              "  616.0698852539062,\n",
              "  610.2853393554688,\n",
              "  613.9660034179688,\n",
              "  615.0896606445312,\n",
              "  617.118408203125,\n",
              "  617.275390625,\n",
              "  616.4600219726562,\n",
              "  620.2509155273438,\n",
              "  616.7020874023438,\n",
              "  616.3712158203125,\n",
              "  619.2984619140625,\n",
              "  611.0965576171875,\n",
              "  617.0502319335938,\n",
              "  621.4149780273438,\n",
              "  618.9417724609375,\n",
              "  617.9918823242188,\n",
              "  618.0682983398438,\n",
              "  617.6621704101562,\n",
              "  616.5967407226562,\n",
              "  614.9229736328125,\n",
              "  615.7491455078125,\n",
              "  616.61328125,\n",
              "  611.3916625976562,\n",
              "  619.732421875,\n",
              "  618.1126708984375,\n",
              "  619.6475219726562,\n",
              "  619.0281372070312,\n",
              "  619.5645141601562,\n",
              "  622.0354614257812,\n",
              "  620.0150146484375,\n",
              "  619.9490356445312,\n",
              "  620.3515014648438,\n",
              "  618.681884765625,\n",
              "  620.2694091796875,\n",
              "  621.876708984375,\n",
              "  618.7407836914062,\n",
              "  622.1548461914062,\n",
              "  614.9974365234375,\n",
              "  617.5372924804688,\n",
              "  625.4246826171875,\n",
              "  623.9462280273438,\n",
              "  617.3319091796875,\n",
              "  622.1596069335938,\n",
              "  620.0560302734375,\n",
              "  626.8023071289062,\n",
              "  621.062744140625,\n",
              "  618.5792236328125,\n",
              "  626.8173828125,\n",
              "  622.4860229492188,\n",
              "  616.8656005859375,\n",
              "  620.0413818359375,\n",
              "  618.4310913085938,\n",
              "  620.3810424804688,\n",
              "  620.6893920898438,\n",
              "  623.930419921875,\n",
              "  625.2084350585938,\n",
              "  622.4564819335938,\n",
              "  624.068115234375,\n",
              "  620.4692993164062,\n",
              "  620.4176635742188,\n",
              "  622.998291015625,\n",
              "  626.7816772460938,\n",
              "  619.4713745117188,\n",
              "  618.6514282226562,\n",
              "  621.603759765625,\n",
              "  617.3240966796875,\n",
              "  624.3900756835938,\n",
              "  619.9849853515625,\n",
              "  626.3441772460938,\n",
              "  623.8629150390625,\n",
              "  626.8087768554688,\n",
              "  624.0731201171875,\n",
              "  624.2216186523438,\n",
              "  620.2307739257812,\n",
              "  621.0760498046875,\n",
              "  621.6421508789062,\n",
              "  620.1735229492188,\n",
              "  626.2769775390625,\n",
              "  623.870361328125,\n",
              "  620.8082885742188,\n",
              "  623.9558715820312,\n",
              "  626.9012451171875,\n",
              "  624.3027954101562,\n",
              "  621.6315307617188,\n",
              "  623.6475219726562,\n",
              "  623.2101440429688,\n",
              "  624.449951171875,\n",
              "  624.61767578125,\n",
              "  624.4118041992188,\n",
              "  626.8455810546875,\n",
              "  624.8869018554688,\n",
              "  629.5032958984375,\n",
              "  625.3455810546875,\n",
              "  627.3458251953125,\n",
              "  628.5397338867188,\n",
              "  627.5902709960938,\n",
              "  626.572265625,\n",
              "  630.5172729492188,\n",
              "  626.7245483398438,\n",
              "  629.7044677734375,\n",
              "  624.474609375,\n",
              "  626.5975341796875,\n",
              "  628.1060791015625,\n",
              "  627.1874389648438,\n",
              "  629.974365234375,\n",
              "  626.2202758789062,\n",
              "  625.7891845703125,\n",
              "  627.925537109375,\n",
              "  625.3649291992188,\n",
              "  629.2063598632812,\n",
              "  628.2196655273438,\n",
              "  624.588623046875,\n",
              "  630.1067504882812,\n",
              "  628.3466796875,\n",
              "  628.5989379882812,\n",
              "  624.3811645507812,\n",
              "  627.099853515625,\n",
              "  629.8306274414062,\n",
              "  628.3633422851562,\n",
              "  625.1152954101562,\n",
              "  626.5642700195312,\n",
              "  629.3870849609375,\n",
              "  629.0341186523438,\n",
              "  630.3869018554688,\n",
              "  630.4747924804688,\n",
              "  628.4347534179688,\n",
              "  629.6810302734375,\n",
              "  628.158203125,\n",
              "  626.4725341796875,\n",
              "  631.1181030273438,\n",
              "  630.1800537109375,\n",
              "  631.5674438476562,\n",
              "  629.2393188476562,\n",
              "  628.8815307617188,\n",
              "  628.428955078125,\n",
              "  629.5780639648438,\n",
              "  626.6530151367188,\n",
              "  630.7329711914062,\n",
              "  631.08984375,\n",
              "  632.449462890625,\n",
              "  628.5167236328125,\n",
              "  629.00830078125,\n",
              "  629.6825561523438,\n",
              "  630.7257080078125,\n",
              "  633.5382080078125,\n",
              "  632.2000122070312,\n",
              "  632.2566528320312,\n",
              "  633.0833740234375,\n",
              "  631.185791015625,\n",
              "  629.9622192382812,\n",
              "  629.6593627929688,\n",
              "  626.1964721679688,\n",
              "  635.0855102539062,\n",
              "  633.2205810546875,\n",
              "  632.3489990234375,\n",
              "  633.0263061523438,\n",
              "  634.8261108398438,\n",
              "  632.64306640625,\n",
              "  630.24755859375,\n",
              "  630.166015625,\n",
              "  633.3421630859375,\n",
              "  630.884521484375,\n",
              "  628.40283203125,\n",
              "  629.718505859375,\n",
              "  632.3872680664062,\n",
              "  631.5506591796875,\n",
              "  631.0997314453125,\n",
              "  630.1956787109375,\n",
              "  629.9397583007812,\n",
              "  633.8005981445312,\n",
              "  634.283447265625,\n",
              "  631.2341918945312,\n",
              "  633.05615234375,\n",
              "  631.343994140625,\n",
              "  632.7129516601562,\n",
              "  631.7269897460938,\n",
              "  631.4445190429688,\n",
              "  633.70068359375,\n",
              "  631.818603515625,\n",
              "  632.003173828125,\n",
              "  635.5296630859375,\n",
              "  631.5575561523438,\n",
              "  633.2278442382812,\n",
              "  631.9481201171875,\n",
              "  629.627685546875,\n",
              "  633.3237915039062,\n",
              "  635.0597534179688,\n",
              "  632.89404296875,\n",
              "  631.4212036132812,\n",
              "  633.4896850585938,\n",
              "  635.5797119140625,\n",
              "  636.636962890625,\n",
              "  634.4800415039062,\n",
              "  628.675048828125,\n",
              "  633.0784301757812,\n",
              "  634.8046875,\n",
              "  628.6785278320312,\n",
              "  629.7930908203125,\n",
              "  635.6836547851562,\n",
              "  632.581787109375,\n",
              "  635.273681640625,\n",
              "  637.0427856445312,\n",
              "  636.8922119140625,\n",
              "  633.3932495117188,\n",
              "  631.5567626953125,\n",
              "  635.8408203125,\n",
              "  634.448974609375,\n",
              "  636.1852416992188,\n",
              "  636.3972778320312,\n",
              "  631.579833984375,\n",
              "  634.901123046875,\n",
              "  634.3546752929688,\n",
              "  633.6262817382812,\n",
              "  637.4244384765625,\n",
              "  635.5857543945312,\n",
              "  637.2564697265625,\n",
              "  636.3545532226562,\n",
              "  631.375732421875,\n",
              "  636.1658325195312,\n",
              "  637.51416015625,\n",
              "  640.3632202148438,\n",
              "  635.0660400390625,\n",
              "  637.6814575195312,\n",
              "  637.8859252929688,\n",
              "  637.0375366210938,\n",
              "  635.2703247070312,\n",
              "  641.63818359375,\n",
              "  631.5520629882812,\n",
              "  637.6376953125,\n",
              "  631.709716796875,\n",
              "  633.80908203125,\n",
              "  639.7531127929688,\n",
              "  635.781494140625,\n",
              "  633.0014038085938,\n",
              "  628.9353637695312,\n",
              "  634.0315551757812,\n",
              "  633.3041381835938,\n",
              "  631.8614501953125,\n",
              "  633.02001953125,\n",
              "  636.5589599609375,\n",
              "  635.24169921875,\n",
              "  638.9096069335938,\n",
              "  634.5323486328125,\n",
              "  634.1217651367188,\n",
              "  631.2736206054688,\n",
              "  635.036376953125,\n",
              "  634.67138671875,\n",
              "  636.8294067382812,\n",
              "  635.7406616210938,\n",
              "  634.7240600585938,\n",
              "  638.1799926757812,\n",
              "  637.3471069335938,\n",
              "  630.8392333984375,\n",
              "  634.6383056640625,\n",
              "  634.2435302734375,\n",
              "  636.078369140625,\n",
              "  636.7732543945312,\n",
              "  636.8671264648438,\n",
              "  635.1002807617188,\n",
              "  630.7935791015625,\n",
              "  638.006591796875,\n",
              "  637.1637573242188,\n",
              "  633.7615356445312,\n",
              "  640.161376953125,\n",
              "  635.2542114257812,\n",
              "  636.6213989257812,\n",
              "  638.7288208007812,\n",
              "  637.594482421875,\n",
              "  639.3198852539062,\n",
              "  636.6802368164062,\n",
              "  638.4501953125,\n",
              "  636.8770751953125,\n",
              "  637.6864624023438,\n",
              "  639.6915893554688,\n",
              "  637.4971313476562,\n",
              "  640.2730102539062,\n",
              "  635.5618286132812,\n",
              "  632.9415283203125,\n",
              "  638.2481079101562,\n",
              "  633.5149536132812,\n",
              "  641.0781860351562,\n",
              "  637.5443725585938,\n",
              "  637.8115234375,\n",
              "  634.7969360351562,\n",
              "  639.0350952148438,\n",
              "  639.6494750976562,\n",
              "  637.6095581054688,\n",
              "  637.8513793945312,\n",
              "  638.9877319335938,\n",
              "  639.1044311523438,\n",
              "  638.9772338867188,\n",
              "  631.8048706054688,\n",
              "  637.8628540039062,\n",
              "  639.232666015625,\n",
              "  641.7009887695312,\n",
              "  635.8889770507812,\n",
              "  636.5305786132812,\n",
              "  640.4531860351562,\n",
              "  637.8796997070312,\n",
              "  637.1773071289062,\n",
              "  640.808837890625,\n",
              "  637.9598999023438,\n",
              "  639.9061889648438,\n",
              "  638.27734375,\n",
              "  634.127685546875,\n",
              "  636.421875,\n",
              "  637.7570190429688,\n",
              "  638.5826416015625,\n",
              "  637.4130249023438,\n",
              "  638.8009643554688,\n",
              "  639.1182861328125,\n",
              "  639.8143920898438,\n",
              "  636.8990478515625,\n",
              "  637.1798706054688,\n",
              "  639.0294799804688,\n",
              "  634.741943359375,\n",
              "  640.8042602539062,\n",
              "  640.3272705078125,\n",
              "  636.4459228515625,\n",
              "  641.3823852539062,\n",
              "  637.6973876953125,\n",
              "  640.9087524414062,\n",
              "  637.0208129882812,\n",
              "  633.7819213867188,\n",
              "  640.00732421875,\n",
              "  640.7947387695312,\n",
              "  636.3612670898438,\n",
              "  637.0076904296875,\n",
              "  637.3748168945312,\n",
              "  636.8889770507812,\n",
              "  637.0140380859375,\n",
              "  639.1241455078125,\n",
              "  640.6148071289062,\n",
              "  638.80908203125,\n",
              "  639.622802734375,\n",
              "  638.6790771484375,\n",
              "  636.6458740234375,\n",
              "  636.9036865234375,\n",
              "  640.388916015625,\n",
              "  638.0836791992188,\n",
              "  638.3219604492188,\n",
              "  640.017333984375,\n",
              "  639.1489868164062,\n",
              "  639.822021484375,\n",
              "  636.9154052734375,\n",
              "  639.4823608398438,\n",
              "  642.1392822265625,\n",
              "  637.3163452148438,\n",
              "  637.6607055664062,\n",
              "  640.7044677734375,\n",
              "  640.3342895507812,\n",
              "  638.8845825195312,\n",
              "  637.5512084960938,\n",
              "  640.4876708984375,\n",
              "  640.3209838867188,\n",
              "  641.0153198242188,\n",
              "  640.359619140625,\n",
              "  636.827880859375,\n",
              "  639.6917114257812,\n",
              "  638.9119262695312,\n",
              "  639.4487915039062,\n",
              "  642.1644897460938,\n",
              "  640.1876831054688,\n",
              "  637.86962890625,\n",
              "  639.115966796875,\n",
              "  642.9189453125,\n",
              "  639.0299072265625,\n",
              "  639.1342163085938,\n",
              "  635.2852783203125,\n",
              "  641.3755493164062,\n",
              "  635.1905517578125,\n",
              "  638.6158447265625,\n",
              "  639.5844116210938,\n",
              "  638.4090576171875,\n",
              "  638.5148315429688,\n",
              "  638.1528930664062,\n",
              "  639.6445922851562,\n",
              "  643.5369262695312,\n",
              "  640.3970947265625,\n",
              "  638.2491455078125,\n",
              "  637.2774658203125,\n",
              "  639.9895629882812,\n",
              "  635.802001953125,\n",
              "  637.8048095703125,\n",
              "  639.2764892578125,\n",
              "  641.4701538085938,\n",
              "  636.3092041015625],\n",
              " 'val_loss': [532883.625,\n",
              "  518962.46875,\n",
              "  542877.6875,\n",
              "  541332.0625,\n",
              "  522400.53125,\n",
              "  525256.625,\n",
              "  531823.6875,\n",
              "  534656.3125,\n",
              "  520450.5,\n",
              "  526909.0625,\n",
              "  512773.5,\n",
              "  515613.15625,\n",
              "  501222.34375,\n",
              "  504054.84375,\n",
              "  519039.90625,\n",
              "  500969.28125,\n",
              "  523504.125,\n",
              "  522633.84375,\n",
              "  496125.84375,\n",
              "  503052.03125,\n",
              "  485071.96875,\n",
              "  499766.65625,\n",
              "  498418.90625,\n",
              "  513125.34375,\n",
              "  479191.625,\n",
              "  497913.15625,\n",
              "  496440.75,\n",
              "  490941.40625,\n",
              "  469376.375,\n",
              "  487810.71875,\n",
              "  498429.21875,\n",
              "  496896.59375,\n",
              "  475520.90625,\n",
              "  489871.78125,\n",
              "  488386.75,\n",
              "  490887.46875,\n",
              "  457975.65625,\n",
              "  480048.15625,\n",
              "  459102.03125,\n",
              "  446020.0,\n",
              "  475694.875,\n",
              "  474260.59375,\n",
              "  476676.21875,\n",
              "  467459.65625,\n",
              "  446875.46875,\n",
              "  464618.34375,\n",
              "  467090.78125,\n",
              "  473218.5,\n",
              "  456711.53125,\n",
              "  470411.625,\n",
              "  472776.15625,\n",
              "  452558.71875,\n",
              "  454822.15625,\n",
              "  446028.46875,\n",
              "  452081.65625,\n",
              "  447112.65625,\n",
              "  449569.40625,\n",
              "  448220.34375,\n",
              "  443044.84375,\n",
              "  426838.78125,\n",
              "  440405.34375,\n",
              "  446295.875,\n",
              "  444952.84375,\n",
              "  425456.03125,\n",
              "  438871.71875,\n",
              "  448085.0,\n",
              "  428718.90625,\n",
              "  456222.40625,\n",
              "  436980.125,\n",
              "  414100.09375,\n",
              "  452104.0,\n",
              "  432990.21875,\n",
              "  410314.03125,\n",
              "  423454.59375,\n",
              "  436029.875,\n",
              "  427795.34375,\n",
              "  426523.65625,\n",
              "  428939.21875,\n",
              "  441326.59375,\n",
              "  409090.71875,\n",
              "  404169.90625,\n",
              "  413402.90625,\n",
              "  412151.25,\n",
              "  414576.84375,\n",
              "  426709.625,\n",
              "  414513.65625,\n",
              "  399967.96875,\n",
              "  412650.84375,\n",
              "  421598.78125,\n",
              "  409561.78125,\n",
              "  382087.78125,\n",
              "  417828.28125,\n",
              "  406475.78125,\n",
              "  408870.59375,\n",
              "  404038.09375,\n",
              "  386420.28125,\n",
              "  378853.90625,\n",
              "  380511.40625,\n",
              "  392894.75,\n",
              "  394477.25,\n",
              "  403191.59375,\n",
              "  378975.65625,\n",
              "  404258.46875,\n",
              "  396238.15625,\n",
              "  376197.09375,\n",
              "  387475.34375,\n",
              "  383630.40625,\n",
              "  382526.375,\n",
              "  403259.125,\n",
              "  380264.65625,\n",
              "  385248.21875,\n",
              "  387614.625,\n",
              "  360828.65625,\n",
              "  387724.375,\n",
              "  374689.875,\n",
              "  379539.15625,\n",
              "  374883.65625,\n",
              "  358600.46875,\n",
              "  347630.375,\n",
              "  384372.15625,\n",
              "  373997.96875,\n",
              "  363632.96875,\n",
              "  362561.90625,\n",
              "  355782.34375,\n",
              "  384546.96875,\n",
              "  359375.0,\n",
              "  358352.0,\n",
              "  366328.09375,\n",
              "  356209.15625,\n",
              "  369822.28125,\n",
              "  357521.71875,\n",
              "  344179.15625,\n",
              "  361089.40625,\n",
              "  365519.90625,\n",
              "  350047.21875,\n",
              "  349128.84375,\n",
              "  356833.5,\n",
              "  361280.59375,\n",
              "  333953.65625,\n",
              "  350445.125,\n",
              "  356027.28125,\n",
              "  343068.90625,\n",
              "  342101.21875,\n",
              "  349715.90625,\n",
              "  340110.90625,\n",
              "  347655.78125,\n",
              "  346657.40625,\n",
              "  345705.125,\n",
              "  331131.09375,\n",
              "  340517.34375,\n",
              "  329261.375,\n",
              "  346801.96875,\n",
              "  335688.03125,\n",
              "  336563.46875,\n",
              "  342089.65625,\n",
              "  324661.15625,\n",
              "  328662.0625,\n",
              "  332558.65625,\n",
              "  326817.15625,\n",
              "  325905.625,\n",
              "  336333.21875,\n",
              "  327293.09375,\n",
              "  319931.09375,\n",
              "  312647.53125,\n",
              "  321428.46875,\n",
              "  336330.71875,\n",
              "  332105.75,\n",
              "  313929.75,\n",
              "  317790.125,\n",
              "  316833.03125,\n",
              "  321872.25,\n",
              "  318272.78125,\n",
              "  309704.34375,\n",
              "  308350.28125,\n",
              "  312686.46875,\n",
              "  308615.1875,\n",
              "  310912.59375,\n",
              "  299389.59375,\n",
              "  313710.15625,\n",
              "  311599.625,\n",
              "  306307.40625,\n",
              "  305407.71875,\n",
              "  316471.53125,\n",
              "  316984.84375,\n",
              "  308583.9375,\n",
              "  310736.6875,\n",
              "  317265.28125,\n",
              "  304864.53125,\n",
              "  303561.53125,\n",
              "  292907.53125,\n",
              "  313631.65625,\n",
              "  302700.90625,\n",
              "  300777.21875,\n",
              "  304136.5625,\n",
              "  295279.5625,\n",
              "  292569.21875,\n",
              "  298785.03125,\n",
              "  304944.5,\n",
              "  293283.03125,\n",
              "  306411.53125,\n",
              "  294763.375,\n",
              "  297835.0625,\n",
              "  293142.53125,\n",
              "  282523.59375,\n",
              "  278036.3125,\n",
              "  294669.375,\n",
              "  296970.21875,\n",
              "  293153.09375,\n",
              "  288613.71875,\n",
              "  278435.5625,\n",
              "  294532.78125,\n",
              "  296819.40625,\n",
              "  279958.34375,\n",
              "  282199.5625,\n",
              "  290857.71875,\n",
              "  277279.46875,\n",
              "  290191.0,\n",
              "  288787.09375,\n",
              "  288621.09375,\n",
              "  278032.125,\n",
              "  290031.75,\n",
              "  279920.15625,\n",
              "  279358.25,\n",
              "  278668.21875,\n",
              "  274805.25,\n",
              "  267882.6875,\n",
              "  276215.96875,\n",
              "  282008.03125,\n",
              "  278231.21875,\n",
              "  268110.15625,\n",
              "  283016.0,\n",
              "  276082.15625,\n",
              "  284454.78125,\n",
              "  271688.71875,\n",
              "  255923.796875,\n",
              "  264599.25,\n",
              "  275760.53125,\n",
              "  272305.6875,\n",
              "  271371.34375,\n",
              "  270900.90625,\n",
              "  267497.40625,\n",
              "  261226.234375,\n",
              "  271784.59375,\n",
              "  271383.21875,\n",
              "  264763.40625,\n",
              "  261616.0,\n",
              "  261181.75,\n",
              "  254540.703125,\n",
              "  259759.625,\n",
              "  267616.03125,\n",
              "  261498.4375,\n",
              "  260354.9375,\n",
              "  255095.125,\n",
              "  262274.34375,\n",
              "  267407.03125,\n",
              "  258841.703125,\n",
              "  258140.203125,\n",
              "  252405.6875,\n",
              "  265007.59375,\n",
              "  261888.390625,\n",
              "  261396.828125,\n",
              "  255458.078125,\n",
              "  257316.171875,\n",
              "  254545.671875,\n",
              "  253878.296875,\n",
              "  245923.4375,\n",
              "  247927.109375,\n",
              "  254800.9375,\n",
              "  256546.265625,\n",
              "  243758.046875,\n",
              "  250884.625,\n",
              "  252735.375,\n",
              "  252502.765625,\n",
              "  239348.0625,\n",
              "  253832.0625,\n",
              "  250894.953125,\n",
              "  252969.625,\n",
              "  251905.171875,\n",
              "  249246.875,\n",
              "  244218.859375,\n",
              "  244178.828125,\n",
              "  243314.046875,\n",
              "  240773.765625,\n",
              "  239728.1875,\n",
              "  241823.453125,\n",
              "  243406.859375,\n",
              "  246186.046875,\n",
              "  238718.296875,\n",
              "  242450.765625,\n",
              "  248590.390625,\n",
              "  243708.984375,\n",
              "  244994.125,\n",
              "  244364.859375,\n",
              "  242418.453125,\n",
              "  242832.546875,\n",
              "  239766.359375,\n",
              "  241630.890625,\n",
              "  238804.671875,\n",
              "  239956.0625,\n",
              "  244075.953125,\n",
              "  240832.046875,\n",
              "  238920.578125,\n",
              "  236080.921875,\n",
              "  232832.984375,\n",
              "  236323.171875,\n",
              "  238560.203125,\n",
              "  238510.421875,\n",
              "  240694.671875,\n",
              "  236384.3125,\n",
              "  235831.6875,\n",
              "  237102.203125,\n",
              "  226391.328125,\n",
              "  236066.921875,\n",
              "  230796.0,\n",
              "  235572.25,\n",
              "  234822.015625,\n",
              "  231060.25,\n",
              "  234283.109375,\n",
              "  232832.1875,\n",
              "  235903.390625,\n",
              "  235559.984375,\n",
              "  225954.140625,\n",
              "  233647.109375,\n",
              "  227708.625,\n",
              "  227407.828125,\n",
              "  230753.125,\n",
              "  230037.3125,\n",
              "  224616.078125,\n",
              "  226089.859375,\n",
              "  226013.578125,\n",
              "  226221.078125,\n",
              "  224710.484375,\n",
              "  227300.3125,\n",
              "  222505.4375,\n",
              "  229019.4375,\n",
              "  227835.671875,\n",
              "  227361.953125,\n",
              "  224635.078125,\n",
              "  223753.078125,\n",
              "  225954.625,\n",
              "  223066.484375,\n",
              "  226374.703125,\n",
              "  227272.875,\n",
              "  219674.0,\n",
              "  222385.953125,\n",
              "  227460.1875,\n",
              "  226797.859375,\n",
              "  221539.25,\n",
              "  224564.015625,\n",
              "  224120.046875,\n",
              "  221156.421875,\n",
              "  221060.8125,\n",
              "  218097.234375,\n",
              "  222208.078125,\n",
              "  219943.359375,\n",
              "  220230.921875,\n",
              "  220284.375,\n",
              "  224432.4375,\n",
              "  223836.0625,\n",
              "  222269.046875,\n",
              "  223988.8125,\n",
              "  221682.984375,\n",
              "  221790.265625,\n",
              "  223958.046875,\n",
              "  218563.171875,\n",
              "  218643.484375,\n",
              "  218710.328125,\n",
              "  222365.0,\n",
              "  222252.765625,\n",
              "  220091.125,\n",
              "  222127.484375,\n",
              "  219875.546875,\n",
              "  215686.5,\n",
              "  217628.828125,\n",
              "  221639.8125,\n",
              "  219224.078125,\n",
              "  217236.796875,\n",
              "  219242.453125,\n",
              "  218533.484375,\n",
              "  217101.0625,\n",
              "  217273.8125,\n",
              "  218118.875,\n",
              "  216214.828125,\n",
              "  216917.75,\n",
              "  219033.625,\n",
              "  216985.078125,\n",
              "  216214.3125,\n",
              "  217357.8125,\n",
              "  218015.875,\n",
              "  218480.609375,\n",
              "  219565.5625,\n",
              "  218710.390625,\n",
              "  218057.625,\n",
              "  216307.984375,\n",
              "  211264.3125,\n",
              "  219169.625,\n",
              "  217666.6875,\n",
              "  217611.046875,\n",
              "  218056.375,\n",
              "  216631.5625,\n",
              "  216947.015625,\n",
              "  214161.546875,\n",
              "  214739.296875,\n",
              "  216719.25,\n",
              "  216147.828125,\n",
              "  217719.921875,\n",
              "  216972.921875,\n",
              "  218564.828125,\n",
              "  216032.078125,\n",
              "  218769.515625,\n",
              "  219510.5625,\n",
              "  218702.859375,\n",
              "  215278.484375,\n",
              "  219530.6875,\n",
              "  215480.515625,\n",
              "  217556.296875,\n",
              "  217532.5,\n",
              "  217131.5,\n",
              "  216875.640625,\n",
              "  217576.328125,\n",
              "  216636.796875,\n",
              "  216924.421875,\n",
              "  217457.125,\n",
              "  218764.890625,\n",
              "  217272.140625,\n",
              "  216605.765625,\n",
              "  216796.984375,\n",
              "  216627.375,\n",
              "  215727.359375,\n",
              "  216874.359375,\n",
              "  216673.75,\n",
              "  218505.921875,\n",
              "  217861.75,\n",
              "  215666.765625,\n",
              "  216936.3125,\n",
              "  215953.296875,\n",
              "  218917.5625,\n",
              "  216147.6875,\n",
              "  217740.171875,\n",
              "  218965.984375,\n",
              "  219583.890625,\n",
              "  218257.671875,\n",
              "  221212.578125,\n",
              "  216689.4375,\n",
              "  216564.828125,\n",
              "  221366.265625,\n",
              "  216908.1875,\n",
              "  216702.171875,\n",
              "  220224.921875,\n",
              "  220347.625,\n",
              "  218864.953125,\n",
              "  217339.796875,\n",
              "  217161.921875,\n",
              "  220854.640625,\n",
              "  221047.296875,\n",
              "  218795.8125,\n",
              "  217393.359375,\n",
              "  221946.1875,\n",
              "  219274.8125,\n",
              "  218094.421875,\n",
              "  219921.296875,\n",
              "  221912.921875,\n",
              "  222304.5625,\n",
              "  224372.109375,\n",
              "  220745.8125,\n",
              "  218312.25,\n",
              "  222118.390625,\n",
              "  221524.953125,\n",
              "  226935.015625,\n",
              "  218631.75,\n",
              "  220934.515625,\n",
              "  222406.890625,\n",
              "  220224.625,\n",
              "  221473.578125,\n",
              "  219114.3125,\n",
              "  222005.75,\n",
              "  225275.171875,\n",
              "  223661.328125,\n",
              "  228610.171875,\n",
              "  224287.5,\n",
              "  222531.453125,\n",
              "  223798.453125,\n",
              "  221091.8125,\n",
              "  226308.9375,\n",
              "  225380.375,\n",
              "  222842.046875,\n",
              "  229334.296875,\n",
              "  223839.5,\n",
              "  224994.25,\n",
              "  229423.4375,\n",
              "  227197.609375,\n",
              "  224950.8125,\n",
              "  224529.453125,\n",
              "  227160.734375,\n",
              "  228896.515625,\n",
              "  229151.578125,\n",
              "  227159.546875,\n",
              "  224495.078125,\n",
              "  225817.1875,\n",
              "  229004.546875,\n",
              "  227846.109375,\n",
              "  232390.890625,\n",
              "  229989.515625,\n",
              "  229930.703125,\n",
              "  235016.375,\n",
              "  230857.078125,\n",
              "  231119.25,\n",
              "  231063.546875,\n",
              "  233523.609375,\n",
              "  228444.546875,\n",
              "  229514.703125,\n",
              "  238595.734375,\n",
              "  234329.6875,\n",
              "  229739.328125,\n",
              "  233176.953125,\n",
              "  235254.9375,\n",
              "  232910.703125,\n",
              "  234909.3125,\n",
              "  229845.078125,\n",
              "  236200.671875,\n",
              "  239917.296875,\n",
              "  234556.109375,\n",
              "  234746.734375,\n",
              "  237835.6875,\n",
              "  233192.0,\n",
              "  240725.015625,\n",
              "  235678.796875,\n",
              "  235988.75,\n",
              "  236324.984375,\n",
              "  236655.0625,\n",
              "  240575.765625,\n",
              "  244205.9375,\n",
              "  238779.046875,\n",
              "  243362.171875,\n",
              "  239732.125,\n",
              "  240445.875,\n",
              "  241132.3125,\n",
              "  236149.1875,\n",
              "  237782.484375,\n",
              "  239186.015625,\n",
              "  237823.359375,\n",
              "  237305.140625,\n",
              "  238143.25,\n",
              "  241147.984375,\n",
              "  238345.25,\n",
              "  248146.828125,\n",
              "  244963.109375,\n",
              "  245732.171875,\n",
              "  241780.578125,\n",
              "  246514.796875,\n",
              "  246555.203125,\n",
              "  244121.609375,\n",
              "  248542.0,\n",
              "  241177.703125,\n",
              "  242318.25,\n",
              "  249731.140625,\n",
              "  247888.265625,\n",
              "  252415.5625,\n",
              "  245667.359375,\n",
              "  247204.0,\n",
              "  246478.6875,\n",
              "  251488.890625,\n",
              "  251515.125,\n",
              "  251959.625,\n",
              "  253488.796875,\n",
              "  248148.828125,\n",
              "  256130.515625,\n",
              "  250437.171875,\n",
              "  247563.515625,\n",
              "  258572.3125,\n",
              "  254649.453125,\n",
              "  254387.0625,\n",
              "  256961.953125,\n",
              "  253964.140625,\n",
              "  254391.203125,\n",
              "  254154.234375,\n",
              "  255236.078125,\n",
              "  266765.28125,\n",
              "  254694.671875,\n",
              "  254808.640625,\n",
              "  260086.3125,\n",
              "  263479.125,\n",
              "  256080.296875,\n",
              "  261181.578125,\n",
              "  257994.171875,\n",
              "  263126.625,\n",
              "  258514.828125,\n",
              "  255606.046875,\n",
              "  268882.90625,\n",
              "  257256.796875,\n",
              "  266861.875,\n",
              "  262474.4375,\n",
              "  257337.046875,\n",
              "  266480.46875,\n",
              "  266346.21875,\n",
              "  263314.21875,\n",
              "  261709.328125,\n",
              "  267439.53125,\n",
              "  272467.75,\n",
              "  269378.09375,\n",
              "  273895.5,\n",
              "  265749.46875,\n",
              "  270312.8125,\n",
              "  270045.9375,\n",
              "  275021.625,\n",
              "  263124.0,\n",
              "  268265.59375,\n",
              "  272355.34375,\n",
              "  267338.03125,\n",
              "  268240.25,\n",
              "  265730.96875,\n",
              "  270075.375,\n",
              "  269653.90625,\n",
              "  274497.28125,\n",
              "  267143.28125,\n",
              "  268466.15625,\n",
              "  268544.65625,\n",
              "  272903.21875,\n",
              "  269045.96875,\n",
              "  278841.90625,\n",
              "  281061.25,\n",
              "  270907.9375,\n",
              "  284041.28125,\n",
              "  285079.75,\n",
              "  277313.8125,\n",
              "  281976.40625,\n",
              "  277904.625,\n",
              "  272561.78125,\n",
              "  283536.6875,\n",
              "  279383.25,\n",
              "  273574.4375,\n",
              "  285860.09375,\n",
              "  280008.25,\n",
              "  281720.375,\n",
              "  281809.71875,\n",
              "  282326.53125,\n",
              "  282772.34375,\n",
              "  286962.34375,\n",
              "  284133.21875,\n",
              "  289112.8125,\n",
              "  285054.53125,\n",
              "  289753.09375,\n",
              "  280480.34375,\n",
              "  291580.09375,\n",
              "  297831.84375,\n",
              "  302996.65625,\n",
              "  293112.03125,\n",
              "  287510.28125,\n",
              "  288648.28125,\n",
              "  283008.71875,\n",
              "  285088.1875,\n",
              "  290244.25,\n",
              "  290773.46875,\n",
              "  296043.15625,\n",
              "  301748.375,\n",
              "  295905.65625,\n",
              "  297150.03125,\n",
              "  292572.5,\n",
              "  300091.71875,\n",
              "  298823.78125,\n",
              "  299330.25,\n",
              "  300549.90625,\n",
              "  295900.21875,\n",
              "  301953.75,\n",
              "  285915.6875,\n",
              "  302000.96875,\n",
              "  297897.34375,\n",
              "  304082.34375,\n",
              "  298990.59375,\n",
              "  293939.0625,\n",
              "  294467.03125,\n",
              "  305754.34375,\n",
              "  307397.71875,\n",
              "  296309.59375,\n",
              "  307576.34375,\n",
              "  302052.40625,\n",
              "  302947.75,\n",
              "  303241.90625,\n",
              "  314989.21875,\n",
              "  315885.9375,\n",
              "  315867.21875,\n",
              "  311536.65625,\n",
              "  299873.75,\n",
              "  311829.40625,\n",
              "  306823.90625,\n",
              "  312440.46875,\n",
              "  302098.4375,\n",
              "  308045.625,\n",
              "  320846.0,\n",
              "  315396.5625,\n",
              "  309889.9375,\n",
              "  310513.75,\n",
              "  310014.78125,\n",
              "  306042.71875,\n",
              "  318262.46875,\n",
              "  324640.15625,\n",
              "  306480.90625,\n",
              "  325218.03125,\n",
              "  313460.0625,\n",
              "  320190.6875,\n",
              "  316057.9375,\n",
              "  308471.34375,\n",
              "  322108.09375,\n",
              "  310420.5,\n",
              "  322211.28125,\n",
              "  316566.03125,\n",
              "  318211.21875,\n",
              "  311956.15625,\n",
              "  319993.78125,\n",
              "  319105.84375,\n",
              "  313428.15625,\n",
              "  320072.46875,\n",
              "  327910.84375,\n",
              "  320685.5,\n",
              "  315133.09375,\n",
              "  340911.84375,\n",
              "  329808.21875,\n",
              "  323036.59375,\n",
              "  335680.46875,\n",
              "  337250.53125,\n",
              "  331843.625,\n",
              "  325076.46875,\n",
              "  326032.875,\n",
              "  333394.90625,\n",
              "  333306.65625,\n",
              "  320454.625,\n",
              "  321350.96875,\n",
              "  335746.96875,\n",
              "  335219.28125,\n",
              "  336176.09375,\n",
              "  328298.75,\n",
              "  330185.0625,\n",
              "  323129.96875,\n",
              "  338166.15625,\n",
              "  345166.625,\n",
              "  332007.40625,\n",
              "  325414.21875,\n",
              "  339095.84375,\n",
              "  339629.125,\n",
              "  327748.71875,\n",
              "  327773.3125,\n",
              "  341024.5,\n",
              "  334792.21875,\n",
              "  341840.78125,\n",
              "  336167.34375,\n",
              "  336515.15625,\n",
              "  329830.3125,\n",
              "  336541.625,\n",
              "  337380.84375,\n",
              "  324783.5625,\n",
              "  344227.46875,\n",
              "  338306.34375,\n",
              "  346576.34375,\n",
              "  353713.84375,\n",
              "  340203.71875,\n",
              "  353452.34375,\n",
              "  355554.21875,\n",
              "  334560.09375,\n",
              "  355650.65625,\n",
              "  349166.28125,\n",
              "  350011.15625,\n",
              "  336260.59375,\n",
              "  349693.78125,\n",
              "  351058.90625,\n",
              "  351423.75,\n",
              "  352738.15625,\n",
              "  352813.875,\n",
              "  345745.21875,\n",
              "  346227.375,\n",
              "  353713.0,\n",
              "  340092.21875,\n",
              "  354998.25,\n",
              "  339909.3125,\n",
              "  347793.53125,\n",
              "  363263.625,\n",
              "  342916.375,\n",
              "  363246.0,\n",
              "  343128.09375,\n",
              "  357769.15625,\n",
              "  351151.21875,\n",
              "  358147.5,\n",
              "  365714.59375,\n",
              "  344607.90625,\n",
              "  352649.15625,\n",
              "  353488.78125,\n",
              "  366979.875,\n",
              "  346083.21875,\n",
              "  360388.09375,\n",
              "  362036.90625,\n",
              "  354373.59375,\n",
              "  354723.71875,\n",
              "  376900.90625,\n",
              "  363182.5,\n",
              "  364265.28125,\n",
              "  348069.65625,\n",
              "  350193.40625,\n",
              "  357429.59375,\n",
              "  350411.78125,\n",
              "  373020.75,\n",
              "  350636.84375,\n",
              "  366549.03125,\n",
              "  359184.125,\n",
              "  352558.46875,\n",
              "  352201.09375,\n",
              "  359445.0,\n",
              "  360582.375,\n",
              "  361091.65625,\n",
              "  375653.40625,\n",
              "  361608.53125,\n",
              "  354372.5,\n",
              "  369724.03125,\n",
              "  376945.84375,\n",
              "  362340.375,\n",
              "  370648.34375,\n",
              "  370826.90625,\n",
              "  371122.875,\n",
              "  378804.90625,\n",
              "  356582.0,\n",
              "  349879.78125,\n",
              "  364624.15625,\n",
              "  380146.84375,\n",
              "  350319.78125,\n",
              "  365495.875,\n",
              "  358781.09375,\n",
              "  367078.46875,\n",
              "  374063.84375,\n",
              "  351688.75,\n",
              "  374906.59375,\n",
              "  375090.40625,\n",
              "  376201.28125,\n",
              "  360704.25,\n",
              "  367921.90625,\n",
              "  368491.625,\n",
              "  369125.125,\n",
              "  384330.40625,\n",
              "  361469.40625,\n",
              "  362547.21875,\n",
              "  385986.28125,\n",
              "  370590.96875,\n",
              "  377846.5,\n",
              "  378903.90625,\n",
              "  378661.46875,\n",
              "  364383.65625,\n",
              "  379903.53125,\n",
              "  379701.15625,\n",
              "  365717.84375,\n",
              "  381066.75,\n",
              "  388261.40625,\n",
              "  380961.84375,\n",
              "  381552.5,\n",
              "  381366.59375,\n",
              "  365851.0,\n",
              "  381266.25,\n",
              "  374793.84375,\n",
              "  374785.34375,\n",
              "  383097.90625,\n",
              "  382898.96875,\n",
              "  390864.96875,\n",
              "  375767.78125,\n",
              "  367467.34375,\n",
              "  383397.84375,\n",
              "  383778.53125,\n",
              "  369176.90625,\n",
              "  375582.125,\n",
              "  392047.75,\n",
              "  377217.90625,\n",
              "  400850.46875,\n",
              "  385209.59375,\n",
              "  377989.75,\n",
              "  378578.625,\n",
              "  386635.34375,\n",
              "  370626.65625,\n",
              "  370274.40625,\n",
              "  378466.84375,\n",
              "  394154.46875,\n",
              "  386629.46875,\n",
              "  379181.09375,\n",
              "  387381.5,\n",
              "  387021.0,\n",
              "  380172.125,\n",
              "  388697.84375,\n",
              "  395986.59375,\n",
              "  380279.34375,\n",
              "  380781.875,\n",
              "  397063.875,\n",
              "  389306.53125,\n",
              "  397258.78125,\n",
              "  405400.78125,\n",
              "  381583.875,\n",
              "  389791.34375,\n",
              "  382173.125,\n",
              "  390515.65625,\n",
              "  390866.71875,\n",
              "  381904.65625,\n",
              "  375502.40625,\n",
              "  390687.78125,\n",
              "  382561.21875,\n",
              "  375523.59375,\n",
              "  383904.5,\n",
              "  391815.09375,\n",
              "  400071.09375,\n",
              "  384307.625,\n",
              "  376330.34375,\n",
              "  384304.78125,\n",
              "  384629.28125,\n",
              "  392664.15625,\n",
              "  385346.28125,\n",
              "  384818.28125,\n",
              "  385379.21875,\n",
              "  378240.75,\n",
              "  393470.40625,\n",
              "  377328.40625,\n",
              "  386073.84375,\n",
              "  393521.59375,\n",
              "  377897.5,\n",
              "  378496.90625,\n",
              "  387190.71875,\n",
              "  402879.40625,\n",
              "  387284.96875,\n",
              "  379344.34375,\n",
              "  387147.25,\n",
              "  380194.71875,\n",
              "  387841.0,\n",
              "  411779.0,\n",
              "  396538.375,\n",
              "  396090.25,\n",
              "  396405.625,\n",
              "  380881.0,\n",
              "  397034.28125,\n",
              "  396661.875,\n",
              "  381341.59375,\n",
              "  397023.15625,\n",
              "  381188.5,\n",
              "  389031.84375,\n",
              "  397717.75,\n",
              "  389833.59375,\n",
              "  406054.28125,\n",
              "  398451.21875,\n",
              "  406050.125,\n",
              "  406681.90625,\n",
              "  390144.75,\n",
              "  389983.75,\n",
              "  390426.96875,\n",
              "  382384.375,\n",
              "  407071.84375,\n",
              "  390462.5,\n",
              "  374358.0,\n",
              "  390584.875,\n",
              "  399419.90625,\n",
              "  400252.46875,\n",
              "  392017.90625,\n",
              "  392039.65625,\n",
              "  407981.625,\n",
              "  383593.21875,\n",
              "  400901.0,\n",
              "  391570.5,\n",
              "  400607.65625,\n",
              "  408920.75,\n",
              "  400843.71875,\n",
              "  391959.375,\n",
              "  393245.90625,\n",
              "  401292.46875,\n",
              "  392481.84375,\n",
              "  409214.875,\n",
              "  394019.25,\n",
              "  385068.25,\n",
              "  394405.5,\n",
              "  418795.78125,\n",
              "  403022.15625,\n",
              "  393675.84375,\n",
              "  394772.21875,\n",
              "  394478.71875,\n",
              "  394575.96875,\n",
              "  386171.5,\n",
              "  411014.90625,\n",
              "  395237.125,\n",
              "  394302.625,\n",
              "  402630.34375,\n",
              "  404072.71875,\n",
              "  404050.625,\n",
              "  378363.53125,\n",
              "  396395.78125,\n",
              "  403860.46875,\n",
              "  395329.84375,\n",
              "  404629.84375,\n",
              "  404019.5,\n",
              "  396363.125,\n",
              "  412240.15625,\n",
              "  405024.71875,\n",
              "  395564.34375,\n",
              "  404582.65625,\n",
              "  404757.46875,\n",
              "  413313.71875,\n",
              "  404831.875,\n",
              "  421502.78125,\n",
              "  396846.625,\n",
              "  396126.25,\n",
              "  413897.84375,\n",
              "  406011.84375,\n",
              "  381074.59375],\n",
              " 'val_mae': [559.5648803710938,\n",
              "  552.5722045898438,\n",
              "  571.7970581054688,\n",
              "  570.91552734375,\n",
              "  552.2836303710938,\n",
              "  556.99365234375,\n",
              "  559.584716796875,\n",
              "  564.5621948242188,\n",
              "  552.900634765625,\n",
              "  554.93701171875,\n",
              "  543.0203247070312,\n",
              "  548.2686157226562,\n",
              "  531.6769409179688,\n",
              "  536.9240112304688,\n",
              "  548.2853393554688,\n",
              "  534.0429077148438,\n",
              "  553.4559936523438,\n",
              "  552.9199829101562,\n",
              "  525.2970581054688,\n",
              "  536.691162109375,\n",
              "  518.3573608398438,\n",
              "  529.1464233398438,\n",
              "  532.110107421875,\n",
              "  543.7565307617188,\n",
              "  512.6539916992188,\n",
              "  530.1484375,\n",
              "  529.0078125,\n",
              "  525.788330078125,\n",
              "  504.892822265625,\n",
              "  517.725830078125,\n",
              "  532.0166625976562,\n",
              "  530.5742797851562,\n",
              "  509.6600646972656,\n",
              "  520.75244140625,\n",
              "  519.3246459960938,\n",
              "  524.6243286132812,\n",
              "  493.2176818847656,\n",
              "  513.0227661132812,\n",
              "  492.413330078125,\n",
              "  480.49951171875,\n",
              "  508.7622985839844,\n",
              "  507.6112365722656,\n",
              "  508.18603515625,\n",
              "  502.4390563964844,\n",
              "  482.0953674316406,\n",
              "  499.3401794433594,\n",
              "  500.2349548339844,\n",
              "  507.2402038574219,\n",
              "  488.9463806152344,\n",
              "  505.2657775878906,\n",
              "  505.0263671875,\n",
              "  484.4134216308594,\n",
              "  489.9701232910156,\n",
              "  480.1243591308594,\n",
              "  487.435791015625,\n",
              "  478.7591247558594,\n",
              "  479.373291015625,\n",
              "  478.236572265625,\n",
              "  474.8148498535156,\n",
              "  461.3538818359375,\n",
              "  473.6386413574219,\n",
              "  480.9670104980469,\n",
              "  480.4538269042969,\n",
              "  460.8182373046875,\n",
              "  472.526123046875,\n",
              "  486.867919921875,\n",
              "  467.1499938964844,\n",
              "  495.9289245605469,\n",
              "  476.2170715332031,\n",
              "  454.5828857421875,\n",
              "  493.6833190917969,\n",
              "  474.0068359375,\n",
              "  452.5836181640625,\n",
              "  464.2646484375,\n",
              "  480.4564208984375,\n",
              "  470.865234375,\n",
              "  470.3592834472656,\n",
              "  471.6642150878906,\n",
              "  488.664306640625,\n",
              "  453.25244140625,\n",
              "  450.969482421875,\n",
              "  461.6458435058594,\n",
              "  461.3645935058594,\n",
              "  463.3958435058594,\n",
              "  479.889892578125,\n",
              "  469.2176208496094,\n",
              "  457.3020935058594,\n",
              "  469.5245056152344,\n",
              "  479.0945129394531,\n",
              "  469.1009216308594,\n",
              "  441.4388427734375,\n",
              "  479.2003479003906,\n",
              "  469.09716796875,\n",
              "  471.3802795410156,\n",
              "  469.3194580078125,\n",
              "  451.6885070800781,\n",
              "  443.8213806152344,\n",
              "  449.7164001464844,\n",
              "  461.6458435058594,\n",
              "  467.1427001953125,\n",
              "  477.2063293457031,\n",
              "  451.2173156738281,\n",
              "  478.2778015136719,\n",
              "  470.4425048828125,\n",
              "  451.9509582519531,\n",
              "  466.9687805175781,\n",
              "  461.0833740234375,\n",
              "  461.3645935058594,\n",
              "  485.7593994140625,\n",
              "  461.3645935058594,\n",
              "  469.135498046875,\n",
              "  471.137939453125,\n",
              "  444.3868103027344,\n",
              "  475.6067199707031,\n",
              "  461.3645935058594,\n",
              "  468.7088928222656,\n",
              "  466.0864562988281,\n",
              "  450.2967834472656,\n",
              "  440.6729431152344,\n",
              "  477.5722351074219,\n",
              "  468.5646057128906,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  452.5015563964844,\n",
              "  484.6490783691406,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  468.0804748535156,\n",
              "  459.0520324707031,\n",
              "  474.96240234375,\n",
              "  461.0833740234375,\n",
              "  450.4205627441406,\n",
              "  468.2180480957031,\n",
              "  474.7325134277344,\n",
              "  459.0520935058594,\n",
              "  459.6145935058594,\n",
              "  467.8207092285156,\n",
              "  474.781005859375,\n",
              "  448.5910339355469,\n",
              "  465.9839172363281,\n",
              "  469.7359619140625,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  467.8997497558594,\n",
              "  459.3333435058594,\n",
              "  467.5611267089844,\n",
              "  467.5323486328125,\n",
              "  467.7842102050781,\n",
              "  453.2234802246094,\n",
              "  465.9768981933594,\n",
              "  453.28125,\n",
              "  473.6915588378906,\n",
              "  461.3645935058594,\n",
              "  465.5803527832031,\n",
              "  469.6133728027344,\n",
              "  453.425537109375,\n",
              "  459.3333435058594,\n",
              "  464.9014587402344,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  469.4405822753906,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  447.6995544433594,\n",
              "  459.6145935058594,\n",
              "  474.9150390625,\n",
              "  472.54443359375,\n",
              "  453.4906921386719,\n",
              "  459.3333740234375,\n",
              "  459.0520935058594,\n",
              "  463.03125,\n",
              "  461.0833435058594,\n",
              "  453.6331481933594,\n",
              "  453.2975158691406,\n",
              "  459.6145935058594,\n",
              "  457.3020935058594,\n",
              "  459.3332824707031,\n",
              "  449.994873046875,\n",
              "  464.8620300292969,\n",
              "  461.6458435058594,\n",
              "  456.1746520996094,\n",
              "  455.9219970703125,\n",
              "  468.8095397949219,\n",
              "  472.4164733886719,\n",
              "  464.6897888183594,\n",
              "  466.4112243652344,\n",
              "  473.7127380371094,\n",
              "  461.3645935058594,\n",
              "  461.0,\n",
              "  452.0888977050781,\n",
              "  472.9212951660156,\n",
              "  464.2083435058594,\n",
              "  461.0833435058594,\n",
              "  466.1830749511719,\n",
              "  456.5754089355469,\n",
              "  457.5833740234375,\n",
              "  464.3468933105469,\n",
              "  471.0528564453125,\n",
              "  459.6145935058594,\n",
              "  473.25048828125,\n",
              "  461.6457824707031,\n",
              "  466.2344970703125,\n",
              "  461.3645935058594,\n",
              "  452.4888000488281,\n",
              "  448.013916015625,\n",
              "  465.8402404785156,\n",
              "  468.1238098144531,\n",
              "  465.7834167480469,\n",
              "  461.0833435058594,\n",
              "  452.9403381347656,\n",
              "  470.031494140625,\n",
              "  472.2873229980469,\n",
              "  457.3020935058594,\n",
              "  459.3333435058594,\n",
              "  467.6148376464844,\n",
              "  455.1424255371094,\n",
              "  470.250244140625,\n",
              "  467.810791015625,\n",
              "  469.8573913574219,\n",
              "  459.3333435058594,\n",
              "  471.7741394042969,\n",
              "  463.0719299316406,\n",
              "  463.3253479003906,\n",
              "  463.2969970703125,\n",
              "  459.6145935058594,\n",
              "  453.3955078125,\n",
              "  461.3645935058594,\n",
              "  469.0640563964844,\n",
              "  465.4672546386719,\n",
              "  455.2591247558594,\n",
              "  471.48876953125,\n",
              "  465.1002502441406,\n",
              "  474.5178527832031,\n",
              "  461.3645935058594,\n",
              "  447.3236999511719,\n",
              "  455.7101745605469,\n",
              "  467.27197265625,\n",
              "  465.2117919921875,\n",
              "  463.3958435058594,\n",
              "  464.8739929199219,\n",
              "  462.8146057128906,\n",
              "  457.3020935058594,\n",
              "  466.8203430175781,\n",
              "  468.4384460449219,\n",
              "  459.7462463378906,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  453.654541015625,\n",
              "  459.0520935058594,\n",
              "  468.1001892089844,\n",
              "  461.3645935058594,\n",
              "  460.71875,\n",
              "  457.5833435058594,\n",
              "  464.1974792480469,\n",
              "  470.1293640136719,\n",
              "  461.6458435058594,\n",
              "  462.6434631347656,\n",
              "  456.3325500488281,\n",
              "  469.9023132324219,\n",
              "  468.0950622558594,\n",
              "  466.8740539550781,\n",
              "  461.3645935058594,\n",
              "  464.22314453125,\n",
              "  461.6458435058594,\n",
              "  461.3646240234375,\n",
              "  453.5047302246094,\n",
              "  456.5874328613281,\n",
              "  464.3642578125,\n",
              "  467.0279235839844,\n",
              "  454.3580627441406,\n",
              "  461.3645935058594,\n",
              "  464.2522277832031,\n",
              "  463.3958435058594,\n",
              "  452.43896484375,\n",
              "  466.200439453125,\n",
              "  463.1145935058594,\n",
              "  466.4244079589844,\n",
              "  465.7503356933594,\n",
              "  464.0555114746094,\n",
              "  459.6145935058594,\n",
              "  459.0108337402344,\n",
              "  459.6145935058594,\n",
              "  457.0359191894531,\n",
              "  457.3020935058594,\n",
              "  459.3333435058594,\n",
              "  461.8274841308594,\n",
              "  463.6770935058594,\n",
              "  457.1767578125,\n",
              "  461.0833435058594,\n",
              "  468.1622619628906,\n",
              "  463.7199401855469,\n",
              "  465.7391662597656,\n",
              "  465.4022521972656,\n",
              "  463.6368713378906,\n",
              "  463.6770935058594,\n",
              "  461.3645935058594,\n",
              "  463.3125,\n",
              "  461.0833435058594,\n",
              "  462.9342346191406,\n",
              "  467.6058349609375,\n",
              "  464.955810546875,\n",
              "  463.1319885253906,\n",
              "  461.0725402832031,\n",
              "  457.6212158203125,\n",
              "  461.3645935058594,\n",
              "  463.6770935058594,\n",
              "  464.9013671875,\n",
              "  467.156982421875,\n",
              "  463.21728515625,\n",
              "  462.90869140625,\n",
              "  464.9587707519531,\n",
              "  454.0439147949219,\n",
              "  463.6770935058594,\n",
              "  459.3333435058594,\n",
              "  464.735595703125,\n",
              "  464.1181335449219,\n",
              "  460.015625,\n",
              "  464.2890319824219,\n",
              "  462.9393005371094,\n",
              "  466.2085876464844,\n",
              "  466.3492126464844,\n",
              "  457.3021240234375,\n",
              "  464.578369140625,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  463.025634765625,\n",
              "  462.4346008300781,\n",
              "  456.9656677246094,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  459.1641540527344,\n",
              "  458.400390625,\n",
              "  461.0833435058594,\n",
              "  457.0208435058594,\n",
              "  463.9068908691406,\n",
              "  462.7497253417969,\n",
              "  462.44140625,\n",
              "  460.1011047363281,\n",
              "  459.3333435058594,\n",
              "  461.6458435058594,\n",
              "  459.0520935058594,\n",
              "  462.6769714355469,\n",
              "  463.3958740234375,\n",
              "  456.419677734375,\n",
              "  459.3333740234375,\n",
              "  464.5034484863281,\n",
              "  463.914306640625,\n",
              "  459.0520935058594,\n",
              "  462.3905944824219,\n",
              "  462.0816345214844,\n",
              "  459.3333435058594,\n",
              "  459.4318542480469,\n",
              "  456.6675109863281,\n",
              "  460.71875,\n",
              "  458.7535705566406,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  463.7454528808594,\n",
              "  463.2068786621094,\n",
              "  462.0880432128906,\n",
              "  463.8116455078125,\n",
              "  461.7535095214844,\n",
              "  462.086181640625,\n",
              "  464.3919372558594,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  459.5835266113281,\n",
              "  463.1728820800781,\n",
              "  463.2225646972656,\n",
              "  461.2513427734375,\n",
              "  463.3958435058594,\n",
              "  461.3087463378906,\n",
              "  457.3020935058594,\n",
              "  459.3333435058594,\n",
              "  463.4266662597656,\n",
              "  461.091064453125,\n",
              "  459.25,\n",
              "  461.3645935058594,\n",
              "  460.6419982910156,\n",
              "  459.437255859375,\n",
              "  459.744873046875,\n",
              "  460.487060546875,\n",
              "  458.6875,\n",
              "  459.6145935058594,\n",
              "  461.8846435546875,\n",
              "  459.8810119628906,\n",
              "  459.0386657714844,\n",
              "  460.158935546875,\n",
              "  461.0155944824219,\n",
              "  461.6458435058594,\n",
              "  462.7119445800781,\n",
              "  462.0762634277344,\n",
              "  461.3645324707031,\n",
              "  459.6145935058594,\n",
              "  454.5430908203125,\n",
              "  462.5769958496094,\n",
              "  461.0802307128906,\n",
              "  461.0547790527344,\n",
              "  461.6458435058594,\n",
              "  460.25927734375,\n",
              "  460.4117126464844,\n",
              "  457.5833435058594,\n",
              "  458.3084411621094,\n",
              "  460.1425476074219,\n",
              "  459.5257263183594,\n",
              "  461.2208251953125,\n",
              "  460.5314636230469,\n",
              "  462.2557678222656,\n",
              "  459.6145935058594,\n",
              "  462.4822692871094,\n",
              "  463.3958435058594,\n",
              "  462.6107482910156,\n",
              "  458.62255859375,\n",
              "  463.3958435058594,\n",
              "  458.7112731933594,\n",
              "  460.969970703125,\n",
              "  460.917724609375,\n",
              "  460.739013671875,\n",
              "  460.213134765625,\n",
              "  461.0,\n",
              "  459.8785400390625,\n",
              "  460.1333923339844,\n",
              "  460.8717346191406,\n",
              "  462.113037109375,\n",
              "  460.3360900878906,\n",
              "  459.6145935058594,\n",
              "  459.72021484375,\n",
              "  459.2335510253906,\n",
              "  458.25244140625,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  460.3538513183594,\n",
              "  457.7850646972656,\n",
              "  459.1447448730469,\n",
              "  457.8849182128906,\n",
              "  461.3645935058594,\n",
              "  457.8793029785156,\n",
              "  459.6864318847656,\n",
              "  461.1283264160156,\n",
              "  461.6458435058594,\n",
              "  459.8798828125,\n",
              "  463.3958435058594,\n",
              "  457.7440490722656,\n",
              "  457.4967956542969,\n",
              "  463.2272033691406,\n",
              "  457.5833435058594,\n",
              "  457.2552490234375,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  459.3721618652344,\n",
              "  457.3286437988281,\n",
              "  457.007080078125,\n",
              "  461.3253173828125,\n",
              "  461.3646240234375,\n",
              "  458.5831604003906,\n",
              "  456.619873046875,\n",
              "  461.7918701171875,\n",
              "  458.6006164550781,\n",
              "  456.8254089355469,\n",
              "  458.8890686035156,\n",
              "  461.0,\n",
              "  461.3645935058594,\n",
              "  463.6962890625,\n",
              "  459.006591796875,\n",
              "  455.9644470214844,\n",
              "  459.9923400878906,\n",
              "  459.2104187011719,\n",
              "  465.8558654785156,\n",
              "  455.546630859375,\n",
              "  458.0909729003906,\n",
              "  459.3333435058594,\n",
              "  456.4886779785156,\n",
              "  457.8531188964844,\n",
              "  454.9895324707031,\n",
              "  458.1618347167969,\n",
              "  461.6458435058594,\n",
              "  459.3333435058594,\n",
              "  465.0508728027344,\n",
              "  460.0895690917969,\n",
              "  457.0208435058594,\n",
              "  458.5700988769531,\n",
              "  455.1625061035156,\n",
              "  461.0000305175781,\n",
              "  459.8797302246094,\n",
              "  456.1560363769531,\n",
              "  463.4276428222656,\n",
              "  457.2195739746094,\n",
              "  458.109130859375,\n",
              "  463.3958435058594,\n",
              "  460.3731689453125,\n",
              "  457.3000183105469,\n",
              "  456.6849365234375,\n",
              "  458.3701171875,\n",
              "  461.0833435058594,\n",
              "  460.4492492675781,\n",
              "  458.1912536621094,\n",
              "  454.6852722167969,\n",
              "  456.3846130371094,\n",
              "  459.8647766113281,\n",
              "  457.8087463378906,\n",
              "  462.6339416503906,\n",
              "  459.3333435058594,\n",
              "  459.0520935058594,\n",
              "  465.0205993652344,\n",
              "  459.9967956542969,\n",
              "  459.9721374511719,\n",
              "  459.66552734375,\n",
              "  461.0568542480469,\n",
              "  455.8345642089844,\n",
              "  455.8083801269531,\n",
              "  467.2269287109375,\n",
              "  461.3645935058594,\n",
              "  455.1700134277344,\n",
              "  459.4070129394531,\n",
              "  461.3645935058594,\n",
              "  457.9751281738281,\n",
              "  459.6145935058594,\n",
              "  453.6393127441406,\n",
              "  461.0833435058594,\n",
              "  465.1351318359375,\n",
              "  457.3020935058594,\n",
              "  457.8271484375,\n",
              "  461.3645935058594,\n",
              "  455.6600036621094,\n",
              "  463.5068054199219,\n",
              "  457.4477233886719,\n",
              "  457.4242858886719,\n",
              "  457.3990783691406,\n",
              "  457.3745422363281,\n",
              "  461.6458435058594,\n",
              "  465.68408203125,\n",
              "  457.5833435058594,\n",
              "  463.419921875,\n",
              "  459.2025146484375,\n",
              "  459.0520324707031,\n",
              "  459.3333435058594,\n",
              "  454.1842346191406,\n",
              "  454.5631408691406,\n",
              "  455.3828430175781,\n",
              "  454.6023864746094,\n",
              "  452.8030090332031,\n",
              "  454.223876953125,\n",
              "  456.7541809082031,\n",
              "  452.6566467285156,\n",
              "  463.7109375,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  454.6048583984375,\n",
              "  459.6145935058594,\n",
              "  459.3332824707031,\n",
              "  456.5621337890625,\n",
              "  461.3645935058594,\n",
              "  451.9460754394531,\n",
              "  453.6502990722656,\n",
              "  461.3645935058594,\n",
              "  459.0382385253906,\n",
              "  462.2447204589844,\n",
              "  454.3668518066406,\n",
              "  456.373779296875,\n",
              "  454.319091796875,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  453.9205627441406,\n",
              "  462.1768493652344,\n",
              "  456.1861267089844,\n",
              "  452.99365234375,\n",
              "  464.2770690917969,\n",
              "  459.0520935058594,\n",
              "  457.3020935058594,\n",
              "  461.3645935058594,\n",
              "  458.0802307128906,\n",
              "  458.0567321777344,\n",
              "  456.282470703125,\n",
              "  458.0105895996094,\n",
              "  469.86572265625,\n",
              "  455.6510009765625,\n",
              "  454.1611328125,\n",
              "  461.0833435058594,\n",
              "  462.8001403808594,\n",
              "  454.0952453613281,\n",
              "  459.6145935058594,\n",
              "  456.0804748535156,\n",
              "  461.6458435058594,\n",
              "  455.7532043457031,\n",
              "  452.4104919433594,\n",
              "  467.0209045410156,\n",
              "  452.8788146972656,\n",
              "  460.9705505371094,\n",
              "  458.2358703613281,\n",
              "  451.9068603515625,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  456.1153869628906,\n",
              "  453.2156677246094,\n",
              "  459.3333435058594,\n",
              "  464.931640625,\n",
              "  461.3645935058594,\n",
              "  465.2584533691406,\n",
              "  455.6993408203125,\n",
              "  459.6145935058594,\n",
              "  459.0520935058594,\n",
              "  463.315673828125,\n",
              "  451.3253479003906,\n",
              "  457.0572204589844,\n",
              "  459.3333435058594,\n",
              "  452.9534606933594,\n",
              "  453.211669921875,\n",
              "  451.39404296875,\n",
              "  455.2008361816406,\n",
              "  453.1484375,\n",
              "  457.3020935058594,\n",
              "  449.4656982421875,\n",
              "  451.4562072753906,\n",
              "  451.1310729980469,\n",
              "  454.7062072753906,\n",
              "  450.7672424316406,\n",
              "  459.6145935058594,\n",
              "  463.6770935058594,\n",
              "  450.9156799316406,\n",
              "  463.0586242675781,\n",
              "  463.7258605957031,\n",
              "  455.4817199707031,\n",
              "  459.6145935058594,\n",
              "  456.6283874511719,\n",
              "  448.3465270996094,\n",
              "  461.0833435058594,\n",
              "  455.0965270996094,\n",
              "  447.9424133300781,\n",
              "  463.1145935058594,\n",
              "  454.4744567871094,\n",
              "  456.766357421875,\n",
              "  454.9963684082031,\n",
              "  454.9753112792969,\n",
              "  454.9571838378906,\n",
              "  457.3020935058594,\n",
              "  455.1999816894531,\n",
              "  461.0833435058594,\n",
              "  456.26220703125,\n",
              "  459.3333740234375,\n",
              "  448.3089294433594,\n",
              "  461.3645935058594,\n",
              "  468.212890625,\n",
              "  472.7915954589844,\n",
              "  461.3645935058594,\n",
              "  452.7076721191406,\n",
              "  456.189453125,\n",
              "  447.4744567871094,\n",
              "  450.027587890625,\n",
              "  454.6621398925781,\n",
              "  454.6418762207031,\n",
              "  459.3333435058594,\n",
              "  464.3456726074219,\n",
              "  457.0208435058594,\n",
              "  459.0520935058594,\n",
              "  452.51513671875,\n",
              "  463.3958435058594,\n",
              "  459.0520935058594,\n",
              "  459.0520935058594,\n",
              "  461.0833435058594,\n",
              "  454.44873046875,\n",
              "  461.0,\n",
              "  442.2528076171875,\n",
              "  459.0520935058594,\n",
              "  454.3752136230469,\n",
              "  461.3645935058594,\n",
              "  454.3353271484375,\n",
              "  449.0257568359375,\n",
              "  448.986083984375,\n",
              "  461.0833435058594,\n",
              "  463.3958435058594,\n",
              "  449.1615905761719,\n",
              "  459.6146240234375,\n",
              "  453.9285583496094,\n",
              "  454.1932067871094,\n",
              "  452.4252014160156,\n",
              "  466.539794921875,\n",
              "  466.8364562988281,\n",
              "  464.825439453125,\n",
              "  463.1145935058594,\n",
              "  448.2745056152344,\n",
              "  459.3333740234375,\n",
              "  452.3006896972656,\n",
              "  459.0520935058594,\n",
              "  446.6640930175781,\n",
              "  453.7185974121094,\n",
              "  466.9985046386719,\n",
              "  459.6146240234375,\n",
              "  452.1960144042969,\n",
              "  453.9302062988281,\n",
              "  451.5997619628906,\n",
              "  448.4593811035156,\n",
              "  463.1145935058594,\n",
              "  467.1198425292969,\n",
              "  446.0453186035156,\n",
              "  466.8707580566406,\n",
              "  451.7803955078125,\n",
              "  461.0833740234375,\n",
              "  456.0900573730469,\n",
              "  445.5970764160156,\n",
              "  459.6145935058594,\n",
              "  447.8447265625,\n",
              "  459.0520935058594,\n",
              "  451.3823547363281,\n",
              "  455.4267272949219,\n",
              "  447.428955078125,\n",
              "  455.9598083496094,\n",
              "  453.3484802246094,\n",
              "  447.3297424316406,\n",
              "  453.317138671875,\n",
              "  463.3958435058594,\n",
              "  453.0016784667969,\n",
              "  445.4608154296875,\n",
              "  473.2403259277344,\n",
              "  463.03125,\n",
              "  453.2218017578125,\n",
              "  463.4293212890625,\n",
              "  467.5084533691406,\n",
              "  461.6458435058594,\n",
              "  454.9085388183594,\n",
              "  455.1736755371094,\n",
              "  463.3958435058594,\n",
              "  459.6145935058594,\n",
              "  446.8661193847656,\n",
              "  447.1186218261719,\n",
              "  463.6770935058594,\n",
              "  461.3645935058594,\n",
              "  461.6458435058594,\n",
              "  450.7145080566406,\n",
              "  453.2925720214844,\n",
              "  444.6327209472656,\n",
              "  461.6458435058594,\n",
              "  469.7599792480469,\n",
              "  453.2364196777344,\n",
              "  444.7984313964844,\n",
              "  458.96875,\n",
              "  459.3333435058594,\n",
              "  447.021240234375,\n",
              "  444.9605407714844,\n",
              "  459.3333435058594,\n",
              "  452.8563232421875,\n",
              "  461.0833435058594,\n",
              "  453.10986328125,\n",
              "  453.099365234375,\n",
              "  444.5234375,\n",
              "  450.7594299316406,\n",
              "  452.7783203125,\n",
              "  438.1567687988281,\n",
              "  457.0208435058594,\n",
              "  452.4555969238281,\n",
              "  459.6145935058594,\n",
              "  469.7379150390625,\n",
              "  450.94580078125,\n",
              "  463.671142578125,\n",
              "  470.0580749511719,\n",
              "  445.98095703125,\n",
              "  467.7724609375,\n",
              "  461.1623840332031,\n",
              "  461.3529357910156,\n",
              "  446.5925598144531,\n",
              "  458.0398254394531,\n",
              "  460.659912109375,\n",
              "  462.6459045410156,\n",
              "  465.183349609375,\n",
              "  461.8150329589844,\n",
              "  455.0185546875,\n",
              "  455.3396911621094,\n",
              "  462.455078125,\n",
              "  449.1170349121094,\n",
              "  464.948486328125,\n",
              "  447.3923034667969,\n",
              "  456.4071044921875,\n",
              "  472.5777282714844,\n",
              "  450.9075012207031,\n",
              "  471.0013732910156,\n",
              "  451.0669860839844,\n",
              "  466.7308044433594,\n",
              "  458.44287109375,\n",
              "  465.3079528808594,\n",
              "  474.1947021484375,\n",
              "  450.4833984375,\n",
              "  457.9849548339844,\n",
              "  460.0561218261719,\n",
              "  473.506591796875,\n",
              "  452.986572265625,\n",
              "  466.9242248535156,\n",
              "  469.424560546875,\n",
              "  460.6777038574219,\n",
              "  460.9035339355469,\n",
              "  483.7603454589844,\n",
              "  470.16259765625,\n",
              "  472.2657775878906,\n",
              "  452.8390808105469,\n",
              "  455.5541687011719,\n",
              "  462.6170349121094,\n",
              "  455.7091979980469,\n",
              "  480.2392578125,\n",
              "  455.8679504394531,\n",
              "  472.2500915527344,\n",
              "  463.7347412109375,\n",
              "  457.0367126464844,\n",
              "  455.4769592285156,\n",
              "  462.5784606933594,\n",
              "  464.6204833984375,\n",
              "  463.5946350097656,\n",
              "  479.1812744140625,\n",
              "  465.267822265625,\n",
              "  458.18017578125,\n",
              "  475.5279846191406,\n",
              "  480.0184631347656,\n",
              "  465.7475891113281,\n",
              "  473.5009765625,\n",
              "  474.9019470214844,\n",
              "  475.0830078125,\n",
              "  483.7385559082031,\n",
              "  458.2950134277344,\n",
              "  451.4971618652344,\n",
              "  467.1752624511719,\n",
              "  482.0780334472656,\n",
              "  451.792724609375,\n",
              "  467.7172546386719,\n",
              "  460.8888854980469,\n",
              "  468.6634826660156,\n",
              "  475.682373046875,\n",
              "  452.646728515625,\n",
              "  477.3828125,\n",
              "  476.3263854980469,\n",
              "  478.1537170410156,\n",
              "  462.0572509765625,\n",
              "  468.0704040527344,\n",
              "  469.5749816894531,\n",
              "  471.1053466796875,\n",
              "  485.9093322753906,\n",
              "  461.4418640136719,\n",
              "  463.2109375,\n",
              "  488.3877868652344,\n",
              "  469.7983703613281,\n",
              "  478.1280822753906,\n",
              "  479.8542785644531,\n",
              "  479.7243347167969,\n",
              "  463.2709045410156,\n",
              "  479.3907470703125,\n",
              "  479.2919921875,\n",
              "  464.1014709472656,\n",
              "  481.1763916015625,\n",
              "  489.4312438964844,\n",
              "  481.1314392089844,\n",
              "  480.4488525390625,\n",
              "  481.381591796875,\n",
              "  465.298828125,\n",
              "  480.326904296875,\n",
              "  473.5508728027344,\n",
              "  472.5492248535156,\n",
              "  481.435791015625,\n",
              "  481.3388366699219,\n",
              "  489.015625,\n",
              "  474.16259765625,\n",
              "  465.3255615234375,\n",
              "  480.6894836425781,\n",
              "  481.8983459472656,\n",
              "  467.314453125,\n",
              "  473.1401062011719,\n",
              "  490.7582092285156,\n",
              "  475.0699768066406,\n",
              "  500.8530578613281,\n",
              "  483.7422790527344,\n",
              "  476.4851989746094,\n",
              "  475.8939514160156,\n",
              "  485.87109375,\n",
              "  468.2238464355469,\n",
              "  468.0272216796875,\n",
              "  475.871826171875,\n",
              "  491.1609191894531,\n",
              "  484.624755859375,\n",
              "  476.315185546875,\n",
              "  484.17529296875,\n",
              "  483.9764099121094,\n",
              "  476.0296936035156,\n",
              "  485.8453674316406,\n",
              "  493.19189453125,\n",
              "  476.1321105957031,\n",
              "  477.3052673339844,\n",
              "  493.832275390625,\n",
              "  486.229248046875,\n",
              "  493.1047058105469,\n",
              "  502.6561584472656,\n",
              "  477.7994384765625,\n",
              "  485.6848449707031,\n",
              "  478.9908447265625,\n",
              "  486.1103820800781,\n",
              "  487.1532897949219,\n",
              "  478.0187072753906,\n",
              "  471.1687927246094,\n",
              "  487.890625,\n",
              "  478.4216613769531,\n",
              "  471.197998046875,\n",
              "  480.352294921875,\n",
              "  487.7373962402344,\n",
              "  495.6683044433594,\n",
              "  478.6632995605469,\n",
              "  470.899658203125,\n",
              "  479.4672546386719,\n",
              "  480.4396057128906,\n",
              "  487.4709167480469,\n",
              "  480.0816955566406,\n",
              "  479.0177917480469,\n",
              "  480.8798522949219,\n",
              "  472.8133239746094,\n",
              "  487.2197265625,\n",
              "  471.5479736328125,\n",
              "  479.7782287597656,\n",
              "  488.0260009765625,\n",
              "  472.6383361816406,\n",
              "  472.2464294433594,\n",
              "  481.9307556152344,\n",
              "  498.112548828125,\n",
              "  481.2590637207031,\n",
              "  473.83349609375,\n",
              "  481.1951599121094,\n",
              "  473.978271484375,\n",
              "  480.8880920410156,\n",
              "  505.7433166503906,\n",
              "  490.8816833496094,\n",
              "  489.5821228027344,\n",
              "  490.4693908691406,\n",
              "  473.693115234375,\n",
              "  490.1369934082031,\n",
              "  489.932373046875,\n",
              "  473.9764709472656,\n",
              "  490.1534729003906,\n",
              "  474.5816345214844,\n",
              "  482.678466796875,\n",
              "  491.9160461425781,\n",
              "  482.7967834472656,\n",
              "  498.614501953125,\n",
              "  491.0025939941406,\n",
              "  499.2914123535156,\n",
              "  499.6522521972656,\n",
              "  483.0018005371094,\n",
              "  483.5685119628906,\n",
              "  483.1710510253906,\n",
              "  475.3013610839844,\n",
              "  500.538330078125,\n",
              "  483.844970703125,\n",
              "  466.8040771484375,\n",
              "  482.6576232910156,\n",
              "  490.9909973144531,\n",
              "  492.09814453125,\n",
              "  484.1056213378906,\n",
              "  483.5024108886719,\n",
              "  500.4434509277344,\n",
              "  476.0126037597656,\n",
              "  493.1000061035156,\n",
              "  483.873046875,\n",
              "  492.3352966308594,\n",
              "  501.5995178222656,\n",
              "  492.478271484375,\n",
              "  483.5089416503906,\n",
              "  484.837890625,\n",
              "  493.3394775390625,\n",
              "  484.4165954589844,\n",
              "  500.0082092285156,\n",
              "  485.8766784667969,\n",
              "  476.3001403808594,\n",
              "  486.0975341796875,\n",
              "  511.1270446777344,\n",
              "  494.3360900878906,\n",
              "  485.1265563964844,\n",
              "  485.7444763183594,\n",
              "  486.1445617675781,\n",
              "  486.5439147949219,\n",
              "  477.5210876464844,\n",
              "  502.240966796875,\n",
              "  486.0198059082031,\n",
              "  485.4983825683594,\n",
              "  493.0324401855469,\n",
              "  494.9456787109375,\n",
              "  494.3908386230469,\n",
              "  469.1990661621094,\n",
              "  487.2275390625,\n",
              "  495.372802734375,\n",
              "  485.5561218261719,\n",
              "  495.802978515625,\n",
              "  494.3924560546875,\n",
              "  486.685302734375,\n",
              "  502.4388732910156,\n",
              "  495.83935546875,\n",
              "  486.2447204589844,\n",
              "  494.730224609375,\n",
              "  494.8349304199219,\n",
              "  503.579833984375,\n",
              "  494.8794860839844,\n",
              "  511.610595703125,\n",
              "  487.8349914550781,\n",
              "  486.0697937011719,\n",
              "  503.9225769042969,\n",
              "  496.0662536621094,\n",
              "  470.7715759277344]}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "7b51aa58-2996-47d7-bda1-6eeac2262257"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/regression.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/regression.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f63331ce-64f9-48a0-e24a-296b7ea390ab"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80fae4dd-d957-4a49-b625-b6f472e9b37e\", \"regression.h5\", 16624120)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrain"
      ],
      "metadata": {
        "id": "owI9dHky6eHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/new/regression.h5')\n",
        "\n",
        "# from efficientnet.layers import Swish, DropConnect\n",
        "# from efficientnet.model import ConvKernalInitializer\n",
        "# from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "# get_custom_objects().update({\n",
        "#     'ConvKernalInitializer': ConvKernalInitializer,\n",
        "#     'Swish': Swish,\n",
        "#     'DropConnect':DropConnect\n",
        "# })"
      ],
      "metadata": {
        "id": "bNX5VLJw54yc"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load model \n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('/content/drive/My Drive/new/regression.h5')\n",
        "# height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "zF2vlFE_54wK"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # set 'multiply_16' and following layers trainable (Unfreeze --> multiply_16 ) ให้เป็น layers ที่ train ชุดข้อมูลใหม่\n",
        "# model.trainable = True\n",
        "\n",
        "# set_trainable = False\n",
        "# for layer in conv_base.layers:\n",
        "#     if layer.name == 'multiply_15':\n",
        "#         set_trainable = True\n",
        "#     if set_trainable:\n",
        "#         layer.trainable = True\n",
        "#     else:\n",
        "#         layer.trainable = False\n",
        "# print('This is the number of trainable layers '\n",
        "#       'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "1OsX50Tk54tx"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "pzptFLAz54rW"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss='mse',\n",
        "#               optimizer=Adam(learning_rate=2e-1),\n",
        "#               metrics=['mae'])\n",
        "# history = model.fit_generator(\n",
        "#       train_generator,\n",
        "#       steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "#       epochs=epochs,\n",
        "#       validation_data=validation_generator,\n",
        "#       validation_steps= NUM_TEST //batch_size,\n",
        "#       verbose=1,\n",
        "#       use_multiprocessing=True,\n",
        "#       workers=4)"
      ],
      "metadata": {
        "id": "5os97-2Q54o2"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "# mae = history.history['mae']\n",
        "# val_mae = history.history['val_mae']\n",
        "\n",
        "# epochs_x = range(len(loss))\n",
        "\n",
        "# plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "# plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "# plt.title('Training and Mean squared error')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "# plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Z-9GR-Qp6lE3"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW_313T26lCd"
      },
      "execution_count": 114,
      "outputs": []
    }
  ]
}