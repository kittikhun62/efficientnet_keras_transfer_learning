{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPMaBS24SmOzgCBQZoM47ke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "6e461f42-9b34-49aa-e46d-ae40da935428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "e9ade157-d8f3-4757-9dc1-1fd5ae802277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87b1a728-ec3d-4d91-a8d2-c47bc6f6b020\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87b1a728-ec3d-4d91-a8d2-c47bc6f6b020')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87b1a728-ec3d-4d91-a8d2-c47bc6f6b020 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87b1a728-ec3d-4d91-a8d2-c47bc6f6b020');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "f3766713-9a4a-44fd-f79b-6b39c81f9728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "48c99386-0759-497f-d8a4-aeb750a197ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "9ced6de0-b057-4e44-96b4-2369b60e7b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "29a4cd94-efbd-4161-a5ff-bd60e171593c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "d5185a91-657d-4148-e809-045daadf07a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b59160-c8bc-4284-8b4b-c23fb49acb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "473fedd8-770a-47d8-ce1e-16b6e368049d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "2a663963-1eee-4f53-e423-3ce72432d307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "8461c75b-0f49-4a35-f9e1-03c761a7e193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "51a6322c-fff9-45f0-e05a-db155632bc33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(lr=2e-5),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f319450-778b-46ea-92e7-e34ef0506c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "<ipython-input-27-5a61ec28a7f4>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 102s 2s/step - loss: 1.5417 - mse: 1.5417 - val_loss: 1.4427 - val_mse: 1.4427\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1.3623 - mse: 1.3623 - val_loss: 1.2926 - val_mse: 1.2926\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1.2126 - mse: 1.2126 - val_loss: 1.1988 - val_mse: 1.1988\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1.0945 - mse: 1.0945 - val_loss: 1.0364 - val_mse: 1.0364\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.9755 - mse: 0.9755 - val_loss: 0.9442 - val_mse: 0.9442\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.8738 - mse: 0.8738 - val_loss: 0.8149 - val_mse: 0.8149\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.7832 - mse: 0.7832 - val_loss: 0.7888 - val_mse: 0.7888\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.7088 - mse: 0.7088 - val_loss: 0.6692 - val_mse: 0.6692\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.6348 - mse: 0.6348 - val_loss: 0.6161 - val_mse: 0.6161\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 0.5734 - mse: 0.5734 - val_loss: 0.5571 - val_mse: 0.5571\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.5249 - mse: 0.5249 - val_loss: 0.5068 - val_mse: 0.5068\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.4700 - mse: 0.4700 - val_loss: 0.4545 - val_mse: 0.4545\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.4348 - mse: 0.4348 - val_loss: 0.4095 - val_mse: 0.4095\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.4015 - mse: 0.4015 - val_loss: 0.4095 - val_mse: 0.4095\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.3752 - mse: 0.3752 - val_loss: 0.3540 - val_mse: 0.3540\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.3492 - mse: 0.3492 - val_loss: 0.3455 - val_mse: 0.3455\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.3257 - mse: 0.3257 - val_loss: 0.3226 - val_mse: 0.3226\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.3149 - mse: 0.3149 - val_loss: 0.3074 - val_mse: 0.3074\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.3014 - mse: 0.3014 - val_loss: 0.2950 - val_mse: 0.2950\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.2892 - val_mse: 0.2892\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2800 - mse: 0.2800 - val_loss: 0.2842 - val_mse: 0.2842\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2732 - mse: 0.2732 - val_loss: 0.2741 - val_mse: 0.2741\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2672 - mse: 0.2672 - val_loss: 0.2688 - val_mse: 0.2688\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2615 - mse: 0.2615 - val_loss: 0.2649 - val_mse: 0.2649\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2593 - mse: 0.2593 - val_loss: 0.2618 - val_mse: 0.2618\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2576 - mse: 0.2576 - val_loss: 0.2537 - val_mse: 0.2537\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 0.2564 - mse: 0.2564 - val_loss: 0.2570 - val_mse: 0.2570\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2568 - val_mse: 0.2568\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2529 - mse: 0.2529 - val_loss: 0.2518 - val_mse: 0.2518\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2526 - mse: 0.2526 - val_loss: 0.2531 - val_mse: 0.2531\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.2525 - val_mse: 0.2525\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2517 - val_mse: 0.2517\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2510 - mse: 0.2510 - val_loss: 0.2507 - val_mse: 0.2507\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2508 - mse: 0.2508 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2508 - val_mse: 0.2508\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2498 - val_mse: 0.2498\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 4s 75ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2498 - val_mse: 0.2498\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.2506 - mse: 0.2506 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2507 - mse: 0.2507 - val_loss: 0.2497 - val_mse: 0.2497\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.2516 - mse: 0.2516 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2498 - val_mse: 0.2498\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2497 - val_mse: 0.2497\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2498 - val_mse: 0.2498\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 3s 87ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2506 - val_mse: 0.2506\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2498 - val_mse: 0.2498\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2505 - val_mse: 0.2505\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2497 - val_mse: 0.2497\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2502 - val_mse: 0.2502\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2499 - val_mse: 0.2499\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2503 - val_mse: 0.2503\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2504 - val_mse: 0.2504\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2501 - val_mse: 0.2501\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2500 - val_mse: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "5f092428-c3de-4179-da5d-2f74ad3381b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.541745901107788,\n",
              "  1.3623071908950806,\n",
              "  1.2125654220581055,\n",
              "  1.094454050064087,\n",
              "  0.97553950548172,\n",
              "  0.8737543821334839,\n",
              "  0.7831557989120483,\n",
              "  0.708800196647644,\n",
              "  0.6348017454147339,\n",
              "  0.5733885765075684,\n",
              "  0.524944007396698,\n",
              "  0.47004520893096924,\n",
              "  0.4347764253616333,\n",
              "  0.4015086591243744,\n",
              "  0.37517473101615906,\n",
              "  0.34921395778656006,\n",
              "  0.32566261291503906,\n",
              "  0.31494152545928955,\n",
              "  0.301399290561676,\n",
              "  0.288849413394928,\n",
              "  0.2800367474555969,\n",
              "  0.2732323110103607,\n",
              "  0.2672470211982727,\n",
              "  0.2614997923374176,\n",
              "  0.25932517647743225,\n",
              "  0.25760573148727417,\n",
              "  0.2563563883304596,\n",
              "  0.2544625401496887,\n",
              "  0.2528984248638153,\n",
              "  0.2525680661201477,\n",
              "  0.25170525908470154,\n",
              "  0.25123801827430725,\n",
              "  0.2510283291339874,\n",
              "  0.2507885992527008,\n",
              "  0.2503601908683777,\n",
              "  0.25016775727272034,\n",
              "  0.2501104474067688,\n",
              "  0.2500673532485962,\n",
              "  0.24995705485343933,\n",
              "  0.2501262426376343,\n",
              "  0.25007134675979614,\n",
              "  0.250211238861084,\n",
              "  0.25003573298454285,\n",
              "  0.2501140534877777,\n",
              "  0.25004473328590393,\n",
              "  0.2500315010547638,\n",
              "  0.2500482201576233,\n",
              "  0.250055193901062,\n",
              "  0.25006118416786194,\n",
              "  0.25017863512039185,\n",
              "  0.24999286234378815,\n",
              "  0.2501259446144104,\n",
              "  0.25004953145980835,\n",
              "  0.25010251998901367,\n",
              "  0.25014984607696533,\n",
              "  0.25006920099258423,\n",
              "  0.250059574842453,\n",
              "  0.24996769428253174,\n",
              "  0.250122606754303,\n",
              "  0.2501140534877777,\n",
              "  0.2500948905944824,\n",
              "  0.2502976953983307,\n",
              "  0.2500050663948059,\n",
              "  0.2500542104244232,\n",
              "  0.25012099742889404,\n",
              "  0.2501237392425537,\n",
              "  0.25010672211647034,\n",
              "  0.25010430812835693,\n",
              "  0.25003039836883545,\n",
              "  0.2501261234283447,\n",
              "  0.25008919835090637,\n",
              "  0.2500503659248352,\n",
              "  0.25004085898399353,\n",
              "  0.2500515282154083,\n",
              "  0.2500174343585968,\n",
              "  0.25015345215797424,\n",
              "  0.25005483627319336,\n",
              "  0.2501334547996521,\n",
              "  0.25019869208335876,\n",
              "  0.2500392198562622,\n",
              "  0.25003886222839355,\n",
              "  0.2501303553581238,\n",
              "  0.2500024437904358,\n",
              "  0.2500269412994385,\n",
              "  0.2499668300151825,\n",
              "  0.2502528429031372,\n",
              "  0.25007888674736023,\n",
              "  0.2499919980764389,\n",
              "  0.25001734495162964,\n",
              "  0.2501715421676636,\n",
              "  0.250091016292572,\n",
              "  0.2501472532749176,\n",
              "  0.25009217858314514,\n",
              "  0.25010308623313904,\n",
              "  0.25018224120140076,\n",
              "  0.2500994801521301,\n",
              "  0.25034812092781067,\n",
              "  0.24996866285800934,\n",
              "  0.2500978410243988,\n",
              "  0.2500326633453369,\n",
              "  0.25005507469177246,\n",
              "  0.2501162588596344,\n",
              "  0.25013843178749084,\n",
              "  0.25012004375457764,\n",
              "  0.25004473328590393,\n",
              "  0.2501831650733948,\n",
              "  0.2500150501728058,\n",
              "  0.25001513957977295,\n",
              "  0.250162273645401,\n",
              "  0.2500472664833069,\n",
              "  0.2500072419643402,\n",
              "  0.25006136298179626,\n",
              "  0.2500969469547272,\n",
              "  0.2501053512096405,\n",
              "  0.25027161836624146,\n",
              "  0.25001072883605957,\n",
              "  0.25022485852241516,\n",
              "  0.25040140748023987,\n",
              "  0.250114768743515,\n",
              "  0.2500709593296051,\n",
              "  0.2500821650028229,\n",
              "  0.25005143880844116,\n",
              "  0.25013673305511475,\n",
              "  0.25007808208465576,\n",
              "  0.25033798813819885,\n",
              "  0.2501974105834961,\n",
              "  0.2502971887588501,\n",
              "  0.2502680718898773,\n",
              "  0.25056079030036926,\n",
              "  0.25022315979003906,\n",
              "  0.2500985860824585,\n",
              "  0.2500549554824829,\n",
              "  0.25017884373664856,\n",
              "  0.25011080503463745,\n",
              "  0.2501145601272583,\n",
              "  0.2501799464225769,\n",
              "  0.2500793933868408,\n",
              "  0.2501493990421295,\n",
              "  0.25010889768600464,\n",
              "  0.2500879466533661,\n",
              "  0.25001630187034607,\n",
              "  0.2500138580799103,\n",
              "  0.250340074300766,\n",
              "  0.2501693367958069,\n",
              "  0.25008729100227356,\n",
              "  0.25010743737220764,\n",
              "  0.2502152919769287,\n",
              "  0.2500769793987274,\n",
              "  0.2501026391983032,\n",
              "  0.2500789165496826,\n",
              "  0.25006240606307983,\n",
              "  0.250139445066452,\n",
              "  0.25010690093040466,\n",
              "  0.25007814168930054,\n",
              "  0.25016340613365173,\n",
              "  0.24999001622200012,\n",
              "  0.25006526708602905,\n",
              "  0.25003504753112793,\n",
              "  0.25010234117507935,\n",
              "  0.250070720911026,\n",
              "  0.2502216398715973,\n",
              "  0.25007686018943787,\n",
              "  0.2500605583190918,\n",
              "  0.2500964403152466,\n",
              "  0.25002801418304443,\n",
              "  0.2501353323459625,\n",
              "  0.2500784695148468,\n",
              "  0.25008878111839294,\n",
              "  0.2500663101673126,\n",
              "  0.250023752450943,\n",
              "  0.2501750588417053,\n",
              "  0.2502270042896271,\n",
              "  0.2504258155822754,\n",
              "  0.25018537044525146,\n",
              "  0.25007879734039307,\n",
              "  0.2501092255115509,\n",
              "  0.2507386803627014,\n",
              "  0.2500968277454376,\n",
              "  0.2501363754272461,\n",
              "  0.250073105096817,\n",
              "  0.2501313090324402,\n",
              "  0.2516373097896576,\n",
              "  0.2501392364501953,\n",
              "  0.25012344121932983,\n",
              "  0.2501594126224518,\n",
              "  0.2502904534339905,\n",
              "  0.2501826286315918,\n",
              "  0.2500404417514801,\n",
              "  0.2501738369464874,\n",
              "  0.25005918741226196,\n",
              "  0.2501491606235504,\n",
              "  0.25012367963790894,\n",
              "  0.2501644194126129,\n",
              "  0.25010228157043457,\n",
              "  0.2501654326915741,\n",
              "  0.2502239942550659,\n",
              "  0.25034061074256897,\n",
              "  0.25007954239845276,\n",
              "  0.25005972385406494,\n",
              "  0.25015684962272644,\n",
              "  0.25031769275665283,\n",
              "  0.2501300275325775,\n",
              "  0.25005805492401123,\n",
              "  0.2500065565109253,\n",
              "  0.2501809895038605,\n",
              "  0.25008872151374817,\n",
              "  0.2503244876861572,\n",
              "  0.2501389682292938,\n",
              "  0.2501034140586853,\n",
              "  0.2501429617404938,\n",
              "  0.25013330578804016,\n",
              "  0.25011950731277466,\n",
              "  0.25012218952178955,\n",
              "  0.25015830993652344,\n",
              "  0.2501581609249115,\n",
              "  0.2500849664211273,\n",
              "  0.250422865152359,\n",
              "  0.25008484721183777,\n",
              "  0.2500624656677246,\n",
              "  0.25005200505256653,\n",
              "  0.25044965744018555,\n",
              "  0.2500540018081665,\n",
              "  0.2504814863204956,\n",
              "  0.2503539025783539,\n",
              "  0.25012117624282837,\n",
              "  0.2501533329486847,\n",
              "  0.2500858008861542,\n",
              "  0.2502955198287964,\n",
              "  0.2501640319824219,\n",
              "  0.2502414286136627,\n",
              "  0.2502082884311676,\n",
              "  0.2501283884048462,\n",
              "  0.25018805265426636,\n",
              "  0.25026044249534607,\n",
              "  0.2501277029514313,\n",
              "  0.2500201463699341,\n",
              "  0.2500669062137604,\n",
              "  0.2500397861003876,\n",
              "  0.2501113712787628,\n",
              "  0.250098317861557,\n",
              "  0.25010934472084045,\n",
              "  0.25016406178474426,\n",
              "  0.2500724196434021,\n",
              "  0.2500573694705963,\n",
              "  0.25007402896881104,\n",
              "  0.2500348389148712,\n",
              "  0.2503584325313568,\n",
              "  0.25018489360809326,\n",
              "  0.2500689625740051,\n",
              "  0.25004804134368896,\n",
              "  0.2501338720321655,\n",
              "  0.2502387762069702,\n",
              "  0.2500459849834442,\n",
              "  0.25022146105766296,\n",
              "  0.25012752413749695,\n",
              "  0.25003302097320557,\n",
              "  0.2502077519893646,\n",
              "  0.2501537799835205,\n",
              "  0.25013962388038635,\n",
              "  0.2501363754272461,\n",
              "  0.25040292739868164,\n",
              "  0.25033870339393616,\n",
              "  0.25017616152763367,\n",
              "  0.250213086605072,\n",
              "  0.25015243887901306,\n",
              "  0.2502235174179077,\n",
              "  0.2501240670681,\n",
              "  0.25004979968070984,\n",
              "  0.2504848837852478,\n",
              "  0.2500464618206024,\n",
              "  0.25023528933525085,\n",
              "  0.2503400146961212,\n",
              "  0.2501349151134491,\n",
              "  0.2500784993171692,\n",
              "  0.25023606419563293,\n",
              "  0.25014546513557434,\n",
              "  0.250095933675766,\n",
              "  0.25016120076179504,\n",
              "  0.25021037459373474,\n",
              "  0.25023123621940613,\n",
              "  0.25003671646118164,\n",
              "  0.25018513202667236,\n",
              "  0.25016242265701294,\n",
              "  0.2501656413078308,\n",
              "  0.25000810623168945,\n",
              "  0.25022372603416443,\n",
              "  0.2500757575035095,\n",
              "  0.2501063644886017,\n",
              "  0.25022757053375244,\n",
              "  0.2504235804080963,\n",
              "  0.25014838576316833,\n",
              "  0.2501976490020752,\n",
              "  0.25008469820022583,\n",
              "  0.2500946819782257,\n",
              "  0.2502457797527313,\n",
              "  0.2502368092536926,\n",
              "  0.2502102553844452,\n",
              "  0.25012555718421936,\n",
              "  0.25003689527511597,\n",
              "  0.2500339448451996,\n",
              "  0.2503330707550049,\n",
              "  0.250140517950058,\n",
              "  0.2500935196876526,\n",
              "  0.25013914704322815,\n",
              "  0.2500746548175812,\n",
              "  0.2500598132610321,\n",
              "  0.25049498677253723,\n",
              "  0.25032737851142883,\n",
              "  0.25026610493659973,\n",
              "  0.2503124177455902,\n",
              "  0.24997779726982117,\n",
              "  0.2502951920032501,\n",
              "  0.2499644011259079,\n",
              "  0.25012943148612976,\n",
              "  0.25016599893569946,\n",
              "  0.25023141503334045,\n",
              "  0.2501707375049591,\n",
              "  0.2501201927661896,\n",
              "  0.25008052587509155,\n",
              "  0.2500937581062317,\n",
              "  0.2500164806842804,\n",
              "  0.2501475512981415,\n",
              "  0.25017380714416504,\n",
              "  0.25011828541755676,\n",
              "  0.25009146332740784,\n",
              "  0.25013238191604614,\n",
              "  0.2501477003097534,\n",
              "  0.2501581013202667,\n",
              "  0.25006163120269775,\n",
              "  0.2500929832458496,\n",
              "  0.2502012848854065,\n",
              "  0.2502857744693756,\n",
              "  0.25019511580467224,\n",
              "  0.25002679228782654,\n",
              "  0.25023338198661804,\n",
              "  0.25005272030830383,\n",
              "  0.2502848505973816,\n",
              "  0.25027158856391907,\n",
              "  0.2501122057437897,\n",
              "  0.2499990016222,\n",
              "  0.2500671446323395,\n",
              "  0.2502007782459259,\n",
              "  0.2503858506679535,\n",
              "  0.2501746416091919,\n",
              "  0.2501989006996155,\n",
              "  0.25004005432128906,\n",
              "  0.25003582239151,\n",
              "  0.25020384788513184,\n",
              "  0.25012364983558655,\n",
              "  0.25003713369369507,\n",
              "  0.25003960728645325,\n",
              "  0.2501639127731323,\n",
              "  0.250081330537796,\n",
              "  0.2500641345977783,\n",
              "  0.25018006563186646,\n",
              "  0.250171422958374,\n",
              "  0.2501488924026489,\n",
              "  0.2500753402709961,\n",
              "  0.25009438395500183,\n",
              "  0.2500480115413666,\n",
              "  0.25014355778694153,\n",
              "  0.2502824664115906,\n",
              "  0.25014781951904297,\n",
              "  0.2500723600387573,\n",
              "  0.2500961720943451,\n",
              "  0.2501162588596344,\n",
              "  0.2500402629375458,\n",
              "  0.2501685619354248,\n",
              "  0.25025346875190735,\n",
              "  0.25017210841178894,\n",
              "  0.25021815299987793,\n",
              "  0.25016674399375916,\n",
              "  0.2500859498977661,\n",
              "  0.2502081096172333,\n",
              "  0.2502944767475128,\n",
              "  0.25003740191459656,\n",
              "  0.2502294182777405,\n",
              "  0.2502370774745941,\n",
              "  0.25042495131492615,\n",
              "  0.25035884976387024,\n",
              "  0.25011345744132996,\n",
              "  0.25027427077293396,\n",
              "  0.2501586973667145,\n",
              "  0.2499973028898239,\n",
              "  0.2501453459262848,\n",
              "  0.25015464425086975,\n",
              "  0.2500559985637665,\n",
              "  0.25008273124694824,\n",
              "  0.2502179741859436,\n",
              "  0.2500606179237366,\n",
              "  0.25011324882507324,\n",
              "  0.2505384087562561,\n",
              "  0.25007641315460205,\n",
              "  0.2500978112220764,\n",
              "  0.2502054274082184,\n",
              "  0.2501600384712219,\n",
              "  0.2500389516353607,\n",
              "  0.250122994184494,\n",
              "  0.25012195110321045,\n",
              "  0.25006523728370667,\n",
              "  0.25050121545791626,\n",
              "  0.250113308429718,\n",
              "  0.2501603960990906,\n",
              "  0.25002017617225647,\n",
              "  0.25018128752708435,\n",
              "  0.25015246868133545,\n",
              "  0.25010889768600464,\n",
              "  0.2500918507575989,\n",
              "  0.2501440644264221,\n",
              "  0.2500273585319519,\n",
              "  0.2505202293395996,\n",
              "  0.2501065731048584,\n",
              "  0.25023457407951355,\n",
              "  0.250065416097641,\n",
              "  0.25007766485214233,\n",
              "  0.2503727972507477,\n",
              "  0.25018012523651123,\n",
              "  0.25000882148742676,\n",
              "  0.25038838386535645,\n",
              "  0.2501758337020874,\n",
              "  0.2501787543296814,\n",
              "  0.2503577172756195,\n",
              "  0.25025737285614014,\n",
              "  0.2502228915691376,\n",
              "  0.2501140832901001,\n",
              "  0.2500271201133728,\n",
              "  0.25017493963241577,\n",
              "  0.2500671148300171,\n",
              "  0.2501327395439148,\n",
              "  0.25009316205978394,\n",
              "  0.2500205934047699,\n",
              "  0.2501051723957062,\n",
              "  0.2500634789466858,\n",
              "  0.2500517666339874,\n",
              "  0.25000685453414917,\n",
              "  0.2501522898674011,\n",
              "  0.25015386939048767,\n",
              "  0.2501208782196045,\n",
              "  0.2501707971096039,\n",
              "  0.2502034306526184,\n",
              "  0.25015902519226074,\n",
              "  0.25010597705841064,\n",
              "  0.2500579357147217,\n",
              "  0.25017213821411133,\n",
              "  0.25033608078956604,\n",
              "  0.2500334084033966,\n",
              "  0.24997752904891968,\n",
              "  0.2503785192966461,\n",
              "  0.2503223717212677,\n",
              "  0.2500283718109131,\n",
              "  0.2500523030757904,\n",
              "  0.25021153688430786,\n",
              "  0.2500949203968048,\n",
              "  0.2500367760658264,\n",
              "  0.2500394880771637,\n",
              "  0.2501363456249237,\n",
              "  0.25009000301361084,\n",
              "  0.25005674362182617,\n",
              "  0.250236451625824,\n",
              "  0.2500992715358734,\n",
              "  0.25031450390815735,\n",
              "  0.2501155138015747,\n",
              "  0.2501171827316284,\n",
              "  0.2500176727771759,\n",
              "  0.25019219517707825,\n",
              "  0.25020134449005127,\n",
              "  0.25007113814353943,\n",
              "  0.25020894408226013,\n",
              "  0.25043201446533203,\n",
              "  0.2502388060092926,\n",
              "  0.2501457929611206,\n",
              "  0.25018608570098877,\n",
              "  0.2500532865524292,\n",
              "  0.25004640221595764,\n",
              "  0.2503184378147125,\n",
              "  0.2500546872615814,\n",
              "  0.2500222623348236,\n",
              "  0.25006094574928284,\n",
              "  0.25007545948028564,\n",
              "  0.2500864565372467,\n",
              "  0.2503215968608856,\n",
              "  0.2501119375228882,\n",
              "  0.2501470744609833,\n",
              "  0.2500574290752411,\n",
              "  0.2501842677593231,\n",
              "  0.2501177489757538,\n",
              "  0.25015613436698914,\n",
              "  0.2501753270626068,\n",
              "  0.25009623169898987,\n",
              "  0.25001099705696106,\n",
              "  0.2500306963920593,\n",
              "  0.2501360774040222,\n",
              "  0.250089168548584,\n",
              "  0.2500608265399933,\n",
              "  0.25036048889160156,\n",
              "  0.2500639855861664,\n",
              "  0.25008442997932434,\n",
              "  0.2500607967376709,\n",
              "  0.2500521242618561,\n",
              "  0.2500518560409546],\n",
              " 'mse': [1.541745901107788,\n",
              "  1.3623071908950806,\n",
              "  1.2125654220581055,\n",
              "  1.094454050064087,\n",
              "  0.97553950548172,\n",
              "  0.8737543821334839,\n",
              "  0.7831557989120483,\n",
              "  0.708800196647644,\n",
              "  0.6348017454147339,\n",
              "  0.5733885765075684,\n",
              "  0.524944007396698,\n",
              "  0.47004520893096924,\n",
              "  0.4347764253616333,\n",
              "  0.4015086591243744,\n",
              "  0.37517473101615906,\n",
              "  0.34921395778656006,\n",
              "  0.32566261291503906,\n",
              "  0.31494152545928955,\n",
              "  0.301399290561676,\n",
              "  0.288849413394928,\n",
              "  0.2800367474555969,\n",
              "  0.2732323110103607,\n",
              "  0.2672470211982727,\n",
              "  0.2614997923374176,\n",
              "  0.25932517647743225,\n",
              "  0.25760573148727417,\n",
              "  0.2563563883304596,\n",
              "  0.2544625401496887,\n",
              "  0.2528984248638153,\n",
              "  0.2525680661201477,\n",
              "  0.25170525908470154,\n",
              "  0.25123801827430725,\n",
              "  0.2510283291339874,\n",
              "  0.2507885992527008,\n",
              "  0.2503601908683777,\n",
              "  0.25016775727272034,\n",
              "  0.2501104474067688,\n",
              "  0.2500673532485962,\n",
              "  0.24995705485343933,\n",
              "  0.2501262426376343,\n",
              "  0.25007134675979614,\n",
              "  0.250211238861084,\n",
              "  0.25003573298454285,\n",
              "  0.2501140534877777,\n",
              "  0.25004473328590393,\n",
              "  0.2500315010547638,\n",
              "  0.2500482201576233,\n",
              "  0.250055193901062,\n",
              "  0.25006118416786194,\n",
              "  0.25017863512039185,\n",
              "  0.24999286234378815,\n",
              "  0.2501259446144104,\n",
              "  0.25004953145980835,\n",
              "  0.25010251998901367,\n",
              "  0.25014984607696533,\n",
              "  0.25006920099258423,\n",
              "  0.250059574842453,\n",
              "  0.24996769428253174,\n",
              "  0.250122606754303,\n",
              "  0.2501140534877777,\n",
              "  0.2500948905944824,\n",
              "  0.2502976953983307,\n",
              "  0.2500050663948059,\n",
              "  0.2500542104244232,\n",
              "  0.25012099742889404,\n",
              "  0.2501237392425537,\n",
              "  0.25010672211647034,\n",
              "  0.25010430812835693,\n",
              "  0.25003039836883545,\n",
              "  0.2501261234283447,\n",
              "  0.25008919835090637,\n",
              "  0.2500503659248352,\n",
              "  0.25004085898399353,\n",
              "  0.2500515282154083,\n",
              "  0.2500174343585968,\n",
              "  0.25015345215797424,\n",
              "  0.25005483627319336,\n",
              "  0.2501334547996521,\n",
              "  0.25019869208335876,\n",
              "  0.2500392198562622,\n",
              "  0.25003886222839355,\n",
              "  0.2501303553581238,\n",
              "  0.2500024437904358,\n",
              "  0.2500269412994385,\n",
              "  0.2499668300151825,\n",
              "  0.2502528429031372,\n",
              "  0.25007888674736023,\n",
              "  0.2499919980764389,\n",
              "  0.25001734495162964,\n",
              "  0.2501715421676636,\n",
              "  0.250091016292572,\n",
              "  0.2501472532749176,\n",
              "  0.25009217858314514,\n",
              "  0.25010308623313904,\n",
              "  0.25018224120140076,\n",
              "  0.2500994801521301,\n",
              "  0.25034812092781067,\n",
              "  0.24996866285800934,\n",
              "  0.2500978410243988,\n",
              "  0.2500326633453369,\n",
              "  0.25005507469177246,\n",
              "  0.2501162588596344,\n",
              "  0.25013843178749084,\n",
              "  0.25012004375457764,\n",
              "  0.25004473328590393,\n",
              "  0.2501831650733948,\n",
              "  0.2500150501728058,\n",
              "  0.25001513957977295,\n",
              "  0.250162273645401,\n",
              "  0.2500472664833069,\n",
              "  0.2500072419643402,\n",
              "  0.25006136298179626,\n",
              "  0.2500969469547272,\n",
              "  0.2501053512096405,\n",
              "  0.25027161836624146,\n",
              "  0.25001072883605957,\n",
              "  0.25022485852241516,\n",
              "  0.25040140748023987,\n",
              "  0.250114768743515,\n",
              "  0.2500709593296051,\n",
              "  0.2500821650028229,\n",
              "  0.25005143880844116,\n",
              "  0.25013673305511475,\n",
              "  0.25007808208465576,\n",
              "  0.25033798813819885,\n",
              "  0.2501974105834961,\n",
              "  0.2502971887588501,\n",
              "  0.2502680718898773,\n",
              "  0.25056079030036926,\n",
              "  0.25022315979003906,\n",
              "  0.2500985860824585,\n",
              "  0.2500549554824829,\n",
              "  0.25017884373664856,\n",
              "  0.25011080503463745,\n",
              "  0.2501145601272583,\n",
              "  0.2501799464225769,\n",
              "  0.2500793933868408,\n",
              "  0.2501493990421295,\n",
              "  0.25010889768600464,\n",
              "  0.2500879466533661,\n",
              "  0.25001630187034607,\n",
              "  0.2500138580799103,\n",
              "  0.250340074300766,\n",
              "  0.2501693367958069,\n",
              "  0.25008729100227356,\n",
              "  0.25010743737220764,\n",
              "  0.2502152919769287,\n",
              "  0.2500769793987274,\n",
              "  0.2501026391983032,\n",
              "  0.2500789165496826,\n",
              "  0.25006240606307983,\n",
              "  0.250139445066452,\n",
              "  0.25010690093040466,\n",
              "  0.25007814168930054,\n",
              "  0.25016340613365173,\n",
              "  0.24999001622200012,\n",
              "  0.25006526708602905,\n",
              "  0.25003504753112793,\n",
              "  0.25010234117507935,\n",
              "  0.250070720911026,\n",
              "  0.2502216398715973,\n",
              "  0.25007686018943787,\n",
              "  0.2500605583190918,\n",
              "  0.2500964403152466,\n",
              "  0.25002801418304443,\n",
              "  0.2501353323459625,\n",
              "  0.2500784695148468,\n",
              "  0.25008878111839294,\n",
              "  0.2500663101673126,\n",
              "  0.250023752450943,\n",
              "  0.2501750588417053,\n",
              "  0.2502270042896271,\n",
              "  0.2504258155822754,\n",
              "  0.25018537044525146,\n",
              "  0.25007879734039307,\n",
              "  0.2501092255115509,\n",
              "  0.2507386803627014,\n",
              "  0.2500968277454376,\n",
              "  0.2501363754272461,\n",
              "  0.250073105096817,\n",
              "  0.2501313090324402,\n",
              "  0.2516373097896576,\n",
              "  0.2501392364501953,\n",
              "  0.25012344121932983,\n",
              "  0.2501594126224518,\n",
              "  0.2502904534339905,\n",
              "  0.2501826286315918,\n",
              "  0.2500404417514801,\n",
              "  0.2501738369464874,\n",
              "  0.25005918741226196,\n",
              "  0.2501491606235504,\n",
              "  0.25012367963790894,\n",
              "  0.2501644194126129,\n",
              "  0.25010228157043457,\n",
              "  0.2501654326915741,\n",
              "  0.2502239942550659,\n",
              "  0.25034061074256897,\n",
              "  0.25007954239845276,\n",
              "  0.25005972385406494,\n",
              "  0.25015684962272644,\n",
              "  0.25031769275665283,\n",
              "  0.2501300275325775,\n",
              "  0.25005805492401123,\n",
              "  0.2500065565109253,\n",
              "  0.2501809895038605,\n",
              "  0.25008872151374817,\n",
              "  0.2503244876861572,\n",
              "  0.2501389682292938,\n",
              "  0.2501034140586853,\n",
              "  0.2501429617404938,\n",
              "  0.25013330578804016,\n",
              "  0.25011950731277466,\n",
              "  0.25012218952178955,\n",
              "  0.25015830993652344,\n",
              "  0.2501581609249115,\n",
              "  0.2500849664211273,\n",
              "  0.250422865152359,\n",
              "  0.25008484721183777,\n",
              "  0.2500624656677246,\n",
              "  0.25005200505256653,\n",
              "  0.25044965744018555,\n",
              "  0.2500540018081665,\n",
              "  0.2504814863204956,\n",
              "  0.2503539025783539,\n",
              "  0.25012117624282837,\n",
              "  0.2501533329486847,\n",
              "  0.2500858008861542,\n",
              "  0.2502955198287964,\n",
              "  0.2501640319824219,\n",
              "  0.2502414286136627,\n",
              "  0.2502082884311676,\n",
              "  0.2501283884048462,\n",
              "  0.25018805265426636,\n",
              "  0.25026044249534607,\n",
              "  0.2501277029514313,\n",
              "  0.2500201463699341,\n",
              "  0.2500669062137604,\n",
              "  0.2500397861003876,\n",
              "  0.2501113712787628,\n",
              "  0.250098317861557,\n",
              "  0.25010934472084045,\n",
              "  0.25016406178474426,\n",
              "  0.2500724196434021,\n",
              "  0.2500573694705963,\n",
              "  0.25007402896881104,\n",
              "  0.2500348389148712,\n",
              "  0.2503584325313568,\n",
              "  0.25018489360809326,\n",
              "  0.2500689625740051,\n",
              "  0.25004804134368896,\n",
              "  0.2501338720321655,\n",
              "  0.2502387762069702,\n",
              "  0.2500459849834442,\n",
              "  0.25022146105766296,\n",
              "  0.25012752413749695,\n",
              "  0.25003302097320557,\n",
              "  0.2502077519893646,\n",
              "  0.2501537799835205,\n",
              "  0.25013962388038635,\n",
              "  0.2501363754272461,\n",
              "  0.25040292739868164,\n",
              "  0.25033870339393616,\n",
              "  0.25017616152763367,\n",
              "  0.250213086605072,\n",
              "  0.25015243887901306,\n",
              "  0.2502235174179077,\n",
              "  0.2501240670681,\n",
              "  0.25004979968070984,\n",
              "  0.2504848837852478,\n",
              "  0.2500464618206024,\n",
              "  0.25023528933525085,\n",
              "  0.2503400146961212,\n",
              "  0.2501349151134491,\n",
              "  0.2500784993171692,\n",
              "  0.25023606419563293,\n",
              "  0.25014546513557434,\n",
              "  0.250095933675766,\n",
              "  0.25016120076179504,\n",
              "  0.25021037459373474,\n",
              "  0.25023123621940613,\n",
              "  0.25003671646118164,\n",
              "  0.25018513202667236,\n",
              "  0.25016242265701294,\n",
              "  0.2501656413078308,\n",
              "  0.25000810623168945,\n",
              "  0.25022372603416443,\n",
              "  0.2500757575035095,\n",
              "  0.2501063644886017,\n",
              "  0.25022757053375244,\n",
              "  0.2504235804080963,\n",
              "  0.25014838576316833,\n",
              "  0.2501976490020752,\n",
              "  0.25008469820022583,\n",
              "  0.2500946819782257,\n",
              "  0.2502457797527313,\n",
              "  0.2502368092536926,\n",
              "  0.2502102553844452,\n",
              "  0.25012555718421936,\n",
              "  0.25003689527511597,\n",
              "  0.2500339448451996,\n",
              "  0.2503330707550049,\n",
              "  0.250140517950058,\n",
              "  0.2500935196876526,\n",
              "  0.25013914704322815,\n",
              "  0.2500746548175812,\n",
              "  0.2500598132610321,\n",
              "  0.25049498677253723,\n",
              "  0.25032737851142883,\n",
              "  0.25026610493659973,\n",
              "  0.2503124177455902,\n",
              "  0.24997779726982117,\n",
              "  0.2502951920032501,\n",
              "  0.2499644011259079,\n",
              "  0.25012943148612976,\n",
              "  0.25016599893569946,\n",
              "  0.25023141503334045,\n",
              "  0.2501707375049591,\n",
              "  0.2501201927661896,\n",
              "  0.25008052587509155,\n",
              "  0.2500937581062317,\n",
              "  0.2500164806842804,\n",
              "  0.2501475512981415,\n",
              "  0.25017380714416504,\n",
              "  0.25011828541755676,\n",
              "  0.25009146332740784,\n",
              "  0.25013238191604614,\n",
              "  0.2501477003097534,\n",
              "  0.2501581013202667,\n",
              "  0.25006163120269775,\n",
              "  0.2500929832458496,\n",
              "  0.2502012848854065,\n",
              "  0.2502857744693756,\n",
              "  0.25019511580467224,\n",
              "  0.25002679228782654,\n",
              "  0.25023338198661804,\n",
              "  0.25005272030830383,\n",
              "  0.2502848505973816,\n",
              "  0.25027158856391907,\n",
              "  0.2501122057437897,\n",
              "  0.2499990016222,\n",
              "  0.2500671446323395,\n",
              "  0.2502007782459259,\n",
              "  0.2503858506679535,\n",
              "  0.2501746416091919,\n",
              "  0.2501989006996155,\n",
              "  0.25004005432128906,\n",
              "  0.25003582239151,\n",
              "  0.25020384788513184,\n",
              "  0.25012364983558655,\n",
              "  0.25003713369369507,\n",
              "  0.25003960728645325,\n",
              "  0.2501639127731323,\n",
              "  0.250081330537796,\n",
              "  0.2500641345977783,\n",
              "  0.25018006563186646,\n",
              "  0.250171422958374,\n",
              "  0.2501488924026489,\n",
              "  0.2500753402709961,\n",
              "  0.25009438395500183,\n",
              "  0.2500480115413666,\n",
              "  0.25014355778694153,\n",
              "  0.2502824664115906,\n",
              "  0.25014781951904297,\n",
              "  0.2500723600387573,\n",
              "  0.2500961720943451,\n",
              "  0.2501162588596344,\n",
              "  0.2500402629375458,\n",
              "  0.2501685619354248,\n",
              "  0.25025346875190735,\n",
              "  0.25017210841178894,\n",
              "  0.25021815299987793,\n",
              "  0.25016674399375916,\n",
              "  0.2500859498977661,\n",
              "  0.2502081096172333,\n",
              "  0.2502944767475128,\n",
              "  0.25003740191459656,\n",
              "  0.2502294182777405,\n",
              "  0.2502370774745941,\n",
              "  0.25042495131492615,\n",
              "  0.25035884976387024,\n",
              "  0.25011345744132996,\n",
              "  0.25027427077293396,\n",
              "  0.2501586973667145,\n",
              "  0.2499973028898239,\n",
              "  0.2501453459262848,\n",
              "  0.25015464425086975,\n",
              "  0.2500559985637665,\n",
              "  0.25008273124694824,\n",
              "  0.2502179741859436,\n",
              "  0.2500606179237366,\n",
              "  0.25011324882507324,\n",
              "  0.2505384087562561,\n",
              "  0.25007641315460205,\n",
              "  0.2500978112220764,\n",
              "  0.2502054274082184,\n",
              "  0.2501600384712219,\n",
              "  0.2500389516353607,\n",
              "  0.250122994184494,\n",
              "  0.25012195110321045,\n",
              "  0.25006523728370667,\n",
              "  0.25050121545791626,\n",
              "  0.250113308429718,\n",
              "  0.2501603960990906,\n",
              "  0.25002017617225647,\n",
              "  0.25018128752708435,\n",
              "  0.25015246868133545,\n",
              "  0.25010889768600464,\n",
              "  0.2500918507575989,\n",
              "  0.2501440644264221,\n",
              "  0.2500273585319519,\n",
              "  0.2505202293395996,\n",
              "  0.2501065731048584,\n",
              "  0.25023457407951355,\n",
              "  0.250065416097641,\n",
              "  0.25007766485214233,\n",
              "  0.2503727972507477,\n",
              "  0.25018012523651123,\n",
              "  0.25000882148742676,\n",
              "  0.25038838386535645,\n",
              "  0.2501758337020874,\n",
              "  0.2501787543296814,\n",
              "  0.2503577172756195,\n",
              "  0.25025737285614014,\n",
              "  0.2502228915691376,\n",
              "  0.2501140832901001,\n",
              "  0.2500271201133728,\n",
              "  0.25017493963241577,\n",
              "  0.2500671148300171,\n",
              "  0.2501327395439148,\n",
              "  0.25009316205978394,\n",
              "  0.2500205934047699,\n",
              "  0.2501051723957062,\n",
              "  0.2500634789466858,\n",
              "  0.2500517666339874,\n",
              "  0.25000685453414917,\n",
              "  0.2501522898674011,\n",
              "  0.25015386939048767,\n",
              "  0.2501208782196045,\n",
              "  0.2501707971096039,\n",
              "  0.2502034306526184,\n",
              "  0.25015902519226074,\n",
              "  0.25010597705841064,\n",
              "  0.2500579357147217,\n",
              "  0.25017213821411133,\n",
              "  0.25033608078956604,\n",
              "  0.2500334084033966,\n",
              "  0.24997752904891968,\n",
              "  0.2503785192966461,\n",
              "  0.2503223717212677,\n",
              "  0.2500283718109131,\n",
              "  0.2500523030757904,\n",
              "  0.25021153688430786,\n",
              "  0.2500949203968048,\n",
              "  0.2500367760658264,\n",
              "  0.2500394880771637,\n",
              "  0.2501363456249237,\n",
              "  0.25009000301361084,\n",
              "  0.25005674362182617,\n",
              "  0.250236451625824,\n",
              "  0.2500992715358734,\n",
              "  0.25031450390815735,\n",
              "  0.2501155138015747,\n",
              "  0.2501171827316284,\n",
              "  0.2500176727771759,\n",
              "  0.25019219517707825,\n",
              "  0.25020134449005127,\n",
              "  0.25007113814353943,\n",
              "  0.25020894408226013,\n",
              "  0.25043201446533203,\n",
              "  0.2502388060092926,\n",
              "  0.2501457929611206,\n",
              "  0.25018608570098877,\n",
              "  0.2500532865524292,\n",
              "  0.25004640221595764,\n",
              "  0.2503184378147125,\n",
              "  0.2500546872615814,\n",
              "  0.2500222623348236,\n",
              "  0.25006094574928284,\n",
              "  0.25007545948028564,\n",
              "  0.2500864565372467,\n",
              "  0.2503215968608856,\n",
              "  0.2501119375228882,\n",
              "  0.2501470744609833,\n",
              "  0.2500574290752411,\n",
              "  0.2501842677593231,\n",
              "  0.2501177489757538,\n",
              "  0.25015613436698914,\n",
              "  0.2501753270626068,\n",
              "  0.25009623169898987,\n",
              "  0.25001099705696106,\n",
              "  0.2500306963920593,\n",
              "  0.2501360774040222,\n",
              "  0.250089168548584,\n",
              "  0.2500608265399933,\n",
              "  0.25036048889160156,\n",
              "  0.2500639855861664,\n",
              "  0.25008442997932434,\n",
              "  0.2500607967376709,\n",
              "  0.2500521242618561,\n",
              "  0.2500518560409546],\n",
              " 'val_loss': [1.442723274230957,\n",
              "  1.292561411857605,\n",
              "  1.198809027671814,\n",
              "  1.0364246368408203,\n",
              "  0.9442346096038818,\n",
              "  0.81488037109375,\n",
              "  0.7888490557670593,\n",
              "  0.6691539883613586,\n",
              "  0.6161338090896606,\n",
              "  0.5570803284645081,\n",
              "  0.5068346858024597,\n",
              "  0.4545241892337799,\n",
              "  0.40952518582344055,\n",
              "  0.409509539604187,\n",
              "  0.3539859354496002,\n",
              "  0.3455216884613037,\n",
              "  0.3226241171360016,\n",
              "  0.3073994815349579,\n",
              "  0.2950311005115509,\n",
              "  0.2892281115055084,\n",
              "  0.28420373797416687,\n",
              "  0.2740778625011444,\n",
              "  0.268785685300827,\n",
              "  0.26491692662239075,\n",
              "  0.26177066564559937,\n",
              "  0.25368326902389526,\n",
              "  0.25697314739227295,\n",
              "  0.25675153732299805,\n",
              "  0.25180724263191223,\n",
              "  0.2531224489212036,\n",
              "  0.2524838447570801,\n",
              "  0.2517152428627014,\n",
              "  0.2507213056087494,\n",
              "  0.2504764795303345,\n",
              "  0.25079646706581116,\n",
              "  0.250220388174057,\n",
              "  0.2501368820667267,\n",
              "  0.25036823749542236,\n",
              "  0.2501307725906372,\n",
              "  0.25029295682907104,\n",
              "  0.2500903606414795,\n",
              "  0.24991704523563385,\n",
              "  0.2500322163105011,\n",
              "  0.25003132224082947,\n",
              "  0.250051349401474,\n",
              "  0.25005465745925903,\n",
              "  0.2500104010105133,\n",
              "  0.24992696940898895,\n",
              "  0.25000932812690735,\n",
              "  0.24975980818271637,\n",
              "  0.2500956356525421,\n",
              "  0.2502990663051605,\n",
              "  0.24992825090885162,\n",
              "  0.24996618926525116,\n",
              "  0.25001049041748047,\n",
              "  0.25000619888305664,\n",
              "  0.2500064969062805,\n",
              "  0.2503161132335663,\n",
              "  0.2499086707830429,\n",
              "  0.2499006986618042,\n",
              "  0.25023144483566284,\n",
              "  0.2500501275062561,\n",
              "  0.249935582280159,\n",
              "  0.25009751319885254,\n",
              "  0.2502499520778656,\n",
              "  0.2500816881656647,\n",
              "  0.2500385344028473,\n",
              "  0.25010380148887634,\n",
              "  0.2499259114265442,\n",
              "  0.25000953674316406,\n",
              "  0.2500036060810089,\n",
              "  0.25001880526542664,\n",
              "  0.2501307427883148,\n",
              "  0.24991391599178314,\n",
              "  0.2498064786195755,\n",
              "  0.2501923143863678,\n",
              "  0.2501610517501831,\n",
              "  0.25004544854164124,\n",
              "  0.2502003014087677,\n",
              "  0.24993665516376495,\n",
              "  0.2501109838485718,\n",
              "  0.25019556283950806,\n",
              "  0.2500321865081787,\n",
              "  0.2498966008424759,\n",
              "  0.25004932284355164,\n",
              "  0.2500218451023102,\n",
              "  0.25005999207496643,\n",
              "  0.25016698241233826,\n",
              "  0.2501341998577118,\n",
              "  0.25012531876564026,\n",
              "  0.2501307725906372,\n",
              "  0.2500011622905731,\n",
              "  0.24992907047271729,\n",
              "  0.25011852383613586,\n",
              "  0.2499660700559616,\n",
              "  0.25007855892181396,\n",
              "  0.25000277161598206,\n",
              "  0.2501406967639923,\n",
              "  0.25002238154411316,\n",
              "  0.2503068447113037,\n",
              "  0.2500498592853546,\n",
              "  0.250294029712677,\n",
              "  0.25050997734069824,\n",
              "  0.25024721026420593,\n",
              "  0.2502705454826355,\n",
              "  0.25023773312568665,\n",
              "  0.25013163685798645,\n",
              "  0.250065416097641,\n",
              "  0.2500338852405548,\n",
              "  0.2499171495437622,\n",
              "  0.25007638335227966,\n",
              "  0.2501792013645172,\n",
              "  0.25007161498069763,\n",
              "  0.25021782517433167,\n",
              "  0.2500027120113373,\n",
              "  0.24989642202854156,\n",
              "  0.2500978708267212,\n",
              "  0.25023576617240906,\n",
              "  0.24989409744739532,\n",
              "  0.250182181596756,\n",
              "  0.25005292892456055,\n",
              "  0.25008413195610046,\n",
              "  0.2500554025173187,\n",
              "  0.2499394416809082,\n",
              "  0.2500341236591339,\n",
              "  0.24991510808467865,\n",
              "  0.2500303089618683,\n",
              "  0.25003090500831604,\n",
              "  0.25013336539268494,\n",
              "  0.24999958276748657,\n",
              "  0.24998605251312256,\n",
              "  0.2500055730342865,\n",
              "  0.25005730986595154,\n",
              "  0.24998541176319122,\n",
              "  0.250123530626297,\n",
              "  0.2499351054430008,\n",
              "  0.25001028180122375,\n",
              "  0.2500789165496826,\n",
              "  0.24994312226772308,\n",
              "  0.24992699921131134,\n",
              "  0.2500436007976532,\n",
              "  0.2500419318675995,\n",
              "  0.2499995231628418,\n",
              "  0.25011685490608215,\n",
              "  0.2500794529914856,\n",
              "  0.25000622868537903,\n",
              "  0.25000250339508057,\n",
              "  0.2500385344028473,\n",
              "  0.250021368265152,\n",
              "  0.249999538064003,\n",
              "  0.24999971687793732,\n",
              "  0.2501920163631439,\n",
              "  0.24998678267002106,\n",
              "  0.2500160038471222,\n",
              "  0.25022631883621216,\n",
              "  0.2500394284725189,\n",
              "  0.2499137669801712,\n",
              "  0.2503295838832855,\n",
              "  0.25002923607826233,\n",
              "  0.2500326335430145,\n",
              "  0.2501964569091797,\n",
              "  0.25008293986320496,\n",
              "  0.25000035762786865,\n",
              "  0.25012895464897156,\n",
              "  0.2500208616256714,\n",
              "  0.250020831823349,\n",
              "  0.25004053115844727,\n",
              "  0.25006699562072754,\n",
              "  0.2499762624502182,\n",
              "  0.2503395080566406,\n",
              "  0.2501104772090912,\n",
              "  0.25000104308128357,\n",
              "  0.24999313056468964,\n",
              "  0.25012338161468506,\n",
              "  0.249997079372406,\n",
              "  0.2503665089607239,\n",
              "  0.24973665177822113,\n",
              "  0.25024673342704773,\n",
              "  0.2498936802148819,\n",
              "  0.2501741647720337,\n",
              "  0.2498873472213745,\n",
              "  0.24999640882015228,\n",
              "  0.25000062584877014,\n",
              "  0.2500074803829193,\n",
              "  0.25000491738319397,\n",
              "  0.24990785121917725,\n",
              "  0.2504682242870331,\n",
              "  0.2499895840883255,\n",
              "  0.25006774067878723,\n",
              "  0.2500581741333008,\n",
              "  0.24997110664844513,\n",
              "  0.24997760355472565,\n",
              "  0.2500070035457611,\n",
              "  0.25000739097595215,\n",
              "  0.2500024139881134,\n",
              "  0.24996918439865112,\n",
              "  0.24990026652812958,\n",
              "  0.25000032782554626,\n",
              "  0.2500402331352234,\n",
              "  0.24989192187786102,\n",
              "  0.25009390711784363,\n",
              "  0.2501543462276459,\n",
              "  0.25020429491996765,\n",
              "  0.2502847909927368,\n",
              "  0.25013890862464905,\n",
              "  0.2500019967556,\n",
              "  0.2500194013118744,\n",
              "  0.2500954866409302,\n",
              "  0.2499459832906723,\n",
              "  0.2500127851963043,\n",
              "  0.24994604289531708,\n",
              "  0.2500264644622803,\n",
              "  0.24995005130767822,\n",
              "  0.24998672306537628,\n",
              "  0.25016751885414124,\n",
              "  0.25002363324165344,\n",
              "  0.2500572204589844,\n",
              "  0.2499765008687973,\n",
              "  0.24997669458389282,\n",
              "  0.24993829429149628,\n",
              "  0.24993205070495605,\n",
              "  0.2500372529029846,\n",
              "  0.25019410252571106,\n",
              "  0.25001874566078186,\n",
              "  0.25001397728919983,\n",
              "  0.25000426173210144,\n",
              "  0.25000837445259094,\n",
              "  0.24998705089092255,\n",
              "  0.24990908801555634,\n",
              "  0.25002405047416687,\n",
              "  0.2500756084918976,\n",
              "  0.250018447637558,\n",
              "  0.2501027286052704,\n",
              "  0.25003114342689514,\n",
              "  0.25000515580177307,\n",
              "  0.2500324547290802,\n",
              "  0.2499760538339615,\n",
              "  0.24998490512371063,\n",
              "  0.24996037781238556,\n",
              "  0.24982786178588867,\n",
              "  0.25004497170448303,\n",
              "  0.25007885694503784,\n",
              "  0.25001585483551025,\n",
              "  0.25009530782699585,\n",
              "  0.24998120963573456,\n",
              "  0.2500418722629547,\n",
              "  0.2500115633010864,\n",
              "  0.25012466311454773,\n",
              "  0.25000903010368347,\n",
              "  0.2500401437282562,\n",
              "  0.2500995099544525,\n",
              "  0.25000011920928955,\n",
              "  0.2500036656856537,\n",
              "  0.25000330805778503,\n",
              "  0.24996338784694672,\n",
              "  0.25000643730163574,\n",
              "  0.24968212842941284,\n",
              "  0.2499396800994873,\n",
              "  0.24992690980434418,\n",
              "  0.24998979270458221,\n",
              "  0.24994099140167236,\n",
              "  0.2499120980501175,\n",
              "  0.24994750320911407,\n",
              "  0.2500542104244232,\n",
              "  0.2500647306442261,\n",
              "  0.2501313388347626,\n",
              "  0.2500132620334625,\n",
              "  0.2500356435775757,\n",
              "  0.25001034140586853,\n",
              "  0.24977195262908936,\n",
              "  0.2503120005130768,\n",
              "  0.24995936453342438,\n",
              "  0.2500002980232239,\n",
              "  0.2500973641872406,\n",
              "  0.25008293986320496,\n",
              "  0.2500388026237488,\n",
              "  0.2500006854534149,\n",
              "  0.24993936717510223,\n",
              "  0.25007185339927673,\n",
              "  0.25010886788368225,\n",
              "  0.25000011920928955,\n",
              "  0.2500661313533783,\n",
              "  0.2500600516796112,\n",
              "  0.2500125765800476,\n",
              "  0.25011560320854187,\n",
              "  0.25003355741500854,\n",
              "  0.2500114142894745,\n",
              "  0.25009027123451233,\n",
              "  0.2500201463699341,\n",
              "  0.2500494420528412,\n",
              "  0.249922513961792,\n",
              "  0.2500062882900238,\n",
              "  0.25001421570777893,\n",
              "  0.25008872151374817,\n",
              "  0.24996782839298248,\n",
              "  0.2500622570514679,\n",
              "  0.250011146068573,\n",
              "  0.24998630583286285,\n",
              "  0.25008872151374817,\n",
              "  0.2500011622905731,\n",
              "  0.2500249445438385,\n",
              "  0.25009962916374207,\n",
              "  0.25029733777046204,\n",
              "  0.2500479817390442,\n",
              "  0.24991166591644287,\n",
              "  0.2501697540283203,\n",
              "  0.2500404119491577,\n",
              "  0.2503163814544678,\n",
              "  0.25003325939178467,\n",
              "  0.2503543198108673,\n",
              "  0.2502448856830597,\n",
              "  0.2503068447113037,\n",
              "  0.2502441704273224,\n",
              "  0.2499147206544876,\n",
              "  0.2501041293144226,\n",
              "  0.2499924749135971,\n",
              "  0.24996745586395264,\n",
              "  0.24987615644931793,\n",
              "  0.25000301003456116,\n",
              "  0.2499486654996872,\n",
              "  0.2500220835208893,\n",
              "  0.25006499886512756,\n",
              "  0.25009775161743164,\n",
              "  0.2502215504646301,\n",
              "  0.2502337396144867,\n",
              "  0.2500125467777252,\n",
              "  0.2500254213809967,\n",
              "  0.2498963624238968,\n",
              "  0.2503828704357147,\n",
              "  0.25015732645988464,\n",
              "  0.25004467368125916,\n",
              "  0.24991250038146973,\n",
              "  0.25000128149986267,\n",
              "  0.2501063942909241,\n",
              "  0.2502707540988922,\n",
              "  0.25001242756843567,\n",
              "  0.2500976026058197,\n",
              "  0.2501283586025238,\n",
              "  0.25001999735832214,\n",
              "  0.250033974647522,\n",
              "  0.2500658631324768,\n",
              "  0.25001609325408936,\n",
              "  0.25002574920654297,\n",
              "  0.25000080466270447,\n",
              "  0.2501707673072815,\n",
              "  0.2501523494720459,\n",
              "  0.249984011054039,\n",
              "  0.25018495321273804,\n",
              "  0.2500004172325134,\n",
              "  0.2500000298023224,\n",
              "  0.2502363324165344,\n",
              "  0.2505562901496887,\n",
              "  0.25001421570777893,\n",
              "  0.25002095103263855,\n",
              "  0.250003844499588,\n",
              "  0.2500103712081909,\n",
              "  0.25005850195884705,\n",
              "  0.25031521916389465,\n",
              "  0.2501930296421051,\n",
              "  0.2499251812696457,\n",
              "  0.24990248680114746,\n",
              "  0.2500101625919342,\n",
              "  0.2500179708003998,\n",
              "  0.25000542402267456,\n",
              "  0.25000128149986267,\n",
              "  0.250076562166214,\n",
              "  0.25019940733909607,\n",
              "  0.24989788234233856,\n",
              "  0.2502377927303314,\n",
              "  0.2500004768371582,\n",
              "  0.25008058547973633,\n",
              "  0.2500873804092407,\n",
              "  0.25002801418304443,\n",
              "  0.24999326467514038,\n",
              "  0.24994798004627228,\n",
              "  0.25004124641418457,\n",
              "  0.2500631511211395,\n",
              "  0.2500055730342865,\n",
              "  0.25013408064842224,\n",
              "  0.25000283122062683,\n",
              "  0.24994920194149017,\n",
              "  0.2499508112668991,\n",
              "  0.25015661120414734,\n",
              "  0.25022366642951965,\n",
              "  0.25005677342414856,\n",
              "  0.25003519654273987,\n",
              "  0.2500327527523041,\n",
              "  0.25028035044670105,\n",
              "  0.25008562207221985,\n",
              "  0.25006964802742004,\n",
              "  0.25003042817115784,\n",
              "  0.25007835030555725,\n",
              "  0.2499847561120987,\n",
              "  0.24993737041950226,\n",
              "  0.2499598115682602,\n",
              "  0.2500056326389313,\n",
              "  0.25017860531806946,\n",
              "  0.250246524810791,\n",
              "  0.2502118945121765,\n",
              "  0.25006455183029175,\n",
              "  0.25031161308288574,\n",
              "  0.250033974647522,\n",
              "  0.25,\n",
              "  0.25018760561943054,\n",
              "  0.25024423003196716,\n",
              "  0.25003698468208313,\n",
              "  0.25002631545066833,\n",
              "  0.24999909102916718,\n",
              "  0.2502662241458893,\n",
              "  0.25001129508018494,\n",
              "  0.24991337954998016,\n",
              "  0.2500006854534149,\n",
              "  0.2502361536026001,\n",
              "  0.2501022517681122,\n",
              "  0.2500685751438141,\n",
              "  0.25017502903938293,\n",
              "  0.25000372529029846,\n",
              "  0.2501198351383209,\n",
              "  0.2502003312110901,\n",
              "  0.2500004470348358,\n",
              "  0.2500264644622803,\n",
              "  0.2500123977661133,\n",
              "  0.2499328851699829,\n",
              "  0.2501651644706726,\n",
              "  0.25000545382499695,\n",
              "  0.25002744793891907,\n",
              "  0.2500864565372467,\n",
              "  0.2501033842563629,\n",
              "  0.25003582239151,\n",
              "  0.2503609359264374,\n",
              "  0.25008320808410645,\n",
              "  0.250322550535202,\n",
              "  0.2500474750995636,\n",
              "  0.24981825053691864,\n",
              "  0.2500866949558258,\n",
              "  0.25005844235420227,\n",
              "  0.25025853514671326,\n",
              "  0.2502743899822235,\n",
              "  0.24992696940898895,\n",
              "  0.24996328353881836,\n",
              "  0.2501264810562134,\n",
              "  0.2501170337200165,\n",
              "  0.2501770853996277,\n",
              "  0.24994266033172607,\n",
              "  0.25011709332466125,\n",
              "  0.2503732144832611,\n",
              "  0.25027838349342346,\n",
              "  0.2501218318939209,\n",
              "  0.25024881958961487,\n",
              "  0.249974325299263,\n",
              "  0.25000011920928955,\n",
              "  0.25005072355270386,\n",
              "  0.2505188286304474,\n",
              "  0.24991662800312042,\n",
              "  0.25012949109077454,\n",
              "  0.2500040829181671,\n",
              "  0.25007644295692444,\n",
              "  0.25001704692840576,\n",
              "  0.24992835521697998,\n",
              "  0.2502609193325043,\n",
              "  0.2500176429748535,\n",
              "  0.2500006854534149,\n",
              "  0.250002920627594,\n",
              "  0.25009337067604065,\n",
              "  0.2500048875808716,\n",
              "  0.25016656517982483,\n",
              "  0.2500338852405548,\n",
              "  0.2500331699848175,\n",
              "  0.25046679377555847,\n",
              "  0.2500673830509186,\n",
              "  0.25000104308128357,\n",
              "  0.25028738379478455,\n",
              "  0.2500201165676117,\n",
              "  0.24973036348819733,\n",
              "  0.2500143051147461,\n",
              "  0.25007686018943787,\n",
              "  0.2500188648700714,\n",
              "  0.2500249147415161,\n",
              "  0.25000283122062683,\n",
              "  0.2500031292438507,\n",
              "  0.25035399198532104,\n",
              "  0.2500627636909485,\n",
              "  0.24995295703411102,\n",
              "  0.25026431679725647,\n",
              "  0.24996669590473175,\n",
              "  0.2500089406967163,\n",
              "  0.24998940527439117,\n",
              "  0.250117689371109,\n",
              "  0.25003981590270996,\n",
              "  0.24996663630008698,\n",
              "  0.2500293552875519,\n",
              "  0.2502143681049347,\n",
              "  0.2499331831932068,\n",
              "  0.25028276443481445,\n",
              "  0.25044867396354675,\n",
              "  0.2500079870223999,\n",
              "  0.2500609755516052,\n",
              "  0.25002261996269226,\n",
              "  0.250026673078537,\n",
              "  0.25002801418304443],\n",
              " 'val_mse': [1.442723274230957,\n",
              "  1.292561411857605,\n",
              "  1.198809027671814,\n",
              "  1.0364246368408203,\n",
              "  0.9442346096038818,\n",
              "  0.81488037109375,\n",
              "  0.7888490557670593,\n",
              "  0.6691539883613586,\n",
              "  0.6161338090896606,\n",
              "  0.5570803284645081,\n",
              "  0.5068346858024597,\n",
              "  0.4545241892337799,\n",
              "  0.40952518582344055,\n",
              "  0.409509539604187,\n",
              "  0.3539859354496002,\n",
              "  0.3455216884613037,\n",
              "  0.3226241171360016,\n",
              "  0.3073994815349579,\n",
              "  0.2950311005115509,\n",
              "  0.2892281115055084,\n",
              "  0.28420373797416687,\n",
              "  0.2740778625011444,\n",
              "  0.268785685300827,\n",
              "  0.26491692662239075,\n",
              "  0.26177066564559937,\n",
              "  0.25368326902389526,\n",
              "  0.25697314739227295,\n",
              "  0.25675153732299805,\n",
              "  0.25180724263191223,\n",
              "  0.2531224489212036,\n",
              "  0.2524838447570801,\n",
              "  0.2517152428627014,\n",
              "  0.2507213056087494,\n",
              "  0.2504764795303345,\n",
              "  0.25079646706581116,\n",
              "  0.250220388174057,\n",
              "  0.2501368820667267,\n",
              "  0.25036823749542236,\n",
              "  0.2501307725906372,\n",
              "  0.25029295682907104,\n",
              "  0.2500903606414795,\n",
              "  0.24991704523563385,\n",
              "  0.2500322163105011,\n",
              "  0.25003132224082947,\n",
              "  0.250051349401474,\n",
              "  0.25005465745925903,\n",
              "  0.2500104010105133,\n",
              "  0.24992696940898895,\n",
              "  0.25000932812690735,\n",
              "  0.24975980818271637,\n",
              "  0.2500956356525421,\n",
              "  0.2502990663051605,\n",
              "  0.24992825090885162,\n",
              "  0.24996618926525116,\n",
              "  0.25001049041748047,\n",
              "  0.25000619888305664,\n",
              "  0.2500064969062805,\n",
              "  0.2503161132335663,\n",
              "  0.2499086707830429,\n",
              "  0.2499006986618042,\n",
              "  0.25023144483566284,\n",
              "  0.2500501275062561,\n",
              "  0.249935582280159,\n",
              "  0.25009751319885254,\n",
              "  0.2502499520778656,\n",
              "  0.2500816881656647,\n",
              "  0.2500385344028473,\n",
              "  0.25010380148887634,\n",
              "  0.2499259114265442,\n",
              "  0.25000953674316406,\n",
              "  0.2500036060810089,\n",
              "  0.25001880526542664,\n",
              "  0.2501307427883148,\n",
              "  0.24991391599178314,\n",
              "  0.2498064786195755,\n",
              "  0.2501923143863678,\n",
              "  0.2501610517501831,\n",
              "  0.25004544854164124,\n",
              "  0.2502003014087677,\n",
              "  0.24993665516376495,\n",
              "  0.2501109838485718,\n",
              "  0.25019556283950806,\n",
              "  0.2500321865081787,\n",
              "  0.2498966008424759,\n",
              "  0.25004932284355164,\n",
              "  0.2500218451023102,\n",
              "  0.25005999207496643,\n",
              "  0.25016698241233826,\n",
              "  0.2501341998577118,\n",
              "  0.25012531876564026,\n",
              "  0.2501307725906372,\n",
              "  0.2500011622905731,\n",
              "  0.24992907047271729,\n",
              "  0.25011852383613586,\n",
              "  0.2499660700559616,\n",
              "  0.25007855892181396,\n",
              "  0.25000277161598206,\n",
              "  0.2501406967639923,\n",
              "  0.25002238154411316,\n",
              "  0.2503068447113037,\n",
              "  0.2500498592853546,\n",
              "  0.250294029712677,\n",
              "  0.25050997734069824,\n",
              "  0.25024721026420593,\n",
              "  0.2502705454826355,\n",
              "  0.25023773312568665,\n",
              "  0.25013163685798645,\n",
              "  0.250065416097641,\n",
              "  0.2500338852405548,\n",
              "  0.2499171495437622,\n",
              "  0.25007638335227966,\n",
              "  0.2501792013645172,\n",
              "  0.25007161498069763,\n",
              "  0.25021782517433167,\n",
              "  0.2500027120113373,\n",
              "  0.24989642202854156,\n",
              "  0.2500978708267212,\n",
              "  0.25023576617240906,\n",
              "  0.24989409744739532,\n",
              "  0.250182181596756,\n",
              "  0.25005292892456055,\n",
              "  0.25008413195610046,\n",
              "  0.2500554025173187,\n",
              "  0.2499394416809082,\n",
              "  0.2500341236591339,\n",
              "  0.24991510808467865,\n",
              "  0.2500303089618683,\n",
              "  0.25003090500831604,\n",
              "  0.25013336539268494,\n",
              "  0.24999958276748657,\n",
              "  0.24998605251312256,\n",
              "  0.2500055730342865,\n",
              "  0.25005730986595154,\n",
              "  0.24998541176319122,\n",
              "  0.250123530626297,\n",
              "  0.2499351054430008,\n",
              "  0.25001028180122375,\n",
              "  0.2500789165496826,\n",
              "  0.24994312226772308,\n",
              "  0.24992699921131134,\n",
              "  0.2500436007976532,\n",
              "  0.2500419318675995,\n",
              "  0.2499995231628418,\n",
              "  0.25011685490608215,\n",
              "  0.2500794529914856,\n",
              "  0.25000622868537903,\n",
              "  0.25000250339508057,\n",
              "  0.2500385344028473,\n",
              "  0.250021368265152,\n",
              "  0.249999538064003,\n",
              "  0.24999971687793732,\n",
              "  0.2501920163631439,\n",
              "  0.24998678267002106,\n",
              "  0.2500160038471222,\n",
              "  0.25022631883621216,\n",
              "  0.2500394284725189,\n",
              "  0.2499137669801712,\n",
              "  0.2503295838832855,\n",
              "  0.25002923607826233,\n",
              "  0.2500326335430145,\n",
              "  0.2501964569091797,\n",
              "  0.25008293986320496,\n",
              "  0.25000035762786865,\n",
              "  0.25012895464897156,\n",
              "  0.2500208616256714,\n",
              "  0.250020831823349,\n",
              "  0.25004053115844727,\n",
              "  0.25006699562072754,\n",
              "  0.2499762624502182,\n",
              "  0.2503395080566406,\n",
              "  0.2501104772090912,\n",
              "  0.25000104308128357,\n",
              "  0.24999313056468964,\n",
              "  0.25012338161468506,\n",
              "  0.249997079372406,\n",
              "  0.2503665089607239,\n",
              "  0.24973665177822113,\n",
              "  0.25024673342704773,\n",
              "  0.2498936802148819,\n",
              "  0.2501741647720337,\n",
              "  0.2498873472213745,\n",
              "  0.24999640882015228,\n",
              "  0.25000062584877014,\n",
              "  0.2500074803829193,\n",
              "  0.25000491738319397,\n",
              "  0.24990785121917725,\n",
              "  0.2504682242870331,\n",
              "  0.2499895840883255,\n",
              "  0.25006774067878723,\n",
              "  0.2500581741333008,\n",
              "  0.24997110664844513,\n",
              "  0.24997760355472565,\n",
              "  0.2500070035457611,\n",
              "  0.25000739097595215,\n",
              "  0.2500024139881134,\n",
              "  0.24996918439865112,\n",
              "  0.24990026652812958,\n",
              "  0.25000032782554626,\n",
              "  0.2500402331352234,\n",
              "  0.24989192187786102,\n",
              "  0.25009390711784363,\n",
              "  0.2501543462276459,\n",
              "  0.25020429491996765,\n",
              "  0.2502847909927368,\n",
              "  0.25013890862464905,\n",
              "  0.2500019967556,\n",
              "  0.2500194013118744,\n",
              "  0.2500954866409302,\n",
              "  0.2499459832906723,\n",
              "  0.2500127851963043,\n",
              "  0.24994604289531708,\n",
              "  0.2500264644622803,\n",
              "  0.24995005130767822,\n",
              "  0.24998672306537628,\n",
              "  0.25016751885414124,\n",
              "  0.25002363324165344,\n",
              "  0.2500572204589844,\n",
              "  0.2499765008687973,\n",
              "  0.24997669458389282,\n",
              "  0.24993829429149628,\n",
              "  0.24993205070495605,\n",
              "  0.2500372529029846,\n",
              "  0.25019410252571106,\n",
              "  0.25001874566078186,\n",
              "  0.25001397728919983,\n",
              "  0.25000426173210144,\n",
              "  0.25000837445259094,\n",
              "  0.24998705089092255,\n",
              "  0.24990908801555634,\n",
              "  0.25002405047416687,\n",
              "  0.2500756084918976,\n",
              "  0.250018447637558,\n",
              "  0.2501027286052704,\n",
              "  0.25003114342689514,\n",
              "  0.25000515580177307,\n",
              "  0.2500324547290802,\n",
              "  0.2499760538339615,\n",
              "  0.24998490512371063,\n",
              "  0.24996037781238556,\n",
              "  0.24982786178588867,\n",
              "  0.25004497170448303,\n",
              "  0.25007885694503784,\n",
              "  0.25001585483551025,\n",
              "  0.25009530782699585,\n",
              "  0.24998120963573456,\n",
              "  0.2500418722629547,\n",
              "  0.2500115633010864,\n",
              "  0.25012466311454773,\n",
              "  0.25000903010368347,\n",
              "  0.2500401437282562,\n",
              "  0.2500995099544525,\n",
              "  0.25000011920928955,\n",
              "  0.2500036656856537,\n",
              "  0.25000330805778503,\n",
              "  0.24996338784694672,\n",
              "  0.25000643730163574,\n",
              "  0.24968212842941284,\n",
              "  0.2499396800994873,\n",
              "  0.24992690980434418,\n",
              "  0.24998979270458221,\n",
              "  0.24994099140167236,\n",
              "  0.2499120980501175,\n",
              "  0.24994750320911407,\n",
              "  0.2500542104244232,\n",
              "  0.2500647306442261,\n",
              "  0.2501313388347626,\n",
              "  0.2500132620334625,\n",
              "  0.2500356435775757,\n",
              "  0.25001034140586853,\n",
              "  0.24977195262908936,\n",
              "  0.2503120005130768,\n",
              "  0.24995936453342438,\n",
              "  0.2500002980232239,\n",
              "  0.2500973641872406,\n",
              "  0.25008293986320496,\n",
              "  0.2500388026237488,\n",
              "  0.2500006854534149,\n",
              "  0.24993936717510223,\n",
              "  0.25007185339927673,\n",
              "  0.25010886788368225,\n",
              "  0.25000011920928955,\n",
              "  0.2500661313533783,\n",
              "  0.2500600516796112,\n",
              "  0.2500125765800476,\n",
              "  0.25011560320854187,\n",
              "  0.25003355741500854,\n",
              "  0.2500114142894745,\n",
              "  0.25009027123451233,\n",
              "  0.2500201463699341,\n",
              "  0.2500494420528412,\n",
              "  0.249922513961792,\n",
              "  0.2500062882900238,\n",
              "  0.25001421570777893,\n",
              "  0.25008872151374817,\n",
              "  0.24996782839298248,\n",
              "  0.2500622570514679,\n",
              "  0.250011146068573,\n",
              "  0.24998630583286285,\n",
              "  0.25008872151374817,\n",
              "  0.2500011622905731,\n",
              "  0.2500249445438385,\n",
              "  0.25009962916374207,\n",
              "  0.25029733777046204,\n",
              "  0.2500479817390442,\n",
              "  0.24991166591644287,\n",
              "  0.2501697540283203,\n",
              "  0.2500404119491577,\n",
              "  0.2503163814544678,\n",
              "  0.25003325939178467,\n",
              "  0.2503543198108673,\n",
              "  0.2502448856830597,\n",
              "  0.2503068447113037,\n",
              "  0.2502441704273224,\n",
              "  0.2499147206544876,\n",
              "  0.2501041293144226,\n",
              "  0.2499924749135971,\n",
              "  0.24996745586395264,\n",
              "  0.24987615644931793,\n",
              "  0.25000301003456116,\n",
              "  0.2499486654996872,\n",
              "  0.2500220835208893,\n",
              "  0.25006499886512756,\n",
              "  0.25009775161743164,\n",
              "  0.2502215504646301,\n",
              "  0.2502337396144867,\n",
              "  0.2500125467777252,\n",
              "  0.2500254213809967,\n",
              "  0.2498963624238968,\n",
              "  0.2503828704357147,\n",
              "  0.25015732645988464,\n",
              "  0.25004467368125916,\n",
              "  0.24991250038146973,\n",
              "  0.25000128149986267,\n",
              "  0.2501063942909241,\n",
              "  0.2502707540988922,\n",
              "  0.25001242756843567,\n",
              "  0.2500976026058197,\n",
              "  0.2501283586025238,\n",
              "  0.25001999735832214,\n",
              "  0.250033974647522,\n",
              "  0.2500658631324768,\n",
              "  0.25001609325408936,\n",
              "  0.25002574920654297,\n",
              "  0.25000080466270447,\n",
              "  0.2501707673072815,\n",
              "  0.2501523494720459,\n",
              "  0.249984011054039,\n",
              "  0.25018495321273804,\n",
              "  0.2500004172325134,\n",
              "  0.2500000298023224,\n",
              "  0.2502363324165344,\n",
              "  0.2505562901496887,\n",
              "  0.25001421570777893,\n",
              "  0.25002095103263855,\n",
              "  0.250003844499588,\n",
              "  0.2500103712081909,\n",
              "  0.25005850195884705,\n",
              "  0.25031521916389465,\n",
              "  0.2501930296421051,\n",
              "  0.2499251812696457,\n",
              "  0.24990248680114746,\n",
              "  0.2500101625919342,\n",
              "  0.2500179708003998,\n",
              "  0.25000542402267456,\n",
              "  0.25000128149986267,\n",
              "  0.250076562166214,\n",
              "  0.25019940733909607,\n",
              "  0.24989788234233856,\n",
              "  0.2502377927303314,\n",
              "  0.2500004768371582,\n",
              "  0.25008058547973633,\n",
              "  0.2500873804092407,\n",
              "  0.25002801418304443,\n",
              "  0.24999326467514038,\n",
              "  0.24994798004627228,\n",
              "  0.25004124641418457,\n",
              "  0.2500631511211395,\n",
              "  0.2500055730342865,\n",
              "  0.25013408064842224,\n",
              "  0.25000283122062683,\n",
              "  0.24994920194149017,\n",
              "  0.2499508112668991,\n",
              "  0.25015661120414734,\n",
              "  0.25022366642951965,\n",
              "  0.25005677342414856,\n",
              "  0.25003519654273987,\n",
              "  0.2500327527523041,\n",
              "  0.25028035044670105,\n",
              "  0.25008562207221985,\n",
              "  0.25006964802742004,\n",
              "  0.25003042817115784,\n",
              "  0.25007835030555725,\n",
              "  0.2499847561120987,\n",
              "  0.24993737041950226,\n",
              "  0.2499598115682602,\n",
              "  0.2500056326389313,\n",
              "  0.25017860531806946,\n",
              "  0.250246524810791,\n",
              "  0.2502118945121765,\n",
              "  0.25006455183029175,\n",
              "  0.25031161308288574,\n",
              "  0.250033974647522,\n",
              "  0.25,\n",
              "  0.25018760561943054,\n",
              "  0.25024423003196716,\n",
              "  0.25003698468208313,\n",
              "  0.25002631545066833,\n",
              "  0.24999909102916718,\n",
              "  0.2502662241458893,\n",
              "  0.25001129508018494,\n",
              "  0.24991337954998016,\n",
              "  0.2500006854534149,\n",
              "  0.2502361536026001,\n",
              "  0.2501022517681122,\n",
              "  0.2500685751438141,\n",
              "  0.25017502903938293,\n",
              "  0.25000372529029846,\n",
              "  0.2501198351383209,\n",
              "  0.2502003312110901,\n",
              "  0.2500004470348358,\n",
              "  0.2500264644622803,\n",
              "  0.2500123977661133,\n",
              "  0.2499328851699829,\n",
              "  0.2501651644706726,\n",
              "  0.25000545382499695,\n",
              "  0.25002744793891907,\n",
              "  0.2500864565372467,\n",
              "  0.2501033842563629,\n",
              "  0.25003582239151,\n",
              "  0.2503609359264374,\n",
              "  0.25008320808410645,\n",
              "  0.250322550535202,\n",
              "  0.2500474750995636,\n",
              "  0.24981825053691864,\n",
              "  0.2500866949558258,\n",
              "  0.25005844235420227,\n",
              "  0.25025853514671326,\n",
              "  0.2502743899822235,\n",
              "  0.24992696940898895,\n",
              "  0.24996328353881836,\n",
              "  0.2501264810562134,\n",
              "  0.2501170337200165,\n",
              "  0.2501770853996277,\n",
              "  0.24994266033172607,\n",
              "  0.25011709332466125,\n",
              "  0.2503732144832611,\n",
              "  0.25027838349342346,\n",
              "  0.2501218318939209,\n",
              "  0.25024881958961487,\n",
              "  0.249974325299263,\n",
              "  0.25000011920928955,\n",
              "  0.25005072355270386,\n",
              "  0.2505188286304474,\n",
              "  0.24991662800312042,\n",
              "  0.25012949109077454,\n",
              "  0.2500040829181671,\n",
              "  0.25007644295692444,\n",
              "  0.25001704692840576,\n",
              "  0.24992835521697998,\n",
              "  0.2502609193325043,\n",
              "  0.2500176429748535,\n",
              "  0.2500006854534149,\n",
              "  0.250002920627594,\n",
              "  0.25009337067604065,\n",
              "  0.2500048875808716,\n",
              "  0.25016656517982483,\n",
              "  0.2500338852405548,\n",
              "  0.2500331699848175,\n",
              "  0.25046679377555847,\n",
              "  0.2500673830509186,\n",
              "  0.25000104308128357,\n",
              "  0.25028738379478455,\n",
              "  0.2500201165676117,\n",
              "  0.24973036348819733,\n",
              "  0.2500143051147461,\n",
              "  0.25007686018943787,\n",
              "  0.2500188648700714,\n",
              "  0.2500249147415161,\n",
              "  0.25000283122062683,\n",
              "  0.2500031292438507,\n",
              "  0.25035399198532104,\n",
              "  0.2500627636909485,\n",
              "  0.24995295703411102,\n",
              "  0.25026431679725647,\n",
              "  0.24996669590473175,\n",
              "  0.2500089406967163,\n",
              "  0.24998940527439117,\n",
              "  0.250117689371109,\n",
              "  0.25003981590270996,\n",
              "  0.24996663630008698,\n",
              "  0.2500293552875519,\n",
              "  0.2502143681049347,\n",
              "  0.2499331831932068,\n",
              "  0.25028276443481445,\n",
              "  0.25044867396354675,\n",
              "  0.2500079870223999,\n",
              "  0.2500609755516052,\n",
              "  0.25002261996269226,\n",
              "  0.250026673078537,\n",
              "  0.25002801418304443]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "40c3534a-bcd6-483c-81cf-3e65a0c8328e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression_freeze_500.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression_freeze_500.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "57c37a85-16d8-4076-9530-4952f50b1178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f03ba9b-65aa-4893-a9dc-a87b0aa46f28\", \"2Class_regression_freeze_500.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}