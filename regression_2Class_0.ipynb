{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPgP6AOR9/aqaX+8t66HOKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb7d8d5-4e63-444b-d4e7-fe6930a4adcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class เพิ่ม 4 paper.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "bebf5ca0-790b-4ef5-93a1-a9d6d80a6265"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "825  826  1-s2.0-S2095268622000210-main   \n",
              "826  827  1-s2.0-S2095268622000210-main   \n",
              "827  828  1-s2.0-S2095268622000210-main   \n",
              "828  829  1-s2.0-S2095268622000210-main   \n",
              "829  830  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "825  Integration of preparation of K, Na-embedded a...   \n",
              "826  Integration of preparation of K, Na-embedded a...   \n",
              "827  Integration of preparation of K, Na-embedded a...   \n",
              "828  Integration of preparation of K, Na-embedded a...   \n",
              "829  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "825  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "826  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "827  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "828  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "829  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "825  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "826  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "827  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "828  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "829  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "825          10         0  \n",
              "826          10         0  \n",
              "827          10         0  \n",
              "828          10         0  \n",
              "829          10         0  \n",
              "\n",
              "[830 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78669bb7-4fc7-446b-80f0-e15eeb763e01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>826</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>827</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>828</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>829</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>830</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>830 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78669bb7-4fc7-446b-80f0-e15eeb763e01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78669bb7-4fc7-446b-80f0-e15eeb763e01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78669bb7-4fc7-446b-80f0-e15eeb763e01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "db504960-8002-4720-bf51-99d3c9e3dd67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "700aedd1-56ae-4e68-a8a3-803d78cb4aab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c74704-d624-4703-9bb1-5d99e89a4e7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val = df[df['No'].between(629,729)]\n",
        "train = df[df['No'].between(1,628)]\n",
        "test = df[df['No'].between(730,830)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new project'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "id": "XyxDPsEi6yYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b881719-8736-4fa7-edef-d9ff9ff1c785"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new project/train\n",
            "/content/drive/My Drive/new project/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 628  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e777bb28-56cb-4497-d031-20f21f1cda01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 1079, done.\u001b[K\n",
            "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 1079 (delta 121), reused 241 (delta 121), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (1079/1079), 13.94 MiB | 12.28 MiB/s, done.\n",
            "Resolving deltas: 100% (618/618), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "c46dc48a-140f-422b-e267-61fd8315afee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722e9b27-2c8d-487e-805a-714758163c87"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca376247-00d7-409b-de9d-0714788acd4d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dec23da-0d82-45f5-8593-1a1a1dc2a1bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8fdf7e-8d9d-4329-ac32-c1fbbb536481"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 628 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=9e-3),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09b30a0-bc77-4ac8-8dbf-fe8985ccb4b5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-0e0394aaa368>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "39/39 [==============================] - 124s 3s/step - loss: 1445241.8750 - mae: 968.3544 - val_loss: 528869.5625 - val_mae: 557.8769\n",
            "Epoch 2/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1442345.8750 - mae: 966.9557 - val_loss: 545494.1875 - val_mae: 574.0803\n",
            "Epoch 3/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 1440865.3750 - mae: 966.4443 - val_loss: 506244.1562 - val_mae: 541.9510\n",
            "Epoch 4/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1452527.5000 - mae: 972.9606 - val_loss: 535284.5625 - val_mae: 564.5291\n",
            "Epoch 5/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 1444905.2500 - mae: 968.6490 - val_loss: 513005.9062 - val_mae: 544.6360\n",
            "Epoch 6/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 1406011.1250 - mae: 951.9712 - val_loss: 525030.8750 - val_mae: 554.4257\n",
            "Epoch 7/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1442729.2500 - mae: 966.1057 - val_loss: 532232.1875 - val_mae: 561.8256\n",
            "Epoch 8/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1416124.2500 - mae: 956.8988 - val_loss: 532243.2500 - val_mae: 562.0644\n",
            "Epoch 9/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1434960.8750 - mae: 962.5458 - val_loss: 544149.2500 - val_mae: 571.5724\n",
            "Epoch 10/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1426287.6250 - mae: 957.8331 - val_loss: 521987.5938 - val_mae: 551.6744\n",
            "Epoch 11/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 1432814.0000 - mae: 962.0333 - val_loss: 521234.6250 - val_mae: 550.9915\n",
            "Epoch 12/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 1431876.1250 - mae: 959.8752 - val_loss: 529096.5625 - val_mae: 559.0215\n",
            "Epoch 13/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1413058.1250 - mae: 953.9845 - val_loss: 528287.0625 - val_mae: 558.0603\n",
            "Epoch 14/1000\n",
            "39/39 [==============================] - 3s 80ms/step - loss: 1412081.6250 - mae: 951.4720 - val_loss: 514867.3438 - val_mae: 546.9002\n",
            "Epoch 15/1000\n",
            "39/39 [==============================] - 8s 200ms/step - loss: 1406752.6250 - mae: 952.0256 - val_loss: 509658.8750 - val_mae: 539.7884\n",
            "Epoch 16/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1428334.1250 - mae: 960.8358 - val_loss: 517448.1562 - val_mae: 547.5447\n",
            "Epoch 17/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1415065.5000 - mae: 954.3406 - val_loss: 525217.9375 - val_mae: 555.3037\n",
            "Epoch 18/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 1407023.3750 - mae: 949.0410 - val_loss: 528614.2500 - val_mae: 556.9480\n",
            "Epoch 19/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1419990.7500 - mae: 954.7605 - val_loss: 536312.5625 - val_mae: 564.4361\n",
            "Epoch 20/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1415495.7500 - mae: 953.9650 - val_loss: 514520.5000 - val_mae: 545.1028\n",
            "Epoch 21/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1429563.6250 - mae: 959.4232 - val_loss: 505511.8750 - val_mae: 539.7879\n",
            "Epoch 22/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 1405164.7500 - mae: 951.6077 - val_loss: 513016.2500 - val_mae: 543.7214\n",
            "Epoch 23/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 1417902.3750 - mae: 954.0664 - val_loss: 504064.2812 - val_mae: 538.6841\n",
            "Epoch 24/1000\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1403220.1250 - mae: 951.7626 - val_loss: 528400.7500 - val_mae: 559.2351\n",
            "Epoch 25/1000\n",
            "39/39 [==============================] - 9s 205ms/step - loss: 1411337.8750 - mae: 949.8445 - val_loss: 519154.1562 - val_mae: 549.8167\n",
            "Epoch 26/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 1384875.2500 - mae: 940.1815 - val_loss: 518508.5000 - val_mae: 549.7104\n",
            "Epoch 27/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 1408057.2500 - mae: 948.6387 - val_loss: 517693.5000 - val_mae: 548.7276\n",
            "Epoch 28/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1404284.8750 - mae: 947.8984 - val_loss: 500114.3438 - val_mae: 530.8738\n",
            "Epoch 29/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1392809.8750 - mae: 942.7770 - val_loss: 507730.8750 - val_mae: 538.3572\n",
            "Epoch 30/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1389433.8750 - mae: 942.9469 - val_loss: 511112.3438 - val_mae: 540.0020\n",
            "Epoch 31/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1398694.8750 - mae: 944.3764 - val_loss: 514721.8438 - val_mae: 546.0132\n",
            "Epoch 32/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1413452.5000 - mae: 953.8215 - val_loss: 501485.0000 - val_mae: 534.2839\n",
            "Epoch 33/1000\n",
            "39/39 [==============================] - 8s 206ms/step - loss: 1403804.0000 - mae: 947.3748 - val_loss: 513229.7188 - val_mae: 544.6450\n",
            "Epoch 34/1000\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 1393608.7500 - mae: 943.6929 - val_loss: 520801.5938 - val_mae: 552.3991\n",
            "Epoch 35/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1390640.6250 - mae: 939.9680 - val_loss: 503376.3438 - val_mae: 534.5403\n",
            "Epoch 36/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 1398988.7500 - mae: 946.6425 - val_loss: 506909.2500 - val_mae: 540.2727\n",
            "Epoch 37/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1369783.3750 - mae: 934.0939 - val_loss: 510300.0312 - val_mae: 542.1929\n",
            "Epoch 38/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 1392435.8750 - mae: 942.0830 - val_loss: 517872.3750 - val_mae: 550.2311\n",
            "Epoch 39/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1388831.2500 - mae: 939.9342 - val_loss: 512767.5312 - val_mae: 542.2905\n",
            "Epoch 40/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1379673.6250 - mae: 936.7772 - val_loss: 499747.7188 - val_mae: 531.1353\n",
            "Epoch 41/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1392721.5000 - mae: 942.5964 - val_loss: 507267.9062 - val_mae: 538.8990\n",
            "Epoch 42/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1399786.3750 - mae: 947.1077 - val_loss: 494260.2500 - val_mae: 527.7242\n",
            "Epoch 43/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 1390638.1250 - mae: 940.2770 - val_loss: 501546.6562 - val_mae: 530.8257\n",
            "Epoch 44/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1389825.1250 - mae: 940.4742 - val_loss: 484622.2500 - val_mae: 517.9234\n",
            "Epoch 45/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 1372978.0000 - mae: 932.8179 - val_loss: 512571.7812 - val_mae: 544.8991\n",
            "Epoch 46/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1365229.7500 - mae: 930.3945 - val_loss: 487387.6250 - val_mae: 522.9760\n",
            "Epoch 47/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1366110.0000 - mae: 933.7869 - val_loss: 511091.3750 - val_mae: 543.5390\n",
            "Epoch 48/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1368829.3750 - mae: 931.1629 - val_loss: 502170.0000 - val_mae: 534.3955\n",
            "Epoch 49/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1371344.7500 - mae: 931.5792 - val_loss: 489226.6562 - val_mae: 522.6863\n",
            "Epoch 50/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 1375137.7500 - mae: 932.8300 - val_loss: 476382.5938 - val_mae: 511.5375\n",
            "Epoch 51/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 1383544.3750 - mae: 937.8580 - val_loss: 500043.5938 - val_mae: 532.6506\n",
            "Epoch 52/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1377814.1250 - mae: 937.1614 - val_loss: 491097.0938 - val_mae: 522.9285\n",
            "Epoch 53/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1357873.2500 - mae: 924.6980 - val_loss: 490363.3438 - val_mae: 521.9778\n",
            "Epoch 54/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1390251.8750 - mae: 940.2267 - val_loss: 497790.7812 - val_mae: 530.0331\n",
            "Epoch 55/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1377420.8750 - mae: 933.7474 - val_loss: 509137.8438 - val_mae: 539.8272\n",
            "Epoch 56/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 1371611.1250 - mae: 932.5933 - val_loss: 508466.5938 - val_mae: 539.7062\n",
            "Epoch 57/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1365580.3750 - mae: 929.3933 - val_loss: 471502.6250 - val_mae: 506.7452\n",
            "Epoch 58/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1350016.8750 - mae: 923.1039 - val_loss: 498894.7812 - val_mae: 529.3450\n",
            "Epoch 59/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 1360318.6250 - mae: 928.4598 - val_loss: 494211.3750 - val_mae: 526.6456\n",
            "Epoch 60/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 1363074.8750 - mae: 928.0781 - val_loss: 481461.3438 - val_mae: 515.2044\n",
            "Epoch 61/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1376877.1250 - mae: 935.2529 - val_loss: 488801.8438 - val_mae: 523.2468\n",
            "Epoch 62/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1350825.8750 - mae: 920.9719 - val_loss: 472089.2812 - val_mae: 505.6794\n",
            "Epoch 63/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 1366801.3750 - mae: 930.9727 - val_loss: 495322.4688 - val_mae: 526.2122\n",
            "Epoch 64/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1339915.3750 - mae: 919.0356 - val_loss: 490657.2500 - val_mae: 523.5129\n",
            "Epoch 65/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1358303.1250 - mae: 926.5856 - val_loss: 489964.2812 - val_mae: 523.1036\n",
            "Epoch 66/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1341948.7500 - mae: 918.9296 - val_loss: 488538.9062 - val_mae: 521.4927\n",
            "Epoch 67/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1367213.6250 - mae: 929.6600 - val_loss: 456715.9688 - val_mae: 491.5035\n",
            "Epoch 68/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1349530.6250 - mae: 921.7167 - val_loss: 495765.0938 - val_mae: 529.2526\n",
            "Epoch 69/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1373656.8750 - mae: 931.3271 - val_loss: 495058.1250 - val_mae: 528.8389\n",
            "Epoch 70/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 1350023.3750 - mae: 921.7390 - val_loss: 474524.6562 - val_mae: 508.6819\n",
            "Epoch 71/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 1348704.8750 - mae: 921.3530 - val_loss: 469917.0000 - val_mae: 506.2514\n",
            "Epoch 72/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1354726.6250 - mae: 923.4706 - val_loss: 477067.4062 - val_mae: 509.0826\n",
            "Epoch 73/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1352547.6250 - mae: 923.7306 - val_loss: 476364.1250 - val_mae: 508.3914\n",
            "Epoch 74/1000\n",
            "39/39 [==============================] - 5s 91ms/step - loss: 1314663.7500 - mae: 907.4474 - val_loss: 503263.5938 - val_mae: 535.6501\n",
            "Epoch 75/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1342041.5000 - mae: 916.8776 - val_loss: 455348.2188 - val_mae: 492.5076\n",
            "Epoch 76/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1323901.1250 - mae: 908.8812 - val_loss: 478283.6562 - val_mae: 513.3539\n",
            "Epoch 77/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1343046.7500 - mae: 917.6138 - val_loss: 461236.2188 - val_mae: 494.8642\n",
            "Epoch 78/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 1336690.6250 - mae: 914.6039 - val_loss: 476908.7500 - val_mae: 512.0130\n",
            "Epoch 79/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 1340641.6250 - mae: 917.0086 - val_loss: 491847.4688 - val_mae: 523.5676\n",
            "Epoch 80/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1343795.0000 - mae: 917.2372 - val_loss: 448150.2188 - val_mae: 482.7110\n",
            "Epoch 81/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 1323389.2500 - mae: 911.6945 - val_loss: 470938.2500 - val_mae: 503.2845\n",
            "Epoch 82/1000\n",
            "39/39 [==============================] - 9s 208ms/step - loss: 1331366.7500 - mae: 912.4092 - val_loss: 470272.1562 - val_mae: 502.8805\n",
            "Epoch 83/1000\n",
            "39/39 [==============================] - 5s 125ms/step - loss: 1336730.6250 - mae: 913.7734 - val_loss: 473452.9062 - val_mae: 508.6270\n",
            "Epoch 84/1000\n",
            "39/39 [==============================] - 4s 78ms/step - loss: 1339608.0000 - mae: 916.0458 - val_loss: 480563.1250 - val_mae: 511.7367\n",
            "Epoch 85/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1345380.5000 - mae: 920.2554 - val_loss: 472078.5312 - val_mae: 507.2742\n",
            "Epoch 86/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1334657.6250 - mae: 912.7617 - val_loss: 483027.4688 - val_mae: 517.3393\n",
            "Epoch 87/1000\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 1331602.2500 - mae: 911.6244 - val_loss: 447498.6562 - val_mae: 480.0052\n",
            "Epoch 88/1000\n",
            "39/39 [==============================] - 3s 79ms/step - loss: 1332651.6250 - mae: 914.4214 - val_loss: 485541.0938 - val_mae: 518.0322\n",
            "Epoch 89/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1335590.2500 - mae: 913.2269 - val_loss: 473217.9062 - val_mae: 506.5831\n",
            "Epoch 90/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1319918.0000 - mae: 909.2195 - val_loss: 445477.1562 - val_mae: 482.6427\n",
            "Epoch 91/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1324303.8750 - mae: 908.6345 - val_loss: 471841.1562 - val_mae: 504.9608\n",
            "Epoch 92/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 1342194.7500 - mae: 916.4174 - val_loss: 475020.7500 - val_mae: 506.0304\n",
            "Epoch 93/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 1318933.7500 - mae: 905.5262 - val_loss: 462812.1250 - val_mae: 495.1456\n",
            "Epoch 94/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 1314019.0000 - mae: 904.5146 - val_loss: 458251.0312 - val_mae: 492.4262\n",
            "Epoch 95/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 1327571.6250 - mae: 911.6776 - val_loss: 449962.7812 - val_mae: 483.3065\n",
            "Epoch 96/1000\n",
            "39/39 [==============================] - 4s 81ms/step - loss: 1322942.5000 - mae: 905.8958 - val_loss: 460794.5938 - val_mae: 493.1041\n",
            "Epoch 97/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 1299092.0000 - mae: 896.1733 - val_loss: 467749.3438 - val_mae: 500.8928\n",
            "Epoch 98/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 1315354.3750 - mae: 904.6450 - val_loss: 467084.5938 - val_mae: 500.4929\n",
            "Epoch 99/1000\n",
            "39/39 [==============================] - 3s 78ms/step - loss: 1302849.2500 - mae: 900.8344 - val_loss: 458191.6562 - val_mae: 491.0049\n",
            "Epoch 100/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1312259.5000 - mae: 901.6655 - val_loss: 469574.1562 - val_mae: 500.8848\n",
            "Epoch 101/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1315783.6250 - mae: 905.7451 - val_loss: 449926.2188 - val_mae: 481.2887\n",
            "Epoch 102/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 1311569.3750 - mae: 903.0054 - val_loss: 464393.2500 - val_mae: 498.0626\n",
            "Epoch 103/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 1324677.8750 - mae: 907.8273 - val_loss: 463703.5938 - val_mae: 497.1038\n",
            "Epoch 104/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 1315080.7500 - mae: 903.9663 - val_loss: 466866.8750 - val_mae: 498.1749\n",
            "Epoch 105/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 1305853.7500 - mae: 897.9269 - val_loss: 450992.6562 - val_mae: 485.0002\n",
            "Epoch 106/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 1300677.8750 - mae: 898.4261 - val_loss: 465542.4062 - val_mae: 497.1109\n",
            "Epoch 107/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 1289216.7500 - mae: 894.5874 - val_loss: 464893.0000 - val_mae: 496.7248\n",
            "Epoch 108/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1297393.7500 - mae: 896.2683 - val_loss: 467852.1562 - val_mae: 502.4586\n",
            "Epoch 109/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1300347.3750 - mae: 895.6221 - val_loss: 459673.7500 - val_mae: 492.7656\n",
            "Epoch 110/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1283925.5000 - mae: 889.2206 - val_loss: 470331.2812 - val_mae: 503.1376\n",
            "Epoch 111/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 1318877.1250 - mae: 906.3461 - val_loss: 462203.6562 - val_mae: 493.7413\n",
            "Epoch 112/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 1297421.3750 - mae: 896.5269 - val_loss: 468951.8750 - val_mae: 501.2258\n",
            "Epoch 113/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 1301858.1250 - mae: 896.4456 - val_loss: 468280.7500 - val_mae: 500.8258\n",
            "Epoch 114/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1299964.2500 - mae: 899.4117 - val_loss: 455730.0000 - val_mae: 489.0253\n",
            "Epoch 115/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 1295516.1250 - mae: 893.8900 - val_loss: 455711.5000 - val_mae: 488.9990\n",
            "Epoch 116/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1296651.8750 - mae: 897.8199 - val_loss: 455037.7812 - val_mae: 488.0390\n",
            "Epoch 117/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1308690.2500 - mae: 900.8172 - val_loss: 450558.4688 - val_mae: 485.0575\n",
            "Epoch 118/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1289984.2500 - mae: 892.9613 - val_loss: 457543.3750 - val_mae: 488.7276\n",
            "Epoch 119/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 1290142.1250 - mae: 892.3697 - val_loss: 456880.6562 - val_mae: 488.0492\n",
            "Epoch 120/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 1300326.1250 - mae: 894.7894 - val_loss: 448603.2188 - val_mae: 483.3101\n",
            "Epoch 121/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1293694.3750 - mae: 892.8542 - val_loss: 436831.6562 - val_mae: 472.4460\n",
            "Epoch 122/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1300541.1250 - mae: 896.2513 - val_loss: 439970.6250 - val_mae: 473.2281\n",
            "Epoch 123/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1280623.2500 - mae: 887.5662 - val_loss: 468868.0938 - val_mae: 502.2455\n",
            "Epoch 124/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 1281542.0000 - mae: 891.0945 - val_loss: 445996.7812 - val_mae: 480.6800\n",
            "Epoch 125/1000\n",
            "39/39 [==============================] - 4s 86ms/step - loss: 1282937.1250 - mae: 889.8014 - val_loss: 449130.0938 - val_mae: 482.6305\n",
            "Epoch 126/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1306847.6250 - mae: 899.5400 - val_loss: 444680.2812 - val_mae: 480.2578\n",
            "Epoch 127/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1264634.7500 - mae: 879.0039 - val_loss: 458891.5000 - val_mae: 492.6613\n",
            "Epoch 128/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1284124.1250 - mae: 888.6294 - val_loss: 436156.4062 - val_mae: 471.1447\n",
            "Epoch 129/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1273565.8750 - mae: 883.4616 - val_loss: 432061.5000 - val_mae: 464.1930\n",
            "Epoch 130/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1263509.1250 - mae: 880.3662 - val_loss: 442115.5312 - val_mae: 478.9161\n",
            "Epoch 131/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 1253991.8750 - mae: 873.3204 - val_loss: 438050.3438 - val_mae: 472.0034\n",
            "Epoch 132/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1276973.6250 - mae: 884.1107 - val_loss: 437431.1562 - val_mae: 471.8402\n",
            "Epoch 133/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1261966.6250 - mae: 875.4867 - val_loss: 422055.9062 - val_mae: 458.8517\n",
            "Epoch 134/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 1266704.3750 - mae: 878.4922 - val_loss: 443355.2812 - val_mae: 479.4906\n",
            "Epoch 135/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1267395.0000 - mae: 880.1770 - val_loss: 420834.7500 - val_mae: 458.2083\n",
            "Epoch 136/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 1291985.7500 - mae: 892.2994 - val_loss: 427141.4062 - val_mae: 461.5862\n",
            "Epoch 137/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1280602.5000 - mae: 886.4404 - val_loss: 441430.7812 - val_mae: 478.5191\n",
            "Epoch 138/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 1279410.7500 - mae: 885.6091 - val_loss: 426145.8438 - val_mae: 465.8307\n",
            "Epoch 139/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1266599.8750 - mae: 878.0246 - val_loss: 436794.9062 - val_mae: 471.5732\n",
            "Epoch 140/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1264314.5000 - mae: 880.5845 - val_loss: 443265.9688 - val_mae: 479.6620\n",
            "Epoch 141/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1270897.8750 - mae: 879.0317 - val_loss: 445992.6562 - val_mae: 485.7352\n",
            "Epoch 142/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1246712.6250 - mae: 870.5776 - val_loss: 420324.6562 - val_mae: 457.9952\n",
            "Epoch 143/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1285660.0000 - mae: 890.6805 - val_loss: 430533.5000 - val_mae: 468.1327\n",
            "Epoch 144/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 1254119.2500 - mae: 879.0045 - val_loss: 436976.5000 - val_mae: 476.2282\n",
            "Epoch 145/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 1248321.7500 - mae: 871.3686 - val_loss: 440080.0938 - val_mae: 477.9060\n",
            "Epoch 146/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 1260307.8750 - mae: 875.5026 - val_loss: 403359.8750 - val_mae: 444.1687\n",
            "Epoch 147/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1265871.7500 - mae: 878.6630 - val_loss: 445839.0312 - val_mae: 485.7025\n",
            "Epoch 148/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1256551.7500 - mae: 874.2271 - val_loss: 431143.4062 - val_mae: 468.3971\n",
            "Epoch 149/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 1233814.6250 - mae: 863.8693 - val_loss: 440825.5312 - val_mae: 482.8331\n",
            "Epoch 150/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1248443.2500 - mae: 870.1520 - val_loss: 450902.7500 - val_mae: 492.9416\n",
            "Epoch 151/1000\n",
            "39/39 [==============================] - 10s 229ms/step - loss: 1247415.1250 - mae: 872.6426 - val_loss: 443272.2812 - val_mae: 484.3552\n",
            "Epoch 152/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 1270067.2500 - mae: 882.4903 - val_loss: 442628.0000 - val_mae: 484.0158\n",
            "Epoch 153/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 1251215.0000 - mae: 872.3427 - val_loss: 424344.3438 - val_mae: 464.7545\n",
            "Epoch 154/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 1250719.0000 - mae: 875.0082 - val_loss: 430660.7188 - val_mae: 472.6992\n",
            "Epoch 155/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1252010.6250 - mae: 871.9668 - val_loss: 419416.1250 - val_mae: 462.2194\n",
            "Epoch 156/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 1217920.2500 - mae: 860.7325 - val_loss: 418791.4688 - val_mae: 461.5281\n",
            "Epoch 157/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1247652.3750 - mae: 868.5108 - val_loss: 435742.5312 - val_mae: 480.3033\n",
            "Epoch 158/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1252472.2500 - mae: 872.5420 - val_loss: 431898.8438 - val_mae: 473.3382\n",
            "Epoch 159/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 1243829.7500 - mae: 865.9877 - val_loss: 410087.9062 - val_mae: 452.2554\n",
            "Epoch 160/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 1241902.1250 - mae: 866.7553 - val_loss: 430674.2500 - val_mae: 472.8813\n",
            "Epoch 161/1000\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 1254466.0000 - mae: 874.7048 - val_loss: 433208.8750 - val_mae: 478.6996\n",
            "Epoch 162/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1222172.8750 - mae: 861.2879 - val_loss: 415158.7812 - val_mae: 459.6830\n",
            "Epoch 163/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1200208.1250 - mae: 851.9863 - val_loss: 428786.3750 - val_mae: 471.3013\n",
            "Epoch 164/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 1225161.0000 - mae: 860.1552 - val_loss: 410280.2812 - val_mae: 457.0208\n",
            "Epoch 165/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1246530.2500 - mae: 869.2770 - val_loss: 430684.5000 - val_mae: 477.3620\n",
            "Epoch 166/1000\n",
            "39/39 [==============================] - 4s 105ms/step - loss: 1237568.5000 - mae: 864.2128 - val_loss: 413317.9062 - val_mae: 454.9893\n",
            "Epoch 167/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 1241623.1250 - mae: 867.2328 - val_loss: 405350.4062 - val_mae: 450.9412\n",
            "Epoch 168/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 1236037.7500 - mae: 865.3207 - val_loss: 415248.1562 - val_mae: 461.0833\n",
            "Epoch 169/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 1224582.2500 - mae: 859.3975 - val_loss: 414645.2812 - val_mae: 461.0833\n",
            "Epoch 170/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1235811.0000 - mae: 863.3955 - val_loss: 424501.8438 - val_mae: 471.7463\n",
            "Epoch 171/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1214694.6250 - mae: 855.0283 - val_loss: 417123.6562 - val_mae: 463.3958\n",
            "Epoch 172/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1234380.3750 - mae: 863.0226 - val_loss: 419593.7812 - val_mae: 469.6865\n",
            "Epoch 173/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1233412.5000 - mae: 863.0972 - val_loss: 412241.2812 - val_mae: 461.0833\n",
            "Epoch 174/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 1234152.1250 - mae: 861.7463 - val_loss: 401265.8750 - val_mae: 451.0396\n",
            "Epoch 175/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1224840.6250 - mae: 857.5117 - val_loss: 414111.9062 - val_mae: 467.6129\n",
            "Epoch 176/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 1239571.3750 - mae: 866.2473 - val_loss: 410453.0938 - val_mae: 461.0833\n",
            "Epoch 177/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1229477.6250 - mae: 858.4583 - val_loss: 416559.7812 - val_mae: 469.3351\n",
            "Epoch 178/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1232773.5000 - mae: 862.4426 - val_loss: 419610.3438 - val_mae: 471.6333\n",
            "Epoch 179/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 1217393.8750 - mae: 853.8804 - val_loss: 402022.0312 - val_mae: 453.1411\n",
            "Epoch 180/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 1239679.6250 - mae: 866.3313 - val_loss: 411107.6562 - val_mae: 467.2616\n",
            "Epoch 181/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1211676.3750 - mae: 853.6340 - val_loss: 400226.1250 - val_mae: 457.3021\n",
            "Epoch 182/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1201531.3750 - mae: 847.7382 - val_loss: 406919.1250 - val_mae: 461.0833\n",
            "Epoch 183/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 1201246.8750 - mae: 848.1064 - val_loss: 412959.4062 - val_mae: 469.2508\n",
            "Epoch 184/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1212398.6250 - mae: 852.3198 - val_loss: 412367.4062 - val_mae: 469.2369\n",
            "Epoch 185/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 1225617.2500 - mae: 858.6714 - val_loss: 415419.6562 - val_mae: 471.5354\n",
            "Epoch 186/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1217797.3750 - mae: 855.2448 - val_loss: 403992.6562 - val_mae: 460.7188\n",
            "Epoch 187/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1226361.8750 - mae: 859.6485 - val_loss: 414183.8438 - val_mae: 470.9446\n",
            "Epoch 188/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1203370.8750 - mae: 847.2545 - val_loss: 406377.4062 - val_mae: 467.4306\n",
            "Epoch 189/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 1213737.7500 - mae: 854.1404 - val_loss: 402855.2500 - val_mae: 461.3646\n",
            "Epoch 190/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1206288.3750 - mae: 850.7841 - val_loss: 402276.6562 - val_mae: 461.3646\n",
            "Epoch 191/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 1172061.3750 - mae: 839.4250 - val_loss: 414739.1250 - val_mae: 476.9132\n",
            "Epoch 192/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1207221.5000 - mae: 851.4426 - val_loss: 407660.8438 - val_mae: 469.4062\n",
            "Epoch 193/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 1181086.6250 - mae: 836.6986 - val_loss: 413545.3750 - val_mae: 476.8573\n",
            "Epoch 194/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1212355.2500 - mae: 850.8348 - val_loss: 403622.6562 - val_mae: 463.6771\n",
            "Epoch 195/1000\n",
            "39/39 [==============================] - 4s 92ms/step - loss: 1212712.3750 - mae: 851.1754 - val_loss: 409498.9062 - val_mae: 471.3953\n",
            "Epoch 196/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1196738.3750 - mae: 845.2327 - val_loss: 392381.7188 - val_mae: 453.6603\n",
            "Epoch 197/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1213667.2500 - mae: 852.6009 - val_loss: 404724.7812 - val_mae: 469.3359\n",
            "Epoch 198/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1201737.1250 - mae: 844.7239 - val_loss: 407699.0938 - val_mae: 470.7908\n",
            "Epoch 199/1000\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 1201072.7500 - mae: 845.9598 - val_loss: 383486.9062 - val_mae: 449.3589\n",
            "Epoch 200/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1209307.3750 - mae: 852.1429 - val_loss: 396515.3750 - val_mae: 461.0833\n",
            "Epoch 201/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 1191903.3750 - mae: 841.3652 - val_loss: 398829.0312 - val_mae: 467.5298\n",
            "Epoch 202/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 1191197.5000 - mae: 842.1920 - val_loss: 418174.8438 - val_mae: 486.5372\n",
            "Epoch 203/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 1185062.6250 - mae: 838.1304 - val_loss: 417617.8750 - val_mae: 487.0577\n",
            "Epoch 204/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1179738.5000 - mae: 837.3180 - val_loss: 400686.4688 - val_mae: 469.5190\n",
            "Epoch 205/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1194540.3750 - mae: 843.4007 - val_loss: 393746.1562 - val_mae: 461.6458\n",
            "Epoch 206/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1198132.7500 - mae: 842.6757 - val_loss: 373309.4688 - val_mae: 441.8925\n",
            "Epoch 207/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1194834.2500 - mae: 842.8602 - val_loss: 405247.3438 - val_mae: 476.4648\n",
            "Epoch 208/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1157406.0000 - mae: 828.0399 - val_loss: 381545.7500 - val_mae: 451.1513\n",
            "Epoch 209/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 1191521.0000 - mae: 841.4064 - val_loss: 397787.4688 - val_mae: 468.8871\n",
            "Epoch 210/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1187297.6250 - mae: 839.0311 - val_loss: 387332.9062 - val_mae: 459.0521\n",
            "Epoch 211/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1184997.7500 - mae: 838.1230 - val_loss: 396699.9688 - val_mae: 469.4216\n",
            "Epoch 212/1000\n",
            "39/39 [==============================] - 4s 85ms/step - loss: 1188954.7500 - mae: 838.0651 - val_loss: 396085.7188 - val_mae: 469.1259\n",
            "Epoch 213/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 1171749.2500 - mae: 830.8496 - val_loss: 401810.0938 - val_mae: 476.8597\n",
            "Epoch 214/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1176225.7500 - mae: 833.2548 - val_loss: 382433.6250 - val_mae: 453.9125\n",
            "Epoch 215/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1184090.5000 - mae: 840.2219 - val_loss: 394365.4062 - val_mae: 468.8029\n",
            "Epoch 216/1000\n",
            "39/39 [==============================] - 4s 104ms/step - loss: 1189308.5000 - mae: 840.5567 - val_loss: 377798.4688 - val_mae: 451.9093\n",
            "Epoch 217/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 1192807.6250 - mae: 843.4198 - val_loss: 403015.7500 - val_mae: 478.7776\n",
            "Epoch 218/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 1173796.0000 - mae: 832.0570 - val_loss: 376704.8438 - val_mae: 451.9378\n",
            "Epoch 219/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 1186872.5000 - mae: 838.9825 - val_loss: 401794.2188 - val_mae: 478.1587\n",
            "Epoch 220/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1168001.7500 - mae: 831.5682 - val_loss: 368520.4062 - val_mae: 447.6223\n",
            "Epoch 221/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1172917.8750 - mae: 831.8909 - val_loss: 384220.2500 - val_mae: 461.0000\n",
            "Epoch 222/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 1168676.7500 - mae: 829.7587 - val_loss: 390371.2812 - val_mae: 468.4232\n",
            "Epoch 223/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 1178093.5000 - mae: 834.2241 - val_loss: 383663.5000 - val_mae: 461.0833\n",
            "Epoch 224/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 1160844.8750 - mae: 829.0738 - val_loss: 389316.5000 - val_mae: 468.9578\n",
            "Epoch 225/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 1179702.2500 - mae: 832.7214 - val_loss: 372975.1562 - val_mae: 452.0354\n",
            "Epoch 226/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1161626.6250 - mae: 824.5183 - val_loss: 378584.4062 - val_mae: 459.6145\n",
            "Epoch 227/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1164489.1250 - mae: 826.8741 - val_loss: 384098.0000 - val_mae: 466.6032\n",
            "Epoch 228/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1170989.8750 - mae: 831.1164 - val_loss: 364366.2500 - val_mae: 448.0147\n",
            "Epoch 229/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 1155475.0000 - mae: 822.6208 - val_loss: 386528.5938 - val_mae: 468.6071\n",
            "Epoch 230/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1175467.5000 - mae: 835.2075 - val_loss: 379951.7812 - val_mae: 461.6458\n",
            "Epoch 231/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1155803.2500 - mae: 823.0662 - val_loss: 395008.5938 - val_mae: 478.1062\n",
            "Epoch 232/1000\n",
            "39/39 [==============================] - 9s 235ms/step - loss: 1169750.8750 - mae: 829.5560 - val_loss: 362294.4062 - val_mae: 448.0703\n",
            "Epoch 233/1000\n",
            "39/39 [==============================] - 9s 209ms/step - loss: 1164406.7500 - mae: 826.9000 - val_loss: 365224.5938 - val_mae: 449.8348\n",
            "Epoch 234/1000\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 1159451.1250 - mae: 824.9928 - val_loss: 393291.2188 - val_mae: 478.0211\n",
            "Epoch 235/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 1162564.7500 - mae: 828.1793 - val_loss: 398741.1250 - val_mae: 485.1522\n",
            "Epoch 236/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1145683.0000 - mae: 821.9115 - val_loss: 382731.1250 - val_mae: 469.0713\n",
            "Epoch 237/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1156345.5000 - mae: 822.2015 - val_loss: 369126.0938 - val_mae: 457.0208\n",
            "Epoch 238/1000\n",
            "39/39 [==============================] - 9s 212ms/step - loss: 1131396.2500 - mae: 813.0333 - val_loss: 378116.5000 - val_mae: 466.7307\n",
            "Epoch 239/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1171085.0000 - mae: 830.4286 - val_loss: 381083.3438 - val_mae: 469.0290\n",
            "Epoch 240/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 1150817.2500 - mae: 820.8807 - val_loss: 371024.7500 - val_mae: 459.0521\n",
            "Epoch 241/1000\n",
            "39/39 [==============================] - 4s 83ms/step - loss: 1155164.5000 - mae: 821.5184 - val_loss: 374040.6562 - val_mae: 461.6459\n",
            "Epoch 242/1000\n",
            "39/39 [==============================] - 9s 215ms/step - loss: 1140122.5000 - mae: 814.3504 - val_loss: 379385.4688 - val_mae: 468.4247\n",
            "Epoch 243/1000\n",
            "39/39 [==============================] - 4s 87ms/step - loss: 1148911.2500 - mae: 820.0161 - val_loss: 372976.9062 - val_mae: 461.6458\n",
            "Epoch 244/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 1144543.7500 - mae: 819.4879 - val_loss: 359583.5312 - val_mae: 450.2699\n",
            "Epoch 245/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 1147952.7500 - mae: 818.7244 - val_loss: 383737.1250 - val_mae: 475.9641\n",
            "Epoch 246/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 1160500.6250 - mae: 825.5239 - val_loss: 377308.0938 - val_mae: 468.9314\n",
            "Epoch 247/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 1143340.5000 - mae: 816.7758 - val_loss: 376769.2188 - val_mae: 468.9173\n",
            "Epoch 248/1000\n",
            "39/39 [==============================] - 4s 84ms/step - loss: 1144351.0000 - mae: 818.7072 - val_loss: 379645.2188 - val_mae: 470.6531\n",
            "Epoch 249/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 1140835.0000 - mae: 815.5115 - val_loss: 372156.7188 - val_mae: 466.2954\n",
            "Epoch 250/1000\n",
            "39/39 [==============================] - 9s 207ms/step - loss: 1147189.1250 - mae: 819.6064 - val_loss: 363400.9688 - val_mae: 454.1350\n",
            "Epoch 251/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 1120346.1250 - mae: 808.4979 - val_loss: 374563.0938 - val_mae: 468.2992\n",
            "Epoch 252/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 1134846.8750 - mae: 811.8739 - val_loss: 377523.7188 - val_mae: 470.5979\n",
            "Epoch 253/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 1128663.1250 - mae: 813.0070 - val_loss: 358441.2812 - val_mae: 452.1453\n",
            "Epoch 254/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 1134883.5000 - mae: 813.3409 - val_loss: 360314.5000 - val_mae: 457.3021\n",
            "Epoch 255/1000\n",
            "39/39 [==============================] - 10s 242ms/step - loss: 1145434.5000 - mae: 816.3444 - val_loss: 387449.0938 - val_mae: 484.3134\n",
            "Epoch 256/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 1156077.7500 - mae: 824.2827 - val_loss: 366150.6562 - val_mae: 461.3646\n",
            "Epoch 257/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1115212.1250 - mae: 807.0089 - val_loss: 371408.2500 - val_mae: 468.4962\n",
            "Epoch 258/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 1133194.2500 - mae: 812.6930 - val_loss: 370846.0000 - val_mae: 468.2012\n",
            "Epoch 259/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 1135486.3750 - mae: 814.9355 - val_loss: 370312.9688 - val_mae: 468.1871\n",
            "Epoch 260/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1127875.5000 - mae: 807.9234 - val_loss: 369788.7500 - val_mae: 468.1731\n",
            "Epoch 261/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 1125405.1250 - mae: 809.7953 - val_loss: 360151.8438 - val_mae: 459.3333\n",
            "Epoch 262/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1123617.0000 - mae: 807.0647 - val_loss: 377891.2500 - val_mae: 477.2384\n",
            "Epoch 263/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1132148.7500 - mae: 810.7090 - val_loss: 371731.3438 - val_mae: 470.7251\n",
            "Epoch 264/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 1125226.2500 - mae: 806.7861 - val_loss: 347266.2500 - val_mae: 445.5463\n",
            "Epoch 265/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1129001.2500 - mae: 808.5680 - val_loss: 367233.2188 - val_mae: 468.3849\n",
            "Epoch 266/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 1127852.5000 - mae: 810.6769 - val_loss: 372338.2812 - val_mae: 475.0954\n",
            "Epoch 267/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1128662.3750 - mae: 809.7205 - val_loss: 375217.4062 - val_mae: 477.0988\n",
            "Epoch 268/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 1123786.8750 - mae: 807.9984 - val_loss: 362257.8438 - val_mae: 466.3113\n",
            "Epoch 269/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1130921.3750 - mae: 810.9824 - val_loss: 370698.6250 - val_mae: 474.7301\n",
            "Epoch 270/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 1126347.5000 - mae: 808.0829 - val_loss: 373711.9062 - val_mae: 477.5771\n",
            "Epoch 271/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 1107671.7500 - mae: 799.2701 - val_loss: 373084.1250 - val_mae: 476.9866\n",
            "Epoch 272/1000\n",
            "39/39 [==============================] - 6s 148ms/step - loss: 1122487.1250 - mae: 806.7939 - val_loss: 378173.2812 - val_mae: 483.8809\n",
            "Epoch 273/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1103429.6250 - mae: 797.3213 - val_loss: 363142.5938 - val_mae: 468.5541\n",
            "Epoch 274/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 1120410.1250 - mae: 806.6649 - val_loss: 359150.4062 - val_mae: 465.9466\n",
            "Epoch 275/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1117566.6250 - mae: 804.3926 - val_loss: 350918.1562 - val_mae: 454.4840\n",
            "Epoch 276/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1111559.8750 - mae: 800.7509 - val_loss: 367043.6250 - val_mae: 474.5357\n",
            "Epoch 277/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 1110819.7500 - mae: 801.1660 - val_loss: 355507.7188 - val_mae: 461.3646\n",
            "Epoch 278/1000\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 1104478.2500 - mae: 796.9600 - val_loss: 363932.1250 - val_mae: 470.2346\n",
            "Epoch 279/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 1118215.0000 - mae: 804.1183 - val_loss: 351193.2812 - val_mae: 459.6146\n",
            "Epoch 280/1000\n",
            "39/39 [==============================] - 8s 205ms/step - loss: 1115176.0000 - mae: 801.5359 - val_loss: 348551.5312 - val_mae: 454.8347\n",
            "Epoch 281/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 1114962.3750 - mae: 802.5328 - val_loss: 359040.4062 - val_mae: 468.1617\n",
            "Epoch 282/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1116302.3750 - mae: 803.2794 - val_loss: 340849.1562 - val_mae: 450.8003\n",
            "Epoch 283/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 1099287.7500 - mae: 794.1085 - val_loss: 347101.7812 - val_mae: 454.8767\n",
            "Epoch 284/1000\n",
            "39/39 [==============================] - 9s 216ms/step - loss: 1109429.5000 - mae: 800.0156 - val_loss: 362894.9688 - val_mae: 474.3123\n",
            "Epoch 285/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 1093166.2500 - mae: 791.4271 - val_loss: 357015.5938 - val_mae: 468.1056\n",
            "Epoch 286/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1117023.0000 - mae: 804.3294 - val_loss: 365205.0938 - val_mae: 476.2867\n",
            "Epoch 287/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 1113855.3750 - mae: 802.5809 - val_loss: 347260.5000 - val_mae: 459.6146\n",
            "Epoch 288/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1093956.6250 - mae: 792.8816 - val_loss: 369608.0312 - val_mae: 482.9297\n",
            "Epoch 289/1000\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 1096686.2500 - mae: 795.1871 - val_loss: 342886.0312 - val_mae: 457.3021\n",
            "Epoch 290/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 1106498.8750 - mae: 799.4670 - val_loss: 342406.2500 - val_mae: 457.3021\n",
            "Epoch 291/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1104594.8750 - mae: 798.7999 - val_loss: 354054.3750 - val_mae: 468.3024\n",
            "Epoch 292/1000\n",
            "39/39 [==============================] - 6s 150ms/step - loss: 1115955.0000 - mae: 803.2747 - val_loss: 342781.9688 - val_mae: 455.0033\n",
            "Epoch 293/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 1093380.6250 - mae: 790.7015 - val_loss: 356342.7500 - val_mae: 470.0244\n",
            "Epoch 294/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1112132.8750 - mae: 801.0947 - val_loss: 355906.9062 - val_mae: 470.2918\n",
            "Epoch 295/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 1096441.7500 - mae: 793.1439 - val_loss: 346727.7812 - val_mae: 461.6459\n",
            "Epoch 296/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 1090389.6250 - mae: 790.6177 - val_loss: 345710.6250 - val_mae: 461.2812\n",
            "Epoch 297/1000\n",
            "39/39 [==============================] - 5s 98ms/step - loss: 1090391.5000 - mae: 790.9491 - val_loss: 347665.4062 - val_mae: 465.6252\n",
            "Epoch 298/1000\n",
            "39/39 [==============================] - 6s 114ms/step - loss: 1101935.8750 - mae: 796.9114 - val_loss: 345291.4688 - val_mae: 461.6458\n",
            "Epoch 299/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 1081513.0000 - mae: 787.1532 - val_loss: 353387.7812 - val_mae: 469.9410\n",
            "Epoch 300/1000\n",
            "39/39 [==============================] - 6s 149ms/step - loss: 1086707.8750 - mae: 787.1477 - val_loss: 344216.2500 - val_mae: 461.0833\n",
            "Epoch 301/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 1073280.5000 - mae: 783.1270 - val_loss: 345714.5938 - val_mae: 465.5692\n",
            "Epoch 302/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1092711.3750 - mae: 790.8823 - val_loss: 348542.4375 - val_mae: 467.5866\n",
            "Epoch 303/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 1097145.8750 - mae: 794.2935 - val_loss: 329079.3750 - val_mae: 446.6360\n",
            "Epoch 304/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 1099072.2500 - mae: 794.0061 - val_loss: 347568.3438 - val_mae: 467.5586\n",
            "Epoch 305/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 1072701.8750 - mae: 780.7602 - val_loss: 326760.5625 - val_mae: 448.8093\n",
            "Epoch 306/1000\n",
            "39/39 [==============================] - 6s 140ms/step - loss: 1073499.1250 - mae: 784.4048 - val_loss: 355191.2500 - val_mae: 476.2906\n",
            "Epoch 307/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 1078387.1250 - mae: 784.5268 - val_loss: 344324.4062 - val_mae: 463.6771\n",
            "Epoch 308/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 1077944.7500 - mae: 784.8544 - val_loss: 354091.1562 - val_mae: 475.6732\n",
            "Epoch 309/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 1073194.0000 - mae: 781.8898 - val_loss: 342006.3438 - val_mae: 466.0206\n",
            "Epoch 310/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 1080195.6250 - mae: 785.6189 - val_loss: 334382.0312 - val_mae: 454.9727\n",
            "Epoch 311/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 1092500.8750 - mae: 793.2344 - val_loss: 344208.2500 - val_mae: 467.4613\n",
            "Epoch 312/1000\n",
            "39/39 [==============================] - 9s 210ms/step - loss: 1076915.0000 - mae: 786.6502 - val_loss: 352167.7188 - val_mae: 475.8421\n",
            "Epoch 313/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1064568.2500 - mae: 780.5746 - val_loss: 346463.2500 - val_mae: 469.1831\n",
            "Epoch 314/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 1081823.6250 - mae: 786.3575 - val_loss: 342770.0938 - val_mae: 467.4192\n",
            "Epoch 315/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1086055.8750 - mae: 789.3752 - val_loss: 345641.1562 - val_mae: 469.7179\n",
            "Epoch 316/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 1079684.8750 - mae: 785.2862 - val_loss: 328449.2188 - val_mae: 453.3065\n",
            "Epoch 317/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 1078133.3750 - mae: 786.7142 - val_loss: 341352.5938 - val_mae: 467.3775\n",
            "Epoch 318/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 1068231.2500 - mae: 782.0011 - val_loss: 332528.0000 - val_mae: 459.0521\n",
            "Epoch 319/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 1060143.3750 - mae: 778.4189 - val_loss: 332212.3438 - val_mae: 459.6146\n",
            "Epoch 320/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 1053579.3750 - mae: 771.5286 - val_loss: 345024.6562 - val_mae: 473.5890\n",
            "Epoch 321/1000\n",
            "39/39 [==============================] - 4s 80ms/step - loss: 1060105.2500 - mae: 775.4476 - val_loss: 331229.9062 - val_mae: 459.3333\n",
            "Epoch 322/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 1078208.7500 - mae: 784.9081 - val_loss: 336694.2500 - val_mae: 462.7500\n",
            "Epoch 323/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1054522.0000 - mae: 771.8727 - val_loss: 333588.6562 - val_mae: 461.3646\n",
            "Epoch 324/1000\n",
            "39/39 [==============================] - 9s 211ms/step - loss: 1069151.1250 - mae: 781.8381 - val_loss: 326641.8438 - val_mae: 457.3021\n",
            "Epoch 325/1000\n",
            "39/39 [==============================] - 9s 213ms/step - loss: 1063892.7500 - mae: 777.1923 - val_loss: 329499.6562 - val_mae: 459.6145\n",
            "Epoch 326/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 1080474.0000 - mae: 787.6301 - val_loss: 340546.5000 - val_mae: 469.8466\n",
            "Epoch 327/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 1047359.2500 - mae: 770.6077 - val_loss: 341761.8125 - val_mae: 473.6760\n",
            "Epoch 328/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 1055265.3750 - mae: 774.7668 - val_loss: 344011.2188 - val_mae: 475.3151\n",
            "Epoch 329/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 1054610.2500 - mae: 773.2293 - val_loss: 322690.5000 - val_mae: 453.2047\n",
            "Epoch 330/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1049463.6250 - mae: 770.2737 - val_loss: 338573.6562 - val_mae: 469.2292\n",
            "Epoch 331/1000\n",
            "39/39 [==============================] - 4s 82ms/step - loss: 1072993.3750 - mae: 784.1170 - val_loss: 326833.2188 - val_mae: 459.6146\n",
            "Epoch 332/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 1057761.6250 - mae: 774.8868 - val_loss: 326320.3750 - val_mae: 459.3333\n",
            "Epoch 333/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 1058694.5000 - mae: 776.1835 - val_loss: 331867.8438 - val_mae: 463.3125\n",
            "Epoch 334/1000\n",
            "39/39 [==============================] - 4s 90ms/step - loss: 1044911.8125 - mae: 773.0789 - val_loss: 322193.5312 - val_mae: 457.3021\n",
            "Epoch 335/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 1060444.2500 - mae: 779.3257 - val_loss: 332951.1562 - val_mae: 466.8464\n",
            "Epoch 336/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 1057887.0000 - mae: 774.1177 - val_loss: 316494.2500 - val_mae: 451.5529\n",
            "Epoch 337/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1052528.7500 - mae: 773.0724 - val_loss: 320804.5938 - val_mae: 457.0208\n",
            "Epoch 338/1000\n",
            "39/39 [==============================] - 4s 91ms/step - loss: 1054801.6250 - mae: 773.8853 - val_loss: 323581.1562 - val_mae: 459.0521\n",
            "Epoch 339/1000\n",
            "39/39 [==============================] - 6s 123ms/step - loss: 1025632.3125 - mae: 759.6838 - val_loss: 339209.3750 - val_mae: 474.8112\n",
            "Epoch 340/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1041594.0625 - mae: 766.8517 - val_loss: 327561.9062 - val_mae: 465.0273\n",
            "Epoch 341/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 1041727.8750 - mae: 767.3271 - val_loss: 330318.7188 - val_mae: 467.0446\n",
            "Epoch 342/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1050178.3750 - mae: 769.9593 - val_loss: 332546.0938 - val_mae: 468.6971\n",
            "Epoch 343/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 1051407.0000 - mae: 773.4138 - val_loss: 329490.8438 - val_mae: 467.2979\n",
            "Epoch 344/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 1043658.5625 - mae: 769.6484 - val_loss: 321047.0312 - val_mae: 459.3333\n",
            "Epoch 345/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 1048483.5625 - mae: 770.7388 - val_loss: 321767.0938 - val_mae: 457.4072\n",
            "Epoch 346/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 1037663.0625 - mae: 766.6723 - val_loss: 328137.8125 - val_mae: 467.2558\n",
            "Epoch 347/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 1053471.2500 - mae: 773.2195 - val_loss: 315047.7812 - val_mae: 453.7373\n",
            "Epoch 348/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 1039220.5000 - mae: 767.9514 - val_loss: 327248.5938 - val_mae: 467.2279\n",
            "Epoch 349/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1040154.0625 - mae: 769.9727 - val_loss: 322069.6562 - val_mae: 461.3646\n",
            "Epoch 350/1000\n",
            "39/39 [==============================] - 10s 245ms/step - loss: 1033063.6250 - mae: 764.3336 - val_loss: 324829.7188 - val_mae: 463.3958\n",
            "Epoch 351/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 1037759.3750 - mae: 765.5020 - val_loss: 330589.8438 - val_mae: 472.7281\n",
            "Epoch 352/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 1035159.5000 - mae: 763.5787 - val_loss: 325583.7188 - val_mae: 467.4543\n",
            "Epoch 353/1000\n",
            "39/39 [==============================] - 6s 150ms/step - loss: 1040249.4375 - mae: 767.5620 - val_loss: 328239.6875 - val_mae: 469.1904\n",
            "Epoch 354/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1044355.6875 - mae: 769.3865 - val_loss: 323111.3750 - val_mae: 463.3958\n",
            "Epoch 355/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 1026281.1250 - mae: 760.0735 - val_loss: 316347.6562 - val_mae: 459.3333\n",
            "Epoch 356/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 1029635.8750 - mae: 762.5800 - val_loss: 323761.6562 - val_mae: 467.1178\n",
            "Epoch 357/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 1038153.3125 - mae: 766.8719 - val_loss: 326414.5938 - val_mae: 468.8541\n",
            "Epoch 358/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 1035365.2500 - mae: 763.9514 - val_loss: 321492.9375 - val_mae: 463.6771\n",
            "Epoch 359/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 1043765.7500 - mae: 769.2521 - val_loss: 319379.5312 - val_mae: 465.3260\n",
            "Epoch 360/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 1037436.2500 - mae: 765.0717 - val_loss: 314180.5312 - val_mae: 459.0521\n",
            "Epoch 361/1000\n",
            "39/39 [==============================] - 6s 139ms/step - loss: 1028543.3750 - mae: 760.2802 - val_loss: 326205.5312 - val_mae: 472.7322\n",
            "Epoch 362/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 1028846.8750 - mae: 762.0147 - val_loss: 333343.3750 - val_mae: 479.8429\n",
            "Epoch 363/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 1020388.0625 - mae: 759.9268 - val_loss: 320732.7500 - val_mae: 467.0206\n",
            "Epoch 364/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 1024709.3125 - mae: 760.0144 - val_loss: 312505.7500 - val_mae: 459.0521\n",
            "Epoch 365/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 1027533.8125 - mae: 761.3097 - val_loss: 312242.2500 - val_mae: 459.6146\n",
            "Epoch 366/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 1032747.3750 - mae: 762.5635 - val_loss: 310325.5625 - val_mae: 455.7508\n",
            "Epoch 367/1000\n",
            "39/39 [==============================] - 9s 233ms/step - loss: 1017908.6875 - mae: 756.7034 - val_loss: 318922.9062 - val_mae: 466.6833\n",
            "Epoch 368/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 1027426.1875 - mae: 760.1749 - val_loss: 313966.3125 - val_mae: 461.0833\n",
            "Epoch 369/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 1026323.3750 - mae: 760.4927 - val_loss: 321205.1250 - val_mae: 468.6869\n",
            "Epoch 370/1000\n",
            "39/39 [==============================] - 10s 242ms/step - loss: 1020368.3125 - mae: 757.7633 - val_loss: 317733.3750 - val_mae: 466.9229\n",
            "Epoch 371/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 1010651.6250 - mae: 752.7990 - val_loss: 329170.2812 - val_mae: 479.1854\n",
            "Epoch 372/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1024376.8750 - mae: 759.0711 - val_loss: 316895.7188 - val_mae: 466.8954\n",
            "Epoch 373/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 1004111.8750 - mae: 749.1154 - val_loss: 313355.5938 - val_mae: 464.8503\n",
            "Epoch 374/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 1023435.5000 - mae: 758.9374 - val_loss: 300897.5625 - val_mae: 451.7990\n",
            "Epoch 375/1000\n",
            "39/39 [==============================] - 5s 93ms/step - loss: 999680.7500 - mae: 749.4196 - val_loss: 315547.5312 - val_mae: 466.5725\n",
            "Epoch 376/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 1015027.0625 - mae: 756.1717 - val_loss: 306344.0312 - val_mae: 455.8890\n",
            "Epoch 377/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 1011815.4375 - mae: 754.9849 - val_loss: 319145.3438 - val_mae: 472.0064\n",
            "Epoch 378/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 999650.0000 - mae: 748.4625 - val_loss: 306867.9688 - val_mae: 459.3333\n",
            "Epoch 379/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 1010951.9375 - mae: 755.5341 - val_loss: 317071.7812 - val_mae: 468.8295\n",
            "Epoch 380/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 999266.3125 - mae: 745.6328 - val_loss: 310459.0938 - val_mae: 464.7533\n",
            "Epoch 381/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 1013033.6250 - mae: 754.0762 - val_loss: 313147.0000 - val_mae: 466.7708\n",
            "Epoch 382/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 1023459.0625 - mae: 760.6902 - val_loss: 311373.4688 - val_mae: 463.1146\n",
            "Epoch 383/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 1012725.2500 - mae: 754.1492 - val_loss: 319678.0312 - val_mae: 473.8709\n",
            "Epoch 384/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 997830.8750 - mae: 744.3491 - val_loss: 311821.0312 - val_mae: 466.4479\n",
            "Epoch 385/1000\n",
            "39/39 [==============================] - 10s 249ms/step - loss: 1005747.7500 - mae: 749.6451 - val_loss: 289319.0312 - val_mae: 444.8503\n",
            "Epoch 386/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 1010869.9375 - mae: 752.5446 - val_loss: 303692.3125 - val_mae: 459.3333\n",
            "Epoch 387/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 1012978.9375 - mae: 754.0240 - val_loss: 317922.0312 - val_mae: 473.4808\n",
            "Epoch 388/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 1007134.0000 - mae: 749.9573 - val_loss: 299924.2500 - val_mae: 457.5833\n",
            "Epoch 389/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 995369.6250 - mae: 746.3950 - val_loss: 305588.2500 - val_mae: 461.3646\n",
            "Epoch 390/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 1004850.7500 - mae: 750.2198 - val_loss: 309483.5312 - val_mae: 466.6466\n",
            "Epoch 391/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 1009508.1875 - mae: 754.8236 - val_loss: 297557.2812 - val_mae: 454.3464\n",
            "Epoch 392/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 989274.1250 - mae: 743.3704 - val_loss: 308575.3438 - val_mae: 466.3374\n",
            "Epoch 393/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 981367.3125 - mae: 737.6406 - val_loss: 300872.6250 - val_mae: 459.0521\n",
            "Epoch 394/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 982583.7500 - mae: 739.6426 - val_loss: 304722.0000 - val_mae: 464.2789\n",
            "Epoch 395/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 1006542.1250 - mae: 751.7692 - val_loss: 293002.7812 - val_mae: 452.3701\n",
            "Epoch 396/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 992825.0000 - mae: 742.6921 - val_loss: 306981.5938 - val_mae: 466.2824\n",
            "Epoch 397/1000\n",
            "39/39 [==============================] - 9s 214ms/step - loss: 989625.9375 - mae: 741.4556 - val_loss: 313754.8750 - val_mae: 473.2043\n",
            "Epoch 398/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 1000067.1250 - mae: 746.9943 - val_loss: 306186.0625 - val_mae: 466.2548\n",
            "Epoch 399/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 996651.6875 - mae: 747.1141 - val_loss: 305694.0938 - val_mae: 465.9597\n",
            "Epoch 400/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 989144.1875 - mae: 739.6558 - val_loss: 298174.3438 - val_mae: 459.0521\n",
            "Epoch 401/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 977891.1250 - mae: 740.5591 - val_loss: 293735.5000 - val_mae: 454.2032\n",
            "Epoch 402/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 991968.8125 - mae: 744.8408 - val_loss: 300656.9688 - val_mae: 461.6458\n",
            "Epoch 403/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 990750.5625 - mae: 743.0925 - val_loss: 289949.5938 - val_mae: 452.2000\n",
            "Epoch 404/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 988235.6250 - mae: 741.3639 - val_loss: 304007.5625 - val_mae: 466.7340\n",
            "Epoch 405/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 1003232.5000 - mae: 749.2758 - val_loss: 299395.8750 - val_mae: 461.3646\n",
            "Epoch 406/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 985599.0625 - mae: 742.0291 - val_loss: 300094.7500 - val_mae: 464.3938\n",
            "Epoch 407/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 988567.3125 - mae: 741.9219 - val_loss: 302736.5625 - val_mae: 466.4113\n",
            "Epoch 408/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 988557.0625 - mae: 742.2742 - val_loss: 284151.3125 - val_mae: 447.5177\n",
            "Epoch 409/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 992642.0000 - mae: 743.3871 - val_loss: 290791.0938 - val_mae: 454.3145\n",
            "Epoch 410/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 979759.0625 - mae: 738.0571 - val_loss: 297516.7812 - val_mae: 461.3646\n",
            "Epoch 411/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 977204.6250 - mae: 740.6189 - val_loss: 301191.7500 - val_mae: 466.3561\n",
            "Epoch 412/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 971004.3125 - mae: 736.1404 - val_loss: 290741.5312 - val_mae: 457.3020\n",
            "Epoch 413/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 959725.6875 - mae: 731.8897 - val_loss: 307358.1562 - val_mae: 473.0424\n",
            "Epoch 414/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 992048.8125 - mae: 744.9726 - val_loss: 303853.7188 - val_mae: 470.7025\n",
            "Epoch 415/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 975081.5000 - mae: 733.3094 - val_loss: 292653.1562 - val_mae: 459.3333\n",
            "Epoch 416/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 982147.0625 - mae: 738.7650 - val_loss: 295296.2812 - val_mae: 461.3646\n",
            "Epoch 417/1000\n",
            "39/39 [==============================] - 6s 145ms/step - loss: 980331.0625 - mae: 736.4583 - val_loss: 309662.9688 - val_mae: 477.5603\n",
            "Epoch 418/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 965868.9375 - mae: 733.0306 - val_loss: 287707.2500 - val_mae: 454.7188\n",
            "Epoch 419/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 986219.3750 - mae: 742.4550 - val_loss: 287342.2188 - val_mae: 454.7331\n",
            "Epoch 420/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 983941.4375 - mae: 740.2101 - val_loss: 304712.8750 - val_mae: 473.1311\n",
            "Epoch 421/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 971877.9375 - mae: 734.2479 - val_loss: 297288.1562 - val_mae: 465.9370\n",
            "Epoch 422/1000\n",
            "39/39 [==============================] - 5s 130ms/step - loss: 965161.1875 - mae: 730.1469 - val_loss: 307645.8125 - val_mae: 477.3532\n",
            "Epoch 423/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 959954.3125 - mae: 730.6714 - val_loss: 285834.9688 - val_mae: 454.5070\n",
            "Epoch 424/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 949230.5625 - mae: 724.3637 - val_loss: 285497.9062 - val_mae: 454.5204\n",
            "Epoch 425/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 958069.4375 - mae: 727.8976 - val_loss: 292127.0938 - val_mae: 461.6458\n",
            "Epoch 426/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 963717.1250 - mae: 727.8046 - val_loss: 287891.5938 - val_mae: 456.8602\n",
            "Epoch 427/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 947601.8750 - mae: 724.8653 - val_loss: 294961.8750 - val_mae: 465.5738\n",
            "Epoch 428/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 961709.0000 - mae: 728.9829 - val_loss: 301639.0000 - val_mae: 472.9120\n",
            "Epoch 429/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 972553.8125 - mae: 733.8748 - val_loss: 301361.1250 - val_mae: 473.1655\n",
            "Epoch 430/1000\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 976905.6250 - mae: 736.2583 - val_loss: 290119.3438 - val_mae: 461.0833\n",
            "Epoch 431/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 977052.6250 - mae: 737.3887 - val_loss: 293684.0938 - val_mae: 466.0806\n",
            "Epoch 432/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 965997.9375 - mae: 729.7654 - val_loss: 296914.8750 - val_mae: 470.2065\n",
            "Epoch 433/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 949136.0000 - mae: 725.6318 - val_loss: 289250.8125 - val_mae: 461.6458\n",
            "Epoch 434/1000\n",
            "39/39 [==============================] - 7s 158ms/step - loss: 948212.8125 - mae: 720.5638 - val_loss: 291762.5312 - val_mae: 463.3958\n",
            "Epoch 435/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 968113.8750 - mae: 732.2023 - val_loss: 285493.5938 - val_mae: 459.3333\n",
            "Epoch 436/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 953842.6250 - mae: 722.8215 - val_loss: 281373.7500 - val_mae: 454.6863\n",
            "Epoch 437/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 976584.3750 - mae: 737.2130 - val_loss: 284090.1562 - val_mae: 457.0127\n",
            "Epoch 438/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 964340.5000 - mae: 729.9215 - val_loss: 284445.2500 - val_mae: 459.3333\n",
            "Epoch 439/1000\n",
            "39/39 [==============================] - 6s 117ms/step - loss: 961824.3125 - mae: 728.1495 - val_loss: 294506.4688 - val_mae: 470.5755\n",
            "Epoch 440/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 948532.3750 - mae: 720.8401 - val_loss: 293349.4688 - val_mae: 467.9874\n",
            "Epoch 441/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 948043.9375 - mae: 722.5956 - val_loss: 300197.6562 - val_mae: 476.5677\n",
            "Epoch 442/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 953670.6875 - mae: 723.9033 - val_loss: 289473.6562 - val_mae: 465.3663\n",
            "Epoch 443/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 955931.9375 - mae: 725.5157 - val_loss: 276104.5000 - val_mae: 452.7515\n",
            "Epoch 444/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 955252.6250 - mae: 723.7805 - val_loss: 291570.0938 - val_mae: 467.8495\n",
            "Epoch 445/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 953491.7500 - mae: 723.3634 - val_loss: 281414.9062 - val_mae: 457.1227\n",
            "Epoch 446/1000\n",
            "39/39 [==============================] - 6s 129ms/step - loss: 952263.0000 - mae: 724.2039 - val_loss: 278161.7812 - val_mae: 455.1052\n",
            "Epoch 447/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 953091.8750 - mae: 724.6625 - val_loss: 287821.2188 - val_mae: 465.5791\n",
            "Epoch 448/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 953568.7500 - mae: 724.2295 - val_loss: 281032.7812 - val_mae: 459.3334\n",
            "Epoch 449/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 951276.8750 - mae: 723.4597 - val_loss: 283485.4062 - val_mae: 461.0833\n",
            "Epoch 450/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 942633.1250 - mae: 721.0955 - val_loss: 280345.5312 - val_mae: 459.3333\n",
            "Epoch 451/1000\n",
            "39/39 [==============================] - 9s 220ms/step - loss: 935066.0625 - mae: 717.2039 - val_loss: 282918.2188 - val_mae: 461.3646\n",
            "Epoch 452/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 939251.5625 - mae: 715.9505 - val_loss: 283271.8438 - val_mae: 463.7600\n",
            "Epoch 453/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 954001.8750 - mae: 725.4924 - val_loss: 285710.5312 - val_mae: 465.4961\n",
            "Epoch 454/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 941306.3750 - mae: 718.4824 - val_loss: 287808.8438 - val_mae: 467.1493\n",
            "Epoch 455/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 932899.0625 - mae: 716.5624 - val_loss: 295030.6562 - val_mae: 476.2704\n",
            "Epoch 456/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 941362.1875 - mae: 721.0237 - val_loss: 291112.5938 - val_mae: 471.8574\n",
            "Epoch 457/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 934969.1875 - mae: 714.7831 - val_loss: 277495.2500 - val_mae: 457.2882\n",
            "Epoch 458/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 935820.4375 - mae: 717.6534 - val_loss: 280690.2812 - val_mae: 461.6458\n",
            "Epoch 459/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 923512.1875 - mae: 709.7593 - val_loss: 283637.4688 - val_mae: 465.4133\n",
            "Epoch 460/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 927975.9375 - mae: 712.4048 - val_loss: 279461.2500 - val_mae: 461.0000\n",
            "Epoch 461/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 935846.8125 - mae: 714.7192 - val_loss: 279469.2812 - val_mae: 461.0833\n",
            "Epoch 462/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 946513.2500 - mae: 722.4324 - val_loss: 292337.7188 - val_mae: 475.7001\n",
            "Epoch 463/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 949287.8125 - mae: 723.1223 - val_loss: 269830.2188 - val_mae: 453.3083\n",
            "Epoch 464/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 927298.7500 - mae: 707.7537 - val_loss: 285391.9688 - val_mae: 469.6055\n",
            "Epoch 465/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 931910.6875 - mae: 712.9625 - val_loss: 271962.5312 - val_mae: 455.0858\n",
            "Epoch 466/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 930744.8125 - mae: 712.9577 - val_loss: 287696.4062 - val_mae: 471.8638\n",
            "Epoch 467/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 925791.6875 - mae: 711.8764 - val_loss: 271780.9062 - val_mae: 457.0209\n",
            "Epoch 468/1000\n",
            "39/39 [==============================] - 6s 135ms/step - loss: 913861.2500 - mae: 705.3186 - val_loss: 277750.3438 - val_mae: 463.2590\n",
            "Epoch 469/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 934190.8750 - mae: 715.4169 - val_loss: 280515.0625 - val_mae: 465.8386\n",
            "Epoch 470/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 917446.1875 - mae: 705.4622 - val_loss: 273414.5938 - val_mae: 457.4664\n",
            "Epoch 471/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 922073.7500 - mae: 708.8190 - val_loss: 282594.9688 - val_mae: 467.5615\n",
            "Epoch 472/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 931754.8750 - mae: 714.2787 - val_loss: 275910.5312 - val_mae: 461.0834\n",
            "Epoch 473/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 935049.0000 - mae: 715.4661 - val_loss: 281801.0625 - val_mae: 467.2526\n",
            "Epoch 474/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 921035.3750 - mae: 707.9042 - val_loss: 281842.5625 - val_mae: 469.0507\n",
            "Epoch 475/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 929513.9375 - mae: 711.6151 - val_loss: 263217.4062 - val_mae: 451.1602\n",
            "Epoch 476/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 933298.9375 - mae: 716.0098 - val_loss: 278209.7812 - val_mae: 465.7423\n",
            "Epoch 477/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 917402.0625 - mae: 705.3642 - val_loss: 268300.2188 - val_mae: 455.2504\n",
            "Epoch 478/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 922745.5000 - mae: 708.7095 - val_loss: 271300.5938 - val_mae: 459.3333\n",
            "Epoch 479/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 925765.2500 - mae: 710.8996 - val_loss: 276647.5312 - val_mae: 463.3958\n",
            "Epoch 480/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 902169.1250 - mae: 699.4796 - val_loss: 270536.1250 - val_mae: 459.0521\n",
            "Epoch 481/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 925016.6875 - mae: 710.2508 - val_loss: 270230.3438 - val_mae: 459.0521\n",
            "Epoch 482/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 923397.2500 - mae: 711.4559 - val_loss: 278819.3125 - val_mae: 467.1277\n",
            "Epoch 483/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 911868.4375 - mae: 703.1330 - val_loss: 281491.0625 - val_mae: 470.5515\n",
            "Epoch 484/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 922449.1250 - mae: 710.4159 - val_loss: 274904.7188 - val_mae: 464.7044\n",
            "Epoch 485/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 909670.6875 - mae: 704.7219 - val_loss: 275039.1875 - val_mae: 465.0555\n",
            "Epoch 486/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 907496.6875 - mae: 703.2916 - val_loss: 271906.0312 - val_mae: 463.0106\n",
            "Epoch 487/1000\n",
            "39/39 [==============================] - 4s 93ms/step - loss: 909413.0000 - mae: 701.9205 - val_loss: 277480.5938 - val_mae: 468.6921\n",
            "Epoch 488/1000\n",
            "39/39 [==============================] - 10s 242ms/step - loss: 915069.6250 - mae: 705.9376 - val_loss: 280095.6562 - val_mae: 470.9775\n",
            "Epoch 489/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 923930.5625 - mae: 710.5791 - val_loss: 267923.9062 - val_mae: 459.3333\n",
            "Epoch 490/1000\n",
            "39/39 [==============================] - 7s 165ms/step - loss: 904466.1875 - mae: 701.5306 - val_loss: 267614.8750 - val_mae: 459.3333\n",
            "Epoch 491/1000\n",
            "39/39 [==============================] - 9s 217ms/step - loss: 909517.6875 - mae: 704.4889 - val_loss: 269991.0312 - val_mae: 461.0833\n",
            "Epoch 492/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 920363.0000 - mae: 710.0173 - val_loss: 275972.8125 - val_mae: 468.8355\n",
            "Epoch 493/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 898404.0625 - mae: 698.5575 - val_loss: 266721.8125 - val_mae: 459.3333\n",
            "Epoch 494/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 914704.6250 - mae: 706.6554 - val_loss: 278259.8438 - val_mae: 471.0937\n",
            "Epoch 495/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 908378.0625 - mae: 701.1083 - val_loss: 272036.2812 - val_mae: 465.1999\n",
            "Epoch 496/1000\n",
            "39/39 [==============================] - 10s 241ms/step - loss: 911097.7500 - mae: 704.6882 - val_loss: 271595.6250 - val_mae: 464.9049\n",
            "Epoch 497/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 907920.9375 - mae: 700.6586 - val_loss: 271560.9062 - val_mae: 465.4540\n",
            "Epoch 498/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 896242.8125 - mae: 698.8175 - val_loss: 268044.4688 - val_mae: 461.3646\n",
            "Epoch 499/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 899951.2500 - mae: 699.7846 - val_loss: 267616.2188 - val_mae: 461.0833\n",
            "Epoch 500/1000\n",
            "39/39 [==============================] - 7s 138ms/step - loss: 914402.6250 - mae: 708.8172 - val_loss: 272841.7500 - val_mae: 466.7981\n",
            "Epoch 501/1000\n",
            "39/39 [==============================] - 6s 124ms/step - loss: 912458.6875 - mae: 704.5699 - val_loss: 267422.7188 - val_mae: 463.0864\n",
            "Epoch 502/1000\n",
            "39/39 [==============================] - 6s 121ms/step - loss: 901014.8125 - mae: 700.0546 - val_loss: 266847.0312 - val_mae: 461.3646\n",
            "Epoch 503/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 906652.7500 - mae: 703.3028 - val_loss: 260879.9531 - val_mae: 457.0208\n",
            "Epoch 504/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 905813.3125 - mae: 702.3586 - val_loss: 257850.2344 - val_mae: 453.8724\n",
            "Epoch 505/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 887914.1250 - mae: 692.3047 - val_loss: 269108.5312 - val_mae: 465.3433\n",
            "Epoch 506/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 897291.1875 - mae: 698.5388 - val_loss: 262928.1562 - val_mae: 459.3333\n",
            "Epoch 507/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 892912.8125 - mae: 693.4551 - val_loss: 277224.2812 - val_mae: 474.6876\n",
            "Epoch 508/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 887093.6250 - mae: 692.9672 - val_loss: 262147.5938 - val_mae: 457.7081\n",
            "Epoch 509/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 889303.6250 - mae: 693.8543 - val_loss: 267652.9375 - val_mae: 464.7266\n",
            "Epoch 510/1000\n",
            "39/39 [==============================] - 4s 88ms/step - loss: 895315.7500 - mae: 695.7404 - val_loss: 273051.6562 - val_mae: 470.3737\n",
            "Epoch 511/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 897710.8750 - mae: 701.6760 - val_loss: 255828.4219 - val_mae: 453.6867\n",
            "Epoch 512/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 897437.8125 - mae: 697.7354 - val_loss: 258443.7656 - val_mae: 456.0130\n",
            "Epoch 513/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 907539.7500 - mae: 703.5166 - val_loss: 258183.3281 - val_mae: 456.0263\n",
            "Epoch 514/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 902516.8125 - mae: 700.2592 - val_loss: 263433.2500 - val_mae: 462.6266\n",
            "Epoch 515/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 898473.1875 - mae: 698.2936 - val_loss: 268909.0938 - val_mae: 468.4862\n",
            "Epoch 516/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 882845.8125 - mae: 690.1526 - val_loss: 251774.9531 - val_mae: 450.4884\n",
            "Epoch 517/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 883019.5000 - mae: 691.2505 - val_loss: 259705.3281 - val_mae: 459.0521\n",
            "Epoch 518/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 885919.5000 - mae: 691.8574 - val_loss: 259586.1875 - val_mae: 458.1259\n",
            "Epoch 519/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 870298.0625 - mae: 687.2948 - val_loss: 264875.7500 - val_mae: 464.8713\n",
            "Epoch 520/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 890482.3125 - mae: 693.5233 - val_loss: 259058.5781 - val_mae: 458.1531\n",
            "Epoch 521/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 887442.5000 - mae: 692.3827 - val_loss: 258901.3594 - val_mae: 459.6146\n",
            "Epoch 522/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 900136.6875 - mae: 700.5195 - val_loss: 258534.8281 - val_mae: 458.1803\n",
            "Epoch 523/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 885635.4375 - mae: 692.7217 - val_loss: 263725.8750 - val_mae: 464.8163\n",
            "Epoch 524/1000\n",
            "39/39 [==============================] - 6s 131ms/step - loss: 866175.5000 - mae: 683.8086 - val_loss: 263298.8438 - val_mae: 464.5213\n",
            "Epoch 525/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 887410.1875 - mae: 693.8363 - val_loss: 257527.9531 - val_mae: 459.0521\n",
            "Epoch 526/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 874528.8750 - mae: 687.2917 - val_loss: 268211.2500 - val_mae: 469.9363\n",
            "Epoch 527/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 892709.3125 - mae: 698.6887 - val_loss: 251674.1875 - val_mae: 453.9051\n",
            "Epoch 528/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 881498.7500 - mae: 691.4662 - val_loss: 262457.7812 - val_mae: 465.0291\n",
            "Epoch 529/1000\n",
            "39/39 [==============================] - 6s 134ms/step - loss: 890440.6875 - mae: 695.4132 - val_loss: 259303.7500 - val_mae: 461.3645\n",
            "Epoch 530/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 885418.0625 - mae: 692.2256 - val_loss: 251065.1250 - val_mae: 454.2272\n",
            "Epoch 531/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 889228.4375 - mae: 694.3177 - val_loss: 250385.3281 - val_mae: 453.8766\n",
            "Epoch 532/1000\n",
            "39/39 [==============================] - 10s 225ms/step - loss: 868682.4375 - mae: 682.2281 - val_loss: 253265.0156 - val_mae: 456.2855\n",
            "Epoch 533/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 884766.4375 - mae: 691.8418 - val_loss: 253009.0469 - val_mae: 456.2995\n",
            "Epoch 534/1000\n",
            "39/39 [==============================] - 10s 250ms/step - loss: 877845.7500 - mae: 690.5173 - val_loss: 255302.4844 - val_mae: 458.0630\n",
            "Epoch 535/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 879702.8750 - mae: 689.5311 - val_loss: 263043.0938 - val_mae: 467.9398\n",
            "Epoch 536/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 868321.6250 - mae: 682.7330 - val_loss: 267949.7812 - val_mae: 472.9366\n",
            "Epoch 537/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 871843.8750 - mae: 685.2842 - val_loss: 259690.2500 - val_mae: 464.3439\n",
            "Epoch 538/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 879760.7500 - mae: 690.2424 - val_loss: 251773.5781 - val_mae: 456.3675\n",
            "Epoch 539/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 875548.3125 - mae: 687.4758 - val_loss: 256677.0781 - val_mae: 461.3646\n",
            "Epoch 540/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 869665.7500 - mae: 683.6481 - val_loss: 253895.0469 - val_mae: 459.6146\n",
            "Epoch 541/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 855197.7500 - mae: 677.5406 - val_loss: 258758.2969 - val_mae: 464.5706\n",
            "Epoch 542/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 871975.4375 - mae: 686.5873 - val_loss: 253242.7344 - val_mae: 459.3333\n",
            "Epoch 543/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 866425.2500 - mae: 682.7206 - val_loss: 265742.8438 - val_mae: 472.3695\n",
            "Epoch 544/1000\n",
            "39/39 [==============================] - 10s 230ms/step - loss: 856634.6250 - mae: 679.6352 - val_loss: 260471.7500 - val_mae: 466.2799\n",
            "Epoch 545/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 867185.9375 - mae: 683.5295 - val_loss: 259780.3281 - val_mae: 465.9014\n",
            "Epoch 546/1000\n",
            "39/39 [==============================] - 6s 125ms/step - loss: 867594.8750 - mae: 684.3743 - val_loss: 244632.1406 - val_mae: 451.3080\n",
            "Epoch 547/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 870368.4375 - mae: 685.4880 - val_loss: 254657.2031 - val_mae: 462.7384\n",
            "Epoch 548/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 879061.5625 - mae: 691.4965 - val_loss: 249349.7500 - val_mae: 456.5046\n",
            "Epoch 549/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 861166.0625 - mae: 680.4745 - val_loss: 261493.0469 - val_mae: 469.0268\n",
            "Epoch 550/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 859178.8750 - mae: 678.5526 - val_loss: 251393.2656 - val_mae: 459.6146\n",
            "Epoch 551/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 857637.0000 - mae: 677.3849 - val_loss: 253786.1875 - val_mae: 460.3261\n",
            "Epoch 552/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 869267.6875 - mae: 685.5832 - val_loss: 250738.3750 - val_mae: 459.3333\n",
            "Epoch 553/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 865351.0000 - mae: 682.3564 - val_loss: 258232.0469 - val_mae: 466.4377\n",
            "Epoch 554/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 853692.1250 - mae: 678.0462 - val_loss: 250246.3594 - val_mae: 459.3333\n",
            "Epoch 555/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 859773.0000 - mae: 679.8828 - val_loss: 257406.1719 - val_mae: 465.8481\n",
            "Epoch 556/1000\n",
            "39/39 [==============================] - 6s 121ms/step - loss: 854696.6875 - mae: 677.7123 - val_loss: 262166.0625 - val_mae: 472.1173\n",
            "Epoch 557/1000\n",
            "39/39 [==============================] - 10s 227ms/step - loss: 857404.1875 - mae: 678.6074 - val_loss: 254423.6406 - val_mae: 464.0710\n",
            "Epoch 558/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 855015.8125 - mae: 676.8186 - val_loss: 247249.4531 - val_mae: 455.6970\n",
            "Epoch 559/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 868002.6875 - mae: 684.8868 - val_loss: 251672.8125 - val_mae: 461.3646\n",
            "Epoch 560/1000\n",
            "39/39 [==============================] - 6s 120ms/step - loss: 857128.2500 - mae: 678.7281 - val_loss: 249198.6719 - val_mae: 458.6989\n",
            "Epoch 561/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 844696.6875 - mae: 673.8429 - val_loss: 258563.4531 - val_mae: 469.2623\n",
            "Epoch 562/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 852293.8750 - mae: 676.9086 - val_loss: 248180.1875 - val_mae: 459.0521\n",
            "Epoch 563/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 852554.6250 - mae: 676.0127 - val_loss: 242883.5625 - val_mae: 454.3127\n",
            "Epoch 564/1000\n",
            "39/39 [==============================] - 5s 125ms/step - loss: 851019.3750 - mae: 674.6562 - val_loss: 245688.2500 - val_mae: 456.7217\n",
            "Epoch 565/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 860088.3125 - mae: 680.1683 - val_loss: 254596.7031 - val_mae: 466.5600\n",
            "Epoch 566/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 853584.0000 - mae: 677.9305 - val_loss: 249573.8281 - val_mae: 461.9176\n",
            "Epoch 567/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 862459.4375 - mae: 682.2413 - val_loss: 247174.0781 - val_mae: 459.3333\n",
            "Epoch 568/1000\n",
            "39/39 [==============================] - 6s 124ms/step - loss: 851994.0625 - mae: 677.2960 - val_loss: 246942.9844 - val_mae: 459.3333\n",
            "Epoch 569/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 851637.0000 - mae: 676.6780 - val_loss: 251484.3750 - val_mae: 463.0312\n",
            "Epoch 570/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 849973.3750 - mae: 675.0630 - val_loss: 246309.0000 - val_mae: 459.0521\n",
            "Epoch 571/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 843267.8125 - mae: 672.4308 - val_loss: 246244.5625 - val_mae: 459.3333\n",
            "Epoch 572/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 860920.1875 - mae: 682.8190 - val_loss: 243752.9844 - val_mae: 456.5507\n",
            "Epoch 573/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 841358.3125 - mae: 673.0718 - val_loss: 244044.2344 - val_mae: 456.1065\n",
            "Epoch 574/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 844749.2500 - mae: 673.0086 - val_loss: 247641.6406 - val_mae: 461.8082\n",
            "Epoch 575/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 841321.5625 - mae: 672.6559 - val_loss: 250138.0000 - val_mae: 464.1067\n",
            "Epoch 576/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 848322.9375 - mae: 676.6553 - val_loss: 247488.4375 - val_mae: 462.3432\n",
            "Epoch 577/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 855002.8750 - mae: 679.7739 - val_loss: 246769.2031 - val_mae: 461.4860\n",
            "Epoch 578/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 845263.1875 - mae: 674.9611 - val_loss: 247077.5156 - val_mae: 461.0833\n",
            "Epoch 579/1000\n",
            "39/39 [==============================] - 10s 223ms/step - loss: 845484.0000 - mae: 675.5232 - val_loss: 247010.9844 - val_mae: 461.3646\n",
            "Epoch 580/1000\n",
            "39/39 [==============================] - 10s 252ms/step - loss: 827688.6875 - mae: 663.4326 - val_loss: 242220.5000 - val_mae: 456.9403\n",
            "Epoch 581/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 844368.0000 - mae: 675.5075 - val_loss: 246004.3281 - val_mae: 461.7129\n",
            "Epoch 582/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 830014.5625 - mae: 668.8171 - val_loss: 252873.0156 - val_mae: 468.1276\n",
            "Epoch 583/1000\n",
            "39/39 [==============================] - 4s 105ms/step - loss: 839274.3125 - mae: 671.4597 - val_loss: 252964.2031 - val_mae: 468.6638\n",
            "Epoch 584/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 838388.1875 - mae: 671.8345 - val_loss: 243949.3906 - val_mae: 459.0258\n",
            "Epoch 585/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 831529.2500 - mae: 667.9000 - val_loss: 240592.3906 - val_mae: 457.3021\n",
            "Epoch 586/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 838817.6875 - mae: 670.2266 - val_loss: 252076.4219 - val_mae: 468.3020\n",
            "Epoch 587/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 813480.7500 - mae: 658.3790 - val_loss: 242894.0781 - val_mae: 459.6146\n",
            "Epoch 588/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 839749.7500 - mae: 671.1696 - val_loss: 239970.8750 - val_mae: 457.3021\n",
            "Epoch 589/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 832151.6250 - mae: 668.3423 - val_loss: 233248.5156 - val_mae: 450.4448\n",
            "Epoch 590/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 833218.3750 - mae: 668.9383 - val_loss: 242095.6250 - val_mae: 459.3333\n",
            "Epoch 591/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 821617.9375 - mae: 664.1232 - val_loss: 248185.1719 - val_mae: 465.8553\n",
            "Epoch 592/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 829077.6875 - mae: 670.1360 - val_loss: 237763.6094 - val_mae: 454.5890\n",
            "Epoch 593/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 830552.1250 - mae: 668.8174 - val_loss: 234372.8125 - val_mae: 452.7717\n",
            "Epoch 594/1000\n",
            "39/39 [==============================] - 10s 252ms/step - loss: 820999.1250 - mae: 662.5597 - val_loss: 241252.5625 - val_mae: 459.3333\n",
            "Epoch 595/1000\n",
            "39/39 [==============================] - 4s 105ms/step - loss: 834923.0625 - mae: 670.3673 - val_loss: 236121.2969 - val_mae: 454.4654\n",
            "Epoch 596/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 831710.3125 - mae: 668.0148 - val_loss: 245346.9219 - val_mae: 463.8225\n",
            "Epoch 597/1000\n",
            "39/39 [==============================] - 6s 133ms/step - loss: 824808.4375 - mae: 665.9743 - val_loss: 247482.5156 - val_mae: 465.5592\n",
            "Epoch 598/1000\n",
            "39/39 [==============================] - 4s 103ms/step - loss: 840127.3125 - mae: 676.3648 - val_loss: 249566.4219 - val_mae: 468.5385\n",
            "Epoch 599/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 838546.0625 - mae: 673.3251 - val_loss: 247204.8750 - val_mae: 465.8130\n",
            "Epoch 600/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 830271.6875 - mae: 669.3607 - val_loss: 246982.0781 - val_mae: 465.7992\n",
            "Epoch 601/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 834390.5625 - mae: 671.1698 - val_loss: 233595.4219 - val_mae: 452.8031\n",
            "Epoch 602/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 828780.0000 - mae: 668.6229 - val_loss: 235338.7656 - val_mae: 455.2073\n",
            "Epoch 603/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 833803.6875 - mae: 671.6891 - val_loss: 242083.1250 - val_mae: 461.6458\n",
            "Epoch 604/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 814076.3125 - mae: 660.5974 - val_loss: 240942.5469 - val_mae: 461.4016\n",
            "Epoch 605/1000\n",
            "39/39 [==============================] - 5s 126ms/step - loss: 804925.4375 - mae: 659.9598 - val_loss: 245900.8281 - val_mae: 465.7316\n",
            "Epoch 606/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 827239.0000 - mae: 668.0974 - val_loss: 238808.6875 - val_mae: 459.3333\n",
            "Epoch 607/1000\n",
            "39/39 [==============================] - 6s 125ms/step - loss: 813605.8125 - mae: 659.6435 - val_loss: 241114.0000 - val_mae: 461.3646\n",
            "Epoch 608/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 835033.0000 - mae: 672.3246 - val_loss: 242767.6875 - val_mae: 463.6596\n",
            "Epoch 609/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 808044.2500 - mae: 658.3864 - val_loss: 240707.6250 - val_mae: 461.3645\n",
            "Epoch 610/1000\n",
            "39/39 [==============================] - 10s 255ms/step - loss: 830594.0000 - mae: 669.8677 - val_loss: 243176.9219 - val_mae: 463.6770\n",
            "Epoch 611/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 818258.6250 - mae: 663.2156 - val_loss: 239496.2031 - val_mae: 461.3071\n",
            "Epoch 612/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 824967.3750 - mae: 668.2351 - val_loss: 244073.4531 - val_mae: 465.0746\n",
            "Epoch 613/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 815426.4375 - mae: 662.4280 - val_loss: 244041.0469 - val_mae: 465.3423\n",
            "Epoch 614/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 811618.3750 - mae: 659.5604 - val_loss: 233169.5469 - val_mae: 455.3690\n",
            "Epoch 615/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 798997.9375 - mae: 652.9408 - val_loss: 236901.7344 - val_mae: 459.0521\n",
            "Epoch 616/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 817158.0625 - mae: 664.4698 - val_loss: 236883.2031 - val_mae: 459.3333\n",
            "Epoch 617/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 827514.1250 - mae: 670.4662 - val_loss: 235108.2344 - val_mae: 457.4411\n",
            "Epoch 618/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 816423.0625 - mae: 665.2565 - val_loss: 236506.0000 - val_mae: 459.3333\n",
            "Epoch 619/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 808006.2500 - mae: 661.1632 - val_loss: 242625.0781 - val_mae: 464.9795\n",
            "Epoch 620/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 824267.6875 - mae: 668.5798 - val_loss: 236127.7656 - val_mae: 459.3333\n",
            "Epoch 621/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 807376.8125 - mae: 657.7192 - val_loss: 235946.9219 - val_mae: 459.3333\n",
            "Epoch 622/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 807977.8750 - mae: 659.5673 - val_loss: 232525.4844 - val_mae: 455.4034\n",
            "Epoch 623/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 815469.0625 - mae: 661.7003 - val_loss: 239372.2344 - val_mae: 462.8944\n",
            "Epoch 624/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 798064.8125 - mae: 653.9795 - val_loss: 231266.0469 - val_mae: 455.2232\n",
            "Epoch 625/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 807065.3125 - mae: 657.8777 - val_loss: 237488.9531 - val_mae: 461.0833\n",
            "Epoch 626/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 796538.7500 - mae: 654.0446 - val_loss: 240635.2969 - val_mae: 465.1877\n",
            "Epoch 627/1000\n",
            "39/39 [==============================] - 11s 265ms/step - loss: 798480.8750 - mae: 656.1342 - val_loss: 234861.8906 - val_mae: 459.3333\n",
            "Epoch 628/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 817652.7500 - mae: 665.9798 - val_loss: 238400.3281 - val_mae: 462.8270\n",
            "Epoch 629/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 805972.6875 - mae: 658.9086 - val_loss: 234684.3594 - val_mae: 459.6146\n",
            "Epoch 630/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 787152.3750 - mae: 652.3787 - val_loss: 232887.3750 - val_mae: 457.6168\n",
            "Epoch 631/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 807457.8750 - mae: 658.7231 - val_loss: 234149.0781 - val_mae: 459.3333\n",
            "Epoch 632/1000\n",
            "39/39 [==============================] - 6s 120ms/step - loss: 795042.5625 - mae: 653.6038 - val_loss: 230960.3125 - val_mae: 455.6729\n",
            "Epoch 633/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 802634.3750 - mae: 657.2405 - val_loss: 237625.5000 - val_mae: 463.0404\n",
            "Epoch 634/1000\n",
            "39/39 [==============================] - 7s 166ms/step - loss: 805482.1250 - mae: 657.0969 - val_loss: 233439.1250 - val_mae: 459.0521\n",
            "Epoch 635/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 797504.9375 - mae: 656.2313 - val_loss: 235198.9844 - val_mae: 461.5446\n",
            "Epoch 636/1000\n",
            "39/39 [==============================] - 10s 245ms/step - loss: 805063.0000 - mae: 658.8844 - val_loss: 237245.2969 - val_mae: 463.2809\n",
            "Epoch 637/1000\n",
            "39/39 [==============================] - 9s 221ms/step - loss: 790191.1875 - mae: 653.1840 - val_loss: 235340.5000 - val_mae: 461.0833\n",
            "Epoch 638/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 798613.0000 - mae: 654.5947 - val_loss: 235350.1719 - val_mae: 461.3646\n",
            "Epoch 639/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 804694.4375 - mae: 658.5876 - val_loss: 236884.8750 - val_mae: 463.5220\n",
            "Epoch 640/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 798521.0000 - mae: 655.2473 - val_loss: 233930.4844 - val_mae: 460.9151\n",
            "Epoch 641/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 785482.6250 - mae: 652.2990 - val_loss: 237842.7500 - val_mae: 465.0639\n",
            "Epoch 642/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 793245.2500 - mae: 652.7209 - val_loss: 235980.9531 - val_mae: 462.9198\n",
            "Epoch 643/1000\n",
            "39/39 [==============================] - 7s 165ms/step - loss: 790013.4375 - mae: 654.2308 - val_loss: 228421.2500 - val_mae: 455.7601\n",
            "Epoch 644/1000\n",
            "39/39 [==============================] - 4s 94ms/step - loss: 792358.4375 - mae: 652.8618 - val_loss: 234157.0469 - val_mae: 461.0833\n",
            "Epoch 645/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 805594.8750 - mae: 660.2115 - val_loss: 234175.2656 - val_mae: 461.3646\n",
            "Epoch 646/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 794593.4375 - mae: 654.5801 - val_loss: 231615.2656 - val_mae: 459.3333\n",
            "Epoch 647/1000\n",
            "39/39 [==============================] - 6s 140ms/step - loss: 801864.5000 - mae: 659.3508 - val_loss: 233841.3906 - val_mae: 461.3646\n",
            "Epoch 648/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 788922.0625 - mae: 651.7072 - val_loss: 229865.6406 - val_mae: 457.5780\n",
            "Epoch 649/1000\n",
            "39/39 [==============================] - 10s 253ms/step - loss: 781178.0625 - mae: 648.3177 - val_loss: 234923.5781 - val_mae: 463.1064\n",
            "Epoch 650/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 794156.5000 - mae: 655.6487 - val_loss: 230971.9219 - val_mae: 459.3333\n",
            "Epoch 651/1000\n",
            "39/39 [==============================] - 10s 242ms/step - loss: 789420.4375 - mae: 652.7040 - val_loss: 236581.6875 - val_mae: 464.5486\n",
            "Epoch 652/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 796076.8750 - mae: 656.7936 - val_loss: 228469.8125 - val_mae: 457.5833\n",
            "Epoch 653/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 787187.6875 - mae: 650.2911 - val_loss: 227950.5156 - val_mae: 456.2379\n",
            "Epoch 654/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 793411.3750 - mae: 655.1017 - val_loss: 229170.1250 - val_mae: 457.9402\n",
            "Epoch 655/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 789707.9375 - mae: 651.2985 - val_loss: 231203.3750 - val_mae: 459.7035\n",
            "Epoch 656/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 791826.4375 - mae: 655.0020 - val_loss: 227663.8125 - val_mae: 457.3021\n",
            "Epoch 657/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 787190.4375 - mae: 652.5583 - val_loss: 231871.0625 - val_mae: 461.0000\n",
            "Epoch 658/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 794144.1875 - mae: 655.6198 - val_loss: 228601.9219 - val_mae: 457.9936\n",
            "Epoch 659/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 786065.6875 - mae: 651.2708 - val_loss: 231739.3750 - val_mae: 461.0833\n",
            "Epoch 660/1000\n",
            "39/39 [==============================] - 5s 96ms/step - loss: 776059.4375 - mae: 645.6298 - val_loss: 235417.5000 - val_mae: 464.9895\n",
            "Epoch 661/1000\n",
            "39/39 [==============================] - 7s 163ms/step - loss: 786979.6250 - mae: 653.0085 - val_loss: 227990.0469 - val_mae: 457.7526\n",
            "Epoch 662/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 772777.9375 - mae: 645.9196 - val_loss: 231467.7969 - val_mae: 461.3646\n",
            "Epoch 663/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 761573.5625 - mae: 640.1748 - val_loss: 226627.6250 - val_mae: 457.3021\n",
            "Epoch 664/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 787432.6250 - mae: 651.7726 - val_loss: 228824.7500 - val_mae: 459.3334\n",
            "Epoch 665/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 776169.2500 - mae: 648.6763 - val_loss: 229977.2031 - val_mae: 460.1187\n",
            "Epoch 666/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 773532.1250 - mae: 648.0862 - val_loss: 228364.5000 - val_mae: 459.2500\n",
            "Epoch 667/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 776216.0000 - mae: 647.3949 - val_loss: 228386.0469 - val_mae: 459.3333\n",
            "Epoch 668/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 778247.8125 - mae: 649.8239 - val_loss: 230578.0781 - val_mae: 461.3646\n",
            "Epoch 669/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 778931.3750 - mae: 649.2894 - val_loss: 227098.3906 - val_mae: 458.1415\n",
            "Epoch 670/1000\n",
            "39/39 [==============================] - 7s 165ms/step - loss: 774333.7500 - mae: 645.6889 - val_loss: 229331.9844 - val_mae: 461.0740\n",
            "Epoch 671/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 774344.4375 - mae: 647.9020 - val_loss: 229181.5156 - val_mae: 461.0607\n",
            "Epoch 672/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 776758.6875 - mae: 648.0161 - val_loss: 232315.1094 - val_mae: 464.2296\n",
            "Epoch 673/1000\n",
            "39/39 [==============================] - 4s 102ms/step - loss: 777583.1875 - mae: 647.2253 - val_loss: 228483.6094 - val_mae: 460.4713\n",
            "Epoch 674/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 761068.0000 - mae: 642.4395 - val_loss: 227193.6250 - val_mae: 459.0521\n",
            "Epoch 675/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 785386.8125 - mae: 653.8492 - val_loss: 230501.2031 - val_mae: 462.4760\n",
            "Epoch 676/1000\n",
            "39/39 [==============================] - 10s 250ms/step - loss: 782511.7500 - mae: 652.0858 - val_loss: 227120.9844 - val_mae: 459.3333\n",
            "Epoch 677/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 777888.9375 - mae: 649.0004 - val_loss: 230201.2344 - val_mae: 462.4493\n",
            "Epoch 678/1000\n",
            "39/39 [==============================] - 7s 159ms/step - loss: 777289.5000 - mae: 650.4777 - val_loss: 230251.0156 - val_mae: 462.7172\n",
            "Epoch 679/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 784364.0625 - mae: 654.0615 - val_loss: 227927.5000 - val_mae: 460.0257\n",
            "Epoch 680/1000\n",
            "39/39 [==============================] - 10s 249ms/step - loss: 762862.6875 - mae: 643.2171 - val_loss: 233133.8594 - val_mae: 465.7664\n",
            "Epoch 681/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 770337.9375 - mae: 645.3845 - val_loss: 224149.5469 - val_mae: 457.3021\n",
            "Epoch 682/1000\n",
            "39/39 [==============================] - 11s 256ms/step - loss: 751005.2500 - mae: 638.0298 - val_loss: 231962.4531 - val_mae: 464.6952\n",
            "Epoch 683/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 750940.6875 - mae: 635.6318 - val_loss: 225343.2031 - val_mae: 458.3285\n",
            "Epoch 684/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 773678.8750 - mae: 647.7068 - val_loss: 228545.3594 - val_mae: 461.6458\n",
            "Epoch 685/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 774264.6250 - mae: 649.2283 - val_loss: 230498.7500 - val_mae: 463.3958\n",
            "Epoch 686/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 769515.3750 - mae: 645.3936 - val_loss: 225791.9219 - val_mae: 459.3333\n",
            "Epoch 687/1000\n",
            "39/39 [==============================] - 10s 259ms/step - loss: 771281.0625 - mae: 645.6820 - val_loss: 231226.2031 - val_mae: 464.6277\n",
            "Epoch 688/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 757918.1875 - mae: 642.0927 - val_loss: 227813.5156 - val_mae: 461.3646\n",
            "Epoch 689/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 767590.3125 - mae: 644.8982 - val_loss: 228660.8281 - val_mae: 462.5696\n",
            "Epoch 690/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 766799.5000 - mae: 644.2933 - val_loss: 225075.8125 - val_mae: 459.0521\n",
            "Epoch 691/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 772313.1250 - mae: 647.6930 - val_loss: 225151.3594 - val_mae: 459.3333\n",
            "Epoch 692/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 762762.1250 - mae: 642.4501 - val_loss: 222756.4531 - val_mae: 457.3021\n",
            "Epoch 693/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 761813.0625 - mae: 642.8207 - val_loss: 228307.6875 - val_mae: 462.7967\n",
            "Epoch 694/1000\n",
            "39/39 [==============================] - 9s 219ms/step - loss: 768747.6875 - mae: 648.3577 - val_loss: 230227.6719 - val_mae: 464.5328\n",
            "Epoch 695/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 760593.0000 - mae: 641.7644 - val_loss: 224451.9219 - val_mae: 459.0521\n",
            "Epoch 696/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 772877.0625 - mae: 649.6435 - val_loss: 226586.9219 - val_mae: 461.0833\n",
            "Epoch 697/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 756117.2500 - mae: 640.7733 - val_loss: 225303.1406 - val_mae: 460.4302\n",
            "Epoch 698/1000\n",
            "39/39 [==============================] - 10s 245ms/step - loss: 759249.0000 - mae: 641.0953 - val_loss: 230558.2969 - val_mae: 465.5623\n",
            "Epoch 699/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 762287.2500 - mae: 643.8154 - val_loss: 223965.6875 - val_mae: 459.0521\n",
            "Epoch 700/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 765687.1250 - mae: 645.3012 - val_loss: 226952.6875 - val_mae: 462.1395\n",
            "Epoch 701/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 740359.5000 - mae: 634.2900 - val_loss: 225984.1094 - val_mae: 461.0833\n",
            "Epoch 702/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 757750.6250 - mae: 642.8564 - val_loss: 226071.8750 - val_mae: 461.3646\n",
            "Epoch 703/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 748010.3750 - mae: 636.2239 - val_loss: 223924.1875 - val_mae: 459.6146\n",
            "Epoch 704/1000\n",
            "39/39 [==============================] - 10s 249ms/step - loss: 754142.1875 - mae: 640.5116 - val_loss: 223809.7031 - val_mae: 459.6146\n",
            "Epoch 705/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 742220.0000 - mae: 637.4133 - val_loss: 225515.0781 - val_mae: 461.0833\n",
            "Epoch 706/1000\n",
            "39/39 [==============================] - 6s 125ms/step - loss: 744854.8750 - mae: 637.9788 - val_loss: 223958.1719 - val_mae: 460.0289\n",
            "Epoch 707/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 742279.1250 - mae: 634.7045 - val_loss: 225858.5156 - val_mae: 461.7657\n",
            "Epoch 708/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 746903.1250 - mae: 636.3110 - val_loss: 222943.0781 - val_mae: 459.0521\n",
            "Epoch 709/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 750667.5000 - mae: 638.9150 - val_loss: 225270.2969 - val_mae: 461.3646\n",
            "Epoch 710/1000\n",
            "39/39 [==============================] - 6s 131ms/step - loss: 748063.5000 - mae: 637.9131 - val_loss: 223476.3906 - val_mae: 459.9759\n",
            "Epoch 711/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 748123.6250 - mae: 639.3687 - val_loss: 224834.0781 - val_mae: 461.0833\n",
            "Epoch 712/1000\n",
            "39/39 [==============================] - 5s 125ms/step - loss: 750025.1875 - mae: 640.6526 - val_loss: 219984.8906 - val_mae: 456.6866\n",
            "Epoch 713/1000\n",
            "39/39 [==============================] - 4s 101ms/step - loss: 752536.1875 - mae: 641.2560 - val_loss: 221389.7969 - val_mae: 457.8484\n",
            "Epoch 714/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 745667.1250 - mae: 635.0861 - val_loss: 223521.0156 - val_mae: 459.9058\n",
            "Epoch 715/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 748916.5625 - mae: 637.8105 - val_loss: 222399.7969 - val_mae: 459.3333\n",
            "Epoch 716/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 731711.1875 - mae: 636.3276 - val_loss: 224506.4219 - val_mae: 461.3646\n",
            "Epoch 717/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 742324.3125 - mae: 634.2880 - val_loss: 225071.7344 - val_mae: 462.1944\n",
            "Epoch 718/1000\n",
            "39/39 [==============================] - 6s 131ms/step - loss: 752981.7500 - mae: 642.0484 - val_loss: 221968.2500 - val_mae: 459.2500\n",
            "Epoch 719/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 740595.6875 - mae: 636.8528 - val_loss: 224626.5156 - val_mae: 461.8864\n",
            "Epoch 720/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 740560.0000 - mae: 637.1942 - val_loss: 224088.1250 - val_mae: 461.3646\n",
            "Epoch 721/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 743002.4375 - mae: 636.8069 - val_loss: 226180.5781 - val_mae: 463.3958\n",
            "Epoch 722/1000\n",
            "39/39 [==============================] - 7s 157ms/step - loss: 745044.9375 - mae: 642.0410 - val_loss: 223884.1094 - val_mae: 461.3646\n",
            "Epoch 723/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 731455.8125 - mae: 633.5936 - val_loss: 221203.2500 - val_mae: 458.8643\n",
            "Epoch 724/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 742613.7500 - mae: 637.1151 - val_loss: 227284.0781 - val_mae: 465.1510\n",
            "Epoch 725/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 738298.4375 - mae: 635.7655 - val_loss: 221777.0156 - val_mae: 459.7762\n",
            "Epoch 726/1000\n",
            "39/39 [==============================] - 6s 143ms/step - loss: 737000.7500 - mae: 635.0960 - val_loss: 221311.0156 - val_mae: 459.3333\n",
            "Epoch 727/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 745004.6875 - mae: 639.9670 - val_loss: 223173.5000 - val_mae: 461.0833\n",
            "Epoch 728/1000\n",
            "39/39 [==============================] - 6s 139ms/step - loss: 738592.8125 - mae: 635.5403 - val_loss: 223850.3281 - val_mae: 462.0483\n",
            "Epoch 729/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 744930.3125 - mae: 641.2961 - val_loss: 226024.6250 - val_mae: 464.1738\n",
            "Epoch 730/1000\n",
            "39/39 [==============================] - 6s 146ms/step - loss: 739962.6875 - mae: 638.0744 - val_loss: 223636.0156 - val_mae: 462.0215\n",
            "Epoch 731/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 747142.2500 - mae: 642.5703 - val_loss: 217845.5469 - val_mae: 456.2964\n",
            "Epoch 732/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 739526.1250 - mae: 637.6357 - val_loss: 223720.2500 - val_mae: 462.3440\n",
            "Epoch 733/1000\n",
            "39/39 [==============================] - 4s 95ms/step - loss: 735013.1250 - mae: 636.8067 - val_loss: 223606.6719 - val_mae: 462.3175\n",
            "Epoch 734/1000\n",
            "39/39 [==============================] - 5s 117ms/step - loss: 727030.3750 - mae: 633.2320 - val_loss: 220300.5156 - val_mae: 459.0110\n",
            "Epoch 735/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 742215.5000 - mae: 639.6385 - val_loss: 225800.9375 - val_mae: 464.6056\n",
            "Epoch 736/1000\n",
            "39/39 [==============================] - 9s 234ms/step - loss: 734915.6875 - mae: 636.6549 - val_loss: 224742.9219 - val_mae: 463.4113\n",
            "Epoch 737/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 725991.0000 - mae: 631.2190 - val_loss: 224858.1406 - val_mae: 463.6790\n",
            "Epoch 738/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 730798.6875 - mae: 632.4917 - val_loss: 224536.8281 - val_mae: 463.3847\n",
            "Epoch 739/1000\n",
            "39/39 [==============================] - 9s 222ms/step - loss: 725098.9375 - mae: 633.8187 - val_loss: 220360.8281 - val_mae: 459.5903\n",
            "Epoch 740/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 736961.1250 - mae: 638.1421 - val_loss: 222635.1719 - val_mae: 461.8898\n",
            "Epoch 741/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 738986.3750 - mae: 639.7617 - val_loss: 222535.5781 - val_mae: 461.8761\n",
            "Epoch 742/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 738569.3125 - mae: 639.7247 - val_loss: 219711.6250 - val_mae: 459.1167\n",
            "Epoch 743/1000\n",
            "39/39 [==============================] - 10s 248ms/step - loss: 734267.3125 - mae: 638.0982 - val_loss: 219642.8750 - val_mae: 459.1297\n",
            "Epoch 744/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 733463.2500 - mae: 636.1129 - val_loss: 222472.9531 - val_mae: 462.1173\n",
            "Epoch 745/1000\n",
            "39/39 [==============================] - 7s 147ms/step - loss: 720888.0625 - mae: 631.3281 - val_loss: 221708.2031 - val_mae: 461.2600\n",
            "Epoch 746/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 722290.6875 - mae: 632.0117 - val_loss: 224558.0469 - val_mae: 464.2864\n",
            "Epoch 747/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 727944.5000 - mae: 633.9177 - val_loss: 222327.1719 - val_mae: 462.2290\n",
            "Epoch 748/1000\n",
            "39/39 [==============================] - 5s 94ms/step - loss: 729284.1875 - mae: 634.6173 - val_loss: 222223.4844 - val_mae: 462.2019\n",
            "Epoch 749/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 735001.3125 - mae: 637.9377 - val_loss: 219671.9844 - val_mae: 459.7389\n",
            "Epoch 750/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 731536.5625 - mae: 636.1519 - val_loss: 219172.3906 - val_mae: 459.2227\n",
            "Epoch 751/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 730017.2500 - mae: 636.6613 - val_loss: 223735.8594 - val_mae: 463.7750\n",
            "Epoch 752/1000\n",
            "39/39 [==============================] - 6s 128ms/step - loss: 730460.4375 - mae: 636.9573 - val_loss: 221232.6406 - val_mae: 461.3646\n",
            "Epoch 753/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 736236.4375 - mae: 639.6077 - val_loss: 218817.2656 - val_mae: 459.0521\n",
            "Epoch 754/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 713784.8125 - mae: 629.2583 - val_loss: 221081.7344 - val_mae: 461.3646\n",
            "Epoch 755/1000\n",
            "39/39 [==============================] - 5s 99ms/step - loss: 714959.8125 - mae: 630.1589 - val_loss: 218861.4531 - val_mae: 459.2891\n",
            "Epoch 756/1000\n",
            "39/39 [==============================] - 6s 138ms/step - loss: 704811.0000 - mae: 623.9919 - val_loss: 222815.0000 - val_mae: 463.1146\n",
            "Epoch 757/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 728003.2500 - mae: 635.5826 - val_loss: 221337.5625 - val_mae: 461.9461\n",
            "Epoch 758/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 720713.6875 - mae: 633.5274 - val_loss: 220794.1406 - val_mae: 461.3646\n",
            "Epoch 759/1000\n",
            "39/39 [==============================] - 10s 248ms/step - loss: 711176.1875 - mae: 630.2061 - val_loss: 223046.1250 - val_mae: 463.6702\n",
            "Epoch 760/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 721921.8750 - mae: 633.3412 - val_loss: 220390.7656 - val_mae: 461.0420\n",
            "Epoch 761/1000\n",
            "39/39 [==============================] - 6s 143ms/step - loss: 717757.1875 - mae: 634.4767 - val_loss: 222419.8906 - val_mae: 463.0808\n",
            "Epoch 762/1000\n",
            "39/39 [==============================] - 10s 230ms/step - loss: 721137.8750 - mae: 634.5211 - val_loss: 218430.6719 - val_mae: 459.3333\n",
            "Epoch 763/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 706378.5000 - mae: 627.5633 - val_loss: 218136.4219 - val_mae: 459.0521\n",
            "Epoch 764/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 711317.7500 - mae: 629.8283 - val_loss: 220719.8750 - val_mae: 461.7804\n",
            "Epoch 765/1000\n",
            "39/39 [==============================] - 10s 250ms/step - loss: 701720.6250 - mae: 625.3014 - val_loss: 215996.6719 - val_mae: 457.1073\n",
            "Epoch 766/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 725047.8125 - mae: 636.3614 - val_loss: 222393.7500 - val_mae: 463.4774\n",
            "Epoch 767/1000\n",
            "39/39 [==============================] - 5s 125ms/step - loss: 716242.8750 - mae: 631.5381 - val_loss: 220327.4531 - val_mae: 461.5335\n",
            "Epoch 768/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 720511.8125 - mae: 637.8907 - val_loss: 219915.3750 - val_mae: 461.1132\n",
            "Epoch 769/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 710636.8125 - mae: 628.8259 - val_loss: 217757.3906 - val_mae: 459.0520\n",
            "Epoch 770/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 709115.5625 - mae: 628.7283 - val_loss: 219582.6250 - val_mae: 460.8711\n",
            "Epoch 771/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 715008.1250 - mae: 632.9737 - val_loss: 221868.1875 - val_mae: 463.2306\n",
            "Epoch 772/1000\n",
            "39/39 [==============================] - 10s 241ms/step - loss: 707874.1875 - mae: 629.4012 - val_loss: 217818.2656 - val_mae: 459.3334\n",
            "Epoch 773/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 709693.8125 - mae: 628.8269 - val_loss: 215465.2969 - val_mae: 457.0208\n",
            "Epoch 774/1000\n",
            "39/39 [==============================] - 10s 254ms/step - loss: 722057.1250 - mae: 637.6017 - val_loss: 219700.0000 - val_mae: 461.2882\n",
            "Epoch 775/1000\n",
            "39/39 [==============================] - 6s 141ms/step - loss: 713791.6250 - mae: 632.5781 - val_loss: 219471.1094 - val_mae: 461.0833\n",
            "Epoch 776/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 712252.0000 - mae: 631.8555 - val_loss: 219923.9219 - val_mae: 461.6963\n",
            "Epoch 777/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 708720.9375 - mae: 630.1312 - val_loss: 221231.6094 - val_mae: 462.9070\n",
            "Epoch 778/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 708070.8750 - mae: 630.4215 - val_loss: 215420.0625 - val_mae: 457.2784\n",
            "Epoch 779/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 711201.0000 - mae: 632.9855 - val_loss: 217437.2500 - val_mae: 459.3333\n",
            "Epoch 780/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 698907.6250 - mae: 627.5247 - val_loss: 221470.6250 - val_mae: 463.3938\n",
            "Epoch 781/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 702103.5000 - mae: 627.4020 - val_loss: 217088.8750 - val_mae: 459.0372\n",
            "Epoch 782/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 702266.1250 - mae: 630.5519 - val_loss: 215508.8750 - val_mae: 457.6113\n",
            "Epoch 783/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 701052.0000 - mae: 626.3419 - val_loss: 219512.8906 - val_mae: 461.6459\n",
            "Epoch 784/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 693823.1875 - mae: 625.0923 - val_loss: 219137.7969 - val_mae: 461.2568\n",
            "Epoch 785/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 707668.6250 - mae: 630.1947 - val_loss: 217146.7031 - val_mae: 459.3333\n",
            "Epoch 786/1000\n",
            "39/39 [==============================] - 11s 257ms/step - loss: 703943.1250 - mae: 628.4990 - val_loss: 219128.2500 - val_mae: 461.3646\n",
            "Epoch 787/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 707652.3125 - mae: 631.9216 - val_loss: 215344.5625 - val_mae: 457.6754\n",
            "Epoch 788/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 710229.9375 - mae: 636.6186 - val_loss: 216772.0625 - val_mae: 459.0521\n",
            "Epoch 789/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 687071.1875 - mae: 619.8966 - val_loss: 221008.9375 - val_mae: 463.3958\n",
            "Epoch 790/1000\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 699031.5625 - mae: 627.8585 - val_loss: 220507.4844 - val_mae: 462.8514\n",
            "Epoch 791/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 693104.1875 - mae: 624.7718 - val_loss: 218593.3125 - val_mae: 461.0000\n",
            "Epoch 792/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 706148.5625 - mae: 632.5839 - val_loss: 218181.7969 - val_mae: 460.5608\n",
            "Epoch 793/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 695108.9375 - mae: 628.8824 - val_loss: 216659.1406 - val_mae: 459.1619\n",
            "Epoch 794/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 696470.3750 - mae: 624.9266 - val_loss: 220233.5469 - val_mae: 462.7457\n",
            "Epoch 795/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 706619.2500 - mae: 632.7538 - val_loss: 217280.3906 - val_mae: 460.0101\n",
            "Epoch 796/1000\n",
            "39/39 [==============================] - 10s 249ms/step - loss: 693958.6250 - mae: 626.3987 - val_loss: 218272.5469 - val_mae: 460.8731\n",
            "Epoch 797/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 704523.6875 - mae: 631.8363 - val_loss: 220221.0469 - val_mae: 462.8912\n",
            "Epoch 798/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 701610.6875 - mae: 631.1327 - val_loss: 218653.8594 - val_mae: 461.4091\n",
            "Epoch 799/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 687239.7500 - mae: 622.5901 - val_loss: 218363.6406 - val_mae: 461.1150\n",
            "Epoch 800/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 699106.9375 - mae: 629.6774 - val_loss: 218286.4844 - val_mae: 461.0833\n",
            "Epoch 801/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 686855.8125 - mae: 627.1099 - val_loss: 219548.5781 - val_mae: 462.2823\n",
            "Epoch 802/1000\n",
            "39/39 [==============================] - 9s 218ms/step - loss: 695168.1875 - mae: 628.6309 - val_loss: 215747.0469 - val_mae: 458.4820\n",
            "Epoch 803/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 698424.3750 - mae: 629.7253 - val_loss: 217687.8281 - val_mae: 460.4802\n",
            "Epoch 804/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 690565.9375 - mae: 628.1649 - val_loss: 218128.8125 - val_mae: 461.0493\n",
            "Epoch 805/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 690207.5000 - mae: 629.2501 - val_loss: 218862.2969 - val_mae: 461.9738\n",
            "Epoch 806/1000\n",
            "39/39 [==============================] - 6s 139ms/step - loss: 690072.0625 - mae: 625.4454 - val_loss: 216587.9219 - val_mae: 459.6146\n",
            "Epoch 807/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 699042.8750 - mae: 631.6889 - val_loss: 215782.1094 - val_mae: 458.6988\n",
            "Epoch 808/1000\n",
            "39/39 [==============================] - 10s 248ms/step - loss: 703157.6250 - mae: 635.0992 - val_loss: 216284.4844 - val_mae: 459.3333\n",
            "Epoch 809/1000\n",
            "39/39 [==============================] - 6s 145ms/step - loss: 695905.0625 - mae: 629.2381 - val_loss: 217916.3750 - val_mae: 460.9845\n",
            "Epoch 810/1000\n",
            "39/39 [==============================] - 10s 230ms/step - loss: 693067.4375 - mae: 629.8406 - val_loss: 217557.1250 - val_mae: 460.5783\n",
            "Epoch 811/1000\n",
            "39/39 [==============================] - 10s 254ms/step - loss: 682077.4375 - mae: 623.2270 - val_loss: 219801.8594 - val_mae: 462.9900\n",
            "Epoch 812/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 698746.8750 - mae: 633.4799 - val_loss: 218044.5781 - val_mae: 461.2266\n",
            "Epoch 813/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 703189.3125 - mae: 636.3583 - val_loss: 219223.8281 - val_mae: 462.4010\n",
            "Epoch 814/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 694485.5625 - mae: 630.4598 - val_loss: 216487.5781 - val_mae: 459.7791\n",
            "Epoch 815/1000\n",
            "39/39 [==============================] - 10s 239ms/step - loss: 678501.9375 - mae: 621.8677 - val_loss: 217810.0781 - val_mae: 461.0833\n",
            "Epoch 816/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 699786.2500 - mae: 634.8702 - val_loss: 218724.0000 - val_mae: 461.8913\n",
            "Epoch 817/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 691726.5000 - mae: 628.8973 - val_loss: 215168.5156 - val_mae: 458.2863\n",
            "Epoch 818/1000\n",
            "39/39 [==============================] - 10s 249ms/step - loss: 693118.8750 - mae: 631.1611 - val_loss: 219931.4375 - val_mae: 463.3958\n",
            "Epoch 819/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 686100.3125 - mae: 626.6271 - val_loss: 215767.5469 - val_mae: 459.0521\n",
            "Epoch 820/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 688609.8125 - mae: 628.1152 - val_loss: 217939.6250 - val_mae: 461.3646\n",
            "Epoch 821/1000\n",
            "39/39 [==============================] - 7s 153ms/step - loss: 684983.8750 - mae: 626.8705 - val_loss: 216665.1719 - val_mae: 460.1515\n",
            "Epoch 822/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 683359.1875 - mae: 625.6506 - val_loss: 219834.5469 - val_mae: 463.3958\n",
            "Epoch 823/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 679661.2500 - mae: 626.0811 - val_loss: 215694.0000 - val_mae: 459.0521\n",
            "Epoch 824/1000\n",
            "39/39 [==============================] - 6s 121ms/step - loss: 689678.5625 - mae: 629.9891 - val_loss: 217141.6250 - val_mae: 460.5076\n",
            "Epoch 825/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 682349.0000 - mae: 626.3995 - val_loss: 217113.0156 - val_mae: 460.4947\n",
            "Epoch 826/1000\n",
            "39/39 [==============================] - 4s 97ms/step - loss: 691269.3125 - mae: 632.5242 - val_loss: 219498.8281 - val_mae: 463.1146\n",
            "Epoch 827/1000\n",
            "39/39 [==============================] - 9s 230ms/step - loss: 691217.5000 - mae: 631.0826 - val_loss: 216559.6719 - val_mae: 459.8545\n",
            "Epoch 828/1000\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 678533.8125 - mae: 623.4023 - val_loss: 215871.5000 - val_mae: 459.3333\n",
            "Epoch 829/1000\n",
            "39/39 [==============================] - 10s 237ms/step - loss: 683712.6875 - mae: 627.7971 - val_loss: 216112.0469 - val_mae: 459.6146\n",
            "Epoch 830/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 662960.9375 - mae: 618.4824 - val_loss: 217510.9375 - val_mae: 461.0833\n",
            "Epoch 831/1000\n",
            "39/39 [==============================] - 7s 154ms/step - loss: 680604.0000 - mae: 627.2139 - val_loss: 215297.0625 - val_mae: 458.6671\n",
            "Epoch 832/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 663804.8125 - mae: 620.8641 - val_loss: 213916.2344 - val_mae: 457.3021\n",
            "Epoch 833/1000\n",
            "39/39 [==============================] - 10s 233ms/step - loss: 681054.1250 - mae: 626.5155 - val_loss: 217166.1406 - val_mae: 460.6730\n",
            "Epoch 834/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 681933.2500 - mae: 628.5360 - val_loss: 216316.7656 - val_mae: 459.6729\n",
            "Epoch 835/1000\n",
            "39/39 [==============================] - 10s 245ms/step - loss: 683254.2500 - mae: 627.5905 - val_loss: 215219.5625 - val_mae: 458.6153\n",
            "Epoch 836/1000\n",
            "39/39 [==============================] - 4s 98ms/step - loss: 680829.2500 - mae: 627.6310 - val_loss: 217235.4844 - val_mae: 461.0756\n",
            "Epoch 837/1000\n",
            "39/39 [==============================] - 6s 147ms/step - loss: 682789.1875 - mae: 628.4494 - val_loss: 216827.9375 - val_mae: 460.3392\n",
            "Epoch 838/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 684009.5000 - mae: 630.4504 - val_loss: 216653.5781 - val_mae: 460.3717\n",
            "Epoch 839/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 676124.3125 - mae: 624.9876 - val_loss: 216660.2344 - val_mae: 460.3855\n",
            "Epoch 840/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 680856.1875 - mae: 629.5835 - val_loss: 217922.4844 - val_mae: 461.6459\n",
            "Epoch 841/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 669847.0000 - mae: 620.7375 - val_loss: 216757.3906 - val_mae: 460.2873\n",
            "Epoch 842/1000\n",
            "39/39 [==============================] - 6s 137ms/step - loss: 672253.6250 - mae: 623.5179 - val_loss: 214540.1719 - val_mae: 458.1111\n",
            "Epoch 843/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 671118.5625 - mae: 622.7844 - val_loss: 217909.4375 - val_mae: 461.6458\n",
            "Epoch 844/1000\n",
            "39/39 [==============================] - 6s 146ms/step - loss: 662501.0625 - mae: 621.1131 - val_loss: 218852.7500 - val_mae: 462.5618\n",
            "Epoch 845/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 673197.1250 - mae: 624.9305 - val_loss: 216686.2656 - val_mae: 460.0096\n",
            "Epoch 846/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 676764.2500 - mae: 627.5339 - val_loss: 218382.6094 - val_mae: 461.9558\n",
            "Epoch 847/1000\n",
            "39/39 [==============================] - 11s 249ms/step - loss: 664913.8750 - mae: 622.5770 - val_loss: 217383.8281 - val_mae: 461.0833\n",
            "Epoch 848/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 663644.4375 - mae: 620.1346 - val_loss: 217902.5781 - val_mae: 461.6458\n",
            "Epoch 849/1000\n",
            "39/39 [==============================] - 6s 118ms/step - loss: 666138.7500 - mae: 621.9398 - val_loss: 216504.0000 - val_mae: 460.2318\n",
            "Epoch 850/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 671931.2500 - mae: 624.5220 - val_loss: 216778.6406 - val_mae: 460.5260\n",
            "Epoch 851/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 671021.1250 - mae: 626.1588 - val_loss: 216533.6875 - val_mae: 460.2579\n",
            "Epoch 852/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 674540.9375 - mae: 627.2535 - val_loss: 216129.3906 - val_mae: 459.4898\n",
            "Epoch 853/1000\n",
            "39/39 [==============================] - 6s 122ms/step - loss: 674697.4375 - mae: 627.6364 - val_loss: 217392.3594 - val_mae: 461.0833\n",
            "Epoch 854/1000\n",
            "39/39 [==============================] - 4s 89ms/step - loss: 662845.1875 - mae: 620.9434 - val_loss: 218732.1719 - val_mae: 462.4323\n",
            "Epoch 855/1000\n",
            "39/39 [==============================] - 6s 146ms/step - loss: 673144.1875 - mae: 628.9749 - val_loss: 216079.3906 - val_mae: 459.4116\n",
            "Epoch 856/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 672444.1875 - mae: 628.1437 - val_loss: 213968.2969 - val_mae: 457.3021\n",
            "Epoch 857/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 673086.1250 - mae: 627.9036 - val_loss: 217675.8906 - val_mae: 461.3646\n",
            "Epoch 858/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 672954.3750 - mae: 628.1019 - val_loss: 216924.0781 - val_mae: 460.6296\n",
            "Epoch 859/1000\n",
            "39/39 [==============================] - 6s 125ms/step - loss: 654413.3750 - mae: 620.2289 - val_loss: 216111.7969 - val_mae: 459.6146\n",
            "Epoch 860/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 653792.8125 - mae: 619.7400 - val_loss: 216592.7500 - val_mae: 460.0429\n",
            "Epoch 861/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 669818.8125 - mae: 628.1115 - val_loss: 214888.6406 - val_mae: 458.3556\n",
            "Epoch 862/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 669767.8125 - mae: 626.4642 - val_loss: 218426.9531 - val_mae: 462.0482\n",
            "Epoch 863/1000\n",
            "39/39 [==============================] - 6s 132ms/step - loss: 662065.8125 - mae: 622.8826 - val_loss: 217731.8750 - val_mae: 461.3646\n",
            "Epoch 864/1000\n",
            "39/39 [==============================] - 10s 232ms/step - loss: 668512.0625 - mae: 625.4874 - val_loss: 219782.3281 - val_mae: 463.8313\n",
            "Epoch 865/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 670836.2500 - mae: 628.0379 - val_loss: 215262.4375 - val_mae: 458.6883\n",
            "Epoch 866/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 660422.0000 - mae: 620.7130 - val_loss: 216597.9219 - val_mae: 459.9658\n",
            "Epoch 867/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 652464.3125 - mae: 620.8756 - val_loss: 217144.3125 - val_mae: 460.7450\n",
            "Epoch 868/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 656955.6875 - mae: 621.7301 - val_loss: 217799.5156 - val_mae: 461.3646\n",
            "Epoch 869/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 659287.8125 - mae: 622.5112 - val_loss: 216610.6719 - val_mae: 459.9276\n",
            "Epoch 870/1000\n",
            "39/39 [==============================] - 7s 160ms/step - loss: 657856.0625 - mae: 622.4450 - val_loss: 214533.4531 - val_mae: 457.6025\n",
            "Epoch 871/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 668928.5000 - mae: 627.9745 - val_loss: 215664.8281 - val_mae: 458.7207\n",
            "Epoch 872/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 660107.7500 - mae: 625.0242 - val_loss: 217598.9375 - val_mae: 461.0833\n",
            "Epoch 873/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 663489.9375 - mae: 626.8641 - val_loss: 214271.3281 - val_mae: 457.3021\n",
            "Epoch 874/1000\n",
            "39/39 [==============================] - 4s 96ms/step - loss: 656833.1250 - mae: 620.7921 - val_loss: 217906.2656 - val_mae: 461.3646\n",
            "Epoch 875/1000\n",
            "39/39 [==============================] - 11s 254ms/step - loss: 651211.0625 - mae: 618.7113 - val_loss: 216392.1250 - val_mae: 459.6146\n",
            "Epoch 876/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 657879.9375 - mae: 624.3152 - val_loss: 217425.1875 - val_mae: 460.8601\n",
            "Epoch 877/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 661369.1250 - mae: 626.4009 - val_loss: 216170.3906 - val_mae: 459.3333\n",
            "Epoch 878/1000\n",
            "39/39 [==============================] - 6s 135ms/step - loss: 651299.0000 - mae: 618.4673 - val_loss: 218755.2500 - val_mae: 462.1249\n",
            "Epoch 879/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 656573.6875 - mae: 621.2570 - val_loss: 214909.1250 - val_mae: 457.7683\n",
            "Epoch 880/1000\n",
            "39/39 [==============================] - 10s 253ms/step - loss: 657422.1875 - mae: 623.2082 - val_loss: 217257.0469 - val_mae: 460.3492\n",
            "Epoch 881/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 655969.3125 - mae: 622.9091 - val_loss: 214942.0000 - val_mae: 457.7423\n",
            "Epoch 882/1000\n",
            "39/39 [==============================] - 6s 129ms/step - loss: 653410.5625 - mae: 621.9897 - val_loss: 214958.9219 - val_mae: 457.7299\n",
            "Epoch 883/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 655054.4375 - mae: 622.5021 - val_loss: 217460.5000 - val_mae: 460.4446\n",
            "Epoch 884/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 656529.6250 - mae: 622.9289 - val_loss: 217736.0469 - val_mae: 460.7001\n",
            "Epoch 885/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 651399.8125 - mae: 619.2039 - val_loss: 220220.0000 - val_mae: 463.6771\n",
            "Epoch 886/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 646993.8125 - mae: 621.5714 - val_loss: 218859.8906 - val_mae: 462.0223\n",
            "Epoch 887/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 641981.6875 - mae: 616.4018 - val_loss: 218602.5781 - val_mae: 461.7288\n",
            "Epoch 888/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 661272.3750 - mae: 627.2593 - val_loss: 218258.8750 - val_mae: 461.3646\n",
            "Epoch 889/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 652963.3125 - mae: 622.3092 - val_loss: 215899.5469 - val_mae: 458.7146\n",
            "Epoch 890/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 654792.7500 - mae: 624.4067 - val_loss: 216617.6406 - val_mae: 459.3766\n",
            "Epoch 891/1000\n",
            "39/39 [==============================] - 6s 132ms/step - loss: 648573.0000 - mae: 621.5697 - val_loss: 218675.8906 - val_mae: 461.6766\n",
            "Epoch 892/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 639729.1250 - mae: 617.3174 - val_loss: 219838.3750 - val_mae: 463.0967\n",
            "Epoch 893/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 652302.2500 - mae: 622.6971 - val_loss: 216386.3594 - val_mae: 459.0521\n",
            "Epoch 894/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 653224.3750 - mae: 623.3539 - val_loss: 215509.2344 - val_mae: 457.8498\n",
            "Epoch 895/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 656618.6875 - mae: 628.4560 - val_loss: 218490.7969 - val_mae: 461.3646\n",
            "Epoch 896/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 639598.4375 - mae: 618.6266 - val_loss: 220001.1250 - val_mae: 463.1146\n",
            "Epoch 897/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 650201.3125 - mae: 624.1829 - val_loss: 215033.3281 - val_mae: 457.2563\n",
            "Epoch 898/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 654968.5625 - mae: 626.2935 - val_loss: 218370.3594 - val_mae: 461.1418\n",
            "Epoch 899/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 641139.7500 - mae: 622.5636 - val_loss: 219911.4844 - val_mae: 462.8215\n",
            "Epoch 900/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 642825.7500 - mae: 619.8392 - val_loss: 216731.9219 - val_mae: 459.1358\n",
            "Epoch 901/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 641561.0000 - mae: 618.2586 - val_loss: 217258.7031 - val_mae: 459.6146\n",
            "Epoch 902/1000\n",
            "39/39 [==============================] - 9s 227ms/step - loss: 652823.3750 - mae: 624.9634 - val_loss: 220037.5781 - val_mae: 462.9427\n",
            "Epoch 903/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 646632.3125 - mae: 622.1729 - val_loss: 217509.4844 - val_mae: 459.7733\n",
            "Epoch 904/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 638513.8750 - mae: 617.8267 - val_loss: 217260.3750 - val_mae: 459.4800\n",
            "Epoch 905/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 655407.7500 - mae: 628.6415 - val_loss: 216876.6875 - val_mae: 459.0521\n",
            "Epoch 906/1000\n",
            "39/39 [==============================] - 5s 102ms/step - loss: 646683.6875 - mae: 621.2427 - val_loss: 216409.9531 - val_mae: 458.2251\n",
            "Epoch 907/1000\n",
            "39/39 [==============================] - 6s 140ms/step - loss: 647714.7500 - mae: 622.9204 - val_loss: 219083.0469 - val_mae: 461.4727\n",
            "Epoch 908/1000\n",
            "39/39 [==============================] - 5s 103ms/step - loss: 643936.8125 - mae: 621.5280 - val_loss: 215673.2031 - val_mae: 457.3974\n",
            "Epoch 909/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 639039.3750 - mae: 620.6434 - val_loss: 217346.5156 - val_mae: 459.3333\n",
            "Epoch 910/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 649272.6250 - mae: 624.6416 - val_loss: 220400.7344 - val_mae: 462.9755\n",
            "Epoch 911/1000\n",
            "39/39 [==============================] - 10s 247ms/step - loss: 630441.4375 - mae: 616.1827 - val_loss: 215787.7969 - val_mae: 457.3593\n",
            "Epoch 912/1000\n",
            "39/39 [==============================] - 9s 232ms/step - loss: 643011.7500 - mae: 621.0264 - val_loss: 216718.5781 - val_mae: 458.2348\n",
            "Epoch 913/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 632656.8125 - mae: 614.7457 - val_loss: 219532.3125 - val_mae: 461.6458\n",
            "Epoch 914/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 639536.1875 - mae: 619.4035 - val_loss: 215623.3125 - val_mae: 457.0409\n",
            "Epoch 915/1000\n",
            "39/39 [==============================] - 6s 142ms/step - loss: 634890.6250 - mae: 616.4743 - val_loss: 219065.8281 - val_mae: 461.0833\n",
            "Epoch 916/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 636323.1875 - mae: 618.8562 - val_loss: 219396.1250 - val_mae: 461.3646\n",
            "Epoch 917/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 638253.7500 - mae: 620.0643 - val_loss: 216014.6250 - val_mae: 457.2660\n",
            "Epoch 918/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 639004.3125 - mae: 621.0148 - val_loss: 216645.2500 - val_mae: 457.8337\n",
            "Epoch 919/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 630205.4375 - mae: 615.8731 - val_loss: 217863.0000 - val_mae: 459.3333\n",
            "Epoch 920/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 645587.0000 - mae: 625.4217 - val_loss: 219382.0469 - val_mae: 461.1395\n",
            "Epoch 921/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 645112.8750 - mae: 624.0815 - val_loss: 216712.0781 - val_mae: 457.7271\n",
            "Epoch 922/1000\n",
            "39/39 [==============================] - 5s 104ms/step - loss: 639987.3750 - mae: 621.4450 - val_loss: 219715.8906 - val_mae: 461.3646\n",
            "Epoch 923/1000\n",
            "39/39 [==============================] - 6s 148ms/step - loss: 620001.8750 - mae: 611.6298 - val_loss: 218561.3281 - val_mae: 459.8022\n",
            "Epoch 924/1000\n",
            "39/39 [==============================] - 9s 233ms/step - loss: 634662.6250 - mae: 620.1108 - val_loss: 221332.7656 - val_mae: 463.2209\n",
            "Epoch 925/1000\n",
            "39/39 [==============================] - 10s 236ms/step - loss: 638259.6875 - mae: 622.2997 - val_loss: 218532.7969 - val_mae: 459.6574\n",
            "Epoch 926/1000\n",
            "39/39 [==============================] - 10s 248ms/step - loss: 630426.6250 - mae: 617.4373 - val_loss: 218267.4531 - val_mae: 459.3333\n",
            "Epoch 927/1000\n",
            "39/39 [==============================] - 9s 233ms/step - loss: 640348.6250 - mae: 622.8016 - val_loss: 218467.9531 - val_mae: 459.4695\n",
            "Epoch 928/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 637749.4375 - mae: 622.6295 - val_loss: 216681.1875 - val_mae: 457.2689\n",
            "Epoch 929/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 634835.6875 - mae: 620.6281 - val_loss: 221680.1250 - val_mae: 463.2847\n",
            "Epoch 930/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 632192.1250 - mae: 619.1530 - val_loss: 218903.2969 - val_mae: 459.7133\n",
            "Epoch 931/1000\n",
            "39/39 [==============================] - 9s 225ms/step - loss: 630516.6250 - mae: 615.6694 - val_loss: 222107.9375 - val_mae: 463.5907\n",
            "Epoch 932/1000\n",
            "39/39 [==============================] - 9s 223ms/step - loss: 636503.7500 - mae: 622.5374 - val_loss: 218716.5156 - val_mae: 459.4073\n",
            "Epoch 933/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 633363.0000 - mae: 620.1368 - val_loss: 218254.4531 - val_mae: 458.8927\n",
            "Epoch 934/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 633255.1875 - mae: 619.0533 - val_loss: 216684.6875 - val_mae: 456.8866\n",
            "Epoch 935/1000\n",
            "39/39 [==============================] - 10s 231ms/step - loss: 630838.1250 - mae: 619.1447 - val_loss: 220494.0781 - val_mae: 461.3646\n",
            "Epoch 936/1000\n",
            "39/39 [==============================] - 6s 139ms/step - loss: 634839.3750 - mae: 622.2433 - val_loss: 220802.0469 - val_mae: 461.5988\n",
            "Epoch 937/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 630151.1875 - mae: 617.4131 - val_loss: 217059.3594 - val_mae: 457.0420\n",
            "Epoch 938/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 632326.8750 - mae: 620.6339 - val_loss: 218749.8750 - val_mae: 459.0492\n",
            "Epoch 939/1000\n",
            "39/39 [==============================] - 10s 238ms/step - loss: 633259.3750 - mae: 619.7277 - val_loss: 222411.3750 - val_mae: 463.4091\n",
            "Epoch 940/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 622704.0625 - mae: 615.6999 - val_loss: 224427.9375 - val_mae: 465.7350\n",
            "Epoch 941/1000\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 628855.6250 - mae: 618.8214 - val_loss: 217281.0000 - val_mae: 456.9821\n",
            "Epoch 942/1000\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 630970.1250 - mae: 620.2237 - val_loss: 220618.2969 - val_mae: 461.0317\n",
            "Epoch 943/1000\n",
            "39/39 [==============================] - 5s 118ms/step - loss: 629072.8125 - mae: 620.2105 - val_loss: 221383.3594 - val_mae: 461.7092\n",
            "Epoch 944/1000\n",
            "39/39 [==============================] - 10s 230ms/step - loss: 623840.1875 - mae: 618.3865 - val_loss: 219021.7031 - val_mae: 458.8994\n",
            "Epoch 945/1000\n",
            "39/39 [==============================] - 4s 100ms/step - loss: 626546.1875 - mae: 618.3147 - val_loss: 221167.3750 - val_mae: 461.3646\n",
            "Epoch 946/1000\n",
            "39/39 [==============================] - 5s 113ms/step - loss: 621011.1250 - mae: 617.2899 - val_loss: 221238.5469 - val_mae: 461.3646\n",
            "Epoch 947/1000\n",
            "39/39 [==============================] - 9s 228ms/step - loss: 620008.0000 - mae: 617.2982 - val_loss: 221308.9531 - val_mae: 461.3646\n",
            "Epoch 948/1000\n",
            "39/39 [==============================] - 9s 229ms/step - loss: 614756.9375 - mae: 610.4136 - val_loss: 217589.4531 - val_mae: 456.7707\n",
            "Epoch 949/1000\n",
            "39/39 [==============================] - 10s 254ms/step - loss: 623526.1250 - mae: 617.1879 - val_loss: 219263.9375 - val_mae: 458.7765\n",
            "Epoch 950/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 625666.5000 - mae: 618.0995 - val_loss: 223144.9844 - val_mae: 463.3958\n",
            "Epoch 951/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 632256.7500 - mae: 624.0536 - val_loss: 219953.5000 - val_mae: 459.2908\n",
            "Epoch 952/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 622426.7500 - mae: 617.4600 - val_loss: 223462.3125 - val_mae: 463.5699\n",
            "Epoch 953/1000\n",
            "39/39 [==============================] - 6s 141ms/step - loss: 623962.0000 - mae: 616.3354 - val_loss: 219953.6250 - val_mae: 459.1465\n",
            "Epoch 954/1000\n",
            "39/39 [==============================] - 6s 134ms/step - loss: 617449.5625 - mae: 615.8356 - val_loss: 221827.9219 - val_mae: 461.3646\n",
            "Epoch 955/1000\n",
            "39/39 [==============================] - 10s 234ms/step - loss: 625920.7500 - mae: 620.5400 - val_loss: 222122.7969 - val_mae: 461.5767\n",
            "Epoch 956/1000\n",
            "39/39 [==============================] - 9s 224ms/step - loss: 619183.5000 - mae: 618.2378 - val_loss: 220383.1250 - val_mae: 459.3333\n",
            "Epoch 957/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 627253.5000 - mae: 620.8592 - val_loss: 221769.3906 - val_mae: 461.0833\n",
            "Epoch 958/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 626636.8750 - mae: 620.8514 - val_loss: 223744.0000 - val_mae: 463.3958\n",
            "Epoch 959/1000\n",
            "39/39 [==============================] - 11s 260ms/step - loss: 619233.1875 - mae: 616.4556 - val_loss: 218473.1094 - val_mae: 456.7590\n",
            "Epoch 960/1000\n",
            "39/39 [==============================] - 5s 126ms/step - loss: 617653.9375 - mae: 615.8873 - val_loss: 222606.0625 - val_mae: 461.6458\n",
            "Epoch 961/1000\n",
            "39/39 [==============================] - 10s 241ms/step - loss: 618430.0625 - mae: 618.6556 - val_loss: 222677.1250 - val_mae: 461.6511\n",
            "Epoch 962/1000\n",
            "39/39 [==============================] - 5s 130ms/step - loss: 623602.7500 - mae: 618.9410 - val_loss: 222473.6875 - val_mae: 461.3646\n",
            "Epoch 963/1000\n",
            "39/39 [==============================] - 5s 106ms/step - loss: 606123.6250 - mae: 613.4858 - val_loss: 218469.7031 - val_mae: 456.4282\n",
            "Epoch 964/1000\n",
            "39/39 [==============================] - 5s 110ms/step - loss: 618651.8750 - mae: 616.4414 - val_loss: 222970.4375 - val_mae: 461.6892\n",
            "Epoch 965/1000\n",
            "39/39 [==============================] - 6s 127ms/step - loss: 617892.7500 - mae: 616.4043 - val_loss: 223022.7031 - val_mae: 461.6458\n",
            "Epoch 966/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 617978.9375 - mae: 616.2563 - val_loss: 222156.5469 - val_mae: 460.7349\n",
            "Epoch 967/1000\n",
            "39/39 [==============================] - 5s 115ms/step - loss: 620270.2500 - mae: 617.9505 - val_loss: 220591.0000 - val_mae: 458.6112\n",
            "Epoch 968/1000\n",
            "39/39 [==============================] - 5s 108ms/step - loss: 620512.1875 - mae: 618.2331 - val_loss: 222079.0781 - val_mae: 459.9876\n",
            "Epoch 969/1000\n",
            "39/39 [==============================] - 5s 100ms/step - loss: 615623.0625 - mae: 616.3822 - val_loss: 223365.6094 - val_mae: 461.6458\n",
            "Epoch 970/1000\n",
            "39/39 [==============================] - 6s 135ms/step - loss: 620864.9375 - mae: 618.9801 - val_loss: 220960.5469 - val_mae: 458.5708\n",
            "Epoch 971/1000\n",
            "39/39 [==============================] - 5s 114ms/step - loss: 598411.0625 - mae: 610.0975 - val_loss: 223545.7500 - val_mae: 461.6458\n",
            "Epoch 972/1000\n",
            "39/39 [==============================] - 4s 99ms/step - loss: 625539.9375 - mae: 623.0610 - val_loss: 221122.5000 - val_mae: 458.5455\n",
            "Epoch 973/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 607907.1250 - mae: 616.2436 - val_loss: 219560.2500 - val_mae: 456.5852\n",
            "Epoch 974/1000\n",
            "39/39 [==============================] - 5s 116ms/step - loss: 610882.5625 - mae: 613.5533 - val_loss: 224268.7500 - val_mae: 462.0937\n",
            "Epoch 975/1000\n",
            "39/39 [==============================] - 6s 126ms/step - loss: 615654.8750 - mae: 617.4529 - val_loss: 223143.8750 - val_mae: 460.9044\n",
            "Epoch 976/1000\n",
            "39/39 [==============================] - 5s 107ms/step - loss: 624694.2500 - mae: 623.7658 - val_loss: 224961.1250 - val_mae: 462.5927\n",
            "Epoch 977/1000\n",
            "39/39 [==============================] - 10s 244ms/step - loss: 606560.9375 - mae: 611.6295 - val_loss: 219595.1406 - val_mae: 456.2540\n",
            "Epoch 978/1000\n",
            "39/39 [==============================] - 5s 109ms/step - loss: 620658.7500 - mae: 620.2701 - val_loss: 221642.2969 - val_mae: 458.6185\n",
            "Epoch 979/1000\n",
            "39/39 [==============================] - 10s 246ms/step - loss: 609900.0000 - mae: 614.9417 - val_loss: 222942.0781 - val_mae: 459.8428\n",
            "Epoch 980/1000\n",
            "39/39 [==============================] - 10s 240ms/step - loss: 622042.3750 - mae: 622.4799 - val_loss: 226455.8906 - val_mae: 464.1999\n",
            "Epoch 981/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 599912.8750 - mae: 611.3547 - val_loss: 225410.5000 - val_mae: 463.1146\n",
            "Epoch 982/1000\n",
            "39/39 [==============================] - 9s 231ms/step - loss: 611240.1875 - mae: 616.4819 - val_loss: 220088.7500 - val_mae: 456.2071\n",
            "Epoch 983/1000\n",
            "39/39 [==============================] - 6s 127ms/step - loss: 610374.3750 - mae: 614.2086 - val_loss: 224367.3906 - val_mae: 461.3646\n",
            "Epoch 984/1000\n",
            "39/39 [==============================] - 5s 105ms/step - loss: 618005.4375 - mae: 619.5376 - val_loss: 221472.5000 - val_mae: 457.9078\n",
            "Epoch 985/1000\n",
            "39/39 [==============================] - 5s 119ms/step - loss: 602334.5625 - mae: 612.0955 - val_loss: 223029.9844 - val_mae: 459.3333\n",
            "Epoch 986/1000\n",
            "39/39 [==============================] - 10s 235ms/step - loss: 607322.2500 - mae: 614.3474 - val_loss: 224357.1719 - val_mae: 461.0833\n",
            "Epoch 987/1000\n",
            "39/39 [==============================] - 11s 260ms/step - loss: 610137.0000 - mae: 615.5988 - val_loss: 223842.4219 - val_mae: 459.9420\n",
            "Epoch 988/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 606625.6875 - mae: 615.5234 - val_loss: 222089.3906 - val_mae: 458.1503\n",
            "Epoch 989/1000\n",
            "39/39 [==============================] - 10s 241ms/step - loss: 604950.3750 - mae: 614.6462 - val_loss: 225593.6250 - val_mae: 461.9977\n",
            "Epoch 990/1000\n",
            "39/39 [==============================] - 6s 147ms/step - loss: 599352.5625 - mae: 608.9719 - val_loss: 226576.8750 - val_mae: 463.3958\n",
            "Epoch 991/1000\n",
            "39/39 [==============================] - 5s 111ms/step - loss: 608875.6875 - mae: 616.0991 - val_loss: 222972.2344 - val_mae: 458.6766\n",
            "Epoch 992/1000\n",
            "39/39 [==============================] - 9s 226ms/step - loss: 601207.8125 - mae: 611.1565 - val_loss: 221241.1250 - val_mae: 456.3517\n",
            "Epoch 993/1000\n",
            "39/39 [==============================] - 5s 101ms/step - loss: 606974.3125 - mae: 614.5164 - val_loss: 227554.6719 - val_mae: 464.0768\n",
            "Epoch 994/1000\n",
            "39/39 [==============================] - 10s 243ms/step - loss: 602904.0000 - mae: 611.2602 - val_loss: 222243.2344 - val_mae: 457.6656\n",
            "Epoch 995/1000\n",
            "39/39 [==============================] - 6s 136ms/step - loss: 603064.0625 - mae: 614.9093 - val_loss: 224054.6250 - val_mae: 459.3333\n",
            "Epoch 996/1000\n",
            "39/39 [==============================] - 5s 127ms/step - loss: 600180.2500 - mae: 613.5735 - val_loss: 223435.9219 - val_mae: 458.6161\n",
            "Epoch 997/1000\n",
            "39/39 [==============================] - 5s 112ms/step - loss: 606959.9375 - mae: 616.5635 - val_loss: 224733.1094 - val_mae: 460.3529\n",
            "Epoch 998/1000\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 611001.3750 - mae: 617.1948 - val_loss: 225886.8125 - val_mae: 461.3646\n",
            "Epoch 999/1000\n",
            "39/39 [==============================] - 6s 130ms/step - loss: 604179.6875 - mae: 613.5715 - val_loss: 221917.8750 - val_mae: 456.2663\n",
            "Epoch 1000/1000\n",
            "39/39 [==============================] - 10s 228ms/step - loss: 599072.1875 - mae: 613.4769 - val_loss: 226098.1875 - val_mae: 461.3646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea77b38-54eb-48fa-d87c-af5e1e4fa741"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1445241.875,\n",
              "  1442345.875,\n",
              "  1440865.375,\n",
              "  1452527.5,\n",
              "  1444905.25,\n",
              "  1406011.125,\n",
              "  1442729.25,\n",
              "  1416124.25,\n",
              "  1434960.875,\n",
              "  1426287.625,\n",
              "  1432814.0,\n",
              "  1431876.125,\n",
              "  1413058.125,\n",
              "  1412081.625,\n",
              "  1406752.625,\n",
              "  1428334.125,\n",
              "  1415065.5,\n",
              "  1407023.375,\n",
              "  1419990.75,\n",
              "  1415495.75,\n",
              "  1429563.625,\n",
              "  1405164.75,\n",
              "  1417902.375,\n",
              "  1403220.125,\n",
              "  1411337.875,\n",
              "  1384875.25,\n",
              "  1408057.25,\n",
              "  1404284.875,\n",
              "  1392809.875,\n",
              "  1389433.875,\n",
              "  1398694.875,\n",
              "  1413452.5,\n",
              "  1403804.0,\n",
              "  1393608.75,\n",
              "  1390640.625,\n",
              "  1398988.75,\n",
              "  1369783.375,\n",
              "  1392435.875,\n",
              "  1388831.25,\n",
              "  1379673.625,\n",
              "  1392721.5,\n",
              "  1399786.375,\n",
              "  1390638.125,\n",
              "  1389825.125,\n",
              "  1372978.0,\n",
              "  1365229.75,\n",
              "  1366110.0,\n",
              "  1368829.375,\n",
              "  1371344.75,\n",
              "  1375137.75,\n",
              "  1383544.375,\n",
              "  1377814.125,\n",
              "  1357873.25,\n",
              "  1390251.875,\n",
              "  1377420.875,\n",
              "  1371611.125,\n",
              "  1365580.375,\n",
              "  1350016.875,\n",
              "  1360318.625,\n",
              "  1363074.875,\n",
              "  1376877.125,\n",
              "  1350825.875,\n",
              "  1366801.375,\n",
              "  1339915.375,\n",
              "  1358303.125,\n",
              "  1341948.75,\n",
              "  1367213.625,\n",
              "  1349530.625,\n",
              "  1373656.875,\n",
              "  1350023.375,\n",
              "  1348704.875,\n",
              "  1354726.625,\n",
              "  1352547.625,\n",
              "  1314663.75,\n",
              "  1342041.5,\n",
              "  1323901.125,\n",
              "  1343046.75,\n",
              "  1336690.625,\n",
              "  1340641.625,\n",
              "  1343795.0,\n",
              "  1323389.25,\n",
              "  1331366.75,\n",
              "  1336730.625,\n",
              "  1339608.0,\n",
              "  1345380.5,\n",
              "  1334657.625,\n",
              "  1331602.25,\n",
              "  1332651.625,\n",
              "  1335590.25,\n",
              "  1319918.0,\n",
              "  1324303.875,\n",
              "  1342194.75,\n",
              "  1318933.75,\n",
              "  1314019.0,\n",
              "  1327571.625,\n",
              "  1322942.5,\n",
              "  1299092.0,\n",
              "  1315354.375,\n",
              "  1302849.25,\n",
              "  1312259.5,\n",
              "  1315783.625,\n",
              "  1311569.375,\n",
              "  1324677.875,\n",
              "  1315080.75,\n",
              "  1305853.75,\n",
              "  1300677.875,\n",
              "  1289216.75,\n",
              "  1297393.75,\n",
              "  1300347.375,\n",
              "  1283925.5,\n",
              "  1318877.125,\n",
              "  1297421.375,\n",
              "  1301858.125,\n",
              "  1299964.25,\n",
              "  1295516.125,\n",
              "  1296651.875,\n",
              "  1308690.25,\n",
              "  1289984.25,\n",
              "  1290142.125,\n",
              "  1300326.125,\n",
              "  1293694.375,\n",
              "  1300541.125,\n",
              "  1280623.25,\n",
              "  1281542.0,\n",
              "  1282937.125,\n",
              "  1306847.625,\n",
              "  1264634.75,\n",
              "  1284124.125,\n",
              "  1273565.875,\n",
              "  1263509.125,\n",
              "  1253991.875,\n",
              "  1276973.625,\n",
              "  1261966.625,\n",
              "  1266704.375,\n",
              "  1267395.0,\n",
              "  1291985.75,\n",
              "  1280602.5,\n",
              "  1279410.75,\n",
              "  1266599.875,\n",
              "  1264314.5,\n",
              "  1270897.875,\n",
              "  1246712.625,\n",
              "  1285660.0,\n",
              "  1254119.25,\n",
              "  1248321.75,\n",
              "  1260307.875,\n",
              "  1265871.75,\n",
              "  1256551.75,\n",
              "  1233814.625,\n",
              "  1248443.25,\n",
              "  1247415.125,\n",
              "  1270067.25,\n",
              "  1251215.0,\n",
              "  1250719.0,\n",
              "  1252010.625,\n",
              "  1217920.25,\n",
              "  1247652.375,\n",
              "  1252472.25,\n",
              "  1243829.75,\n",
              "  1241902.125,\n",
              "  1254466.0,\n",
              "  1222172.875,\n",
              "  1200208.125,\n",
              "  1225161.0,\n",
              "  1246530.25,\n",
              "  1237568.5,\n",
              "  1241623.125,\n",
              "  1236037.75,\n",
              "  1224582.25,\n",
              "  1235811.0,\n",
              "  1214694.625,\n",
              "  1234380.375,\n",
              "  1233412.5,\n",
              "  1234152.125,\n",
              "  1224840.625,\n",
              "  1239571.375,\n",
              "  1229477.625,\n",
              "  1232773.5,\n",
              "  1217393.875,\n",
              "  1239679.625,\n",
              "  1211676.375,\n",
              "  1201531.375,\n",
              "  1201246.875,\n",
              "  1212398.625,\n",
              "  1225617.25,\n",
              "  1217797.375,\n",
              "  1226361.875,\n",
              "  1203370.875,\n",
              "  1213737.75,\n",
              "  1206288.375,\n",
              "  1172061.375,\n",
              "  1207221.5,\n",
              "  1181086.625,\n",
              "  1212355.25,\n",
              "  1212712.375,\n",
              "  1196738.375,\n",
              "  1213667.25,\n",
              "  1201737.125,\n",
              "  1201072.75,\n",
              "  1209307.375,\n",
              "  1191903.375,\n",
              "  1191197.5,\n",
              "  1185062.625,\n",
              "  1179738.5,\n",
              "  1194540.375,\n",
              "  1198132.75,\n",
              "  1194834.25,\n",
              "  1157406.0,\n",
              "  1191521.0,\n",
              "  1187297.625,\n",
              "  1184997.75,\n",
              "  1188954.75,\n",
              "  1171749.25,\n",
              "  1176225.75,\n",
              "  1184090.5,\n",
              "  1189308.5,\n",
              "  1192807.625,\n",
              "  1173796.0,\n",
              "  1186872.5,\n",
              "  1168001.75,\n",
              "  1172917.875,\n",
              "  1168676.75,\n",
              "  1178093.5,\n",
              "  1160844.875,\n",
              "  1179702.25,\n",
              "  1161626.625,\n",
              "  1164489.125,\n",
              "  1170989.875,\n",
              "  1155475.0,\n",
              "  1175467.5,\n",
              "  1155803.25,\n",
              "  1169750.875,\n",
              "  1164406.75,\n",
              "  1159451.125,\n",
              "  1162564.75,\n",
              "  1145683.0,\n",
              "  1156345.5,\n",
              "  1131396.25,\n",
              "  1171085.0,\n",
              "  1150817.25,\n",
              "  1155164.5,\n",
              "  1140122.5,\n",
              "  1148911.25,\n",
              "  1144543.75,\n",
              "  1147952.75,\n",
              "  1160500.625,\n",
              "  1143340.5,\n",
              "  1144351.0,\n",
              "  1140835.0,\n",
              "  1147189.125,\n",
              "  1120346.125,\n",
              "  1134846.875,\n",
              "  1128663.125,\n",
              "  1134883.5,\n",
              "  1145434.5,\n",
              "  1156077.75,\n",
              "  1115212.125,\n",
              "  1133194.25,\n",
              "  1135486.375,\n",
              "  1127875.5,\n",
              "  1125405.125,\n",
              "  1123617.0,\n",
              "  1132148.75,\n",
              "  1125226.25,\n",
              "  1129001.25,\n",
              "  1127852.5,\n",
              "  1128662.375,\n",
              "  1123786.875,\n",
              "  1130921.375,\n",
              "  1126347.5,\n",
              "  1107671.75,\n",
              "  1122487.125,\n",
              "  1103429.625,\n",
              "  1120410.125,\n",
              "  1117566.625,\n",
              "  1111559.875,\n",
              "  1110819.75,\n",
              "  1104478.25,\n",
              "  1118215.0,\n",
              "  1115176.0,\n",
              "  1114962.375,\n",
              "  1116302.375,\n",
              "  1099287.75,\n",
              "  1109429.5,\n",
              "  1093166.25,\n",
              "  1117023.0,\n",
              "  1113855.375,\n",
              "  1093956.625,\n",
              "  1096686.25,\n",
              "  1106498.875,\n",
              "  1104594.875,\n",
              "  1115955.0,\n",
              "  1093380.625,\n",
              "  1112132.875,\n",
              "  1096441.75,\n",
              "  1090389.625,\n",
              "  1090391.5,\n",
              "  1101935.875,\n",
              "  1081513.0,\n",
              "  1086707.875,\n",
              "  1073280.5,\n",
              "  1092711.375,\n",
              "  1097145.875,\n",
              "  1099072.25,\n",
              "  1072701.875,\n",
              "  1073499.125,\n",
              "  1078387.125,\n",
              "  1077944.75,\n",
              "  1073194.0,\n",
              "  1080195.625,\n",
              "  1092500.875,\n",
              "  1076915.0,\n",
              "  1064568.25,\n",
              "  1081823.625,\n",
              "  1086055.875,\n",
              "  1079684.875,\n",
              "  1078133.375,\n",
              "  1068231.25,\n",
              "  1060143.375,\n",
              "  1053579.375,\n",
              "  1060105.25,\n",
              "  1078208.75,\n",
              "  1054522.0,\n",
              "  1069151.125,\n",
              "  1063892.75,\n",
              "  1080474.0,\n",
              "  1047359.25,\n",
              "  1055265.375,\n",
              "  1054610.25,\n",
              "  1049463.625,\n",
              "  1072993.375,\n",
              "  1057761.625,\n",
              "  1058694.5,\n",
              "  1044911.8125,\n",
              "  1060444.25,\n",
              "  1057887.0,\n",
              "  1052528.75,\n",
              "  1054801.625,\n",
              "  1025632.3125,\n",
              "  1041594.0625,\n",
              "  1041727.875,\n",
              "  1050178.375,\n",
              "  1051407.0,\n",
              "  1043658.5625,\n",
              "  1048483.5625,\n",
              "  1037663.0625,\n",
              "  1053471.25,\n",
              "  1039220.5,\n",
              "  1040154.0625,\n",
              "  1033063.625,\n",
              "  1037759.375,\n",
              "  1035159.5,\n",
              "  1040249.4375,\n",
              "  1044355.6875,\n",
              "  1026281.125,\n",
              "  1029635.875,\n",
              "  1038153.3125,\n",
              "  1035365.25,\n",
              "  1043765.75,\n",
              "  1037436.25,\n",
              "  1028543.375,\n",
              "  1028846.875,\n",
              "  1020388.0625,\n",
              "  1024709.3125,\n",
              "  1027533.8125,\n",
              "  1032747.375,\n",
              "  1017908.6875,\n",
              "  1027426.1875,\n",
              "  1026323.375,\n",
              "  1020368.3125,\n",
              "  1010651.625,\n",
              "  1024376.875,\n",
              "  1004111.875,\n",
              "  1023435.5,\n",
              "  999680.75,\n",
              "  1015027.0625,\n",
              "  1011815.4375,\n",
              "  999650.0,\n",
              "  1010951.9375,\n",
              "  999266.3125,\n",
              "  1013033.625,\n",
              "  1023459.0625,\n",
              "  1012725.25,\n",
              "  997830.875,\n",
              "  1005747.75,\n",
              "  1010869.9375,\n",
              "  1012978.9375,\n",
              "  1007134.0,\n",
              "  995369.625,\n",
              "  1004850.75,\n",
              "  1009508.1875,\n",
              "  989274.125,\n",
              "  981367.3125,\n",
              "  982583.75,\n",
              "  1006542.125,\n",
              "  992825.0,\n",
              "  989625.9375,\n",
              "  1000067.125,\n",
              "  996651.6875,\n",
              "  989144.1875,\n",
              "  977891.125,\n",
              "  991968.8125,\n",
              "  990750.5625,\n",
              "  988235.625,\n",
              "  1003232.5,\n",
              "  985599.0625,\n",
              "  988567.3125,\n",
              "  988557.0625,\n",
              "  992642.0,\n",
              "  979759.0625,\n",
              "  977204.625,\n",
              "  971004.3125,\n",
              "  959725.6875,\n",
              "  992048.8125,\n",
              "  975081.5,\n",
              "  982147.0625,\n",
              "  980331.0625,\n",
              "  965868.9375,\n",
              "  986219.375,\n",
              "  983941.4375,\n",
              "  971877.9375,\n",
              "  965161.1875,\n",
              "  959954.3125,\n",
              "  949230.5625,\n",
              "  958069.4375,\n",
              "  963717.125,\n",
              "  947601.875,\n",
              "  961709.0,\n",
              "  972553.8125,\n",
              "  976905.625,\n",
              "  977052.625,\n",
              "  965997.9375,\n",
              "  949136.0,\n",
              "  948212.8125,\n",
              "  968113.875,\n",
              "  953842.625,\n",
              "  976584.375,\n",
              "  964340.5,\n",
              "  961824.3125,\n",
              "  948532.375,\n",
              "  948043.9375,\n",
              "  953670.6875,\n",
              "  955931.9375,\n",
              "  955252.625,\n",
              "  953491.75,\n",
              "  952263.0,\n",
              "  953091.875,\n",
              "  953568.75,\n",
              "  951276.875,\n",
              "  942633.125,\n",
              "  935066.0625,\n",
              "  939251.5625,\n",
              "  954001.875,\n",
              "  941306.375,\n",
              "  932899.0625,\n",
              "  941362.1875,\n",
              "  934969.1875,\n",
              "  935820.4375,\n",
              "  923512.1875,\n",
              "  927975.9375,\n",
              "  935846.8125,\n",
              "  946513.25,\n",
              "  949287.8125,\n",
              "  927298.75,\n",
              "  931910.6875,\n",
              "  930744.8125,\n",
              "  925791.6875,\n",
              "  913861.25,\n",
              "  934190.875,\n",
              "  917446.1875,\n",
              "  922073.75,\n",
              "  931754.875,\n",
              "  935049.0,\n",
              "  921035.375,\n",
              "  929513.9375,\n",
              "  933298.9375,\n",
              "  917402.0625,\n",
              "  922745.5,\n",
              "  925765.25,\n",
              "  902169.125,\n",
              "  925016.6875,\n",
              "  923397.25,\n",
              "  911868.4375,\n",
              "  922449.125,\n",
              "  909670.6875,\n",
              "  907496.6875,\n",
              "  909413.0,\n",
              "  915069.625,\n",
              "  923930.5625,\n",
              "  904466.1875,\n",
              "  909517.6875,\n",
              "  920363.0,\n",
              "  898404.0625,\n",
              "  914704.625,\n",
              "  908378.0625,\n",
              "  911097.75,\n",
              "  907920.9375,\n",
              "  896242.8125,\n",
              "  899951.25,\n",
              "  914402.625,\n",
              "  912458.6875,\n",
              "  901014.8125,\n",
              "  906652.75,\n",
              "  905813.3125,\n",
              "  887914.125,\n",
              "  897291.1875,\n",
              "  892912.8125,\n",
              "  887093.625,\n",
              "  889303.625,\n",
              "  895315.75,\n",
              "  897710.875,\n",
              "  897437.8125,\n",
              "  907539.75,\n",
              "  902516.8125,\n",
              "  898473.1875,\n",
              "  882845.8125,\n",
              "  883019.5,\n",
              "  885919.5,\n",
              "  870298.0625,\n",
              "  890482.3125,\n",
              "  887442.5,\n",
              "  900136.6875,\n",
              "  885635.4375,\n",
              "  866175.5,\n",
              "  887410.1875,\n",
              "  874528.875,\n",
              "  892709.3125,\n",
              "  881498.75,\n",
              "  890440.6875,\n",
              "  885418.0625,\n",
              "  889228.4375,\n",
              "  868682.4375,\n",
              "  884766.4375,\n",
              "  877845.75,\n",
              "  879702.875,\n",
              "  868321.625,\n",
              "  871843.875,\n",
              "  879760.75,\n",
              "  875548.3125,\n",
              "  869665.75,\n",
              "  855197.75,\n",
              "  871975.4375,\n",
              "  866425.25,\n",
              "  856634.625,\n",
              "  867185.9375,\n",
              "  867594.875,\n",
              "  870368.4375,\n",
              "  879061.5625,\n",
              "  861166.0625,\n",
              "  859178.875,\n",
              "  857637.0,\n",
              "  869267.6875,\n",
              "  865351.0,\n",
              "  853692.125,\n",
              "  859773.0,\n",
              "  854696.6875,\n",
              "  857404.1875,\n",
              "  855015.8125,\n",
              "  868002.6875,\n",
              "  857128.25,\n",
              "  844696.6875,\n",
              "  852293.875,\n",
              "  852554.625,\n",
              "  851019.375,\n",
              "  860088.3125,\n",
              "  853584.0,\n",
              "  862459.4375,\n",
              "  851994.0625,\n",
              "  851637.0,\n",
              "  849973.375,\n",
              "  843267.8125,\n",
              "  860920.1875,\n",
              "  841358.3125,\n",
              "  844749.25,\n",
              "  841321.5625,\n",
              "  848322.9375,\n",
              "  855002.875,\n",
              "  845263.1875,\n",
              "  845484.0,\n",
              "  827688.6875,\n",
              "  844368.0,\n",
              "  830014.5625,\n",
              "  839274.3125,\n",
              "  838388.1875,\n",
              "  831529.25,\n",
              "  838817.6875,\n",
              "  813480.75,\n",
              "  839749.75,\n",
              "  832151.625,\n",
              "  833218.375,\n",
              "  821617.9375,\n",
              "  829077.6875,\n",
              "  830552.125,\n",
              "  820999.125,\n",
              "  834923.0625,\n",
              "  831710.3125,\n",
              "  824808.4375,\n",
              "  840127.3125,\n",
              "  838546.0625,\n",
              "  830271.6875,\n",
              "  834390.5625,\n",
              "  828780.0,\n",
              "  833803.6875,\n",
              "  814076.3125,\n",
              "  804925.4375,\n",
              "  827239.0,\n",
              "  813605.8125,\n",
              "  835033.0,\n",
              "  808044.25,\n",
              "  830594.0,\n",
              "  818258.625,\n",
              "  824967.375,\n",
              "  815426.4375,\n",
              "  811618.375,\n",
              "  798997.9375,\n",
              "  817158.0625,\n",
              "  827514.125,\n",
              "  816423.0625,\n",
              "  808006.25,\n",
              "  824267.6875,\n",
              "  807376.8125,\n",
              "  807977.875,\n",
              "  815469.0625,\n",
              "  798064.8125,\n",
              "  807065.3125,\n",
              "  796538.75,\n",
              "  798480.875,\n",
              "  817652.75,\n",
              "  805972.6875,\n",
              "  787152.375,\n",
              "  807457.875,\n",
              "  795042.5625,\n",
              "  802634.375,\n",
              "  805482.125,\n",
              "  797504.9375,\n",
              "  805063.0,\n",
              "  790191.1875,\n",
              "  798613.0,\n",
              "  804694.4375,\n",
              "  798521.0,\n",
              "  785482.625,\n",
              "  793245.25,\n",
              "  790013.4375,\n",
              "  792358.4375,\n",
              "  805594.875,\n",
              "  794593.4375,\n",
              "  801864.5,\n",
              "  788922.0625,\n",
              "  781178.0625,\n",
              "  794156.5,\n",
              "  789420.4375,\n",
              "  796076.875,\n",
              "  787187.6875,\n",
              "  793411.375,\n",
              "  789707.9375,\n",
              "  791826.4375,\n",
              "  787190.4375,\n",
              "  794144.1875,\n",
              "  786065.6875,\n",
              "  776059.4375,\n",
              "  786979.625,\n",
              "  772777.9375,\n",
              "  761573.5625,\n",
              "  787432.625,\n",
              "  776169.25,\n",
              "  773532.125,\n",
              "  776216.0,\n",
              "  778247.8125,\n",
              "  778931.375,\n",
              "  774333.75,\n",
              "  774344.4375,\n",
              "  776758.6875,\n",
              "  777583.1875,\n",
              "  761068.0,\n",
              "  785386.8125,\n",
              "  782511.75,\n",
              "  777888.9375,\n",
              "  777289.5,\n",
              "  784364.0625,\n",
              "  762862.6875,\n",
              "  770337.9375,\n",
              "  751005.25,\n",
              "  750940.6875,\n",
              "  773678.875,\n",
              "  774264.625,\n",
              "  769515.375,\n",
              "  771281.0625,\n",
              "  757918.1875,\n",
              "  767590.3125,\n",
              "  766799.5,\n",
              "  772313.125,\n",
              "  762762.125,\n",
              "  761813.0625,\n",
              "  768747.6875,\n",
              "  760593.0,\n",
              "  772877.0625,\n",
              "  756117.25,\n",
              "  759249.0,\n",
              "  762287.25,\n",
              "  765687.125,\n",
              "  740359.5,\n",
              "  757750.625,\n",
              "  748010.375,\n",
              "  754142.1875,\n",
              "  742220.0,\n",
              "  744854.875,\n",
              "  742279.125,\n",
              "  746903.125,\n",
              "  750667.5,\n",
              "  748063.5,\n",
              "  748123.625,\n",
              "  750025.1875,\n",
              "  752536.1875,\n",
              "  745667.125,\n",
              "  748916.5625,\n",
              "  731711.1875,\n",
              "  742324.3125,\n",
              "  752981.75,\n",
              "  740595.6875,\n",
              "  740560.0,\n",
              "  743002.4375,\n",
              "  745044.9375,\n",
              "  731455.8125,\n",
              "  742613.75,\n",
              "  738298.4375,\n",
              "  737000.75,\n",
              "  745004.6875,\n",
              "  738592.8125,\n",
              "  744930.3125,\n",
              "  739962.6875,\n",
              "  747142.25,\n",
              "  739526.125,\n",
              "  735013.125,\n",
              "  727030.375,\n",
              "  742215.5,\n",
              "  734915.6875,\n",
              "  725991.0,\n",
              "  730798.6875,\n",
              "  725098.9375,\n",
              "  736961.125,\n",
              "  738986.375,\n",
              "  738569.3125,\n",
              "  734267.3125,\n",
              "  733463.25,\n",
              "  720888.0625,\n",
              "  722290.6875,\n",
              "  727944.5,\n",
              "  729284.1875,\n",
              "  735001.3125,\n",
              "  731536.5625,\n",
              "  730017.25,\n",
              "  730460.4375,\n",
              "  736236.4375,\n",
              "  713784.8125,\n",
              "  714959.8125,\n",
              "  704811.0,\n",
              "  728003.25,\n",
              "  720713.6875,\n",
              "  711176.1875,\n",
              "  721921.875,\n",
              "  717757.1875,\n",
              "  721137.875,\n",
              "  706378.5,\n",
              "  711317.75,\n",
              "  701720.625,\n",
              "  725047.8125,\n",
              "  716242.875,\n",
              "  720511.8125,\n",
              "  710636.8125,\n",
              "  709115.5625,\n",
              "  715008.125,\n",
              "  707874.1875,\n",
              "  709693.8125,\n",
              "  722057.125,\n",
              "  713791.625,\n",
              "  712252.0,\n",
              "  708720.9375,\n",
              "  708070.875,\n",
              "  711201.0,\n",
              "  698907.625,\n",
              "  702103.5,\n",
              "  702266.125,\n",
              "  701052.0,\n",
              "  693823.1875,\n",
              "  707668.625,\n",
              "  703943.125,\n",
              "  707652.3125,\n",
              "  710229.9375,\n",
              "  687071.1875,\n",
              "  699031.5625,\n",
              "  693104.1875,\n",
              "  706148.5625,\n",
              "  695108.9375,\n",
              "  696470.375,\n",
              "  706619.25,\n",
              "  693958.625,\n",
              "  704523.6875,\n",
              "  701610.6875,\n",
              "  687239.75,\n",
              "  699106.9375,\n",
              "  686855.8125,\n",
              "  695168.1875,\n",
              "  698424.375,\n",
              "  690565.9375,\n",
              "  690207.5,\n",
              "  690072.0625,\n",
              "  699042.875,\n",
              "  703157.625,\n",
              "  695905.0625,\n",
              "  693067.4375,\n",
              "  682077.4375,\n",
              "  698746.875,\n",
              "  703189.3125,\n",
              "  694485.5625,\n",
              "  678501.9375,\n",
              "  699786.25,\n",
              "  691726.5,\n",
              "  693118.875,\n",
              "  686100.3125,\n",
              "  688609.8125,\n",
              "  684983.875,\n",
              "  683359.1875,\n",
              "  679661.25,\n",
              "  689678.5625,\n",
              "  682349.0,\n",
              "  691269.3125,\n",
              "  691217.5,\n",
              "  678533.8125,\n",
              "  683712.6875,\n",
              "  662960.9375,\n",
              "  680604.0,\n",
              "  663804.8125,\n",
              "  681054.125,\n",
              "  681933.25,\n",
              "  683254.25,\n",
              "  680829.25,\n",
              "  682789.1875,\n",
              "  684009.5,\n",
              "  676124.3125,\n",
              "  680856.1875,\n",
              "  669847.0,\n",
              "  672253.625,\n",
              "  671118.5625,\n",
              "  662501.0625,\n",
              "  673197.125,\n",
              "  676764.25,\n",
              "  664913.875,\n",
              "  663644.4375,\n",
              "  666138.75,\n",
              "  671931.25,\n",
              "  671021.125,\n",
              "  674540.9375,\n",
              "  674697.4375,\n",
              "  662845.1875,\n",
              "  673144.1875,\n",
              "  672444.1875,\n",
              "  673086.125,\n",
              "  672954.375,\n",
              "  654413.375,\n",
              "  653792.8125,\n",
              "  669818.8125,\n",
              "  669767.8125,\n",
              "  662065.8125,\n",
              "  668512.0625,\n",
              "  670836.25,\n",
              "  660422.0,\n",
              "  652464.3125,\n",
              "  656955.6875,\n",
              "  659287.8125,\n",
              "  657856.0625,\n",
              "  668928.5,\n",
              "  660107.75,\n",
              "  663489.9375,\n",
              "  656833.125,\n",
              "  651211.0625,\n",
              "  657879.9375,\n",
              "  661369.125,\n",
              "  651299.0,\n",
              "  656573.6875,\n",
              "  657422.1875,\n",
              "  655969.3125,\n",
              "  653410.5625,\n",
              "  655054.4375,\n",
              "  656529.625,\n",
              "  651399.8125,\n",
              "  646993.8125,\n",
              "  641981.6875,\n",
              "  661272.375,\n",
              "  652963.3125,\n",
              "  654792.75,\n",
              "  648573.0,\n",
              "  639729.125,\n",
              "  652302.25,\n",
              "  653224.375,\n",
              "  656618.6875,\n",
              "  639598.4375,\n",
              "  650201.3125,\n",
              "  654968.5625,\n",
              "  641139.75,\n",
              "  642825.75,\n",
              "  641561.0,\n",
              "  652823.375,\n",
              "  646632.3125,\n",
              "  638513.875,\n",
              "  655407.75,\n",
              "  646683.6875,\n",
              "  647714.75,\n",
              "  643936.8125,\n",
              "  639039.375,\n",
              "  649272.625,\n",
              "  630441.4375,\n",
              "  643011.75,\n",
              "  632656.8125,\n",
              "  639536.1875,\n",
              "  634890.625,\n",
              "  636323.1875,\n",
              "  638253.75,\n",
              "  639004.3125,\n",
              "  630205.4375,\n",
              "  645587.0,\n",
              "  645112.875,\n",
              "  639987.375,\n",
              "  620001.875,\n",
              "  634662.625,\n",
              "  638259.6875,\n",
              "  630426.625,\n",
              "  640348.625,\n",
              "  637749.4375,\n",
              "  634835.6875,\n",
              "  632192.125,\n",
              "  630516.625,\n",
              "  636503.75,\n",
              "  633363.0,\n",
              "  633255.1875,\n",
              "  630838.125,\n",
              "  634839.375,\n",
              "  630151.1875,\n",
              "  632326.875,\n",
              "  633259.375,\n",
              "  622704.0625,\n",
              "  628855.625,\n",
              "  630970.125,\n",
              "  629072.8125,\n",
              "  623840.1875,\n",
              "  626546.1875,\n",
              "  621011.125,\n",
              "  620008.0,\n",
              "  614756.9375,\n",
              "  623526.125,\n",
              "  625666.5,\n",
              "  632256.75,\n",
              "  622426.75,\n",
              "  623962.0,\n",
              "  617449.5625,\n",
              "  625920.75,\n",
              "  619183.5,\n",
              "  627253.5,\n",
              "  626636.875,\n",
              "  619233.1875,\n",
              "  617653.9375,\n",
              "  618430.0625,\n",
              "  623602.75,\n",
              "  606123.625,\n",
              "  618651.875,\n",
              "  617892.75,\n",
              "  617978.9375,\n",
              "  620270.25,\n",
              "  620512.1875,\n",
              "  615623.0625,\n",
              "  620864.9375,\n",
              "  598411.0625,\n",
              "  625539.9375,\n",
              "  607907.125,\n",
              "  610882.5625,\n",
              "  615654.875,\n",
              "  624694.25,\n",
              "  606560.9375,\n",
              "  620658.75,\n",
              "  609900.0,\n",
              "  622042.375,\n",
              "  599912.875,\n",
              "  611240.1875,\n",
              "  610374.375,\n",
              "  618005.4375,\n",
              "  602334.5625,\n",
              "  607322.25,\n",
              "  610137.0,\n",
              "  606625.6875,\n",
              "  604950.375,\n",
              "  599352.5625,\n",
              "  608875.6875,\n",
              "  601207.8125,\n",
              "  606974.3125,\n",
              "  602904.0,\n",
              "  603064.0625,\n",
              "  600180.25,\n",
              "  606959.9375,\n",
              "  611001.375,\n",
              "  604179.6875,\n",
              "  599072.1875],\n",
              " 'mae': [968.3543701171875,\n",
              "  966.9556884765625,\n",
              "  966.4443359375,\n",
              "  972.9605712890625,\n",
              "  968.6489868164062,\n",
              "  951.97119140625,\n",
              "  966.105712890625,\n",
              "  956.8988037109375,\n",
              "  962.5458374023438,\n",
              "  957.8331298828125,\n",
              "  962.0332641601562,\n",
              "  959.8751831054688,\n",
              "  953.9844970703125,\n",
              "  951.4720458984375,\n",
              "  952.025634765625,\n",
              "  960.8357543945312,\n",
              "  954.340576171875,\n",
              "  949.0409545898438,\n",
              "  954.760498046875,\n",
              "  953.9650268554688,\n",
              "  959.4232177734375,\n",
              "  951.6077270507812,\n",
              "  954.06640625,\n",
              "  951.7626342773438,\n",
              "  949.844482421875,\n",
              "  940.1814575195312,\n",
              "  948.638671875,\n",
              "  947.8984375,\n",
              "  942.7769775390625,\n",
              "  942.9468994140625,\n",
              "  944.3764038085938,\n",
              "  953.8214721679688,\n",
              "  947.3748168945312,\n",
              "  943.6929321289062,\n",
              "  939.9679565429688,\n",
              "  946.6424560546875,\n",
              "  934.0938720703125,\n",
              "  942.0830078125,\n",
              "  939.9342041015625,\n",
              "  936.7771606445312,\n",
              "  942.596435546875,\n",
              "  947.1077270507812,\n",
              "  940.2769775390625,\n",
              "  940.4742431640625,\n",
              "  932.8179321289062,\n",
              "  930.39453125,\n",
              "  933.786865234375,\n",
              "  931.1629028320312,\n",
              "  931.5792236328125,\n",
              "  932.8299560546875,\n",
              "  937.8580322265625,\n",
              "  937.161376953125,\n",
              "  924.697998046875,\n",
              "  940.2267456054688,\n",
              "  933.7474365234375,\n",
              "  932.59326171875,\n",
              "  929.393310546875,\n",
              "  923.1039428710938,\n",
              "  928.4598388671875,\n",
              "  928.078125,\n",
              "  935.2528686523438,\n",
              "  920.971923828125,\n",
              "  930.97265625,\n",
              "  919.03564453125,\n",
              "  926.5855712890625,\n",
              "  918.9296264648438,\n",
              "  929.6600341796875,\n",
              "  921.7166748046875,\n",
              "  931.3270874023438,\n",
              "  921.7389526367188,\n",
              "  921.3529663085938,\n",
              "  923.4706420898438,\n",
              "  923.7305908203125,\n",
              "  907.4473876953125,\n",
              "  916.8775634765625,\n",
              "  908.8812255859375,\n",
              "  917.61376953125,\n",
              "  914.6039428710938,\n",
              "  917.0086059570312,\n",
              "  917.2372436523438,\n",
              "  911.6944580078125,\n",
              "  912.4092407226562,\n",
              "  913.7733764648438,\n",
              "  916.0457763671875,\n",
              "  920.2554321289062,\n",
              "  912.76171875,\n",
              "  911.6243896484375,\n",
              "  914.42138671875,\n",
              "  913.2269287109375,\n",
              "  909.2195434570312,\n",
              "  908.634521484375,\n",
              "  916.4173583984375,\n",
              "  905.5262451171875,\n",
              "  904.5146484375,\n",
              "  911.6776123046875,\n",
              "  905.895751953125,\n",
              "  896.1732788085938,\n",
              "  904.6449584960938,\n",
              "  900.8343505859375,\n",
              "  901.66552734375,\n",
              "  905.7451171875,\n",
              "  903.0054321289062,\n",
              "  907.8273315429688,\n",
              "  903.96630859375,\n",
              "  897.9268798828125,\n",
              "  898.4261474609375,\n",
              "  894.58740234375,\n",
              "  896.268310546875,\n",
              "  895.6221313476562,\n",
              "  889.2205810546875,\n",
              "  906.3460693359375,\n",
              "  896.52685546875,\n",
              "  896.445556640625,\n",
              "  899.4117431640625,\n",
              "  893.8900146484375,\n",
              "  897.8199462890625,\n",
              "  900.8171997070312,\n",
              "  892.9613037109375,\n",
              "  892.3696899414062,\n",
              "  894.7894287109375,\n",
              "  892.8541870117188,\n",
              "  896.2513427734375,\n",
              "  887.566162109375,\n",
              "  891.094482421875,\n",
              "  889.8013916015625,\n",
              "  899.5400390625,\n",
              "  879.00390625,\n",
              "  888.62939453125,\n",
              "  883.4616088867188,\n",
              "  880.3662109375,\n",
              "  873.3203735351562,\n",
              "  884.1107177734375,\n",
              "  875.4866943359375,\n",
              "  878.4922485351562,\n",
              "  880.177001953125,\n",
              "  892.2994384765625,\n",
              "  886.4403686523438,\n",
              "  885.6090698242188,\n",
              "  878.0245971679688,\n",
              "  880.5845336914062,\n",
              "  879.03173828125,\n",
              "  870.57763671875,\n",
              "  890.6804809570312,\n",
              "  879.0045166015625,\n",
              "  871.3685913085938,\n",
              "  875.5025634765625,\n",
              "  878.6629638671875,\n",
              "  874.2271118164062,\n",
              "  863.8692626953125,\n",
              "  870.1519775390625,\n",
              "  872.6426391601562,\n",
              "  882.4902954101562,\n",
              "  872.3427124023438,\n",
              "  875.0081787109375,\n",
              "  871.966796875,\n",
              "  860.7325439453125,\n",
              "  868.5108032226562,\n",
              "  872.5419921875,\n",
              "  865.9876708984375,\n",
              "  866.7553100585938,\n",
              "  874.7047729492188,\n",
              "  861.2879028320312,\n",
              "  851.986328125,\n",
              "  860.1551513671875,\n",
              "  869.2769775390625,\n",
              "  864.2128295898438,\n",
              "  867.2328491210938,\n",
              "  865.3206787109375,\n",
              "  859.3974609375,\n",
              "  863.3955078125,\n",
              "  855.0282592773438,\n",
              "  863.0226440429688,\n",
              "  863.0972290039062,\n",
              "  861.746337890625,\n",
              "  857.51171875,\n",
              "  866.247314453125,\n",
              "  858.4583129882812,\n",
              "  862.442626953125,\n",
              "  853.88037109375,\n",
              "  866.331298828125,\n",
              "  853.6339721679688,\n",
              "  847.7381591796875,\n",
              "  848.1063842773438,\n",
              "  852.3197631835938,\n",
              "  858.6714477539062,\n",
              "  855.2448120117188,\n",
              "  859.6484985351562,\n",
              "  847.2545166015625,\n",
              "  854.1404418945312,\n",
              "  850.7840576171875,\n",
              "  839.425048828125,\n",
              "  851.442626953125,\n",
              "  836.6986083984375,\n",
              "  850.8347778320312,\n",
              "  851.1754150390625,\n",
              "  845.232666015625,\n",
              "  852.6008911132812,\n",
              "  844.723876953125,\n",
              "  845.9598388671875,\n",
              "  852.1428833007812,\n",
              "  841.365234375,\n",
              "  842.1919555664062,\n",
              "  838.13037109375,\n",
              "  837.3179931640625,\n",
              "  843.4006958007812,\n",
              "  842.6756591796875,\n",
              "  842.8601684570312,\n",
              "  828.0398559570312,\n",
              "  841.4064331054688,\n",
              "  839.0311279296875,\n",
              "  838.1229858398438,\n",
              "  838.0650634765625,\n",
              "  830.849609375,\n",
              "  833.2548217773438,\n",
              "  840.221923828125,\n",
              "  840.5567016601562,\n",
              "  843.4197998046875,\n",
              "  832.0570068359375,\n",
              "  838.9825439453125,\n",
              "  831.5682373046875,\n",
              "  831.8909301757812,\n",
              "  829.7586669921875,\n",
              "  834.2240600585938,\n",
              "  829.0737915039062,\n",
              "  832.7213745117188,\n",
              "  824.518310546875,\n",
              "  826.8740844726562,\n",
              "  831.1163940429688,\n",
              "  822.620849609375,\n",
              "  835.20751953125,\n",
              "  823.066162109375,\n",
              "  829.5560302734375,\n",
              "  826.9000244140625,\n",
              "  824.9927978515625,\n",
              "  828.1792602539062,\n",
              "  821.9114990234375,\n",
              "  822.2014770507812,\n",
              "  813.0332641601562,\n",
              "  830.4285888671875,\n",
              "  820.8807373046875,\n",
              "  821.5183715820312,\n",
              "  814.3504028320312,\n",
              "  820.0160522460938,\n",
              "  819.4879150390625,\n",
              "  818.724365234375,\n",
              "  825.5238647460938,\n",
              "  816.7757568359375,\n",
              "  818.7072143554688,\n",
              "  815.511474609375,\n",
              "  819.6064453125,\n",
              "  808.4979248046875,\n",
              "  811.8739013671875,\n",
              "  813.0070190429688,\n",
              "  813.3408813476562,\n",
              "  816.3443603515625,\n",
              "  824.2826538085938,\n",
              "  807.0089111328125,\n",
              "  812.6929931640625,\n",
              "  814.9354858398438,\n",
              "  807.9234008789062,\n",
              "  809.7953491210938,\n",
              "  807.064697265625,\n",
              "  810.708984375,\n",
              "  806.7860717773438,\n",
              "  808.5679931640625,\n",
              "  810.6768798828125,\n",
              "  809.720458984375,\n",
              "  807.9984130859375,\n",
              "  810.9823608398438,\n",
              "  808.0828857421875,\n",
              "  799.2701416015625,\n",
              "  806.7939453125,\n",
              "  797.3212890625,\n",
              "  806.6649169921875,\n",
              "  804.392578125,\n",
              "  800.7508544921875,\n",
              "  801.1659545898438,\n",
              "  796.9599609375,\n",
              "  804.1182861328125,\n",
              "  801.5359497070312,\n",
              "  802.5327758789062,\n",
              "  803.2793579101562,\n",
              "  794.1085205078125,\n",
              "  800.0155639648438,\n",
              "  791.4271240234375,\n",
              "  804.3294067382812,\n",
              "  802.5808715820312,\n",
              "  792.881591796875,\n",
              "  795.1870727539062,\n",
              "  799.4669799804688,\n",
              "  798.7998657226562,\n",
              "  803.274658203125,\n",
              "  790.7015380859375,\n",
              "  801.0947265625,\n",
              "  793.1439208984375,\n",
              "  790.61767578125,\n",
              "  790.9490966796875,\n",
              "  796.911376953125,\n",
              "  787.1531982421875,\n",
              "  787.147705078125,\n",
              "  783.1270141601562,\n",
              "  790.88232421875,\n",
              "  794.29345703125,\n",
              "  794.006103515625,\n",
              "  780.7601928710938,\n",
              "  784.40478515625,\n",
              "  784.5267944335938,\n",
              "  784.8543701171875,\n",
              "  781.8898315429688,\n",
              "  785.618896484375,\n",
              "  793.2344360351562,\n",
              "  786.6502075195312,\n",
              "  780.5746459960938,\n",
              "  786.3574829101562,\n",
              "  789.375244140625,\n",
              "  785.2861938476562,\n",
              "  786.7141723632812,\n",
              "  782.0010986328125,\n",
              "  778.4188842773438,\n",
              "  771.528564453125,\n",
              "  775.4475708007812,\n",
              "  784.9080810546875,\n",
              "  771.8726806640625,\n",
              "  781.8380737304688,\n",
              "  777.1922607421875,\n",
              "  787.630126953125,\n",
              "  770.6077270507812,\n",
              "  774.7667846679688,\n",
              "  773.2293090820312,\n",
              "  770.273681640625,\n",
              "  784.1170043945312,\n",
              "  774.8867797851562,\n",
              "  776.1835327148438,\n",
              "  773.0789184570312,\n",
              "  779.3257446289062,\n",
              "  774.11767578125,\n",
              "  773.0724487304688,\n",
              "  773.8853149414062,\n",
              "  759.683837890625,\n",
              "  766.8517456054688,\n",
              "  767.3270874023438,\n",
              "  769.9592895507812,\n",
              "  773.4137573242188,\n",
              "  769.6483764648438,\n",
              "  770.73876953125,\n",
              "  766.6723022460938,\n",
              "  773.219482421875,\n",
              "  767.951416015625,\n",
              "  769.97265625,\n",
              "  764.3335571289062,\n",
              "  765.501953125,\n",
              "  763.5786743164062,\n",
              "  767.5619506835938,\n",
              "  769.3865356445312,\n",
              "  760.073486328125,\n",
              "  762.5799560546875,\n",
              "  766.8718872070312,\n",
              "  763.9513549804688,\n",
              "  769.2521362304688,\n",
              "  765.0717163085938,\n",
              "  760.2801513671875,\n",
              "  762.0147094726562,\n",
              "  759.9267578125,\n",
              "  760.014404296875,\n",
              "  761.3096923828125,\n",
              "  762.5635375976562,\n",
              "  756.703369140625,\n",
              "  760.1748657226562,\n",
              "  760.49267578125,\n",
              "  757.7633056640625,\n",
              "  752.7990112304688,\n",
              "  759.0711059570312,\n",
              "  749.1153564453125,\n",
              "  758.9373779296875,\n",
              "  749.4195556640625,\n",
              "  756.1716918945312,\n",
              "  754.98486328125,\n",
              "  748.4625244140625,\n",
              "  755.5340576171875,\n",
              "  745.6327514648438,\n",
              "  754.076171875,\n",
              "  760.6902465820312,\n",
              "  754.1492309570312,\n",
              "  744.3490600585938,\n",
              "  749.6451416015625,\n",
              "  752.5445556640625,\n",
              "  754.0240478515625,\n",
              "  749.9573364257812,\n",
              "  746.39501953125,\n",
              "  750.2197875976562,\n",
              "  754.8236083984375,\n",
              "  743.370361328125,\n",
              "  737.6405639648438,\n",
              "  739.6426391601562,\n",
              "  751.7692260742188,\n",
              "  742.6920776367188,\n",
              "  741.45556640625,\n",
              "  746.9943237304688,\n",
              "  747.1141357421875,\n",
              "  739.6558227539062,\n",
              "  740.55908203125,\n",
              "  744.8407592773438,\n",
              "  743.0924682617188,\n",
              "  741.3638916015625,\n",
              "  749.2757568359375,\n",
              "  742.029052734375,\n",
              "  741.921875,\n",
              "  742.274169921875,\n",
              "  743.3871459960938,\n",
              "  738.0570678710938,\n",
              "  740.618896484375,\n",
              "  736.1404418945312,\n",
              "  731.8897094726562,\n",
              "  744.9725952148438,\n",
              "  733.3093872070312,\n",
              "  738.7649536132812,\n",
              "  736.4583129882812,\n",
              "  733.0306396484375,\n",
              "  742.4549560546875,\n",
              "  740.2101440429688,\n",
              "  734.2478637695312,\n",
              "  730.1469116210938,\n",
              "  730.67138671875,\n",
              "  724.3637084960938,\n",
              "  727.8975830078125,\n",
              "  727.8045654296875,\n",
              "  724.8652954101562,\n",
              "  728.98291015625,\n",
              "  733.8748168945312,\n",
              "  736.25830078125,\n",
              "  737.388671875,\n",
              "  729.765380859375,\n",
              "  725.6318359375,\n",
              "  720.5637817382812,\n",
              "  732.2023315429688,\n",
              "  722.821533203125,\n",
              "  737.2130126953125,\n",
              "  729.9215087890625,\n",
              "  728.1495361328125,\n",
              "  720.8401489257812,\n",
              "  722.5956420898438,\n",
              "  723.9033203125,\n",
              "  725.5157470703125,\n",
              "  723.7804565429688,\n",
              "  723.3634033203125,\n",
              "  724.2039184570312,\n",
              "  724.6624755859375,\n",
              "  724.2294921875,\n",
              "  723.4596557617188,\n",
              "  721.095458984375,\n",
              "  717.2039184570312,\n",
              "  715.9505004882812,\n",
              "  725.4923706054688,\n",
              "  718.482421875,\n",
              "  716.5624389648438,\n",
              "  721.023681640625,\n",
              "  714.7831420898438,\n",
              "  717.6533813476562,\n",
              "  709.7593383789062,\n",
              "  712.4048461914062,\n",
              "  714.71923828125,\n",
              "  722.432373046875,\n",
              "  723.1222534179688,\n",
              "  707.7537231445312,\n",
              "  712.9624633789062,\n",
              "  712.9577026367188,\n",
              "  711.8764038085938,\n",
              "  705.318603515625,\n",
              "  715.4168701171875,\n",
              "  705.462158203125,\n",
              "  708.8189697265625,\n",
              "  714.2787475585938,\n",
              "  715.4661254882812,\n",
              "  707.9042358398438,\n",
              "  711.6150512695312,\n",
              "  716.009765625,\n",
              "  705.3641967773438,\n",
              "  708.7095336914062,\n",
              "  710.8995971679688,\n",
              "  699.4795532226562,\n",
              "  710.2507934570312,\n",
              "  711.4558715820312,\n",
              "  703.1329956054688,\n",
              "  710.4158935546875,\n",
              "  704.721923828125,\n",
              "  703.2915649414062,\n",
              "  701.9204711914062,\n",
              "  705.9376220703125,\n",
              "  710.5791015625,\n",
              "  701.5306396484375,\n",
              "  704.4888916015625,\n",
              "  710.017333984375,\n",
              "  698.5574951171875,\n",
              "  706.6553955078125,\n",
              "  701.1082763671875,\n",
              "  704.6881713867188,\n",
              "  700.6586303710938,\n",
              "  698.8175048828125,\n",
              "  699.7846069335938,\n",
              "  708.8171997070312,\n",
              "  704.5699462890625,\n",
              "  700.0545654296875,\n",
              "  703.3027954101562,\n",
              "  702.358642578125,\n",
              "  692.3047485351562,\n",
              "  698.538818359375,\n",
              "  693.455078125,\n",
              "  692.9671630859375,\n",
              "  693.8543090820312,\n",
              "  695.7404174804688,\n",
              "  701.6759643554688,\n",
              "  697.7353515625,\n",
              "  703.5166015625,\n",
              "  700.2592163085938,\n",
              "  698.2936401367188,\n",
              "  690.152587890625,\n",
              "  691.25048828125,\n",
              "  691.8573608398438,\n",
              "  687.2947998046875,\n",
              "  693.5232543945312,\n",
              "  692.3826904296875,\n",
              "  700.51953125,\n",
              "  692.7216796875,\n",
              "  683.80859375,\n",
              "  693.8363037109375,\n",
              "  687.291748046875,\n",
              "  698.6886596679688,\n",
              "  691.4661865234375,\n",
              "  695.4132080078125,\n",
              "  692.2256469726562,\n",
              "  694.3177490234375,\n",
              "  682.2281494140625,\n",
              "  691.841796875,\n",
              "  690.517333984375,\n",
              "  689.5310668945312,\n",
              "  682.7329711914062,\n",
              "  685.2842407226562,\n",
              "  690.2423706054688,\n",
              "  687.4757690429688,\n",
              "  683.6481323242188,\n",
              "  677.5405883789062,\n",
              "  686.5873413085938,\n",
              "  682.7205810546875,\n",
              "  679.6351928710938,\n",
              "  683.5294799804688,\n",
              "  684.374267578125,\n",
              "  685.4879760742188,\n",
              "  691.4965209960938,\n",
              "  680.4745483398438,\n",
              "  678.5525512695312,\n",
              "  677.3849487304688,\n",
              "  685.5831909179688,\n",
              "  682.3564453125,\n",
              "  678.0462036132812,\n",
              "  679.8827514648438,\n",
              "  677.7122802734375,\n",
              "  678.607421875,\n",
              "  676.818603515625,\n",
              "  684.8868408203125,\n",
              "  678.7281494140625,\n",
              "  673.8428955078125,\n",
              "  676.9085693359375,\n",
              "  676.0126953125,\n",
              "  674.65625,\n",
              "  680.1682739257812,\n",
              "  677.9304809570312,\n",
              "  682.2412719726562,\n",
              "  677.2959594726562,\n",
              "  676.6780395507812,\n",
              "  675.06298828125,\n",
              "  672.4307861328125,\n",
              "  682.8190307617188,\n",
              "  673.0718383789062,\n",
              "  673.0086059570312,\n",
              "  672.6559448242188,\n",
              "  676.6553344726562,\n",
              "  679.7738647460938,\n",
              "  674.9610595703125,\n",
              "  675.523193359375,\n",
              "  663.4326171875,\n",
              "  675.5075073242188,\n",
              "  668.8170776367188,\n",
              "  671.459716796875,\n",
              "  671.8345336914062,\n",
              "  667.8999633789062,\n",
              "  670.2265625,\n",
              "  658.3790283203125,\n",
              "  671.1695556640625,\n",
              "  668.3423461914062,\n",
              "  668.9382934570312,\n",
              "  664.1231689453125,\n",
              "  670.1360473632812,\n",
              "  668.8173828125,\n",
              "  662.5596923828125,\n",
              "  670.3673095703125,\n",
              "  668.0148315429688,\n",
              "  665.9743041992188,\n",
              "  676.3648071289062,\n",
              "  673.3250732421875,\n",
              "  669.3606567382812,\n",
              "  671.1697998046875,\n",
              "  668.6228637695312,\n",
              "  671.6890869140625,\n",
              "  660.5973510742188,\n",
              "  659.9598388671875,\n",
              "  668.0973510742188,\n",
              "  659.6434936523438,\n",
              "  672.3245849609375,\n",
              "  658.3864135742188,\n",
              "  669.8677368164062,\n",
              "  663.2156372070312,\n",
              "  668.235107421875,\n",
              "  662.4280395507812,\n",
              "  659.5604248046875,\n",
              "  652.9407958984375,\n",
              "  664.4698486328125,\n",
              "  670.4661865234375,\n",
              "  665.2565307617188,\n",
              "  661.1632080078125,\n",
              "  668.579833984375,\n",
              "  657.71923828125,\n",
              "  659.5673217773438,\n",
              "  661.7003173828125,\n",
              "  653.9794921875,\n",
              "  657.877685546875,\n",
              "  654.0446166992188,\n",
              "  656.1342163085938,\n",
              "  665.9797973632812,\n",
              "  658.9086303710938,\n",
              "  652.378662109375,\n",
              "  658.72314453125,\n",
              "  653.603759765625,\n",
              "  657.240478515625,\n",
              "  657.096923828125,\n",
              "  656.2313232421875,\n",
              "  658.8843994140625,\n",
              "  653.1839599609375,\n",
              "  654.5946655273438,\n",
              "  658.587646484375,\n",
              "  655.2472534179688,\n",
              "  652.2990112304688,\n",
              "  652.720947265625,\n",
              "  654.2307739257812,\n",
              "  652.86181640625,\n",
              "  660.2115478515625,\n",
              "  654.5801391601562,\n",
              "  659.3507690429688,\n",
              "  651.7072143554688,\n",
              "  648.3177490234375,\n",
              "  655.6487426757812,\n",
              "  652.7039794921875,\n",
              "  656.7935791015625,\n",
              "  650.2910766601562,\n",
              "  655.1017456054688,\n",
              "  651.2984619140625,\n",
              "  655.0020141601562,\n",
              "  652.558349609375,\n",
              "  655.6197509765625,\n",
              "  651.270751953125,\n",
              "  645.6297607421875,\n",
              "  653.008544921875,\n",
              "  645.9195556640625,\n",
              "  640.1748046875,\n",
              "  651.7725830078125,\n",
              "  648.67626953125,\n",
              "  648.0862426757812,\n",
              "  647.3948974609375,\n",
              "  649.8239135742188,\n",
              "  649.2894287109375,\n",
              "  645.6889038085938,\n",
              "  647.9020385742188,\n",
              "  648.01611328125,\n",
              "  647.2252807617188,\n",
              "  642.4395141601562,\n",
              "  653.8492431640625,\n",
              "  652.0857543945312,\n",
              "  649.0004272460938,\n",
              "  650.4777221679688,\n",
              "  654.0614624023438,\n",
              "  643.2171020507812,\n",
              "  645.384521484375,\n",
              "  638.0298461914062,\n",
              "  635.6318359375,\n",
              "  647.7068481445312,\n",
              "  649.228271484375,\n",
              "  645.3936157226562,\n",
              "  645.6820068359375,\n",
              "  642.0927124023438,\n",
              "  644.898193359375,\n",
              "  644.2933349609375,\n",
              "  647.6929931640625,\n",
              "  642.4501342773438,\n",
              "  642.8207397460938,\n",
              "  648.3577270507812,\n",
              "  641.764404296875,\n",
              "  649.6434936523438,\n",
              "  640.7733154296875,\n",
              "  641.0952758789062,\n",
              "  643.8153686523438,\n",
              "  645.3012084960938,\n",
              "  634.2900390625,\n",
              "  642.8564453125,\n",
              "  636.2239379882812,\n",
              "  640.5115966796875,\n",
              "  637.4132690429688,\n",
              "  637.9788208007812,\n",
              "  634.7044677734375,\n",
              "  636.31103515625,\n",
              "  638.9150390625,\n",
              "  637.9130859375,\n",
              "  639.36865234375,\n",
              "  640.6526489257812,\n",
              "  641.2559814453125,\n",
              "  635.0861206054688,\n",
              "  637.810546875,\n",
              "  636.32763671875,\n",
              "  634.2879638671875,\n",
              "  642.0484008789062,\n",
              "  636.8528442382812,\n",
              "  637.1941528320312,\n",
              "  636.8069458007812,\n",
              "  642.041015625,\n",
              "  633.5936279296875,\n",
              "  637.1150512695312,\n",
              "  635.7655029296875,\n",
              "  635.0960083007812,\n",
              "  639.9669799804688,\n",
              "  635.540283203125,\n",
              "  641.2960815429688,\n",
              "  638.0744018554688,\n",
              "  642.5702514648438,\n",
              "  637.6356811523438,\n",
              "  636.8067016601562,\n",
              "  633.2319946289062,\n",
              "  639.6384887695312,\n",
              "  636.6549072265625,\n",
              "  631.218994140625,\n",
              "  632.49169921875,\n",
              "  633.8186645507812,\n",
              "  638.14208984375,\n",
              "  639.7616577148438,\n",
              "  639.7246704101562,\n",
              "  638.0982055664062,\n",
              "  636.1129150390625,\n",
              "  631.328125,\n",
              "  632.01171875,\n",
              "  633.9176635742188,\n",
              "  634.6173095703125,\n",
              "  637.9376831054688,\n",
              "  636.1519165039062,\n",
              "  636.6612548828125,\n",
              "  636.9573364257812,\n",
              "  639.607666015625,\n",
              "  629.25830078125,\n",
              "  630.158935546875,\n",
              "  623.991943359375,\n",
              "  635.5825805664062,\n",
              "  633.5274047851562,\n",
              "  630.2061157226562,\n",
              "  633.3411865234375,\n",
              "  634.4767456054688,\n",
              "  634.5211181640625,\n",
              "  627.5632934570312,\n",
              "  629.8283081054688,\n",
              "  625.3013916015625,\n",
              "  636.3613891601562,\n",
              "  631.5381469726562,\n",
              "  637.8907470703125,\n",
              "  628.825927734375,\n",
              "  628.728271484375,\n",
              "  632.9736938476562,\n",
              "  629.4011840820312,\n",
              "  628.826904296875,\n",
              "  637.6016845703125,\n",
              "  632.578125,\n",
              "  631.85546875,\n",
              "  630.1312255859375,\n",
              "  630.4215087890625,\n",
              "  632.9854736328125,\n",
              "  627.5247192382812,\n",
              "  627.4020385742188,\n",
              "  630.5519409179688,\n",
              "  626.3419189453125,\n",
              "  625.09228515625,\n",
              "  630.1947021484375,\n",
              "  628.4989624023438,\n",
              "  631.921630859375,\n",
              "  636.6185913085938,\n",
              "  619.8966064453125,\n",
              "  627.8584594726562,\n",
              "  624.7717895507812,\n",
              "  632.5839233398438,\n",
              "  628.8823852539062,\n",
              "  624.9266357421875,\n",
              "  632.7538452148438,\n",
              "  626.398681640625,\n",
              "  631.8363037109375,\n",
              "  631.1326904296875,\n",
              "  622.5901489257812,\n",
              "  629.6774291992188,\n",
              "  627.1099243164062,\n",
              "  628.630859375,\n",
              "  629.725341796875,\n",
              "  628.1649169921875,\n",
              "  629.2501220703125,\n",
              "  625.4454345703125,\n",
              "  631.6889038085938,\n",
              "  635.0991821289062,\n",
              "  629.2380981445312,\n",
              "  629.840576171875,\n",
              "  623.2269897460938,\n",
              "  633.4798583984375,\n",
              "  636.3582763671875,\n",
              "  630.4597778320312,\n",
              "  621.86767578125,\n",
              "  634.8701782226562,\n",
              "  628.8973388671875,\n",
              "  631.1611328125,\n",
              "  626.6271362304688,\n",
              "  628.1151733398438,\n",
              "  626.8705444335938,\n",
              "  625.6505737304688,\n",
              "  626.0811157226562,\n",
              "  629.9890747070312,\n",
              "  626.3994750976562,\n",
              "  632.5242309570312,\n",
              "  631.0825805664062,\n",
              "  623.4022827148438,\n",
              "  627.797119140625,\n",
              "  618.4823608398438,\n",
              "  627.2138671875,\n",
              "  620.8641357421875,\n",
              "  626.5155029296875,\n",
              "  628.5360107421875,\n",
              "  627.5904541015625,\n",
              "  627.6309814453125,\n",
              "  628.4494018554688,\n",
              "  630.450439453125,\n",
              "  624.9876098632812,\n",
              "  629.58349609375,\n",
              "  620.737548828125,\n",
              "  623.5178833007812,\n",
              "  622.784423828125,\n",
              "  621.1130981445312,\n",
              "  624.9304809570312,\n",
              "  627.5338745117188,\n",
              "  622.5769653320312,\n",
              "  620.1346435546875,\n",
              "  621.9398193359375,\n",
              "  624.5220336914062,\n",
              "  626.1587524414062,\n",
              "  627.2535400390625,\n",
              "  627.6364135742188,\n",
              "  620.9434204101562,\n",
              "  628.9749145507812,\n",
              "  628.1436767578125,\n",
              "  627.903564453125,\n",
              "  628.1019287109375,\n",
              "  620.2288818359375,\n",
              "  619.739990234375,\n",
              "  628.1114501953125,\n",
              "  626.4642333984375,\n",
              "  622.8826293945312,\n",
              "  625.4873657226562,\n",
              "  628.0379028320312,\n",
              "  620.7129516601562,\n",
              "  620.8756103515625,\n",
              "  621.7301025390625,\n",
              "  622.51123046875,\n",
              "  622.4450073242188,\n",
              "  627.9745483398438,\n",
              "  625.0242309570312,\n",
              "  626.8641357421875,\n",
              "  620.7921142578125,\n",
              "  618.7113037109375,\n",
              "  624.315185546875,\n",
              "  626.4009399414062,\n",
              "  618.4673461914062,\n",
              "  621.2570190429688,\n",
              "  623.2081909179688,\n",
              "  622.9091186523438,\n",
              "  621.98974609375,\n",
              "  622.5021362304688,\n",
              "  622.9288940429688,\n",
              "  619.203857421875,\n",
              "  621.5714111328125,\n",
              "  616.4017944335938,\n",
              "  627.25927734375,\n",
              "  622.3092041015625,\n",
              "  624.4066772460938,\n",
              "  621.5697021484375,\n",
              "  617.3174438476562,\n",
              "  622.6970825195312,\n",
              "  623.3539428710938,\n",
              "  628.4559936523438,\n",
              "  618.6266479492188,\n",
              "  624.182861328125,\n",
              "  626.29345703125,\n",
              "  622.5635986328125,\n",
              "  619.8392333984375,\n",
              "  618.2586059570312,\n",
              "  624.9634399414062,\n",
              "  622.1729125976562,\n",
              "  617.8267211914062,\n",
              "  628.6414794921875,\n",
              "  621.2427368164062,\n",
              "  622.92041015625,\n",
              "  621.5279541015625,\n",
              "  620.6434326171875,\n",
              "  624.6416015625,\n",
              "  616.1827392578125,\n",
              "  621.0264282226562,\n",
              "  614.7456665039062,\n",
              "  619.4035034179688,\n",
              "  616.4743041992188,\n",
              "  618.856201171875,\n",
              "  620.0643310546875,\n",
              "  621.0147705078125,\n",
              "  615.8731079101562,\n",
              "  625.4216918945312,\n",
              "  624.0814819335938,\n",
              "  621.4450073242188,\n",
              "  611.6298217773438,\n",
              "  620.1107788085938,\n",
              "  622.2996826171875,\n",
              "  617.4373168945312,\n",
              "  622.8015747070312,\n",
              "  622.6295166015625,\n",
              "  620.6281127929688,\n",
              "  619.1529541015625,\n",
              "  615.6693725585938,\n",
              "  622.537353515625,\n",
              "  620.1368408203125,\n",
              "  619.0532836914062,\n",
              "  619.1446533203125,\n",
              "  622.2432861328125,\n",
              "  617.4131469726562,\n",
              "  620.6339111328125,\n",
              "  619.7277221679688,\n",
              "  615.6998901367188,\n",
              "  618.8214111328125,\n",
              "  620.2236938476562,\n",
              "  620.2105102539062,\n",
              "  618.3865356445312,\n",
              "  618.314697265625,\n",
              "  617.2898559570312,\n",
              "  617.2982177734375,\n",
              "  610.41357421875,\n",
              "  617.1879272460938,\n",
              "  618.0995483398438,\n",
              "  624.0535888671875,\n",
              "  617.4599609375,\n",
              "  616.33544921875,\n",
              "  615.8355712890625,\n",
              "  620.5400390625,\n",
              "  618.23779296875,\n",
              "  620.8591918945312,\n",
              "  620.8513793945312,\n",
              "  616.45556640625,\n",
              "  615.8872680664062,\n",
              "  618.6555786132812,\n",
              "  618.9410400390625,\n",
              "  613.4857788085938,\n",
              "  616.44140625,\n",
              "  616.404296875,\n",
              "  616.25634765625,\n",
              "  617.9505004882812,\n",
              "  618.2330932617188,\n",
              "  616.3822021484375,\n",
              "  618.9801025390625,\n",
              "  610.0974731445312,\n",
              "  623.06103515625,\n",
              "  616.2435913085938,\n",
              "  613.5532836914062,\n",
              "  617.4529418945312,\n",
              "  623.7658081054688,\n",
              "  611.6294555664062,\n",
              "  620.2701416015625,\n",
              "  614.9417114257812,\n",
              "  622.4798583984375,\n",
              "  611.354736328125,\n",
              "  616.48193359375,\n",
              "  614.2086181640625,\n",
              "  619.53759765625,\n",
              "  612.0955200195312,\n",
              "  614.3473510742188,\n",
              "  615.5988159179688,\n",
              "  615.5233764648438,\n",
              "  614.646240234375,\n",
              "  608.971923828125,\n",
              "  616.09912109375,\n",
              "  611.156494140625,\n",
              "  614.516357421875,\n",
              "  611.2601928710938,\n",
              "  614.9093017578125,\n",
              "  613.573486328125,\n",
              "  616.5635375976562,\n",
              "  617.19482421875,\n",
              "  613.5714721679688,\n",
              "  613.4768676757812],\n",
              " 'val_loss': [528869.5625,\n",
              "  545494.1875,\n",
              "  506244.15625,\n",
              "  535284.5625,\n",
              "  513005.90625,\n",
              "  525030.875,\n",
              "  532232.1875,\n",
              "  532243.25,\n",
              "  544149.25,\n",
              "  521987.59375,\n",
              "  521234.625,\n",
              "  529096.5625,\n",
              "  528287.0625,\n",
              "  514867.34375,\n",
              "  509658.875,\n",
              "  517448.15625,\n",
              "  525217.9375,\n",
              "  528614.25,\n",
              "  536312.5625,\n",
              "  514520.5,\n",
              "  505511.875,\n",
              "  513016.25,\n",
              "  504064.28125,\n",
              "  528400.75,\n",
              "  519154.15625,\n",
              "  518508.5,\n",
              "  517693.5,\n",
              "  500114.34375,\n",
              "  507730.875,\n",
              "  511112.34375,\n",
              "  514721.84375,\n",
              "  501485.0,\n",
              "  513229.71875,\n",
              "  520801.59375,\n",
              "  503376.34375,\n",
              "  506909.25,\n",
              "  510300.03125,\n",
              "  517872.375,\n",
              "  512767.53125,\n",
              "  499747.71875,\n",
              "  507267.90625,\n",
              "  494260.25,\n",
              "  501546.65625,\n",
              "  484622.25,\n",
              "  512571.78125,\n",
              "  487387.625,\n",
              "  511091.375,\n",
              "  502170.0,\n",
              "  489226.65625,\n",
              "  476382.59375,\n",
              "  500043.59375,\n",
              "  491097.09375,\n",
              "  490363.34375,\n",
              "  497790.78125,\n",
              "  509137.84375,\n",
              "  508466.59375,\n",
              "  471502.625,\n",
              "  498894.78125,\n",
              "  494211.375,\n",
              "  481461.34375,\n",
              "  488801.84375,\n",
              "  472089.28125,\n",
              "  495322.46875,\n",
              "  490657.25,\n",
              "  489964.28125,\n",
              "  488538.90625,\n",
              "  456715.96875,\n",
              "  495765.09375,\n",
              "  495058.125,\n",
              "  474524.65625,\n",
              "  469917.0,\n",
              "  477067.40625,\n",
              "  476364.125,\n",
              "  503263.59375,\n",
              "  455348.21875,\n",
              "  478283.65625,\n",
              "  461236.21875,\n",
              "  476908.75,\n",
              "  491847.46875,\n",
              "  448150.21875,\n",
              "  470938.25,\n",
              "  470272.15625,\n",
              "  473452.90625,\n",
              "  480563.125,\n",
              "  472078.53125,\n",
              "  483027.46875,\n",
              "  447498.65625,\n",
              "  485541.09375,\n",
              "  473217.90625,\n",
              "  445477.15625,\n",
              "  471841.15625,\n",
              "  475020.75,\n",
              "  462812.125,\n",
              "  458251.03125,\n",
              "  449962.78125,\n",
              "  460794.59375,\n",
              "  467749.34375,\n",
              "  467084.59375,\n",
              "  458191.65625,\n",
              "  469574.15625,\n",
              "  449926.21875,\n",
              "  464393.25,\n",
              "  463703.59375,\n",
              "  466866.875,\n",
              "  450992.65625,\n",
              "  465542.40625,\n",
              "  464893.0,\n",
              "  467852.15625,\n",
              "  459673.75,\n",
              "  470331.28125,\n",
              "  462203.65625,\n",
              "  468951.875,\n",
              "  468280.75,\n",
              "  455730.0,\n",
              "  455711.5,\n",
              "  455037.78125,\n",
              "  450558.46875,\n",
              "  457543.375,\n",
              "  456880.65625,\n",
              "  448603.21875,\n",
              "  436831.65625,\n",
              "  439970.625,\n",
              "  468868.09375,\n",
              "  445996.78125,\n",
              "  449130.09375,\n",
              "  444680.28125,\n",
              "  458891.5,\n",
              "  436156.40625,\n",
              "  432061.5,\n",
              "  442115.53125,\n",
              "  438050.34375,\n",
              "  437431.15625,\n",
              "  422055.90625,\n",
              "  443355.28125,\n",
              "  420834.75,\n",
              "  427141.40625,\n",
              "  441430.78125,\n",
              "  426145.84375,\n",
              "  436794.90625,\n",
              "  443265.96875,\n",
              "  445992.65625,\n",
              "  420324.65625,\n",
              "  430533.5,\n",
              "  436976.5,\n",
              "  440080.09375,\n",
              "  403359.875,\n",
              "  445839.03125,\n",
              "  431143.40625,\n",
              "  440825.53125,\n",
              "  450902.75,\n",
              "  443272.28125,\n",
              "  442628.0,\n",
              "  424344.34375,\n",
              "  430660.71875,\n",
              "  419416.125,\n",
              "  418791.46875,\n",
              "  435742.53125,\n",
              "  431898.84375,\n",
              "  410087.90625,\n",
              "  430674.25,\n",
              "  433208.875,\n",
              "  415158.78125,\n",
              "  428786.375,\n",
              "  410280.28125,\n",
              "  430684.5,\n",
              "  413317.90625,\n",
              "  405350.40625,\n",
              "  415248.15625,\n",
              "  414645.28125,\n",
              "  424501.84375,\n",
              "  417123.65625,\n",
              "  419593.78125,\n",
              "  412241.28125,\n",
              "  401265.875,\n",
              "  414111.90625,\n",
              "  410453.09375,\n",
              "  416559.78125,\n",
              "  419610.34375,\n",
              "  402022.03125,\n",
              "  411107.65625,\n",
              "  400226.125,\n",
              "  406919.125,\n",
              "  412959.40625,\n",
              "  412367.40625,\n",
              "  415419.65625,\n",
              "  403992.65625,\n",
              "  414183.84375,\n",
              "  406377.40625,\n",
              "  402855.25,\n",
              "  402276.65625,\n",
              "  414739.125,\n",
              "  407660.84375,\n",
              "  413545.375,\n",
              "  403622.65625,\n",
              "  409498.90625,\n",
              "  392381.71875,\n",
              "  404724.78125,\n",
              "  407699.09375,\n",
              "  383486.90625,\n",
              "  396515.375,\n",
              "  398829.03125,\n",
              "  418174.84375,\n",
              "  417617.875,\n",
              "  400686.46875,\n",
              "  393746.15625,\n",
              "  373309.46875,\n",
              "  405247.34375,\n",
              "  381545.75,\n",
              "  397787.46875,\n",
              "  387332.90625,\n",
              "  396699.96875,\n",
              "  396085.71875,\n",
              "  401810.09375,\n",
              "  382433.625,\n",
              "  394365.40625,\n",
              "  377798.46875,\n",
              "  403015.75,\n",
              "  376704.84375,\n",
              "  401794.21875,\n",
              "  368520.40625,\n",
              "  384220.25,\n",
              "  390371.28125,\n",
              "  383663.5,\n",
              "  389316.5,\n",
              "  372975.15625,\n",
              "  378584.40625,\n",
              "  384098.0,\n",
              "  364366.25,\n",
              "  386528.59375,\n",
              "  379951.78125,\n",
              "  395008.59375,\n",
              "  362294.40625,\n",
              "  365224.59375,\n",
              "  393291.21875,\n",
              "  398741.125,\n",
              "  382731.125,\n",
              "  369126.09375,\n",
              "  378116.5,\n",
              "  381083.34375,\n",
              "  371024.75,\n",
              "  374040.65625,\n",
              "  379385.46875,\n",
              "  372976.90625,\n",
              "  359583.53125,\n",
              "  383737.125,\n",
              "  377308.09375,\n",
              "  376769.21875,\n",
              "  379645.21875,\n",
              "  372156.71875,\n",
              "  363400.96875,\n",
              "  374563.09375,\n",
              "  377523.71875,\n",
              "  358441.28125,\n",
              "  360314.5,\n",
              "  387449.09375,\n",
              "  366150.65625,\n",
              "  371408.25,\n",
              "  370846.0,\n",
              "  370312.96875,\n",
              "  369788.75,\n",
              "  360151.84375,\n",
              "  377891.25,\n",
              "  371731.34375,\n",
              "  347266.25,\n",
              "  367233.21875,\n",
              "  372338.28125,\n",
              "  375217.40625,\n",
              "  362257.84375,\n",
              "  370698.625,\n",
              "  373711.90625,\n",
              "  373084.125,\n",
              "  378173.28125,\n",
              "  363142.59375,\n",
              "  359150.40625,\n",
              "  350918.15625,\n",
              "  367043.625,\n",
              "  355507.71875,\n",
              "  363932.125,\n",
              "  351193.28125,\n",
              "  348551.53125,\n",
              "  359040.40625,\n",
              "  340849.15625,\n",
              "  347101.78125,\n",
              "  362894.96875,\n",
              "  357015.59375,\n",
              "  365205.09375,\n",
              "  347260.5,\n",
              "  369608.03125,\n",
              "  342886.03125,\n",
              "  342406.25,\n",
              "  354054.375,\n",
              "  342781.96875,\n",
              "  356342.75,\n",
              "  355906.90625,\n",
              "  346727.78125,\n",
              "  345710.625,\n",
              "  347665.40625,\n",
              "  345291.46875,\n",
              "  353387.78125,\n",
              "  344216.25,\n",
              "  345714.59375,\n",
              "  348542.4375,\n",
              "  329079.375,\n",
              "  347568.34375,\n",
              "  326760.5625,\n",
              "  355191.25,\n",
              "  344324.40625,\n",
              "  354091.15625,\n",
              "  342006.34375,\n",
              "  334382.03125,\n",
              "  344208.25,\n",
              "  352167.71875,\n",
              "  346463.25,\n",
              "  342770.09375,\n",
              "  345641.15625,\n",
              "  328449.21875,\n",
              "  341352.59375,\n",
              "  332528.0,\n",
              "  332212.34375,\n",
              "  345024.65625,\n",
              "  331229.90625,\n",
              "  336694.25,\n",
              "  333588.65625,\n",
              "  326641.84375,\n",
              "  329499.65625,\n",
              "  340546.5,\n",
              "  341761.8125,\n",
              "  344011.21875,\n",
              "  322690.5,\n",
              "  338573.65625,\n",
              "  326833.21875,\n",
              "  326320.375,\n",
              "  331867.84375,\n",
              "  322193.53125,\n",
              "  332951.15625,\n",
              "  316494.25,\n",
              "  320804.59375,\n",
              "  323581.15625,\n",
              "  339209.375,\n",
              "  327561.90625,\n",
              "  330318.71875,\n",
              "  332546.09375,\n",
              "  329490.84375,\n",
              "  321047.03125,\n",
              "  321767.09375,\n",
              "  328137.8125,\n",
              "  315047.78125,\n",
              "  327248.59375,\n",
              "  322069.65625,\n",
              "  324829.71875,\n",
              "  330589.84375,\n",
              "  325583.71875,\n",
              "  328239.6875,\n",
              "  323111.375,\n",
              "  316347.65625,\n",
              "  323761.65625,\n",
              "  326414.59375,\n",
              "  321492.9375,\n",
              "  319379.53125,\n",
              "  314180.53125,\n",
              "  326205.53125,\n",
              "  333343.375,\n",
              "  320732.75,\n",
              "  312505.75,\n",
              "  312242.25,\n",
              "  310325.5625,\n",
              "  318922.90625,\n",
              "  313966.3125,\n",
              "  321205.125,\n",
              "  317733.375,\n",
              "  329170.28125,\n",
              "  316895.71875,\n",
              "  313355.59375,\n",
              "  300897.5625,\n",
              "  315547.53125,\n",
              "  306344.03125,\n",
              "  319145.34375,\n",
              "  306867.96875,\n",
              "  317071.78125,\n",
              "  310459.09375,\n",
              "  313147.0,\n",
              "  311373.46875,\n",
              "  319678.03125,\n",
              "  311821.03125,\n",
              "  289319.03125,\n",
              "  303692.3125,\n",
              "  317922.03125,\n",
              "  299924.25,\n",
              "  305588.25,\n",
              "  309483.53125,\n",
              "  297557.28125,\n",
              "  308575.34375,\n",
              "  300872.625,\n",
              "  304722.0,\n",
              "  293002.78125,\n",
              "  306981.59375,\n",
              "  313754.875,\n",
              "  306186.0625,\n",
              "  305694.09375,\n",
              "  298174.34375,\n",
              "  293735.5,\n",
              "  300656.96875,\n",
              "  289949.59375,\n",
              "  304007.5625,\n",
              "  299395.875,\n",
              "  300094.75,\n",
              "  302736.5625,\n",
              "  284151.3125,\n",
              "  290791.09375,\n",
              "  297516.78125,\n",
              "  301191.75,\n",
              "  290741.53125,\n",
              "  307358.15625,\n",
              "  303853.71875,\n",
              "  292653.15625,\n",
              "  295296.28125,\n",
              "  309662.96875,\n",
              "  287707.25,\n",
              "  287342.21875,\n",
              "  304712.875,\n",
              "  297288.15625,\n",
              "  307645.8125,\n",
              "  285834.96875,\n",
              "  285497.90625,\n",
              "  292127.09375,\n",
              "  287891.59375,\n",
              "  294961.875,\n",
              "  301639.0,\n",
              "  301361.125,\n",
              "  290119.34375,\n",
              "  293684.09375,\n",
              "  296914.875,\n",
              "  289250.8125,\n",
              "  291762.53125,\n",
              "  285493.59375,\n",
              "  281373.75,\n",
              "  284090.15625,\n",
              "  284445.25,\n",
              "  294506.46875,\n",
              "  293349.46875,\n",
              "  300197.65625,\n",
              "  289473.65625,\n",
              "  276104.5,\n",
              "  291570.09375,\n",
              "  281414.90625,\n",
              "  278161.78125,\n",
              "  287821.21875,\n",
              "  281032.78125,\n",
              "  283485.40625,\n",
              "  280345.53125,\n",
              "  282918.21875,\n",
              "  283271.84375,\n",
              "  285710.53125,\n",
              "  287808.84375,\n",
              "  295030.65625,\n",
              "  291112.59375,\n",
              "  277495.25,\n",
              "  280690.28125,\n",
              "  283637.46875,\n",
              "  279461.25,\n",
              "  279469.28125,\n",
              "  292337.71875,\n",
              "  269830.21875,\n",
              "  285391.96875,\n",
              "  271962.53125,\n",
              "  287696.40625,\n",
              "  271780.90625,\n",
              "  277750.34375,\n",
              "  280515.0625,\n",
              "  273414.59375,\n",
              "  282594.96875,\n",
              "  275910.53125,\n",
              "  281801.0625,\n",
              "  281842.5625,\n",
              "  263217.40625,\n",
              "  278209.78125,\n",
              "  268300.21875,\n",
              "  271300.59375,\n",
              "  276647.53125,\n",
              "  270536.125,\n",
              "  270230.34375,\n",
              "  278819.3125,\n",
              "  281491.0625,\n",
              "  274904.71875,\n",
              "  275039.1875,\n",
              "  271906.03125,\n",
              "  277480.59375,\n",
              "  280095.65625,\n",
              "  267923.90625,\n",
              "  267614.875,\n",
              "  269991.03125,\n",
              "  275972.8125,\n",
              "  266721.8125,\n",
              "  278259.84375,\n",
              "  272036.28125,\n",
              "  271595.625,\n",
              "  271560.90625,\n",
              "  268044.46875,\n",
              "  267616.21875,\n",
              "  272841.75,\n",
              "  267422.71875,\n",
              "  266847.03125,\n",
              "  260879.953125,\n",
              "  257850.234375,\n",
              "  269108.53125,\n",
              "  262928.15625,\n",
              "  277224.28125,\n",
              "  262147.59375,\n",
              "  267652.9375,\n",
              "  273051.65625,\n",
              "  255828.421875,\n",
              "  258443.765625,\n",
              "  258183.328125,\n",
              "  263433.25,\n",
              "  268909.09375,\n",
              "  251774.953125,\n",
              "  259705.328125,\n",
              "  259586.1875,\n",
              "  264875.75,\n",
              "  259058.578125,\n",
              "  258901.359375,\n",
              "  258534.828125,\n",
              "  263725.875,\n",
              "  263298.84375,\n",
              "  257527.953125,\n",
              "  268211.25,\n",
              "  251674.1875,\n",
              "  262457.78125,\n",
              "  259303.75,\n",
              "  251065.125,\n",
              "  250385.328125,\n",
              "  253265.015625,\n",
              "  253009.046875,\n",
              "  255302.484375,\n",
              "  263043.09375,\n",
              "  267949.78125,\n",
              "  259690.25,\n",
              "  251773.578125,\n",
              "  256677.078125,\n",
              "  253895.046875,\n",
              "  258758.296875,\n",
              "  253242.734375,\n",
              "  265742.84375,\n",
              "  260471.75,\n",
              "  259780.328125,\n",
              "  244632.140625,\n",
              "  254657.203125,\n",
              "  249349.75,\n",
              "  261493.046875,\n",
              "  251393.265625,\n",
              "  253786.1875,\n",
              "  250738.375,\n",
              "  258232.046875,\n",
              "  250246.359375,\n",
              "  257406.171875,\n",
              "  262166.0625,\n",
              "  254423.640625,\n",
              "  247249.453125,\n",
              "  251672.8125,\n",
              "  249198.671875,\n",
              "  258563.453125,\n",
              "  248180.1875,\n",
              "  242883.5625,\n",
              "  245688.25,\n",
              "  254596.703125,\n",
              "  249573.828125,\n",
              "  247174.078125,\n",
              "  246942.984375,\n",
              "  251484.375,\n",
              "  246309.0,\n",
              "  246244.5625,\n",
              "  243752.984375,\n",
              "  244044.234375,\n",
              "  247641.640625,\n",
              "  250138.0,\n",
              "  247488.4375,\n",
              "  246769.203125,\n",
              "  247077.515625,\n",
              "  247010.984375,\n",
              "  242220.5,\n",
              "  246004.328125,\n",
              "  252873.015625,\n",
              "  252964.203125,\n",
              "  243949.390625,\n",
              "  240592.390625,\n",
              "  252076.421875,\n",
              "  242894.078125,\n",
              "  239970.875,\n",
              "  233248.515625,\n",
              "  242095.625,\n",
              "  248185.171875,\n",
              "  237763.609375,\n",
              "  234372.8125,\n",
              "  241252.5625,\n",
              "  236121.296875,\n",
              "  245346.921875,\n",
              "  247482.515625,\n",
              "  249566.421875,\n",
              "  247204.875,\n",
              "  246982.078125,\n",
              "  233595.421875,\n",
              "  235338.765625,\n",
              "  242083.125,\n",
              "  240942.546875,\n",
              "  245900.828125,\n",
              "  238808.6875,\n",
              "  241114.0,\n",
              "  242767.6875,\n",
              "  240707.625,\n",
              "  243176.921875,\n",
              "  239496.203125,\n",
              "  244073.453125,\n",
              "  244041.046875,\n",
              "  233169.546875,\n",
              "  236901.734375,\n",
              "  236883.203125,\n",
              "  235108.234375,\n",
              "  236506.0,\n",
              "  242625.078125,\n",
              "  236127.765625,\n",
              "  235946.921875,\n",
              "  232525.484375,\n",
              "  239372.234375,\n",
              "  231266.046875,\n",
              "  237488.953125,\n",
              "  240635.296875,\n",
              "  234861.890625,\n",
              "  238400.328125,\n",
              "  234684.359375,\n",
              "  232887.375,\n",
              "  234149.078125,\n",
              "  230960.3125,\n",
              "  237625.5,\n",
              "  233439.125,\n",
              "  235198.984375,\n",
              "  237245.296875,\n",
              "  235340.5,\n",
              "  235350.171875,\n",
              "  236884.875,\n",
              "  233930.484375,\n",
              "  237842.75,\n",
              "  235980.953125,\n",
              "  228421.25,\n",
              "  234157.046875,\n",
              "  234175.265625,\n",
              "  231615.265625,\n",
              "  233841.390625,\n",
              "  229865.640625,\n",
              "  234923.578125,\n",
              "  230971.921875,\n",
              "  236581.6875,\n",
              "  228469.8125,\n",
              "  227950.515625,\n",
              "  229170.125,\n",
              "  231203.375,\n",
              "  227663.8125,\n",
              "  231871.0625,\n",
              "  228601.921875,\n",
              "  231739.375,\n",
              "  235417.5,\n",
              "  227990.046875,\n",
              "  231467.796875,\n",
              "  226627.625,\n",
              "  228824.75,\n",
              "  229977.203125,\n",
              "  228364.5,\n",
              "  228386.046875,\n",
              "  230578.078125,\n",
              "  227098.390625,\n",
              "  229331.984375,\n",
              "  229181.515625,\n",
              "  232315.109375,\n",
              "  228483.609375,\n",
              "  227193.625,\n",
              "  230501.203125,\n",
              "  227120.984375,\n",
              "  230201.234375,\n",
              "  230251.015625,\n",
              "  227927.5,\n",
              "  233133.859375,\n",
              "  224149.546875,\n",
              "  231962.453125,\n",
              "  225343.203125,\n",
              "  228545.359375,\n",
              "  230498.75,\n",
              "  225791.921875,\n",
              "  231226.203125,\n",
              "  227813.515625,\n",
              "  228660.828125,\n",
              "  225075.8125,\n",
              "  225151.359375,\n",
              "  222756.453125,\n",
              "  228307.6875,\n",
              "  230227.671875,\n",
              "  224451.921875,\n",
              "  226586.921875,\n",
              "  225303.140625,\n",
              "  230558.296875,\n",
              "  223965.6875,\n",
              "  226952.6875,\n",
              "  225984.109375,\n",
              "  226071.875,\n",
              "  223924.1875,\n",
              "  223809.703125,\n",
              "  225515.078125,\n",
              "  223958.171875,\n",
              "  225858.515625,\n",
              "  222943.078125,\n",
              "  225270.296875,\n",
              "  223476.390625,\n",
              "  224834.078125,\n",
              "  219984.890625,\n",
              "  221389.796875,\n",
              "  223521.015625,\n",
              "  222399.796875,\n",
              "  224506.421875,\n",
              "  225071.734375,\n",
              "  221968.25,\n",
              "  224626.515625,\n",
              "  224088.125,\n",
              "  226180.578125,\n",
              "  223884.109375,\n",
              "  221203.25,\n",
              "  227284.078125,\n",
              "  221777.015625,\n",
              "  221311.015625,\n",
              "  223173.5,\n",
              "  223850.328125,\n",
              "  226024.625,\n",
              "  223636.015625,\n",
              "  217845.546875,\n",
              "  223720.25,\n",
              "  223606.671875,\n",
              "  220300.515625,\n",
              "  225800.9375,\n",
              "  224742.921875,\n",
              "  224858.140625,\n",
              "  224536.828125,\n",
              "  220360.828125,\n",
              "  222635.171875,\n",
              "  222535.578125,\n",
              "  219711.625,\n",
              "  219642.875,\n",
              "  222472.953125,\n",
              "  221708.203125,\n",
              "  224558.046875,\n",
              "  222327.171875,\n",
              "  222223.484375,\n",
              "  219671.984375,\n",
              "  219172.390625,\n",
              "  223735.859375,\n",
              "  221232.640625,\n",
              "  218817.265625,\n",
              "  221081.734375,\n",
              "  218861.453125,\n",
              "  222815.0,\n",
              "  221337.5625,\n",
              "  220794.140625,\n",
              "  223046.125,\n",
              "  220390.765625,\n",
              "  222419.890625,\n",
              "  218430.671875,\n",
              "  218136.421875,\n",
              "  220719.875,\n",
              "  215996.671875,\n",
              "  222393.75,\n",
              "  220327.453125,\n",
              "  219915.375,\n",
              "  217757.390625,\n",
              "  219582.625,\n",
              "  221868.1875,\n",
              "  217818.265625,\n",
              "  215465.296875,\n",
              "  219700.0,\n",
              "  219471.109375,\n",
              "  219923.921875,\n",
              "  221231.609375,\n",
              "  215420.0625,\n",
              "  217437.25,\n",
              "  221470.625,\n",
              "  217088.875,\n",
              "  215508.875,\n",
              "  219512.890625,\n",
              "  219137.796875,\n",
              "  217146.703125,\n",
              "  219128.25,\n",
              "  215344.5625,\n",
              "  216772.0625,\n",
              "  221008.9375,\n",
              "  220507.484375,\n",
              "  218593.3125,\n",
              "  218181.796875,\n",
              "  216659.140625,\n",
              "  220233.546875,\n",
              "  217280.390625,\n",
              "  218272.546875,\n",
              "  220221.046875,\n",
              "  218653.859375,\n",
              "  218363.640625,\n",
              "  218286.484375,\n",
              "  219548.578125,\n",
              "  215747.046875,\n",
              "  217687.828125,\n",
              "  218128.8125,\n",
              "  218862.296875,\n",
              "  216587.921875,\n",
              "  215782.109375,\n",
              "  216284.484375,\n",
              "  217916.375,\n",
              "  217557.125,\n",
              "  219801.859375,\n",
              "  218044.578125,\n",
              "  219223.828125,\n",
              "  216487.578125,\n",
              "  217810.078125,\n",
              "  218724.0,\n",
              "  215168.515625,\n",
              "  219931.4375,\n",
              "  215767.546875,\n",
              "  217939.625,\n",
              "  216665.171875,\n",
              "  219834.546875,\n",
              "  215694.0,\n",
              "  217141.625,\n",
              "  217113.015625,\n",
              "  219498.828125,\n",
              "  216559.671875,\n",
              "  215871.5,\n",
              "  216112.046875,\n",
              "  217510.9375,\n",
              "  215297.0625,\n",
              "  213916.234375,\n",
              "  217166.140625,\n",
              "  216316.765625,\n",
              "  215219.5625,\n",
              "  217235.484375,\n",
              "  216827.9375,\n",
              "  216653.578125,\n",
              "  216660.234375,\n",
              "  217922.484375,\n",
              "  216757.390625,\n",
              "  214540.171875,\n",
              "  217909.4375,\n",
              "  218852.75,\n",
              "  216686.265625,\n",
              "  218382.609375,\n",
              "  217383.828125,\n",
              "  217902.578125,\n",
              "  216504.0,\n",
              "  216778.640625,\n",
              "  216533.6875,\n",
              "  216129.390625,\n",
              "  217392.359375,\n",
              "  218732.171875,\n",
              "  216079.390625,\n",
              "  213968.296875,\n",
              "  217675.890625,\n",
              "  216924.078125,\n",
              "  216111.796875,\n",
              "  216592.75,\n",
              "  214888.640625,\n",
              "  218426.953125,\n",
              "  217731.875,\n",
              "  219782.328125,\n",
              "  215262.4375,\n",
              "  216597.921875,\n",
              "  217144.3125,\n",
              "  217799.515625,\n",
              "  216610.671875,\n",
              "  214533.453125,\n",
              "  215664.828125,\n",
              "  217598.9375,\n",
              "  214271.328125,\n",
              "  217906.265625,\n",
              "  216392.125,\n",
              "  217425.1875,\n",
              "  216170.390625,\n",
              "  218755.25,\n",
              "  214909.125,\n",
              "  217257.046875,\n",
              "  214942.0,\n",
              "  214958.921875,\n",
              "  217460.5,\n",
              "  217736.046875,\n",
              "  220220.0,\n",
              "  218859.890625,\n",
              "  218602.578125,\n",
              "  218258.875,\n",
              "  215899.546875,\n",
              "  216617.640625,\n",
              "  218675.890625,\n",
              "  219838.375,\n",
              "  216386.359375,\n",
              "  215509.234375,\n",
              "  218490.796875,\n",
              "  220001.125,\n",
              "  215033.328125,\n",
              "  218370.359375,\n",
              "  219911.484375,\n",
              "  216731.921875,\n",
              "  217258.703125,\n",
              "  220037.578125,\n",
              "  217509.484375,\n",
              "  217260.375,\n",
              "  216876.6875,\n",
              "  216409.953125,\n",
              "  219083.046875,\n",
              "  215673.203125,\n",
              "  217346.515625,\n",
              "  220400.734375,\n",
              "  215787.796875,\n",
              "  216718.578125,\n",
              "  219532.3125,\n",
              "  215623.3125,\n",
              "  219065.828125,\n",
              "  219396.125,\n",
              "  216014.625,\n",
              "  216645.25,\n",
              "  217863.0,\n",
              "  219382.046875,\n",
              "  216712.078125,\n",
              "  219715.890625,\n",
              "  218561.328125,\n",
              "  221332.765625,\n",
              "  218532.796875,\n",
              "  218267.453125,\n",
              "  218467.953125,\n",
              "  216681.1875,\n",
              "  221680.125,\n",
              "  218903.296875,\n",
              "  222107.9375,\n",
              "  218716.515625,\n",
              "  218254.453125,\n",
              "  216684.6875,\n",
              "  220494.078125,\n",
              "  220802.046875,\n",
              "  217059.359375,\n",
              "  218749.875,\n",
              "  222411.375,\n",
              "  224427.9375,\n",
              "  217281.0,\n",
              "  220618.296875,\n",
              "  221383.359375,\n",
              "  219021.703125,\n",
              "  221167.375,\n",
              "  221238.546875,\n",
              "  221308.953125,\n",
              "  217589.453125,\n",
              "  219263.9375,\n",
              "  223144.984375,\n",
              "  219953.5,\n",
              "  223462.3125,\n",
              "  219953.625,\n",
              "  221827.921875,\n",
              "  222122.796875,\n",
              "  220383.125,\n",
              "  221769.390625,\n",
              "  223744.0,\n",
              "  218473.109375,\n",
              "  222606.0625,\n",
              "  222677.125,\n",
              "  222473.6875,\n",
              "  218469.703125,\n",
              "  222970.4375,\n",
              "  223022.703125,\n",
              "  222156.546875,\n",
              "  220591.0,\n",
              "  222079.078125,\n",
              "  223365.609375,\n",
              "  220960.546875,\n",
              "  223545.75,\n",
              "  221122.5,\n",
              "  219560.25,\n",
              "  224268.75,\n",
              "  223143.875,\n",
              "  224961.125,\n",
              "  219595.140625,\n",
              "  221642.296875,\n",
              "  222942.078125,\n",
              "  226455.890625,\n",
              "  225410.5,\n",
              "  220088.75,\n",
              "  224367.390625,\n",
              "  221472.5,\n",
              "  223029.984375,\n",
              "  224357.171875,\n",
              "  223842.421875,\n",
              "  222089.390625,\n",
              "  225593.625,\n",
              "  226576.875,\n",
              "  222972.234375,\n",
              "  221241.125,\n",
              "  227554.671875,\n",
              "  222243.234375,\n",
              "  224054.625,\n",
              "  223435.921875,\n",
              "  224733.109375,\n",
              "  225886.8125,\n",
              "  221917.875,\n",
              "  226098.1875],\n",
              " 'val_mae': [557.8768920898438,\n",
              "  574.080322265625,\n",
              "  541.9509887695312,\n",
              "  564.5291137695312,\n",
              "  544.635986328125,\n",
              "  554.4257202148438,\n",
              "  561.8256225585938,\n",
              "  562.0643920898438,\n",
              "  571.5724487304688,\n",
              "  551.6743774414062,\n",
              "  550.9915161132812,\n",
              "  559.0215454101562,\n",
              "  558.060302734375,\n",
              "  546.9002075195312,\n",
              "  539.7883911132812,\n",
              "  547.5447387695312,\n",
              "  555.3037109375,\n",
              "  556.947998046875,\n",
              "  564.4360961914062,\n",
              "  545.1028442382812,\n",
              "  539.7879028320312,\n",
              "  543.7213745117188,\n",
              "  538.68408203125,\n",
              "  559.235107421875,\n",
              "  549.816650390625,\n",
              "  549.71044921875,\n",
              "  548.7276000976562,\n",
              "  530.873779296875,\n",
              "  538.3572387695312,\n",
              "  540.0020141601562,\n",
              "  546.01318359375,\n",
              "  534.283935546875,\n",
              "  544.64501953125,\n",
              "  552.3991088867188,\n",
              "  534.5403442382812,\n",
              "  540.272705078125,\n",
              "  542.19287109375,\n",
              "  550.2311401367188,\n",
              "  542.2904663085938,\n",
              "  531.1353149414062,\n",
              "  538.8989868164062,\n",
              "  527.7241821289062,\n",
              "  530.8257446289062,\n",
              "  517.9234008789062,\n",
              "  544.8991088867188,\n",
              "  522.9760131835938,\n",
              "  543.5390014648438,\n",
              "  534.3955078125,\n",
              "  522.686279296875,\n",
              "  511.5375061035156,\n",
              "  532.650634765625,\n",
              "  522.928466796875,\n",
              "  521.9778442382812,\n",
              "  530.0330810546875,\n",
              "  539.8272094726562,\n",
              "  539.7062377929688,\n",
              "  506.7452087402344,\n",
              "  529.344970703125,\n",
              "  526.6455688476562,\n",
              "  515.2044067382812,\n",
              "  523.246826171875,\n",
              "  505.679443359375,\n",
              "  526.212158203125,\n",
              "  523.512939453125,\n",
              "  523.1035766601562,\n",
              "  521.4927368164062,\n",
              "  491.5035095214844,\n",
              "  529.2526245117188,\n",
              "  528.8389282226562,\n",
              "  508.681884765625,\n",
              "  506.2513732910156,\n",
              "  509.0826110839844,\n",
              "  508.391357421875,\n",
              "  535.650146484375,\n",
              "  492.507568359375,\n",
              "  513.3539428710938,\n",
              "  494.8641662597656,\n",
              "  512.0130004882812,\n",
              "  523.5675659179688,\n",
              "  482.7110290527344,\n",
              "  503.2845153808594,\n",
              "  502.8805236816406,\n",
              "  508.626953125,\n",
              "  511.7367248535156,\n",
              "  507.2742004394531,\n",
              "  517.3392944335938,\n",
              "  480.0051574707031,\n",
              "  518.0321655273438,\n",
              "  506.5830993652344,\n",
              "  482.6426696777344,\n",
              "  504.9607849121094,\n",
              "  506.0303649902344,\n",
              "  495.1455993652344,\n",
              "  492.4261779785156,\n",
              "  483.3065490722656,\n",
              "  493.1040954589844,\n",
              "  500.892822265625,\n",
              "  500.492919921875,\n",
              "  491.0048828125,\n",
              "  500.884765625,\n",
              "  481.2887268066406,\n",
              "  498.0625915527344,\n",
              "  497.103759765625,\n",
              "  498.1748962402344,\n",
              "  485.0001525878906,\n",
              "  497.1109313964844,\n",
              "  496.7247619628906,\n",
              "  502.4586486816406,\n",
              "  492.765625,\n",
              "  503.1376037597656,\n",
              "  493.7413024902344,\n",
              "  501.225830078125,\n",
              "  500.8258361816406,\n",
              "  489.0252685546875,\n",
              "  488.9990234375,\n",
              "  488.0389709472656,\n",
              "  485.0575256347656,\n",
              "  488.7276306152344,\n",
              "  488.0492248535156,\n",
              "  483.31005859375,\n",
              "  472.446044921875,\n",
              "  473.2281188964844,\n",
              "  502.2455139160156,\n",
              "  480.6800231933594,\n",
              "  482.6305236816406,\n",
              "  480.2578125,\n",
              "  492.6613464355469,\n",
              "  471.1446533203125,\n",
              "  464.1930236816406,\n",
              "  478.9161376953125,\n",
              "  472.0034484863281,\n",
              "  471.8401794433594,\n",
              "  458.8517150878906,\n",
              "  479.4905700683594,\n",
              "  458.208251953125,\n",
              "  461.5862121582031,\n",
              "  478.5191345214844,\n",
              "  465.8306884765625,\n",
              "  471.5732421875,\n",
              "  479.6619567871094,\n",
              "  485.7352294921875,\n",
              "  457.9952087402344,\n",
              "  468.1326599121094,\n",
              "  476.2281799316406,\n",
              "  477.906005859375,\n",
              "  444.1686706542969,\n",
              "  485.7025451660156,\n",
              "  468.3970947265625,\n",
              "  482.8330993652344,\n",
              "  492.9416198730469,\n",
              "  484.3551940917969,\n",
              "  484.0158386230469,\n",
              "  464.7544860839844,\n",
              "  472.69921875,\n",
              "  462.2193603515625,\n",
              "  461.528076171875,\n",
              "  480.3032531738281,\n",
              "  473.3381652832031,\n",
              "  452.25537109375,\n",
              "  472.88134765625,\n",
              "  478.6995544433594,\n",
              "  459.6829528808594,\n",
              "  471.30126953125,\n",
              "  457.0208435058594,\n",
              "  477.3619689941406,\n",
              "  454.9892578125,\n",
              "  450.941162109375,\n",
              "  461.0833435058594,\n",
              "  461.0833435058594,\n",
              "  471.7463073730469,\n",
              "  463.3958435058594,\n",
              "  469.6864929199219,\n",
              "  461.0833435058594,\n",
              "  451.0395812988281,\n",
              "  467.6129455566406,\n",
              "  461.0833435058594,\n",
              "  469.3350830078125,\n",
              "  471.63330078125,\n",
              "  453.1411437988281,\n",
              "  467.2616271972656,\n",
              "  457.3021240234375,\n",
              "  461.0833435058594,\n",
              "  469.2508239746094,\n",
              "  469.2369079589844,\n",
              "  471.535400390625,\n",
              "  460.7187805175781,\n",
              "  470.9446105957031,\n",
              "  467.4306335449219,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  476.9131774902344,\n",
              "  469.40625,\n",
              "  476.8573303222656,\n",
              "  463.6770935058594,\n",
              "  471.3952941894531,\n",
              "  453.6603088378906,\n",
              "  469.3359375,\n",
              "  470.790771484375,\n",
              "  449.3588562011719,\n",
              "  461.0833435058594,\n",
              "  467.52978515625,\n",
              "  486.5372009277344,\n",
              "  487.0577392578125,\n",
              "  469.51904296875,\n",
              "  461.6458435058594,\n",
              "  441.8924865722656,\n",
              "  476.46484375,\n",
              "  451.1512756347656,\n",
              "  468.8870544433594,\n",
              "  459.0520935058594,\n",
              "  469.4216003417969,\n",
              "  469.1258850097656,\n",
              "  476.8597106933594,\n",
              "  453.9125061035156,\n",
              "  468.8028869628906,\n",
              "  451.9093322753906,\n",
              "  478.7776184082031,\n",
              "  451.9377746582031,\n",
              "  478.15869140625,\n",
              "  447.622314453125,\n",
              "  461.0,\n",
              "  468.4231872558594,\n",
              "  461.0833435058594,\n",
              "  468.957763671875,\n",
              "  452.035400390625,\n",
              "  459.6145324707031,\n",
              "  466.6031799316406,\n",
              "  448.0147399902344,\n",
              "  468.6070861816406,\n",
              "  461.6458435058594,\n",
              "  478.106201171875,\n",
              "  448.0703430175781,\n",
              "  449.8348083496094,\n",
              "  478.0210876464844,\n",
              "  485.1521911621094,\n",
              "  469.0713195800781,\n",
              "  457.0208435058594,\n",
              "  466.7307434082031,\n",
              "  469.0290222167969,\n",
              "  459.0520935058594,\n",
              "  461.6458740234375,\n",
              "  468.4246520996094,\n",
              "  461.6458435058594,\n",
              "  450.2698669433594,\n",
              "  475.9640808105469,\n",
              "  468.9313659667969,\n",
              "  468.9172668457031,\n",
              "  470.6531066894531,\n",
              "  466.2954406738281,\n",
              "  454.1349792480469,\n",
              "  468.2992248535156,\n",
              "  470.5978698730469,\n",
              "  452.145263671875,\n",
              "  457.3021240234375,\n",
              "  484.3133850097656,\n",
              "  461.3645935058594,\n",
              "  468.4962158203125,\n",
              "  468.2012023925781,\n",
              "  468.1871032714844,\n",
              "  468.1731262207031,\n",
              "  459.3333435058594,\n",
              "  477.2384338378906,\n",
              "  470.72509765625,\n",
              "  445.5462951660156,\n",
              "  468.3848571777344,\n",
              "  475.0953674316406,\n",
              "  477.0987854003906,\n",
              "  466.311279296875,\n",
              "  474.7301330566406,\n",
              "  477.5770568847656,\n",
              "  476.986572265625,\n",
              "  483.8808898925781,\n",
              "  468.5541076660156,\n",
              "  465.9465637207031,\n",
              "  454.4840087890625,\n",
              "  474.5357360839844,\n",
              "  461.3645935058594,\n",
              "  470.2346496582031,\n",
              "  459.6145935058594,\n",
              "  454.8346862792969,\n",
              "  468.1616516113281,\n",
              "  450.80029296875,\n",
              "  454.8767395019531,\n",
              "  474.312255859375,\n",
              "  468.1055603027344,\n",
              "  476.2867126464844,\n",
              "  459.6145935058594,\n",
              "  482.9296569824219,\n",
              "  457.3020935058594,\n",
              "  457.3020935058594,\n",
              "  468.3023681640625,\n",
              "  455.0033264160156,\n",
              "  470.0244445800781,\n",
              "  470.2918395996094,\n",
              "  461.6458740234375,\n",
              "  461.28125,\n",
              "  465.625244140625,\n",
              "  461.6458435058594,\n",
              "  469.9410400390625,\n",
              "  461.0833435058594,\n",
              "  465.5692443847656,\n",
              "  467.5865783691406,\n",
              "  446.635986328125,\n",
              "  467.55859375,\n",
              "  448.809326171875,\n",
              "  476.2906188964844,\n",
              "  463.6771240234375,\n",
              "  475.6731872558594,\n",
              "  466.0206298828125,\n",
              "  454.9726867675781,\n",
              "  467.4612731933594,\n",
              "  475.8421325683594,\n",
              "  469.18310546875,\n",
              "  467.4192199707031,\n",
              "  469.7179260253906,\n",
              "  453.3064880371094,\n",
              "  467.3775329589844,\n",
              "  459.0520935058594,\n",
              "  459.6145935058594,\n",
              "  473.5889892578125,\n",
              "  459.3333435058594,\n",
              "  462.75,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  459.6145324707031,\n",
              "  469.8466491699219,\n",
              "  473.6759948730469,\n",
              "  475.3150939941406,\n",
              "  453.2047424316406,\n",
              "  469.2292175292969,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  463.3125,\n",
              "  457.3020935058594,\n",
              "  466.846435546875,\n",
              "  451.5528869628906,\n",
              "  457.0208435058594,\n",
              "  459.0520935058594,\n",
              "  474.8111572265625,\n",
              "  465.0272521972656,\n",
              "  467.0445861816406,\n",
              "  468.6970520019531,\n",
              "  467.2978820800781,\n",
              "  459.3333435058594,\n",
              "  457.4072265625,\n",
              "  467.2557678222656,\n",
              "  453.7373046875,\n",
              "  467.2279357910156,\n",
              "  461.3645935058594,\n",
              "  463.3958435058594,\n",
              "  472.7281494140625,\n",
              "  467.454345703125,\n",
              "  469.1904296875,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  467.1177673339844,\n",
              "  468.8540954589844,\n",
              "  463.6770935058594,\n",
              "  465.3260192871094,\n",
              "  459.0520935058594,\n",
              "  472.7322082519531,\n",
              "  479.8429260253906,\n",
              "  467.0205993652344,\n",
              "  459.0520935058594,\n",
              "  459.6145935058594,\n",
              "  455.7508239746094,\n",
              "  466.6833190917969,\n",
              "  461.0833435058594,\n",
              "  468.6868896484375,\n",
              "  466.9229431152344,\n",
              "  479.1854248046875,\n",
              "  466.8953857421875,\n",
              "  464.8502502441406,\n",
              "  451.7989807128906,\n",
              "  466.5724792480469,\n",
              "  455.8890380859375,\n",
              "  472.0064392089844,\n",
              "  459.3333435058594,\n",
              "  468.8294982910156,\n",
              "  464.7533264160156,\n",
              "  466.770751953125,\n",
              "  463.1145935058594,\n",
              "  473.8709411621094,\n",
              "  466.4479064941406,\n",
              "  444.850341796875,\n",
              "  459.3333435058594,\n",
              "  473.4808044433594,\n",
              "  457.5833435058594,\n",
              "  461.3645935058594,\n",
              "  466.6465759277344,\n",
              "  454.346435546875,\n",
              "  466.33740234375,\n",
              "  459.0521240234375,\n",
              "  464.2789306640625,\n",
              "  452.3701477050781,\n",
              "  466.2823791503906,\n",
              "  473.204345703125,\n",
              "  466.2547607421875,\n",
              "  465.9597473144531,\n",
              "  459.0520935058594,\n",
              "  454.2031555175781,\n",
              "  461.6458435058594,\n",
              "  452.1999816894531,\n",
              "  466.7340393066406,\n",
              "  461.3645935058594,\n",
              "  464.3938293457031,\n",
              "  466.4112854003906,\n",
              "  447.5177307128906,\n",
              "  454.3144836425781,\n",
              "  461.3645935058594,\n",
              "  466.3561096191406,\n",
              "  457.3020324707031,\n",
              "  473.0423583984375,\n",
              "  470.7024841308594,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  477.5603332519531,\n",
              "  454.7187805175781,\n",
              "  454.7331237792969,\n",
              "  473.1310729980469,\n",
              "  465.9370422363281,\n",
              "  477.3531799316406,\n",
              "  454.5070495605469,\n",
              "  454.5204162597656,\n",
              "  461.6458435058594,\n",
              "  456.8602294921875,\n",
              "  465.5738220214844,\n",
              "  472.9119873046875,\n",
              "  473.16552734375,\n",
              "  461.0833435058594,\n",
              "  466.08056640625,\n",
              "  470.20654296875,\n",
              "  461.6457824707031,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  454.686279296875,\n",
              "  457.0127258300781,\n",
              "  459.3333435058594,\n",
              "  470.5754699707031,\n",
              "  467.9874267578125,\n",
              "  476.5677185058594,\n",
              "  465.3663024902344,\n",
              "  452.75146484375,\n",
              "  467.8495178222656,\n",
              "  457.1227111816406,\n",
              "  455.105224609375,\n",
              "  465.5790710449219,\n",
              "  459.3333740234375,\n",
              "  461.0833435058594,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  463.760009765625,\n",
              "  465.49609375,\n",
              "  467.1492614746094,\n",
              "  476.2703857421875,\n",
              "  471.857421875,\n",
              "  457.2881774902344,\n",
              "  461.6458435058594,\n",
              "  465.413330078125,\n",
              "  461.0,\n",
              "  461.0833435058594,\n",
              "  475.7001037597656,\n",
              "  453.308349609375,\n",
              "  469.6054992675781,\n",
              "  455.0858154296875,\n",
              "  471.86376953125,\n",
              "  457.0208740234375,\n",
              "  463.259033203125,\n",
              "  465.8385925292969,\n",
              "  457.4664001464844,\n",
              "  467.5615234375,\n",
              "  461.0833740234375,\n",
              "  467.2525634765625,\n",
              "  469.0506896972656,\n",
              "  451.1602478027344,\n",
              "  465.7423400878906,\n",
              "  455.2503967285156,\n",
              "  459.3333435058594,\n",
              "  463.3958435058594,\n",
              "  459.0520935058594,\n",
              "  459.0520935058594,\n",
              "  467.1277160644531,\n",
              "  470.551513671875,\n",
              "  464.7044372558594,\n",
              "  465.0555419921875,\n",
              "  463.0105895996094,\n",
              "  468.692138671875,\n",
              "  470.9775390625,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  461.0833435058594,\n",
              "  468.8354797363281,\n",
              "  459.3333435058594,\n",
              "  471.0936584472656,\n",
              "  465.1998596191406,\n",
              "  464.9049072265625,\n",
              "  465.4540100097656,\n",
              "  461.3645935058594,\n",
              "  461.0833435058594,\n",
              "  466.798095703125,\n",
              "  463.0863952636719,\n",
              "  461.3645935058594,\n",
              "  457.0208435058594,\n",
              "  453.8724060058594,\n",
              "  465.3432922363281,\n",
              "  459.3333435058594,\n",
              "  474.6875915527344,\n",
              "  457.7080993652344,\n",
              "  464.7265625,\n",
              "  470.3736572265625,\n",
              "  453.6866760253906,\n",
              "  456.0129699707031,\n",
              "  456.0262756347656,\n",
              "  462.6266174316406,\n",
              "  468.4862365722656,\n",
              "  450.4884338378906,\n",
              "  459.0521240234375,\n",
              "  458.1258850097656,\n",
              "  464.8713073730469,\n",
              "  458.1531066894531,\n",
              "  459.6145935058594,\n",
              "  458.1802673339844,\n",
              "  464.8163146972656,\n",
              "  464.5213317871094,\n",
              "  459.0520935058594,\n",
              "  469.936279296875,\n",
              "  453.9050598144531,\n",
              "  465.029052734375,\n",
              "  461.3645324707031,\n",
              "  454.2272033691406,\n",
              "  453.8765869140625,\n",
              "  456.2855224609375,\n",
              "  456.2994689941406,\n",
              "  458.06298828125,\n",
              "  467.9397888183594,\n",
              "  472.9365539550781,\n",
              "  464.3439025878906,\n",
              "  456.3675231933594,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  464.5705871582031,\n",
              "  459.3333435058594,\n",
              "  472.3694763183594,\n",
              "  466.2798767089844,\n",
              "  465.9013671875,\n",
              "  451.3080139160156,\n",
              "  462.7384033203125,\n",
              "  456.5046081542969,\n",
              "  469.0268249511719,\n",
              "  459.6145935058594,\n",
              "  460.3260803222656,\n",
              "  459.3333435058594,\n",
              "  466.437744140625,\n",
              "  459.3333435058594,\n",
              "  465.8480529785156,\n",
              "  472.1173095703125,\n",
              "  464.071044921875,\n",
              "  455.697021484375,\n",
              "  461.3645935058594,\n",
              "  458.6988525390625,\n",
              "  469.2622985839844,\n",
              "  459.0520935058594,\n",
              "  454.312744140625,\n",
              "  456.7217102050781,\n",
              "  466.5600280761719,\n",
              "  461.9176330566406,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  463.03125,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  456.5506591796875,\n",
              "  456.1065368652344,\n",
              "  461.8081970214844,\n",
              "  464.106689453125,\n",
              "  462.3431701660156,\n",
              "  461.4859924316406,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  456.9403381347656,\n",
              "  461.7129211425781,\n",
              "  468.1275939941406,\n",
              "  468.6637878417969,\n",
              "  459.0257873535156,\n",
              "  457.3020935058594,\n",
              "  468.302001953125,\n",
              "  459.6145935058594,\n",
              "  457.3020935058594,\n",
              "  450.4447937011719,\n",
              "  459.3333435058594,\n",
              "  465.8553161621094,\n",
              "  454.5889892578125,\n",
              "  452.7716979980469,\n",
              "  459.3333435058594,\n",
              "  454.4653625488281,\n",
              "  463.822509765625,\n",
              "  465.5592041015625,\n",
              "  468.5384826660156,\n",
              "  465.81298828125,\n",
              "  465.7992248535156,\n",
              "  452.8031311035156,\n",
              "  455.2073059082031,\n",
              "  461.6458435058594,\n",
              "  461.4015808105469,\n",
              "  465.7315979003906,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  463.6595764160156,\n",
              "  461.3645324707031,\n",
              "  463.6770324707031,\n",
              "  461.30712890625,\n",
              "  465.0745544433594,\n",
              "  465.34228515625,\n",
              "  455.3690185546875,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  457.4411315917969,\n",
              "  459.3332824707031,\n",
              "  464.9794921875,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  455.4034118652344,\n",
              "  462.8944091796875,\n",
              "  455.2232360839844,\n",
              "  461.0833435058594,\n",
              "  465.1877136230469,\n",
              "  459.3333435058594,\n",
              "  462.8269958496094,\n",
              "  459.6145935058594,\n",
              "  457.6167907714844,\n",
              "  459.3333435058594,\n",
              "  455.6728820800781,\n",
              "  463.0403747558594,\n",
              "  459.0520935058594,\n",
              "  461.5446472167969,\n",
              "  463.2808532714844,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  463.5220031738281,\n",
              "  460.9151306152344,\n",
              "  465.0638732910156,\n",
              "  462.9198303222656,\n",
              "  455.7601318359375,\n",
              "  461.0833435058594,\n",
              "  461.3646240234375,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  457.5780334472656,\n",
              "  463.1063537597656,\n",
              "  459.3333435058594,\n",
              "  464.5485534667969,\n",
              "  457.5833435058594,\n",
              "  456.2379455566406,\n",
              "  457.9401550292969,\n",
              "  459.7034912109375,\n",
              "  457.3020935058594,\n",
              "  461.0,\n",
              "  457.9936218261719,\n",
              "  461.0833435058594,\n",
              "  464.989501953125,\n",
              "  457.7525939941406,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  459.3333740234375,\n",
              "  460.1187438964844,\n",
              "  459.25,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  458.1415100097656,\n",
              "  461.073974609375,\n",
              "  461.0606994628906,\n",
              "  464.2295837402344,\n",
              "  460.4712829589844,\n",
              "  459.0520935058594,\n",
              "  462.4759521484375,\n",
              "  459.3333435058594,\n",
              "  462.4493103027344,\n",
              "  462.7171936035156,\n",
              "  460.0256652832031,\n",
              "  465.7664489746094,\n",
              "  457.3020935058594,\n",
              "  464.6951599121094,\n",
              "  458.3284606933594,\n",
              "  461.6458435058594,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  464.6276550292969,\n",
              "  461.3645935058594,\n",
              "  462.569580078125,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  457.3020935058594,\n",
              "  462.7966613769531,\n",
              "  464.5328369140625,\n",
              "  459.0520935058594,\n",
              "  461.0833435058594,\n",
              "  460.4302062988281,\n",
              "  465.5622863769531,\n",
              "  459.0520935058594,\n",
              "  462.1394958496094,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  459.6145935058594,\n",
              "  461.0833435058594,\n",
              "  460.0289306640625,\n",
              "  461.7657165527344,\n",
              "  459.0520935058594,\n",
              "  461.3645935058594,\n",
              "  459.9759216308594,\n",
              "  461.0833435058594,\n",
              "  456.6866149902344,\n",
              "  457.8484191894531,\n",
              "  459.9057922363281,\n",
              "  459.3333435058594,\n",
              "  461.3646240234375,\n",
              "  462.1943664550781,\n",
              "  459.25,\n",
              "  461.8863525390625,\n",
              "  461.3645935058594,\n",
              "  463.3958435058594,\n",
              "  461.3645935058594,\n",
              "  458.8642883300781,\n",
              "  465.1509704589844,\n",
              "  459.7761535644531,\n",
              "  459.3333435058594,\n",
              "  461.0833435058594,\n",
              "  462.04833984375,\n",
              "  464.173828125,\n",
              "  462.021484375,\n",
              "  456.29638671875,\n",
              "  462.3439636230469,\n",
              "  462.3175048828125,\n",
              "  459.010986328125,\n",
              "  464.6056213378906,\n",
              "  463.4113464355469,\n",
              "  463.6790466308594,\n",
              "  463.3846740722656,\n",
              "  459.5903015136719,\n",
              "  461.8897705078125,\n",
              "  461.8760986328125,\n",
              "  459.11669921875,\n",
              "  459.1296691894531,\n",
              "  462.1173400878906,\n",
              "  461.2600402832031,\n",
              "  464.286376953125,\n",
              "  462.2290344238281,\n",
              "  462.2019348144531,\n",
              "  459.7388610839844,\n",
              "  459.2227478027344,\n",
              "  463.7749938964844,\n",
              "  461.3645935058594,\n",
              "  459.0520935058594,\n",
              "  461.3645935058594,\n",
              "  459.2890930175781,\n",
              "  463.1145935058594,\n",
              "  461.9460754394531,\n",
              "  461.3645935058594,\n",
              "  463.670166015625,\n",
              "  461.0420227050781,\n",
              "  463.0808410644531,\n",
              "  459.3333435058594,\n",
              "  459.0520935058594,\n",
              "  461.7803649902344,\n",
              "  457.1072692871094,\n",
              "  463.4774475097656,\n",
              "  461.5334777832031,\n",
              "  461.1131896972656,\n",
              "  459.0520324707031,\n",
              "  460.8711242675781,\n",
              "  463.2305908203125,\n",
              "  459.3333740234375,\n",
              "  457.0207824707031,\n",
              "  461.2881774902344,\n",
              "  461.0832824707031,\n",
              "  461.6962890625,\n",
              "  462.9070129394531,\n",
              "  457.2783508300781,\n",
              "  459.3333435058594,\n",
              "  463.3937683105469,\n",
              "  459.0372009277344,\n",
              "  457.611328125,\n",
              "  461.6458740234375,\n",
              "  461.2568054199219,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  457.6754455566406,\n",
              "  459.0520935058594,\n",
              "  463.3958435058594,\n",
              "  462.8514099121094,\n",
              "  461.0,\n",
              "  460.560791015625,\n",
              "  459.1618957519531,\n",
              "  462.7456970214844,\n",
              "  460.0101318359375,\n",
              "  460.8730773925781,\n",
              "  462.8912048339844,\n",
              "  461.4090576171875,\n",
              "  461.114990234375,\n",
              "  461.0832824707031,\n",
              "  462.2822570800781,\n",
              "  458.4820251464844,\n",
              "  460.480224609375,\n",
              "  461.04931640625,\n",
              "  461.9737548828125,\n",
              "  459.6145935058594,\n",
              "  458.6987609863281,\n",
              "  459.3333435058594,\n",
              "  460.9844665527344,\n",
              "  460.5782775878906,\n",
              "  462.989990234375,\n",
              "  461.2265930175781,\n",
              "  462.4010009765625,\n",
              "  459.7790832519531,\n",
              "  461.0833435058594,\n",
              "  461.8912658691406,\n",
              "  458.2862854003906,\n",
              "  463.3958435058594,\n",
              "  459.0520935058594,\n",
              "  461.3645935058594,\n",
              "  460.1514587402344,\n",
              "  463.3958435058594,\n",
              "  459.0520935058594,\n",
              "  460.507568359375,\n",
              "  460.4946594238281,\n",
              "  463.1145935058594,\n",
              "  459.8544616699219,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  461.0833435058594,\n",
              "  458.6671447753906,\n",
              "  457.3020935058594,\n",
              "  460.6730041503906,\n",
              "  459.6728820800781,\n",
              "  458.6152648925781,\n",
              "  461.0755615234375,\n",
              "  460.3392333984375,\n",
              "  460.3717346191406,\n",
              "  460.3854675292969,\n",
              "  461.6458740234375,\n",
              "  460.2872619628906,\n",
              "  458.111083984375,\n",
              "  461.6458435058594,\n",
              "  462.5617980957031,\n",
              "  460.0096130371094,\n",
              "  461.955810546875,\n",
              "  461.0833435058594,\n",
              "  461.6458435058594,\n",
              "  460.2318420410156,\n",
              "  460.5260314941406,\n",
              "  460.2579345703125,\n",
              "  459.4897766113281,\n",
              "  461.0833435058594,\n",
              "  462.4323425292969,\n",
              "  459.41162109375,\n",
              "  457.3020935058594,\n",
              "  461.3645935058594,\n",
              "  460.6296081542969,\n",
              "  459.6145935058594,\n",
              "  460.0428771972656,\n",
              "  458.3556213378906,\n",
              "  462.0482482910156,\n",
              "  461.3645935058594,\n",
              "  463.8313293457031,\n",
              "  458.6882629394531,\n",
              "  459.9657897949219,\n",
              "  460.7449645996094,\n",
              "  461.3645935058594,\n",
              "  459.9275817871094,\n",
              "  457.6025085449219,\n",
              "  458.7206726074219,\n",
              "  461.0833435058594,\n",
              "  457.3020935058594,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  460.8600769042969,\n",
              "  459.3333435058594,\n",
              "  462.1249084472656,\n",
              "  457.7683410644531,\n",
              "  460.3491516113281,\n",
              "  457.7423400878906,\n",
              "  457.7298583984375,\n",
              "  460.444580078125,\n",
              "  460.7001037597656,\n",
              "  463.6771240234375,\n",
              "  462.0223083496094,\n",
              "  461.728759765625,\n",
              "  461.3645935058594,\n",
              "  458.7146301269531,\n",
              "  459.3765563964844,\n",
              "  461.6766052246094,\n",
              "  463.0966796875,\n",
              "  459.0520935058594,\n",
              "  457.8497619628906,\n",
              "  461.3646240234375,\n",
              "  463.1145935058594,\n",
              "  457.25634765625,\n",
              "  461.141845703125,\n",
              "  462.821533203125,\n",
              "  459.1357727050781,\n",
              "  459.6145935058594,\n",
              "  462.9427490234375,\n",
              "  459.7733459472656,\n",
              "  459.4800109863281,\n",
              "  459.0520935058594,\n",
              "  458.2251281738281,\n",
              "  461.47265625,\n",
              "  457.3974304199219,\n",
              "  459.3333435058594,\n",
              "  462.9754943847656,\n",
              "  457.3592834472656,\n",
              "  458.2347717285156,\n",
              "  461.6458435058594,\n",
              "  457.0408935546875,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  457.2660217285156,\n",
              "  457.833740234375,\n",
              "  459.3333435058594,\n",
              "  461.1395263671875,\n",
              "  457.7270812988281,\n",
              "  461.3645935058594,\n",
              "  459.8021545410156,\n",
              "  463.2208557128906,\n",
              "  459.6573791503906,\n",
              "  459.3333435058594,\n",
              "  459.469482421875,\n",
              "  457.2688903808594,\n",
              "  463.28466796875,\n",
              "  459.7133483886719,\n",
              "  463.5906677246094,\n",
              "  459.4073181152344,\n",
              "  458.8926696777344,\n",
              "  456.8865966796875,\n",
              "  461.3645935058594,\n",
              "  461.5987854003906,\n",
              "  457.0419921875,\n",
              "  459.0492248535156,\n",
              "  463.4090576171875,\n",
              "  465.7349853515625,\n",
              "  456.9821472167969,\n",
              "  461.03173828125,\n",
              "  461.7091979980469,\n",
              "  458.8994140625,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  456.7706604003906,\n",
              "  458.7765197753906,\n",
              "  463.3958435058594,\n",
              "  459.290771484375,\n",
              "  463.5699157714844,\n",
              "  459.1464538574219,\n",
              "  461.3645935058594,\n",
              "  461.5766906738281,\n",
              "  459.3333435058594,\n",
              "  461.0833435058594,\n",
              "  463.3958435058594,\n",
              "  456.7590026855469,\n",
              "  461.6458435058594,\n",
              "  461.651123046875,\n",
              "  461.3645935058594,\n",
              "  456.42822265625,\n",
              "  461.689208984375,\n",
              "  461.6458435058594,\n",
              "  460.73486328125,\n",
              "  458.6112060546875,\n",
              "  459.9875793457031,\n",
              "  461.6458435058594,\n",
              "  458.57080078125,\n",
              "  461.6458435058594,\n",
              "  458.5455322265625,\n",
              "  456.5852355957031,\n",
              "  462.0937194824219,\n",
              "  460.9043884277344,\n",
              "  462.5926818847656,\n",
              "  456.2539978027344,\n",
              "  458.6184997558594,\n",
              "  459.8428039550781,\n",
              "  464.1999206542969,\n",
              "  463.1145935058594,\n",
              "  456.2070617675781,\n",
              "  461.3645935058594,\n",
              "  457.9078063964844,\n",
              "  459.3333435058594,\n",
              "  461.0833435058594,\n",
              "  459.9420471191406,\n",
              "  458.1502990722656,\n",
              "  461.9976501464844,\n",
              "  463.3958435058594,\n",
              "  458.6766357421875,\n",
              "  456.3516540527344,\n",
              "  464.0767517089844,\n",
              "  457.6656188964844,\n",
              "  459.3333435058594,\n",
              "  458.6160583496094,\n",
              "  460.3529052734375,\n",
              "  461.3645935058594,\n",
              "  456.2663269042969,\n",
              "  461.3645935058594]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "cd552de6-9ea0-44a1-9c9d-fa5c378d4e06"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/regression.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/regression.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8fac5158-26af-4d31-bb65-7df86fa83a95"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_41480b31-2af0-4383-bdce-b4f9db968cce\", \"regression.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrain"
      ],
      "metadata": {
        "id": "owI9dHky6eHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/new/regression.h5')\n",
        "\n",
        "# from efficientnet.layers import Swish, DropConnect\n",
        "# from efficientnet.model import ConvKernalInitializer\n",
        "# from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "# get_custom_objects().update({\n",
        "#     'ConvKernalInitializer': ConvKernalInitializer,\n",
        "#     'Swish': Swish,\n",
        "#     'DropConnect':DropConnect\n",
        "# })"
      ],
      "metadata": {
        "id": "bNX5VLJw54yc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load model \n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('/content/drive/My Drive/new/regression.h5')\n",
        "# height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "zF2vlFE_54wK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # set 'multiply_16' and following layers trainable (Unfreeze --> multiply_16 ) ให้เป็น layers ที่ train ชุดข้อมูลใหม่\n",
        "# model.trainable = True\n",
        "\n",
        "# set_trainable = False\n",
        "# for layer in conv_base.layers:\n",
        "#     if layer.name == 'multiply_15':\n",
        "#         set_trainable = True\n",
        "#     if set_trainable:\n",
        "#         layer.trainable = True\n",
        "#     else:\n",
        "#         layer.trainable = False\n",
        "# print('This is the number of trainable layers '\n",
        "#       'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "1OsX50Tk54tx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "pzptFLAz54rW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss='mse',\n",
        "#               optimizer=Adam(learning_rate=2e-1),\n",
        "#               metrics=['mae'])\n",
        "# history = model.fit_generator(\n",
        "#       train_generator,\n",
        "#       steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "#       epochs=epochs,\n",
        "#       validation_data=validation_generator,\n",
        "#       validation_steps= NUM_TEST //batch_size,\n",
        "#       verbose=1,\n",
        "#       use_multiprocessing=True,\n",
        "#       workers=4)"
      ],
      "metadata": {
        "id": "5os97-2Q54o2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "# mae = history.history['mae']\n",
        "# val_mae = history.history['val_mae']\n",
        "\n",
        "# epochs_x = range(len(loss))\n",
        "\n",
        "# plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "# plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "# plt.title('Training and Mean squared error')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "# plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Z-9GR-Qp6lE3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW_313T26lCd"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}