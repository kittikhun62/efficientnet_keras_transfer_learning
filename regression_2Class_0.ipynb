{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvOsEpxoRF+A8DYGo5eu+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "55a76127-05a8-4ab6-b93f-a046cabeaea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "584a4805-10c2-4c9a-8c9e-c5b45455dbdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d90ebc4-ba4e-4998-bd9b-dd549b5b3f50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d90ebc4-ba4e-4998-bd9b-dd549b5b3f50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d90ebc4-ba4e-4998-bd9b-dd549b5b3f50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d90ebc4-ba4e-4998-bd9b-dd549b5b3f50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "1d529f42-7a9f-459b-ac6d-058e56018b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "8a5170a6-af8a-469f-e95c-aa17770737e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "c2e60a31-e55c-41e0-8260-ef48d1b10f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "41926d3b-82f0-4287-97a9-23f5af71a25e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "d2517b09-efe0-4c4f-b236-5fa101195200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 18.74 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "outputId": "3ea19def-2322-4336-ac11-9b6b5a1087a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "04ebca5b-1cbd-4e80-cac4-341a6fe6ced4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "a864ac71-05e5-49fa-a72c-ac30f4fc0b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "f5e7a30b-a9ab-49c9-f7d0-144c688757da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "a7d47a27-97ef-400d-95ce-cf804b1f59b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=2e-2),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3c8753-d54b-40c7-8cfd-2f08823a11fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-28c75c78f62a>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 144s 3s/step - loss: 1514655.2500 - mae: 1007.2507 - val_loss: 520760.2812 - val_mae: 549.7438\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1499907.6250 - mae: 999.3065 - val_loss: 510891.5000 - val_mae: 544.2238\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1510851.6250 - mae: 1005.3285 - val_loss: 517560.0000 - val_mae: 546.8254\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1486000.6250 - mae: 995.8356 - val_loss: 529248.6250 - val_mae: 560.8005\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1488147.0000 - mae: 996.5640 - val_loss: 514770.2500 - val_mae: 548.2967\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1492266.5000 - mae: 995.6510 - val_loss: 551461.5000 - val_mae: 579.1321\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 70ms/step - loss: 1490570.7500 - mae: 994.8224 - val_loss: 519882.9688 - val_mae: 549.9996\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1479627.6250 - mae: 990.4570 - val_loss: 504934.6562 - val_mae: 537.4407\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 1491138.8750 - mae: 995.1667 - val_loss: 521126.6562 - val_mae: 553.5117\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 8s 204ms/step - loss: 1486623.3750 - mae: 992.6097 - val_loss: 497677.8750 - val_mae: 532.2151\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1480745.7500 - mae: 989.0745 - val_loss: 534643.6875 - val_mae: 563.6758\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1469009.0000 - mae: 982.8166 - val_loss: 520305.2812 - val_mae: 550.6226\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1476101.2500 - mae: 986.6421 - val_loss: 539775.4375 - val_mae: 568.9534\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1460271.6250 - mae: 980.6345 - val_loss: 492274.0000 - val_mae: 527.0811\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 1465507.6250 - mae: 981.2482 - val_loss: 523972.0938 - val_mae: 555.0190\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1474563.7500 - mae: 985.1324 - val_loss: 518177.4688 - val_mae: 547.7407\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1446695.6250 - mae: 979.1754 - val_loss: 520854.1562 - val_mae: 552.4467\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1474772.2500 - mae: 986.5471 - val_loss: 502610.0000 - val_mae: 533.8231\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1461580.5000 - mae: 984.5085 - val_loss: 505148.7188 - val_mae: 534.6956\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1440443.1250 - mae: 976.1420 - val_loss: 499474.1250 - val_mae: 530.6334\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1460667.3750 - mae: 980.5885 - val_loss: 493940.7188 - val_mae: 527.1763\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1441983.8750 - mae: 969.0306 - val_loss: 504744.0000 - val_mae: 536.7984\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1453496.0000 - mae: 976.5579 - val_loss: 499078.4688 - val_mae: 532.7295\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1444258.2500 - mae: 972.3196 - val_loss: 477285.1562 - val_mae: 512.6652\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1418206.5000 - mae: 962.7435 - val_loss: 475837.2500 - val_mae: 511.2511\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1441178.7500 - mae: 972.2122 - val_loss: 506731.1562 - val_mae: 539.5131\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1436243.1250 - mae: 970.1752 - val_loss: 517286.5000 - val_mae: 548.8343\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1427563.7500 - mae: 965.7701 - val_loss: 491462.5000 - val_mae: 521.4905\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1427555.7500 - mae: 965.0244 - val_loss: 473985.5938 - val_mae: 507.5509\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 1441077.5000 - mae: 971.8252 - val_loss: 492603.7188 - val_mae: 525.6204\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1428017.7500 - mae: 965.1386 - val_loss: 475036.0938 - val_mae: 506.7038\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1436140.8750 - mae: 967.9891 - val_loss: 489545.6562 - val_mae: 522.4502\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1418636.2500 - mae: 959.9507 - val_loss: 480090.3750 - val_mae: 512.2962\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1410827.3750 - mae: 958.0867 - val_loss: 486527.3750 - val_mae: 519.0447\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1417703.6250 - mae: 959.8750 - val_loss: 485032.5938 - val_mae: 517.6028\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1424022.5000 - mae: 963.3915 - val_loss: 459980.5000 - val_mae: 495.5006\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1431338.2500 - mae: 966.7517 - val_loss: 493211.9062 - val_mae: 525.1204\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1405583.7500 - mae: 953.7964 - val_loss: 492396.2500 - val_mae: 524.3492\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1401230.2500 - mae: 953.4102 - val_loss: 479174.9062 - val_mae: 512.4290\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1390016.5000 - mae: 949.3983 - val_loss: 481630.1562 - val_mae: 512.7783\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1388073.2500 - mae: 945.6280 - val_loss: 487921.7812 - val_mae: 520.0650\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1396928.8750 - mae: 950.8741 - val_loss: 494188.4062 - val_mae: 527.1109\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1392059.0000 - mae: 949.1426 - val_loss: 477273.3438 - val_mae: 508.5122\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1386859.0000 - mae: 945.6246 - val_loss: 475834.9688 - val_mae: 507.0959\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1383058.2500 - mae: 942.1024 - val_loss: 478201.2188 - val_mae: 512.3909\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1384532.3750 - mae: 943.8873 - val_loss: 473004.1562 - val_mae: 504.8234\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1364116.6250 - mae: 935.3255 - val_loss: 475325.5000 - val_mae: 509.8411\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1377550.7500 - mae: 939.3500 - val_loss: 447248.1562 - val_mae: 480.4899\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 1357483.2500 - mae: 936.5236 - val_loss: 468763.9688 - val_mae: 500.3407\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1385331.6250 - mae: 945.4033 - val_loss: 444514.2188 - val_mae: 477.3710\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1374512.7500 - mae: 940.1168 - val_loss: 458358.4062 - val_mae: 488.4544\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1368675.5000 - mae: 936.8480 - val_loss: 456989.5938 - val_mae: 487.0512\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1350416.5000 - mae: 930.3828 - val_loss: 436213.2188 - val_mae: 466.3308\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1355535.0000 - mae: 930.9390 - val_loss: 446591.9688 - val_mae: 480.4420\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1369216.3750 - mae: 938.7453 - val_loss: 463908.2812 - val_mae: 498.2484\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1355766.5000 - mae: 932.7045 - val_loss: 440063.2188 - val_mae: 475.8538\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1349434.7500 - mae: 925.6489 - val_loss: 457529.0000 - val_mae: 488.7129\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1361949.3750 - mae: 934.2164 - val_loss: 444988.7812 - val_mae: 476.5359\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1349767.8750 - mae: 928.4720 - val_loss: 443662.0938 - val_mae: 475.4147\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1343654.8750 - mae: 927.7582 - val_loss: 449608.7188 - val_mae: 482.8787\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1331952.7500 - mae: 919.9726 - val_loss: 448289.7188 - val_mae: 482.4254\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1342600.6250 - mae: 922.7110 - val_loss: 421391.0000 - val_mae: 453.9008\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1325673.7500 - mae: 915.9200 - val_loss: 434580.4062 - val_mae: 470.4771\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1336309.5000 - mae: 920.3289 - val_loss: 444250.4062 - val_mae: 480.0858\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1320611.0000 - mae: 914.4959 - val_loss: 442916.4688 - val_mae: 479.5013\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1330557.6250 - mae: 919.5898 - val_loss: 434423.1250 - val_mae: 470.3175\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1330872.1250 - mae: 917.3621 - val_loss: 458220.4062 - val_mae: 497.0000\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1308282.7500 - mae: 908.7977 - val_loss: 435552.3750 - val_mae: 470.8207\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1340319.3750 - mae: 924.5516 - val_loss: 441312.0938 - val_mae: 478.5868\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1342688.3750 - mae: 923.7755 - val_loss: 407599.5312 - val_mae: 446.4743\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1326823.6250 - mae: 916.1868 - val_loss: 417121.2812 - val_mae: 456.2732\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1325505.0000 - mae: 916.5372 - val_loss: 437333.2812 - val_mae: 476.4691\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1330264.7500 - mae: 918.7776 - val_loss: 403882.2500 - val_mae: 444.4344\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1308615.7500 - mae: 909.0618 - val_loss: 437913.5312 - val_mae: 481.1665\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1310133.7500 - mae: 908.6750 - val_loss: 429681.0938 - val_mae: 472.4955\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1312323.7500 - mae: 907.3315 - val_loss: 421468.3438 - val_mae: 463.1674\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1310443.8750 - mae: 908.2430 - val_loss: 409631.4688 - val_mae: 452.2274\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1297995.3750 - mae: 902.0386 - val_loss: 422103.4688 - val_mae: 468.1757\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1300601.5000 - mae: 903.4335 - val_loss: 424517.7500 - val_mae: 469.2376\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1294728.1250 - mae: 900.7044 - val_loss: 419549.0938 - val_mae: 467.7390\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1289156.8750 - mae: 900.6893 - val_loss: 411489.7188 - val_mae: 459.0521\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1302315.3750 - mae: 903.7595 - val_loss: 417595.0000 - val_mae: 463.6771\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 1280760.3750 - mae: 893.2343 - val_loss: 426159.0938 - val_mae: 478.0000\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1289765.8750 - mae: 900.2198 - val_loss: 404125.7812 - val_mae: 457.5833\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1292866.2500 - mae: 901.2019 - val_loss: 406509.7500 - val_mae: 459.3333\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1270691.2500 - mae: 888.1147 - val_loss: 415645.4688 - val_mae: 469.5947\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1286785.7500 - mae: 896.7752 - val_loss: 390813.9062 - val_mae: 443.2129\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1271025.6250 - mae: 893.4382 - val_loss: 409486.6250 - val_mae: 467.2234\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1286222.1250 - mae: 896.7096 - val_loss: 405279.5312 - val_mae: 461.3646\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1274037.2500 - mae: 891.2881 - val_loss: 393869.9062 - val_mae: 451.2206\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1275366.1250 - mae: 890.5726 - val_loss: 406496.6562 - val_mae: 463.3958\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1255722.2500 - mae: 879.5214 - val_loss: 404590.6250 - val_mae: 467.3879\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1270242.5000 - mae: 888.1077 - val_loss: 403370.6250 - val_mae: 467.0780\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1271164.7500 - mae: 889.3063 - val_loss: 402156.9688 - val_mae: 467.3296\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1249264.3750 - mae: 878.6368 - val_loss: 401682.3438 - val_mae: 463.3959\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1253284.8750 - mae: 882.4308 - val_loss: 406949.2188 - val_mae: 471.3343\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1264214.5000 - mae: 883.3069 - val_loss: 395741.2500 - val_mae: 461.3646\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1237879.3750 - mae: 872.2147 - val_loss: 400944.2188 - val_mae: 469.2446\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1245712.1250 - mae: 877.0573 - val_loss: 403337.6562 - val_mae: 471.2472\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1232332.2500 - mae: 870.5749 - val_loss: 388647.4688 - val_mae: 459.3333\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1248612.3750 - mae: 878.9524 - val_loss: 397362.4688 - val_mae: 468.8766\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1257489.5000 - mae: 882.4798 - val_loss: 392620.1562 - val_mae: 466.8163\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1266511.1250 - mae: 886.9874 - val_loss: 388714.7188 - val_mae: 461.3646\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1225661.6250 - mae: 866.9139 - val_loss: 393813.5000 - val_mae: 468.7892\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1225169.7500 - mae: 867.8264 - val_loss: 386416.0000 - val_mae: 461.0833\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1213561.2500 - mae: 862.8766 - val_loss: 401152.9688 - val_mae: 477.8477\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1222152.1250 - mae: 866.3290 - val_loss: 393841.9688 - val_mae: 470.7334\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1240763.8750 - mae: 874.8906 - val_loss: 373360.2500 - val_mae: 451.7433\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1226583.5000 - mae: 866.2061 - val_loss: 381872.3438 - val_mae: 461.0833\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1201939.1250 - mae: 857.8160 - val_loss: 377311.7188 - val_mae: 459.6146\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1210614.5000 - mae: 860.9679 - val_loss: 395324.7500 - val_mae: 478.1218\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1232218.1250 - mae: 870.2087 - val_loss: 394183.2500 - val_mae: 478.3450\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1219079.6250 - mae: 863.6519 - val_loss: 373947.6562 - val_mae: 459.3333\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1216041.6250 - mae: 861.0594 - val_loss: 376358.5000 - val_mae: 461.6458\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1202780.0000 - mae: 860.3430 - val_loss: 368257.8438 - val_mae: 457.3021\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1212423.3750 - mae: 860.4766 - val_loss: 371611.0938 - val_mae: 456.0379\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1213785.1250 - mae: 862.8135 - val_loss: 372949.2188 - val_mae: 461.0833\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1201692.5000 - mae: 859.9293 - val_loss: 368417.8438 - val_mae: 459.3333\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1203221.2500 - mae: 856.0336 - val_loss: 370792.9062 - val_mae: 461.3646\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1211328.5000 - mae: 860.3571 - val_loss: 384867.5938 - val_mae: 477.5974\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1178257.7500 - mae: 844.2916 - val_loss: 374435.4688 - val_mae: 468.2959\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1186780.7500 - mae: 849.5720 - val_loss: 360654.6562 - val_mae: 457.3021\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1203301.6250 - mae: 859.3421 - val_loss: 375695.5938 - val_mae: 470.5499\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1189194.6250 - mae: 848.8334 - val_loss: 374553.9062 - val_mae: 470.2397\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1190406.0000 - mae: 850.1911 - val_loss: 366700.1250 - val_mae: 466.7111\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1172030.2500 - mae: 842.6258 - val_loss: 368957.0938 - val_mae: 468.1510\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1186785.8750 - mae: 847.8435 - val_loss: 364458.4062 - val_mae: 466.0908\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1186679.7500 - mae: 848.1666 - val_loss: 357742.3438 - val_mae: 459.3333\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1171631.6250 - mae: 842.6994 - val_loss: 360113.5938 - val_mae: 461.3646\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1180940.6250 - mae: 847.5695 - val_loss: 356830.0312 - val_mae: 456.4437\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1170413.1250 - mae: 839.2689 - val_loss: 352453.3438 - val_mae: 454.7229\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1184126.0000 - mae: 846.0009 - val_loss: 359121.4688 - val_mae: 465.9458\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1150577.8750 - mae: 832.8087 - val_loss: 370414.6562 - val_mae: 476.8449\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1179592.0000 - mae: 846.2165 - val_loss: 348144.2812 - val_mae: 457.3021\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1157384.3750 - mae: 835.7167 - val_loss: 344977.2188 - val_mae: 452.5266\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1160656.6250 - mae: 834.5836 - val_loss: 340683.2812 - val_mae: 450.8052\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1145320.5000 - mae: 829.6094 - val_loss: 362687.7500 - val_mae: 474.3010\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1159412.5000 - mae: 835.9728 - val_loss: 352924.5938 - val_mae: 466.0538\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1163856.3750 - mae: 835.9802 - val_loss: 337716.2188 - val_mae: 450.8928\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1159715.1250 - mae: 835.2089 - val_loss: 342048.4062 - val_mae: 457.0208\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1166522.0000 - mae: 837.7542 - val_loss: 353159.7188 - val_mae: 467.9975\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1148540.0000 - mae: 832.0577 - val_loss: 343513.1562 - val_mae: 459.6146\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1139026.3750 - mae: 823.9878 - val_loss: 337196.3750 - val_mae: 453.0392\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1149919.0000 - mae: 831.0490 - val_loss: 353440.8438 - val_mae: 469.9425\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1132555.7500 - mae: 823.9681 - val_loss: 335221.1562 - val_mae: 452.8156\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1125066.6250 - mae: 820.0810 - val_loss: 353318.2188 - val_mae: 474.0619\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1146305.0000 - mae: 828.9163 - val_loss: 333397.0000 - val_mae: 453.1540\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1126918.8750 - mae: 819.8779 - val_loss: 354633.2500 - val_mae: 476.2592\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1133882.5000 - mae: 822.5980 - val_loss: 353409.3438 - val_mae: 475.3570\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1128672.2500 - mae: 819.4229 - val_loss: 335685.1562 - val_mae: 459.3333\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1130339.2500 - mae: 821.2101 - val_loss: 338016.3750 - val_mae: 461.3646\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1125033.6250 - mae: 818.7704 - val_loss: 342261.7188 - val_mae: 467.9630\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1132929.5000 - mae: 821.6603 - val_loss: 330556.0938 - val_mae: 454.9930\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1117403.0000 - mae: 813.7968 - val_loss: 338518.6562 - val_mae: 463.6771\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1126071.3750 - mae: 818.4819 - val_loss: 334157.4062 - val_mae: 461.0833\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1104925.7500 - mae: 807.2213 - val_loss: 324552.5312 - val_mae: 453.0478\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1120973.0000 - mae: 815.6830 - val_loss: 340494.2500 - val_mae: 469.0075\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1134564.5000 - mae: 823.0461 - val_loss: 319964.0938 - val_mae: 451.1571\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1103045.1250 - mae: 808.8256 - val_loss: 330485.5312 - val_mae: 461.3646\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1109908.3750 - mae: 812.3898 - val_loss: 337646.0312 - val_mae: 469.2013\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1118774.3750 - mae: 816.2432 - val_loss: 338376.7812 - val_mae: 473.1989\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1117579.3750 - mae: 812.9908 - val_loss: 332602.9062 - val_mae: 467.3938\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1083211.6250 - mae: 800.2680 - val_loss: 328375.4062 - val_mae: 465.0525\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1091870.1250 - mae: 801.5396 - val_loss: 333949.3438 - val_mae: 469.3678\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1095639.3750 - mae: 802.6640 - val_loss: 320227.9375 - val_mae: 455.7032\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1106231.2500 - mae: 807.4630 - val_loss: 324090.2188 - val_mae: 461.3646\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1079704.8750 - mae: 793.3007 - val_loss: 331145.0312 - val_mae: 469.2813\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1095387.2500 - mae: 802.8398 - val_loss: 314442.7500 - val_mae: 453.7574\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 1086258.5000 - mae: 800.7598 - val_loss: 322904.5625 - val_mae: 464.8813\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1090108.3750 - mae: 802.0668 - val_loss: 337656.8438 - val_mae: 480.2330\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1089252.3750 - mae: 798.3984 - val_loss: 329006.1562 - val_mae: 472.9068\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1077987.0000 - mae: 796.1682 - val_loss: 323431.2188 - val_mae: 467.1072\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 1083713.0000 - mae: 797.1722 - val_loss: 314826.4375 - val_mae: 459.6146\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 1095573.5000 - mae: 803.2216 - val_loss: 304727.1562 - val_mae: 448.2449\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1084507.5000 - mae: 799.6584 - val_loss: 306561.9688 - val_mae: 449.9684\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1072406.5000 - mae: 795.8190 - val_loss: 322923.5312 - val_mae: 468.7425\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1060932.7500 - mae: 785.3873 - val_loss: 309901.0938 - val_mae: 455.7654\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1082926.6250 - mae: 797.2215 - val_loss: 309052.9375 - val_mae: 455.7947\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 1086522.6250 - mae: 800.4246 - val_loss: 320232.7188 - val_mae: 468.6552\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1072627.0000 - mae: 791.1383 - val_loss: 308729.5938 - val_mae: 459.3333\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1053656.1250 - mae: 785.3979 - val_loss: 318499.5312 - val_mae: 468.5984\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1053024.6250 - mae: 783.7712 - val_loss: 318841.2500 - val_mae: 471.7112\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1059932.7500 - mae: 784.8883 - val_loss: 318153.1562 - val_mae: 472.2175\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1041203.8750 - mae: 776.7629 - val_loss: 308524.3750 - val_mae: 461.3646\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1066782.0000 - mae: 787.8627 - val_loss: 307695.2812 - val_mae: 461.3646\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1061477.1250 - mae: 788.6707 - val_loss: 303778.6562 - val_mae: 459.3333\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1058882.0000 - mae: 784.4419 - val_loss: 317733.3438 - val_mae: 474.0188\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1065434.2500 - mae: 787.7435 - val_loss: 294872.1875 - val_mae: 452.3010\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1062396.3750 - mae: 787.4373 - val_loss: 297060.3125 - val_mae: 454.0800\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1019927.3125 - mae: 765.5974 - val_loss: 293225.3750 - val_mae: 452.0771\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1047066.9375 - mae: 779.0401 - val_loss: 309976.7500 - val_mae: 468.3115\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1052641.6250 - mae: 781.4380 - val_loss: 317628.6562 - val_mae: 478.9010\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1036283.9375 - mae: 774.2846 - val_loss: 300709.2188 - val_mae: 461.0000\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1044402.8125 - mae: 777.0564 - val_loss: 293229.2812 - val_mae: 454.2222\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1027469.3125 - mae: 770.4292 - val_loss: 310706.5938 - val_mae: 472.9983\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1018624.8750 - mae: 766.1263 - val_loss: 294846.3438 - val_mae: 456.5917\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1038759.0625 - mae: 773.0447 - val_loss: 295049.7188 - val_mae: 459.3333\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1027303.6875 - mae: 770.4376 - val_loss: 301356.5938 - val_mae: 466.3620\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1024592.1875 - mae: 767.5978 - val_loss: 289479.0312 - val_mae: 454.3649\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1021756.1250 - mae: 767.9965 - val_loss: 298871.1875 - val_mae: 463.6771\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1001686.1250 - mae: 758.3573 - val_loss: 309833.0312 - val_mae: 477.8484\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1012302.0625 - mae: 762.7172 - val_loss: 291333.8125 - val_mae: 459.6146\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1022407.9375 - mae: 765.4543 - val_loss: 286654.1562 - val_mae: 454.7601\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1038612.1250 - mae: 775.4187 - val_loss: 296615.7812 - val_mae: 466.1897\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 999092.3125 - mae: 753.3234 - val_loss: 282231.6250 - val_mae: 452.7860\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1006771.2500 - mae: 756.6569 - val_loss: 288156.0625 - val_mae: 459.0521\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1011673.6250 - mae: 759.9186 - val_loss: 289734.1875 - val_mae: 458.9362\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 1020977.9375 - mae: 764.7527 - val_loss: 297188.8750 - val_mae: 470.2265\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1019631.9375 - mae: 764.8005 - val_loss: 295316.3438 - val_mae: 467.7151\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 998463.6875 - mae: 753.5338 - val_loss: 298698.5625 - val_mae: 472.4254\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 997764.6250 - mae: 755.0226 - val_loss: 284761.5312 - val_mae: 459.6146\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1015143.6875 - mae: 763.4963 - val_loss: 284036.9688 - val_mae: 459.6146\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1009578.5625 - mae: 760.3602 - val_loss: 282540.5000 - val_mae: 457.0762\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1002159.0625 - mae: 757.5505 - val_loss: 299418.9062 - val_mae: 477.0216\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1005177.3125 - mae: 759.0748 - val_loss: 278995.6562 - val_mae: 457.5833\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 993112.0000 - mae: 751.5315 - val_loss: 290582.5938 - val_mae: 467.8810\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 987511.0000 - mae: 747.8458 - val_loss: 286701.5312 - val_mae: 465.2585\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1000586.6875 - mae: 754.7297 - val_loss: 279027.7812 - val_mae: 456.9367\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 973886.8750 - mae: 742.3181 - val_loss: 285498.5938 - val_mae: 465.7643\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 994523.5000 - mae: 751.9381 - val_loss: 284901.9688 - val_mae: 466.0171\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 986089.8750 - mae: 746.5987 - val_loss: 267851.0312 - val_mae: 448.8974\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 981008.5000 - mae: 744.8078 - val_loss: 280381.8438 - val_mae: 463.3671\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 993590.3125 - mae: 750.9733 - val_loss: 273543.6562 - val_mae: 457.5833\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 979482.3125 - mae: 743.8660 - val_loss: 275645.4688 - val_mae: 459.3333\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 980439.4375 - mae: 741.9784 - val_loss: 277276.7188 - val_mae: 459.1670\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 981801.8125 - mae: 743.8963 - val_loss: 283559.4688 - val_mae: 467.8773\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 981806.0625 - mae: 745.0519 - val_loss: 277018.4062 - val_mae: 463.5046\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 981230.9375 - mae: 742.8971 - val_loss: 281504.4688 - val_mae: 466.8945\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 976340.4375 - mae: 741.5313 - val_loss: 274254.9688 - val_mae: 458.9157\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 972018.5625 - mae: 738.7103 - val_loss: 280472.1562 - val_mae: 466.9213\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 958913.1875 - mae: 730.9420 - val_loss: 283327.5938 - val_mae: 471.5141\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 977196.8750 - mae: 742.2060 - val_loss: 276276.3125 - val_mae: 464.8327\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 968982.7500 - mae: 737.5943 - val_loss: 281930.0938 - val_mae: 471.3999\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 971453.6875 - mae: 739.2448 - val_loss: 275195.4375 - val_mae: 465.3380\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 968139.0000 - mae: 737.8976 - val_loss: 271342.4062 - val_mae: 461.3646\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 961575.1250 - mae: 735.6577 - val_loss: 273874.9062 - val_mae: 465.2807\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 954490.1875 - mae: 731.0703 - val_loss: 279052.9688 - val_mae: 470.8904\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 936724.8125 - mae: 723.0298 - val_loss: 275606.5625 - val_mae: 468.8044\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 958543.3750 - mae: 734.4733 - val_loss: 269300.1250 - val_mae: 463.4464\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 953119.4375 - mae: 731.3011 - val_loss: 273840.8438 - val_mae: 466.6368\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 957971.3750 - mae: 734.0594 - val_loss: 264577.5938 - val_mae: 457.5889\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 941734.1875 - mae: 725.1683 - val_loss: 269222.4062 - val_mae: 462.7500\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 949134.3750 - mae: 729.8448 - val_loss: 266428.6250 - val_mae: 461.3646\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 942973.8125 - mae: 723.5762 - val_loss: 260301.0781 - val_mae: 457.3021\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 920776.9375 - mae: 715.4194 - val_loss: 265445.3438 - val_mae: 462.9956\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 936316.8125 - mae: 723.9955 - val_loss: 270541.5938 - val_mae: 468.6329\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 945600.7500 - mae: 725.8550 - val_loss: 266817.0312 - val_mae: 463.3958\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 943939.1875 - mae: 727.7343 - val_loss: 269132.0312 - val_mae: 466.9740\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 930823.0625 - mae: 721.0707 - val_loss: 260022.7969 - val_mae: 459.0521\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 942300.6875 - mae: 726.1276 - val_loss: 259010.7969 - val_mae: 458.6875\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 929539.6250 - mae: 716.2803 - val_loss: 264472.4375 - val_mae: 463.3958\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 926052.6250 - mae: 717.3542 - val_loss: 250274.3750 - val_mae: 450.6535\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 920825.8125 - mae: 714.5642 - val_loss: 263473.6562 - val_mae: 463.6771\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 932403.0000 - mae: 720.2329 - val_loss: 262528.5625 - val_mae: 464.2098\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 934282.5625 - mae: 722.0659 - val_loss: 267674.9375 - val_mae: 470.1543\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 927116.4375 - mae: 719.4810 - val_loss: 258820.2969 - val_mae: 462.4034\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 918444.3750 - mae: 713.4980 - val_loss: 258107.9531 - val_mae: 462.0940\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 925178.3125 - mae: 717.6228 - val_loss: 257870.6406 - val_mae: 461.3646\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 904497.8750 - mae: 709.3203 - val_loss: 257183.2969 - val_mae: 461.0833\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 913127.2500 - mae: 713.5632 - val_loss: 259415.1406 - val_mae: 464.6039\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 906927.6875 - mae: 711.5560 - val_loss: 253655.7344 - val_mae: 458.1532\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 912427.3125 - mae: 711.7830 - val_loss: 250481.9219 - val_mae: 456.1502\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 906544.6875 - mae: 706.4949 - val_loss: 252547.4219 - val_mae: 459.3333\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 918468.0000 - mae: 713.9775 - val_loss: 254415.2344 - val_mae: 461.2812\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 904100.8125 - mae: 710.8637 - val_loss: 256670.7500 - val_mae: 464.4630\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 908597.4375 - mae: 708.5331 - val_loss: 250991.3281 - val_mae: 459.3334\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 897676.1250 - mae: 702.1514 - val_loss: 257919.1250 - val_mae: 467.1682\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 886003.9375 - mae: 696.1396 - val_loss: 257356.6094 - val_mae: 467.1110\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 896660.3750 - mae: 704.7780 - val_loss: 254393.4844 - val_mae: 464.0695\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 902284.3750 - mae: 705.7520 - val_loss: 249153.7031 - val_mae: 459.6146\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 884306.6250 - mae: 697.0035 - val_loss: 253498.5000 - val_mae: 464.2940\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 901014.8750 - mae: 706.6504 - val_loss: 252811.6719 - val_mae: 463.9838\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 884687.3750 - mae: 696.2642 - val_loss: 252466.9531 - val_mae: 464.2374\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 874124.4375 - mae: 693.6708 - val_loss: 256388.4531 - val_mae: 468.2416\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 889848.5000 - mae: 700.9196 - val_loss: 253403.8750 - val_mae: 466.4335\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 889794.5000 - mae: 698.7512 - val_loss: 248275.9844 - val_mae: 461.0000\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 884070.1875 - mae: 695.3623 - val_loss: 240964.5469 - val_mae: 454.8239\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 880019.7500 - mae: 694.0770 - val_loss: 250117.5781 - val_mae: 464.3775\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 875343.8750 - mae: 693.2532 - val_loss: 244547.6094 - val_mae: 459.0521\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 887226.6250 - mae: 699.7379 - val_loss: 244264.0469 - val_mae: 459.3333\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 893408.6875 - mae: 703.5240 - val_loss: 243803.5469 - val_mae: 459.3333\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 871875.1875 - mae: 693.8851 - val_loss: 239266.2344 - val_mae: 454.3751\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 873940.8750 - mae: 690.7932 - val_loss: 245174.2500 - val_mae: 462.2064\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 866580.6875 - mae: 690.8717 - val_loss: 244707.4375 - val_mae: 462.1781\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 863338.1875 - mae: 688.7710 - val_loss: 251240.6094 - val_mae: 468.4701\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 863529.5000 - mae: 686.8521 - val_loss: 245609.3125 - val_mae: 463.2273\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 867855.1250 - mae: 690.4940 - val_loss: 241350.0781 - val_mae: 459.6146\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 872564.5625 - mae: 691.9932 - val_loss: 245090.7500 - val_mae: 463.5359\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 871265.6250 - mae: 692.4493 - val_loss: 247504.8281 - val_mae: 466.1022\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 875151.6875 - mae: 694.3824 - val_loss: 246705.9531 - val_mae: 465.5115\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 856382.9375 - mae: 683.3162 - val_loss: 237747.3750 - val_mae: 457.2456\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 868546.4375 - mae: 692.8032 - val_loss: 247876.0000 - val_mae: 468.0771\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 868509.1250 - mae: 692.1188 - val_loss: 241174.2500 - val_mae: 461.3646\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 859022.4375 - mae: 685.9648 - val_loss: 244303.3594 - val_mae: 465.6546\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 855629.7500 - mae: 686.0591 - val_loss: 240356.3281 - val_mae: 461.3646\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 866676.8125 - mae: 692.6057 - val_loss: 235848.0781 - val_mae: 457.3850\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 858291.1250 - mae: 687.3939 - val_loss: 239565.9219 - val_mae: 461.3646\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 858108.4375 - mae: 687.3966 - val_loss: 238973.9844 - val_mae: 461.0833\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 844205.2500 - mae: 681.6534 - val_loss: 241045.5469 - val_mae: 463.1146\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 849038.4375 - mae: 683.1120 - val_loss: 241814.7500 - val_mae: 465.5995\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 843925.1250 - mae: 682.9479 - val_loss: 237232.2500 - val_mae: 461.4224\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 829190.0625 - mae: 671.9172 - val_loss: 240778.4531 - val_mae: 465.2065\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 850625.9375 - mae: 685.8841 - val_loss: 228446.8750 - val_mae: 453.5186\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 848829.0000 - mae: 683.7613 - val_loss: 238491.5625 - val_mae: 463.3704\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 849280.8750 - mae: 683.3997 - val_loss: 233889.6719 - val_mae: 459.0521\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 842060.0625 - mae: 678.9680 - val_loss: 232314.7969 - val_mae: 457.6639\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 831529.8125 - mae: 674.6569 - val_loss: 241302.0781 - val_mae: 467.2400\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 827379.8125 - mae: 672.1134 - val_loss: 234537.6719 - val_mae: 461.2280\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 841811.2500 - mae: 680.6375 - val_loss: 237711.3281 - val_mae: 464.5349\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 841580.8125 - mae: 679.4805 - val_loss: 234698.0156 - val_mae: 461.3646\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 830876.0000 - mae: 675.6440 - val_loss: 235810.2969 - val_mae: 463.1742\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 821596.0625 - mae: 669.3596 - val_loss: 235075.0469 - val_mae: 462.5844\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 830664.6875 - mae: 674.6837 - val_loss: 235279.0781 - val_mae: 463.4005\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 811887.1875 - mae: 667.4536 - val_loss: 230943.5469 - val_mae: 459.3333\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 829495.1875 - mae: 673.3502 - val_loss: 237934.8906 - val_mae: 466.5120\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 832152.5625 - mae: 675.5903 - val_loss: 227926.6719 - val_mae: 457.3021\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 827023.8750 - mae: 674.3761 - val_loss: 236035.4219 - val_mae: 465.0388\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 815335.7500 - mae: 670.7681 - val_loss: 234832.9531 - val_mae: 464.8768\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 825897.8125 - mae: 674.5146 - val_loss: 233886.5469 - val_mae: 463.9766\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 820230.3750 - mae: 668.8605 - val_loss: 230121.8906 - val_mae: 459.8048\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 820631.1875 - mae: 670.9264 - val_loss: 232309.3750 - val_mae: 462.8958\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 823988.4375 - mae: 673.4330 - val_loss: 232738.6719 - val_mae: 463.0312\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 816059.1875 - mae: 668.7119 - val_loss: 233791.1250 - val_mae: 464.5905\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 802402.0625 - mae: 663.4780 - val_loss: 229189.4531 - val_mae: 460.1969\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 819110.5000 - mae: 670.9601 - val_loss: 227552.7031 - val_mae: 459.3333\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 812623.4375 - mae: 667.2643 - val_loss: 229574.9844 - val_mae: 461.3646\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 812662.6875 - mae: 668.3853 - val_loss: 230395.4219 - val_mae: 462.7301\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 803681.2500 - mae: 664.5943 - val_loss: 224399.1719 - val_mae: 457.3021\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 804575.5625 - mae: 664.4229 - val_loss: 229988.0625 - val_mae: 462.9564\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 812624.6875 - mae: 669.1000 - val_loss: 221981.7969 - val_mae: 455.0172\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 796776.7500 - mae: 660.1074 - val_loss: 225889.1875 - val_mae: 459.3333\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 809157.7500 - mae: 669.0101 - val_loss: 228694.1719 - val_mae: 462.3108\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 796066.8125 - mae: 661.1935 - val_loss: 226666.7031 - val_mae: 460.1642\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 805230.5000 - mae: 666.5356 - val_loss: 224152.1719 - val_mae: 458.1606\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 798150.6875 - mae: 661.9717 - val_loss: 228763.4375 - val_mae: 463.3726\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 789581.3125 - mae: 656.1399 - val_loss: 227352.8125 - val_mae: 461.9193\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 789296.0625 - mae: 658.0546 - val_loss: 225229.6719 - val_mae: 460.4226\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 793250.5000 - mae: 660.2788 - val_loss: 223244.5781 - val_mae: 458.2712\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 787437.2500 - mae: 658.1171 - val_loss: 230032.4844 - val_mae: 465.4646\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 784668.1250 - mae: 654.6761 - val_loss: 228113.3750 - val_mae: 463.3958\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 781294.3750 - mae: 655.1299 - val_loss: 221160.8281 - val_mae: 457.3021\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 786995.1250 - mae: 659.7614 - val_loss: 225747.9531 - val_mae: 461.7539\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 775821.6250 - mae: 651.8083 - val_loss: 223907.9219 - val_mae: 460.5396\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 784856.9375 - mae: 658.0673 - val_loss: 228402.8281 - val_mae: 464.9088\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 782851.9375 - mae: 655.6802 - val_loss: 222509.3906 - val_mae: 459.3333\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 758471.0000 - mae: 647.5806 - val_loss: 222292.2969 - val_mae: 459.3333\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 782839.1875 - mae: 658.4214 - val_loss: 219221.2969 - val_mae: 456.4865\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 779073.2500 - mae: 657.5134 - val_loss: 224717.9531 - val_mae: 462.1533\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 776143.1250 - mae: 656.7601 - val_loss: 224888.4531 - val_mae: 462.6058\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 776487.4375 - mae: 657.4926 - val_loss: 223876.1875 - val_mae: 461.6458\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 766258.6875 - mae: 651.5818 - val_loss: 221283.7969 - val_mae: 459.3333\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 779613.2500 - mae: 657.3759 - val_loss: 222926.6406 - val_mae: 460.9670\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 768427.5625 - mae: 651.4291 - val_loss: 223065.4531 - val_mae: 461.3645\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 774695.5000 - mae: 658.4274 - val_loss: 225326.1094 - val_mae: 463.7387\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 769407.1875 - mae: 653.2531 - val_loss: 220524.6094 - val_mae: 459.3333\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 752253.3750 - mae: 645.1653 - val_loss: 223428.3906 - val_mae: 462.5045\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 771275.1250 - mae: 658.2657 - val_loss: 217798.7344 - val_mae: 457.0420\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 765072.1250 - mae: 653.2424 - val_loss: 220218.6250 - val_mae: 459.6146\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 753181.5625 - mae: 645.7258 - val_loss: 224667.5469 - val_mae: 464.0879\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 761369.3125 - mae: 650.6016 - val_loss: 224074.8125 - val_mae: 463.5740\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 763800.6250 - mae: 653.8815 - val_loss: 219617.5625 - val_mae: 459.4832\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 746301.1250 - mae: 647.7002 - val_loss: 221787.2969 - val_mae: 461.7690\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 762743.2500 - mae: 656.0961 - val_loss: 223575.0781 - val_mae: 463.5872\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 755556.8125 - mae: 649.4128 - val_loss: 223476.5781 - val_mae: 463.6771\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 751387.5625 - mae: 649.9385 - val_loss: 223617.1250 - val_mae: 464.0405\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 754602.0625 - mae: 649.2064 - val_loss: 221073.3125 - val_mae: 461.6590\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 763683.8750 - mae: 655.6725 - val_loss: 221120.7500 - val_mae: 461.8985\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 755091.1250 - mae: 649.0958 - val_loss: 222600.4219 - val_mae: 463.3543\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 755176.5000 - mae: 652.6919 - val_loss: 220089.2969 - val_mae: 461.0000\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 756701.7500 - mae: 652.5266 - val_loss: 222033.7969 - val_mae: 463.0178\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 745987.1875 - mae: 648.8806 - val_loss: 220137.0781 - val_mae: 461.3646\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 746717.5625 - mae: 647.7266 - val_loss: 220235.2031 - val_mae: 461.6459\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 735000.0000 - mae: 645.0757 - val_loss: 217895.0000 - val_mae: 459.4352\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 739382.0000 - mae: 645.1389 - val_loss: 219931.5469 - val_mae: 461.5700\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 745546.1875 - mae: 650.1522 - val_loss: 219638.6094 - val_mae: 461.3646\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 751645.5625 - mae: 653.9923 - val_loss: 221354.2969 - val_mae: 463.1357\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 745261.9375 - mae: 649.2367 - val_loss: 219637.4219 - val_mae: 461.6331\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 745282.8125 - mae: 651.2081 - val_loss: 217557.4219 - val_mae: 459.6822\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 744655.1250 - mae: 651.6386 - val_loss: 217114.8281 - val_mae: 459.2718\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 736056.0625 - mae: 645.2106 - val_loss: 219334.8906 - val_mae: 461.6458\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 735069.0000 - mae: 647.8989 - val_loss: 218904.8281 - val_mae: 461.2491\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 738941.9375 - mae: 648.0009 - val_loss: 217003.5156 - val_mae: 459.4764\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 739005.8750 - mae: 648.5965 - val_loss: 216421.9219 - val_mae: 458.8814\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 730632.7500 - mae: 645.9470 - val_loss: 218725.7344 - val_mae: 461.3646\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 728140.8125 - mae: 642.6261 - val_loss: 216642.2500 - val_mae: 459.3333\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 731159.1875 - mae: 646.7100 - val_loss: 220593.6406 - val_mae: 463.4256\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 726196.8750 - mae: 642.5839 - val_loss: 218709.3281 - val_mae: 461.6432\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 708779.1875 - mae: 633.8998 - val_loss: 220391.7500 - val_mae: 463.3720\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 736466.3750 - mae: 651.2408 - val_loss: 215848.0625 - val_mae: 458.7198\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 718464.6875 - mae: 642.2609 - val_loss: 216054.6719 - val_mae: 459.0521\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 719506.2500 - mae: 644.4952 - val_loss: 214273.7031 - val_mae: 457.3021\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 718447.9375 - mae: 641.6250 - val_loss: 216522.2656 - val_mae: 459.7470\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 717792.6875 - mae: 641.7347 - val_loss: 217737.5000 - val_mae: 460.9246\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 707993.3750 - mae: 636.9645 - val_loss: 219994.8125 - val_mae: 463.3958\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 717124.0625 - mae: 641.7765 - val_loss: 216046.1250 - val_mae: 459.3333\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 713221.9375 - mae: 640.3568 - val_loss: 217274.2344 - val_mae: 460.5632\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 713574.1875 - mae: 641.3304 - val_loss: 215965.7344 - val_mae: 459.3333\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 709650.7500 - mae: 639.8624 - val_loss: 219580.6250 - val_mae: 463.1035\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 705317.1875 - mae: 636.9137 - val_loss: 220006.3281 - val_mae: 463.6771\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 711521.1250 - mae: 641.8498 - val_loss: 218954.4531 - val_mae: 462.4880\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 714440.8750 - mae: 642.2377 - val_loss: 218018.1875 - val_mae: 461.6458\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 710687.7500 - mae: 641.5449 - val_loss: 216933.3594 - val_mae: 460.4041\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 716285.4375 - mae: 646.6041 - val_loss: 217209.4375 - val_mae: 461.0276\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 696575.1875 - mae: 636.0758 - val_loss: 217694.7500 - val_mae: 461.3646\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 707796.4375 - mae: 641.7642 - val_loss: 217677.4531 - val_mae: 461.3646\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 693291.5000 - mae: 636.0364 - val_loss: 218403.0625 - val_mae: 462.0471\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 708342.4375 - mae: 643.3569 - val_loss: 213626.6719 - val_mae: 457.0208\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 695316.7500 - mae: 636.0988 - val_loss: 216707.2656 - val_mae: 460.2431\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 699251.8750 - mae: 638.4619 - val_loss: 215065.8594 - val_mae: 458.4662\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 691163.7500 - mae: 636.8715 - val_loss: 217803.0469 - val_mae: 461.3273\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 687631.7500 - mae: 634.7279 - val_loss: 216789.2344 - val_mae: 460.5350\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 699566.8125 - mae: 639.4579 - val_loss: 215854.6406 - val_mae: 459.1885\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 688997.3750 - mae: 633.8195 - val_loss: 217400.2031 - val_mae: 461.0833\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 697629.1875 - mae: 638.5516 - val_loss: 216600.8906 - val_mae: 460.0830\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 692999.3125 - mae: 636.7659 - val_loss: 218522.4844 - val_mae: 462.3906\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 693981.4375 - mae: 636.7798 - val_loss: 217974.1250 - val_mae: 461.6458\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 687776.0000 - mae: 636.6317 - val_loss: 220441.8594 - val_mae: 464.4761\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 690742.6250 - mae: 637.6863 - val_loss: 215661.0469 - val_mae: 458.9688\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 686481.3125 - mae: 635.4445 - val_loss: 217787.4531 - val_mae: 461.3646\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 688679.4375 - mae: 636.5669 - val_loss: 218964.0156 - val_mae: 462.5160\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 687167.3125 - mae: 635.8658 - val_loss: 216043.6406 - val_mae: 459.3333\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 685978.7500 - mae: 636.0460 - val_loss: 218449.7969 - val_mae: 461.9004\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 687872.5000 - mae: 636.7671 - val_loss: 219213.0000 - val_mae: 462.8862\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 681476.8750 - mae: 634.8752 - val_loss: 218482.0781 - val_mae: 461.8479\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 684874.8125 - mae: 637.9205 - val_loss: 216711.8281 - val_mae: 459.7903\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 678162.5625 - mae: 633.7590 - val_loss: 218798.9531 - val_mae: 462.0761\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 688220.8750 - mae: 639.7756 - val_loss: 217047.2031 - val_mae: 460.0178\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 679728.5625 - mae: 636.8439 - val_loss: 215309.6719 - val_mae: 457.9592\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 689176.9375 - mae: 642.2813 - val_loss: 217629.3906 - val_mae: 460.7347\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 670738.0625 - mae: 629.3011 - val_loss: 215403.7031 - val_mae: 457.9061\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 668900.2500 - mae: 632.0127 - val_loss: 215792.7344 - val_mae: 458.1922\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 671430.0625 - mae: 632.1537 - val_loss: 218189.1406 - val_mae: 461.0833\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 668609.0000 - mae: 630.2311 - val_loss: 218006.5781 - val_mae: 460.8385\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 671526.8125 - mae: 634.9566 - val_loss: 217371.9844 - val_mae: 459.8327\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 662679.3125 - mae: 630.9440 - val_loss: 220714.3750 - val_mae: 463.6771\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 667479.5625 - mae: 633.9230 - val_loss: 215032.0469 - val_mae: 457.0208\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 668869.1875 - mae: 633.3261 - val_loss: 217137.0625 - val_mae: 459.3333\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 673743.6250 - mae: 636.2084 - val_loss: 216181.8906 - val_mae: 457.9778\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 670888.0000 - mae: 633.9761 - val_loss: 218516.7031 - val_mae: 460.7188\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 671053.5000 - mae: 635.3628 - val_loss: 219144.4844 - val_mae: 461.3646\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 671016.2500 - mae: 635.2482 - val_loss: 220921.5156 - val_mae: 463.3628\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 658615.9375 - mae: 630.6720 - val_loss: 217399.6875 - val_mae: 458.9688\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 645531.9375 - mae: 627.1317 - val_loss: 219477.9531 - val_mae: 461.3846\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 669766.9375 - mae: 636.0060 - val_loss: 216131.6406 - val_mae: 457.2560\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 667631.3750 - mae: 634.4918 - val_loss: 216509.0469 - val_mae: 457.5117\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 655889.0625 - mae: 629.2641 - val_loss: 217925.5469 - val_mae: 459.1505\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 647332.1250 - mae: 623.8207 - val_loss: 217988.9531 - val_mae: 458.9688\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 659148.0625 - mae: 632.2411 - val_loss: 216086.3906 - val_mae: 456.7200\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 662041.5625 - mae: 633.4854 - val_loss: 216922.9531 - val_mae: 457.4070\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 647745.4375 - mae: 627.3962 - val_loss: 216669.5625 - val_mae: 457.0208\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 658108.1250 - mae: 631.5453 - val_loss: 216623.0469 - val_mae: 456.8449\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 647819.8750 - mae: 628.0333 - val_loss: 218917.3594 - val_mae: 459.3601\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 651522.3125 - mae: 629.4782 - val_loss: 220440.3125 - val_mae: 460.9992\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 648347.5625 - mae: 629.4646 - val_loss: 217533.7031 - val_mae: 457.3021\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 653453.5000 - mae: 630.5735 - val_loss: 218637.2031 - val_mae: 458.6676\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 650883.0000 - mae: 628.8839 - val_loss: 219396.3906 - val_mae: 459.2559\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 653912.6875 - mae: 631.5203 - val_loss: 221124.4375 - val_mae: 461.1033\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 648107.2500 - mae: 630.4294 - val_loss: 217773.8125 - val_mae: 456.9118\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 642559.2500 - mae: 626.8976 - val_loss: 219942.3125 - val_mae: 459.3333\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 645455.8750 - mae: 628.3572 - val_loss: 220405.6094 - val_mae: 459.6146\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 648420.0000 - mae: 628.4133 - val_loss: 219264.1719 - val_mae: 458.3556\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 642561.7500 - mae: 628.9467 - val_loss: 222337.1094 - val_mae: 461.6458\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 645708.5000 - mae: 628.2281 - val_loss: 218197.6875 - val_mae: 456.5021\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 654763.2500 - mae: 635.7466 - val_loss: 219617.6406 - val_mae: 458.2005\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 644237.0625 - mae: 628.6248 - val_loss: 220646.3750 - val_mae: 458.9893\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 650400.1250 - mae: 634.1318 - val_loss: 221148.6250 - val_mae: 459.3334\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 652648.6875 - mae: 636.4607 - val_loss: 220976.7656 - val_mae: 458.9686\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 635176.4375 - mae: 624.9283 - val_loss: 222411.0781 - val_mae: 460.6922\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 631874.6875 - mae: 621.2635 - val_loss: 222130.7031 - val_mae: 459.7487\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 640428.7500 - mae: 628.6315 - val_loss: 219908.1719 - val_mae: 456.8596\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 649637.8750 - mae: 635.0745 - val_loss: 221163.5000 - val_mae: 458.3985\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 636221.8125 - mae: 626.7452 - val_loss: 227747.8906 - val_mae: 466.2005\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 632150.1875 - mae: 623.9852 - val_loss: 222806.1719 - val_mae: 459.6146\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 638258.0625 - mae: 629.1392 - val_loss: 221726.7031 - val_mae: 457.8480\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 628563.6250 - mae: 625.7800 - val_loss: 222034.5625 - val_mae: 458.4818\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 627827.6250 - mae: 621.8080 - val_loss: 223123.5000 - val_mae: 459.3333\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 628070.4375 - mae: 622.7484 - val_loss: 223637.2500 - val_mae: 459.6146\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 631643.0000 - mae: 625.2059 - val_loss: 222591.6250 - val_mae: 458.4051\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 613347.4375 - mae: 615.5441 - val_loss: 228988.4531 - val_mae: 466.0990\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 626744.1875 - mae: 624.9704 - val_loss: 223983.0781 - val_mae: 459.3333\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 638521.5000 - mae: 632.2935 - val_loss: 225254.3125 - val_mae: 460.3385\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 8s 225ms/step - loss: 624501.5625 - mae: 621.7191 - val_loss: 222499.1406 - val_mae: 456.8338\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 628399.8750 - mae: 625.2756 - val_loss: 220211.9219 - val_mae: 454.4158\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 626163.1875 - mae: 624.3278 - val_loss: 223416.7500 - val_mae: 457.3021\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 621637.4375 - mae: 621.6664 - val_loss: 221671.6875 - val_mae: 455.3709\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 624390.1875 - mae: 625.0380 - val_loss: 228337.9531 - val_mae: 463.3958\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 623019.8750 - mae: 623.8525 - val_loss: 225612.2500 - val_mae: 459.3333\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 617234.7500 - mae: 624.7810 - val_loss: 222261.2656 - val_mae: 455.2163\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 617579.4375 - mae: 620.3882 - val_loss: 225483.5781 - val_mae: 458.6878\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 629169.6250 - mae: 628.4125 - val_loss: 228773.8906 - val_mae: 462.3172\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 620616.5625 - mae: 622.4581 - val_loss: 225311.9531 - val_mae: 458.0734\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 619038.9375 - mae: 622.8821 - val_loss: 227430.0781 - val_mae: 460.2766\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 621748.0000 - mae: 624.7676 - val_loss: 224969.4375 - val_mae: 456.5532\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 605375.7500 - mae: 618.2795 - val_loss: 227722.4375 - val_mae: 459.6146\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 608410.9375 - mae: 617.5322 - val_loss: 224495.5625 - val_mae: 455.6587\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 626539.0625 - mae: 629.5381 - val_loss: 224752.9531 - val_mae: 456.2738\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 607994.5625 - mae: 620.3471 - val_loss: 229964.1875 - val_mae: 461.6458\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 624165.1250 - mae: 629.9478 - val_loss: 227003.5625 - val_mae: 457.8930\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 615287.9375 - mae: 622.2407 - val_loss: 228665.8281 - val_mae: 459.8993\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 616417.1250 - mae: 623.6359 - val_loss: 227319.8125 - val_mae: 457.4769\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 623323.5000 - mae: 630.3641 - val_loss: 229321.3281 - val_mae: 459.3333\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 609277.4375 - mae: 620.6036 - val_loss: 232602.4375 - val_mae: 462.9059\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 596212.3125 - mae: 612.7516 - val_loss: 228608.4844 - val_mae: 458.0475\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 607073.6250 - mae: 620.3820 - val_loss: 230247.0781 - val_mae: 460.0551\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 606656.1250 - mae: 620.2453 - val_loss: 234402.3125 - val_mae: 464.3669\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 599536.6250 - mae: 618.1368 - val_loss: 226642.6719 - val_mae: 454.5820\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 603030.7500 - mae: 618.7964 - val_loss: 232441.7500 - val_mae: 460.7177\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 602934.3125 - mae: 617.7152 - val_loss: 233062.2500 - val_mae: 461.6458\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 613877.1250 - mae: 625.3392 - val_loss: 228851.0781 - val_mae: 455.8665\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 601974.6250 - mae: 618.1467 - val_loss: 233320.2031 - val_mae: 461.3646\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 602911.3125 - mae: 619.0113 - val_loss: 234641.7344 - val_mae: 463.1146\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 601253.2500 - mae: 619.5924 - val_loss: 226324.2656 - val_mae: 452.4940\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 602029.7500 - mae: 620.4990 - val_loss: 229128.0469 - val_mae: 455.7023\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 601297.0625 - mae: 618.0259 - val_loss: 230287.8906 - val_mae: 455.7423\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 605684.3125 - mae: 623.3436 - val_loss: 234529.4531 - val_mae: 461.0833\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 4s 112ms/step - loss: 600163.8750 - mae: 620.0068 - val_loss: 232222.8281 - val_mae: 457.7234\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 594460.6875 - mae: 615.2374 - val_loss: 237132.7969 - val_mae: 462.9990\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 593097.7500 - mae: 616.0119 - val_loss: 235788.0000 - val_mae: 461.3646\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 594704.5625 - mae: 616.3798 - val_loss: 234239.7656 - val_mae: 459.3175\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 597642.8750 - mae: 617.9869 - val_loss: 234788.0156 - val_mae: 459.0521\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 601880.6875 - mae: 621.1379 - val_loss: 234036.3281 - val_mae: 457.8835\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 599514.7500 - mae: 622.0359 - val_loss: 236129.5469 - val_mae: 459.6146\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 592870.3750 - mae: 618.4320 - val_loss: 238719.4375 - val_mae: 463.3958\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 585305.3750 - mae: 615.1176 - val_loss: 238275.5469 - val_mae: 461.1391\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 592825.1875 - mae: 618.8066 - val_loss: 231818.9219 - val_mae: 453.6426\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 586423.0625 - mae: 613.6670 - val_loss: 235263.9375 - val_mae: 457.4794\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 595959.1875 - mae: 619.1445 - val_loss: 233368.8750 - val_mae: 455.2926\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 587719.2500 - mae: 615.8766 - val_loss: 238153.5469 - val_mae: 459.6146\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 587049.7500 - mae: 615.4828 - val_loss: 230721.8750 - val_mae: 451.2437\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 591650.7500 - mae: 617.6083 - val_loss: 238500.4531 - val_mae: 459.3333\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 585624.6875 - mae: 617.1931 - val_loss: 231292.7500 - val_mae: 451.0963\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 597131.8750 - mae: 625.5445 - val_loss: 232926.4375 - val_mae: 452.4562\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 591102.8125 - mae: 619.6121 - val_loss: 241194.6094 - val_mae: 461.6458\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 593374.6250 - mae: 622.0812 - val_loss: 239632.8281 - val_mae: 459.0521\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 585647.0625 - mae: 616.9048 - val_loss: 241072.7656 - val_mae: 460.7188\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 582345.3750 - mae: 615.5839 - val_loss: 242114.4844 - val_mae: 461.2812\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 592987.1875 - mae: 622.5907 - val_loss: 240163.8281 - val_mae: 459.2419\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 581818.3750 - mae: 616.2980 - val_loss: 248579.7969 - val_mae: 467.9729\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 586131.3750 - mae: 619.1746 - val_loss: 236392.9531 - val_mae: 454.1477\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 576289.0000 - mae: 613.5139 - val_loss: 237412.8594 - val_mae: 454.6626\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 570720.2500 - mae: 611.5104 - val_loss: 242199.0156 - val_mae: 459.0521\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 584602.3125 - mae: 618.4918 - val_loss: 241885.8281 - val_mae: 459.1219\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 584594.1875 - mae: 619.3107 - val_loss: 239833.2344 - val_mae: 455.0350\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 576367.1250 - mae: 614.8503 - val_loss: 243686.0781 - val_mae: 459.3333\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 578234.3125 - mae: 616.1474 - val_loss: 238695.9219 - val_mae: 454.1448\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 578692.3125 - mae: 616.5119 - val_loss: 238574.0625 - val_mae: 452.6248\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 571553.6875 - mae: 614.0961 - val_loss: 245895.0781 - val_mae: 461.0000\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 563939.5625 - mae: 610.9591 - val_loss: 236484.0469 - val_mae: 449.8683\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 577241.1875 - mae: 616.4813 - val_loss: 247210.7031 - val_mae: 459.9914\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 576307.2500 - mae: 617.7853 - val_loss: 246373.7500 - val_mae: 459.6146\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 575275.4375 - mae: 617.5328 - val_loss: 246755.1719 - val_mae: 459.6146\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 580118.0000 - mae: 621.5723 - val_loss: 244321.3594 - val_mae: 456.8534\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 575175.9375 - mae: 617.5074 - val_loss: 243199.6406 - val_mae: 454.5161\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 577021.8125 - mae: 617.8453 - val_loss: 246484.6094 - val_mae: 457.3021\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 572535.0000 - mae: 615.0849 - val_loss: 252134.7500 - val_mae: 464.1985\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 10s 235ms/step - loss: 573893.6875 - mae: 619.5440 - val_loss: 247000.5156 - val_mae: 458.7895\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 571359.6250 - mae: 615.8100 - val_loss: 247369.4531 - val_mae: 458.7667\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 571876.5000 - mae: 616.2540 - val_loss: 242499.5781 - val_mae: 451.7759\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 573960.4375 - mae: 617.9678 - val_loss: 253482.7500 - val_mae: 464.0107\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 564145.0625 - mae: 614.1081 - val_loss: 247070.6719 - val_mae: 456.3817\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 573519.6250 - mae: 619.7506 - val_loss: 254718.3750 - val_mae: 464.3388\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 569593.8125 - mae: 617.4873 - val_loss: 252075.2656 - val_mae: 461.3646\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 567327.7500 - mae: 615.6345 - val_loss: 244746.3125 - val_mae: 451.8218\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 571313.6250 - mae: 619.6855 - val_loss: 245871.7500 - val_mae: 453.5251\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 560446.3750 - mae: 613.8661 - val_loss: 245876.4375 - val_mae: 452.0099\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 566588.2500 - mae: 615.1932 - val_loss: 250565.0781 - val_mae: 458.2728\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 575049.3750 - mae: 623.0255 - val_loss: 251339.2344 - val_mae: 458.5305\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 565546.3750 - mae: 614.7905 - val_loss: 249190.4375 - val_mae: 454.1637\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 554852.0625 - mae: 612.5441 - val_loss: 254368.6094 - val_mae: 459.6146\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 568093.7500 - mae: 619.3530 - val_loss: 250759.1250 - val_mae: 455.8660\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 560110.9375 - mae: 614.4760 - val_loss: 259231.4375 - val_mae: 464.5730\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 562569.8125 - mae: 615.6803 - val_loss: 254961.5781 - val_mae: 459.0521\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 560096.7500 - mae: 614.8438 - val_loss: 255053.4531 - val_mae: 457.5833\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 561952.3125 - mae: 616.0062 - val_loss: 260647.6406 - val_mae: 464.6439\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 565146.8125 - mae: 619.1167 - val_loss: 255048.7344 - val_mae: 458.6240\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 562088.3750 - mae: 616.9578 - val_loss: 254037.3750 - val_mae: 456.2888\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 554693.5625 - mae: 610.5394 - val_loss: 257132.3594 - val_mae: 459.0521\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 557523.3750 - mae: 616.2529 - val_loss: 257847.3125 - val_mae: 458.9688\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 559300.9375 - mae: 615.8582 - val_loss: 258047.5781 - val_mae: 459.0521\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 561592.2500 - mae: 618.5027 - val_loss: 254291.6250 - val_mae: 453.8873\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 559619.7500 - mae: 616.4875 - val_loss: 261469.3125 - val_mae: 460.4591\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 550375.5625 - mae: 611.4814 - val_loss: 259387.8594 - val_mae: 459.0521\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 554024.1875 - mae: 615.1438 - val_loss: 262823.5938 - val_mae: 460.7862\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 556324.1250 - mae: 615.2078 - val_loss: 257945.9531 - val_mae: 457.4950\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 552661.8750 - mae: 614.2463 - val_loss: 264415.8438 - val_mae: 462.5805\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 559897.0000 - mae: 618.6224 - val_loss: 253261.9844 - val_mae: 450.2006\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 562454.1250 - mae: 621.8090 - val_loss: 253675.2344 - val_mae: 450.1565\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 557995.2500 - mae: 619.1500 - val_loss: 258802.1250 - val_mae: 455.7386\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 557147.3125 - mae: 619.3486 - val_loss: 258246.6250 - val_mae: 453.6851\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 556204.1250 - mae: 618.1611 - val_loss: 264439.3750 - val_mae: 461.3646\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 543872.6875 - mae: 612.5819 - val_loss: 256729.5781 - val_mae: 452.2915\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 555562.0000 - mae: 617.7105 - val_loss: 264371.4688 - val_mae: 459.3333\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 548382.2500 - mae: 612.6424 - val_loss: 269234.7188 - val_mae: 464.7874\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 545740.5625 - mae: 613.5330 - val_loss: 265298.2188 - val_mae: 459.3334\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 551756.5625 - mae: 616.9744 - val_loss: 262661.7188 - val_mae: 456.1471\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 556106.3750 - mae: 619.0672 - val_loss: 266284.0000 - val_mae: 459.3333\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 552231.8125 - mae: 617.1795 - val_loss: 271547.0000 - val_mae: 464.7956\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 553735.3125 - mae: 618.1593 - val_loss: 267850.2188 - val_mae: 461.0833\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 550597.5625 - mae: 618.8265 - val_loss: 258465.2031 - val_mae: 449.3415\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 547956.6250 - mae: 614.7510 - val_loss: 259295.0781 - val_mae: 449.5786\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 552863.5625 - mae: 618.3828 - val_loss: 270063.7500 - val_mae: 461.6459\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 556524.1875 - mae: 622.4712 - val_loss: 270582.8125 - val_mae: 461.6458\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 537975.2500 - mae: 610.9169 - val_loss: 260657.6719 - val_mae: 449.4441\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 547806.9375 - mae: 616.8726 - val_loss: 265220.2500 - val_mae: 453.3497\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 542473.3750 - mae: 614.2959 - val_loss: 271578.2812 - val_mae: 461.3646\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 541989.3125 - mae: 617.2410 - val_loss: 270218.6562 - val_mae: 457.3021\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 539372.8750 - mae: 611.6507 - val_loss: 267479.6875 - val_mae: 455.3192\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 543064.4375 - mae: 615.3599 - val_loss: 273049.5312 - val_mae: 461.3646\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 541613.5000 - mae: 614.8672 - val_loss: 279091.3438 - val_mae: 469.2010\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 542672.2500 - mae: 615.0497 - val_loss: 273099.5000 - val_mae: 459.3333\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 546279.3750 - mae: 617.5991 - val_loss: 273625.4375 - val_mae: 459.3333\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 544288.8750 - mae: 617.2037 - val_loss: 270738.2188 - val_mae: 457.2427\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 546840.6875 - mae: 617.5043 - val_loss: 275468.8750 - val_mae: 461.3646\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 544472.1250 - mae: 617.0069 - val_loss: 267650.4375 - val_mae: 451.8501\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 542886.1875 - mae: 616.7261 - val_loss: 284665.2812 - val_mae: 469.4500\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 542753.8125 - mae: 615.8300 - val_loss: 271693.0312 - val_mae: 455.1285\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 547537.6250 - mae: 619.6106 - val_loss: 268216.2188 - val_mae: 451.1614\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 537543.5000 - mae: 616.2467 - val_loss: 272183.3438 - val_mae: 453.3358\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 545017.9375 - mae: 619.2717 - val_loss: 283327.2812 - val_mae: 467.3843\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 541138.9375 - mae: 615.8373 - val_loss: 279354.4062 - val_mae: 461.6458\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 540361.8125 - mae: 616.9508 - val_loss: 264757.3438 - val_mae: 444.6483\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 544221.1875 - mae: 620.0306 - val_loss: 279428.4375 - val_mae: 459.6146\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 540398.3125 - mae: 617.1111 - val_loss: 280005.7812 - val_mae: 461.0833\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 538717.5000 - mae: 618.3917 - val_loss: 266447.7500 - val_mae: 444.7481\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 541389.7500 - mae: 618.4705 - val_loss: 286393.2812 - val_mae: 467.5081\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 527928.5625 - mae: 612.2551 - val_loss: 281013.2188 - val_mae: 459.3333\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 537694.5000 - mae: 617.4338 - val_loss: 276135.7812 - val_mae: 452.8683\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 541825.6250 - mae: 621.6790 - val_loss: 268242.0000 - val_mae: 444.4998\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 536388.9375 - mae: 615.9053 - val_loss: 286812.8125 - val_mae: 463.5279\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 537804.6250 - mae: 617.4850 - val_loss: 274207.2188 - val_mae: 450.6228\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 537516.9375 - mae: 618.7432 - val_loss: 285590.5938 - val_mae: 463.3125\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 530273.1250 - mae: 614.7527 - val_loss: 279425.6250 - val_mae: 454.7966\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 537559.0625 - mae: 619.0245 - val_loss: 293662.1562 - val_mae: 468.1683\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 535080.6250 - mae: 617.3257 - val_loss: 274467.9062 - val_mae: 447.8657\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 534379.5000 - mae: 617.6898 - val_loss: 280450.3438 - val_mae: 454.4562\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 528668.5625 - mae: 615.0927 - val_loss: 282222.4062 - val_mae: 456.7456\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 533727.6875 - mae: 617.8082 - val_loss: 277496.5312 - val_mae: 450.3393\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 537465.0625 - mae: 621.0527 - val_loss: 287254.4375 - val_mae: 459.3333\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 534059.5625 - mae: 619.3466 - val_loss: 288155.6562 - val_mae: 459.6146\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 535571.1875 - mae: 619.9307 - val_loss: 287902.9688 - val_mae: 459.0521\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 527128.3125 - mae: 614.4923 - val_loss: 288792.3438 - val_mae: 459.3333\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 533969.1875 - mae: 619.9373 - val_loss: 274687.0312 - val_mae: 443.6416\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 535907.0000 - mae: 622.2405 - val_loss: 289525.7500 - val_mae: 459.0521\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 528101.8750 - mae: 616.3496 - val_loss: 274850.1562 - val_mae: 442.9519\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 531483.6875 - mae: 619.3071 - val_loss: 292178.2188 - val_mae: 461.6458\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 532904.0000 - mae: 620.5634 - val_loss: 280756.9688 - val_mae: 447.6563\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 532256.7500 - mae: 621.0273 - val_loss: 296992.0938 - val_mae: 464.1754\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 531118.9375 - mae: 619.9663 - val_loss: 282958.0312 - val_mae: 449.8850\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 535765.2500 - mae: 623.3220 - val_loss: 287274.9062 - val_mae: 452.4195\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 531183.5000 - mae: 619.7681 - val_loss: 293108.2812 - val_mae: 458.6875\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 527655.5625 - mae: 621.2747 - val_loss: 289126.5625 - val_mae: 452.9427\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 529123.2500 - mae: 620.1141 - val_loss: 295154.4062 - val_mae: 459.6146\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 530820.1250 - mae: 621.6700 - val_loss: 290530.4688 - val_mae: 454.6512\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 531278.0000 - mae: 622.9578 - val_loss: 291028.5938 - val_mae: 454.6321\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 524259.0938 - mae: 618.2123 - val_loss: 302870.1250 - val_mae: 468.3964\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 531036.0625 - mae: 621.9886 - val_loss: 296061.3438 - val_mae: 457.3021\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 528284.5625 - mae: 621.0911 - val_loss: 292528.3438 - val_mae: 454.5750\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 530266.1875 - mae: 623.4834 - val_loss: 293036.2188 - val_mae: 454.5558\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 528454.9375 - mae: 621.9760 - val_loss: 288278.0938 - val_mae: 449.4604\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 519424.7812 - mae: 618.3336 - val_loss: 298873.2812 - val_mae: 459.3333\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 528938.0000 - mae: 623.2231 - val_loss: 293437.7500 - val_mae: 452.1859\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 524457.3125 - mae: 620.0342 - val_loss: 290619.9062 - val_mae: 449.9062\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 519653.0625 - mae: 618.3283 - val_loss: 301176.8750 - val_mae: 461.3646\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 529790.6875 - mae: 626.8809 - val_loss: 312629.6562 - val_mae: 473.4581\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 523599.2812 - mae: 620.3936 - val_loss: 301118.5000 - val_mae: 459.0521\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 523073.4375 - mae: 621.1981 - val_loss: 303165.9688 - val_mae: 461.6458\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 525557.3750 - mae: 622.5201 - val_loss: 298264.5000 - val_mae: 456.4180\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 525874.7500 - mae: 623.0292 - val_loss: 303511.7188 - val_mae: 459.6146\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 525961.8125 - mae: 622.7306 - val_loss: 298224.4688 - val_mae: 454.0665\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 523152.2188 - mae: 621.4873 - val_loss: 309117.9062 - val_mae: 463.9713\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 524090.6250 - mae: 623.3744 - val_loss: 294204.9688 - val_mae: 449.0057\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 515873.2500 - mae: 621.1241 - val_loss: 311670.6875 - val_mae: 468.4376\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 521079.1562 - mae: 621.7363 - val_loss: 300721.2188 - val_mae: 454.2727\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 512479.6250 - mae: 618.0446 - val_loss: 313842.2812 - val_mae: 470.7870\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 519865.4688 - mae: 620.1642 - val_loss: 307233.1250 - val_mae: 459.6146\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 523697.4375 - mae: 623.6523 - val_loss: 302096.1250 - val_mae: 453.8568\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 519556.5625 - mae: 622.3457 - val_loss: 313632.0312 - val_mae: 466.4946\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 523434.7188 - mae: 623.4449 - val_loss: 309672.6250 - val_mae: 463.3958\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 518707.5312 - mae: 620.5883 - val_loss: 314693.6250 - val_mae: 466.5299\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 520864.9375 - mae: 622.1002 - val_loss: 309585.9688 - val_mae: 461.0833\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 523417.6875 - mae: 624.1592 - val_loss: 298770.7812 - val_mae: 446.9026\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 520024.4062 - mae: 623.0333 - val_loss: 316095.5938 - val_mae: 464.8329\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 520603.3750 - mae: 623.4456 - val_loss: 310917.4062 - val_mae: 459.3333\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 518706.0938 - mae: 624.2056 - val_loss: 300778.9062 - val_mae: 447.0703\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 523842.6250 - mae: 626.6971 - val_loss: 323694.6250 - val_mae: 472.1917\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 517483.0938 - mae: 621.1756 - val_loss: 319780.0938 - val_mae: 470.7194\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 522613.2812 - mae: 626.3530 - val_loss: 318484.3438 - val_mae: 464.6434\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 513548.9062 - mae: 621.5565 - val_loss: 313113.0000 - val_mae: 457.3021\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 517436.9062 - mae: 624.6086 - val_loss: 315280.3125 - val_mae: 461.6458\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 522319.3438 - mae: 626.3781 - val_loss: 308933.0000 - val_mae: 453.6881\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 520549.9375 - mae: 625.7742 - val_loss: 309926.2812 - val_mae: 453.9503\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 524385.0000 - mae: 628.8726 - val_loss: 304114.8125 - val_mae: 447.9710\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 515589.1250 - mae: 623.2514 - val_loss: 298535.3438 - val_mae: 440.4912\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 520627.4688 - mae: 626.0735 - val_loss: 305974.9062 - val_mae: 448.4641\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 520521.2812 - mae: 626.5063 - val_loss: 311966.7812 - val_mae: 453.8810\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 513253.9375 - mae: 623.0927 - val_loss: 311410.3438 - val_mae: 451.5529\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 520196.5000 - mae: 626.2957 - val_loss: 311903.4688 - val_mae: 451.5363\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 519468.3125 - mae: 626.8839 - val_loss: 306422.9062 - val_mae: 445.7378\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 508697.1250 - mae: 621.9300 - val_loss: 319972.4062 - val_mae: 461.3646\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 514309.1562 - mae: 624.6975 - val_loss: 307743.5000 - val_mae: 445.9583\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 518134.7188 - mae: 626.3367 - val_loss: 321164.3750 - val_mae: 463.1146\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 516188.9062 - mae: 625.8040 - val_loss: 313838.6562 - val_mae: 451.1762\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 249ms/step - loss: 511391.6562 - mae: 622.3529 - val_loss: 315786.8438 - val_mae: 453.7532\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 519449.7812 - mae: 628.1447 - val_loss: 321392.4375 - val_mae: 457.3021\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 512727.3125 - mae: 623.5969 - val_loss: 329096.4688 - val_mae: 467.2596\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 514561.1562 - mae: 623.9586 - val_loss: 322410.7812 - val_mae: 457.3021\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 506318.7500 - mae: 621.3327 - val_loss: 328605.7188 - val_mae: 462.9483\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 508089.2812 - mae: 622.0509 - val_loss: 330169.0312 - val_mae: 467.0253\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 514978.1562 - mae: 625.2377 - val_loss: 318147.7500 - val_mae: 451.6266\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 514735.5000 - mae: 626.2306 - val_loss: 311633.0938 - val_mae: 445.0719\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 516809.9375 - mae: 627.6875 - val_loss: 325927.5000 - val_mae: 459.6146\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 507924.1875 - mae: 624.9832 - val_loss: 318752.4375 - val_mae: 451.0158\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 518558.4062 - mae: 629.3757 - val_loss: 307668.4062 - val_mae: 439.5234\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 516415.5000 - mae: 628.1386 - val_loss: 320715.1250 - val_mae: 453.2964\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 515987.0938 - mae: 628.7327 - val_loss: 321124.2188 - val_mae: 451.5307\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 509766.1562 - mae: 625.2188 - val_loss: 315338.2188 - val_mae: 445.4473\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 511578.0625 - mae: 626.5986 - val_loss: 323087.0000 - val_mae: 455.5635\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 510793.1250 - mae: 626.3791 - val_loss: 329881.9688 - val_mae: 461.6458\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 505974.0938 - mae: 623.9247 - val_loss: 323037.4688 - val_mae: 451.4698\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 514663.9062 - mae: 627.2880 - val_loss: 324487.4688 - val_mae: 453.7666\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 510925.3125 - mae: 625.6083 - val_loss: 317631.0312 - val_mae: 445.2968\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 505426.5312 - mae: 624.4707 - val_loss: 330410.0938 - val_mae: 457.3021\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 510559.9375 - mae: 625.4255 - val_loss: 338334.5625 - val_mae: 467.5409\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 514884.9062 - mae: 627.9968 - val_loss: 346210.2500 - val_mae: 479.5564\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 506363.2188 - mae: 625.5599 - val_loss: 320507.4688 - val_mae: 447.4838\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 512316.9062 - mae: 627.7021 - val_loss: 333874.5938 - val_mae: 461.6458\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 516011.6562 - mae: 630.4590 - val_loss: 314098.2188 - val_mae: 438.8708\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 515658.4375 - mae: 630.5528 - val_loss: 328437.5938 - val_mae: 455.3932\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 509837.1875 - mae: 625.8649 - val_loss: 333929.1562 - val_mae: 457.3021\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 514664.8438 - mae: 630.3537 - val_loss: 335493.2188 - val_mae: 461.3646\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 509630.9062 - mae: 628.3752 - val_loss: 335941.2812 - val_mae: 459.6146\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 516247.1562 - mae: 632.4612 - val_loss: 335952.7812 - val_mae: 459.3333\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 506951.2500 - mae: 627.2070 - val_loss: 342964.0312 - val_mae: 467.0461\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 512671.0000 - mae: 630.0203 - val_loss: 337467.6562 - val_mae: 459.6146\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 510553.0312 - mae: 628.2681 - val_loss: 344024.6875 - val_mae: 467.4409\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 510092.8125 - mae: 627.7662 - val_loss: 324260.9062 - val_mae: 446.3115\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 6s 142ms/step - loss: 512990.2500 - mae: 629.8572 - val_loss: 332318.5312 - val_mae: 451.4768\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 514970.5000 - mae: 632.1876 - val_loss: 332755.7188 - val_mae: 451.4635\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 504343.9688 - mae: 626.6596 - val_loss: 326576.6562 - val_mae: 445.0353\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 505791.0000 - mae: 627.6777 - val_loss: 340689.8438 - val_mae: 463.3958\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 505702.9688 - mae: 626.3047 - val_loss: 346939.7500 - val_mae: 467.5254\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 504417.7500 - mae: 628.0632 - val_loss: 334515.2812 - val_mae: 453.1598\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 513349.6562 - mae: 632.7200 - val_loss: 334938.7500 - val_mae: 453.1470\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 513152.5938 - mae: 632.8702 - val_loss: 334524.0312 - val_mae: 452.5693\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 509064.6250 - mae: 629.0739 - val_loss: 328756.8438 - val_mae: 444.5898\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 498022.3125 - mae: 624.2804 - val_loss: 335864.4688 - val_mae: 452.8239\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 504076.8125 - mae: 625.4073 - val_loss: 350041.5938 - val_mae: 469.6333\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 510347.6562 - mae: 630.5441 - val_loss: 350977.2500 - val_mae: 469.9281\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 507436.9688 - mae: 627.6274 - val_loss: 344296.1562 - val_mae: 461.3646\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 506986.4062 - mae: 629.0570 - val_loss: 352042.5312 - val_mae: 469.9586\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 514214.4375 - mae: 633.8217 - val_loss: 338186.3750 - val_mae: 452.7542\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 507613.1875 - mae: 630.2182 - val_loss: 345046.1562 - val_mae: 457.3021\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 511318.3750 - mae: 632.8021 - val_loss: 338648.3438 - val_mae: 450.6969\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 502487.4688 - mae: 626.3340 - val_loss: 346344.4062 - val_mae: 459.3333\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 503293.1562 - mae: 627.8370 - val_loss: 347695.6562 - val_mae: 463.3958\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 509153.5000 - mae: 630.7043 - val_loss: 348631.5625 - val_mae: 463.6771\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 508189.8438 - mae: 629.8334 - val_loss: 348133.5312 - val_mae: 461.3646\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 508138.0625 - mae: 630.6407 - val_loss: 347735.0312 - val_mae: 457.3021\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 514065.8750 - mae: 634.7843 - val_loss: 348712.1562 - val_mae: 457.5833\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 506457.9062 - mae: 632.3849 - val_loss: 356842.4062 - val_mae: 470.1047\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 513342.5000 - mae: 634.9187 - val_loss: 356874.1562 - val_mae: 470.1427\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 501846.7500 - mae: 627.6813 - val_loss: 357778.7500 - val_mae: 470.7070\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 507900.0938 - mae: 631.3577 - val_loss: 336194.2500 - val_mae: 444.8145\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 505374.4062 - mae: 629.6744 - val_loss: 343929.4062 - val_mae: 453.8037\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 508563.0938 - mae: 631.7635 - val_loss: 337508.3438 - val_mae: 445.6797\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 508085.4062 - mae: 632.2996 - val_loss: 366093.8750 - val_mae: 478.0597\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 508796.6875 - mae: 632.5533 - val_loss: 345582.4062 - val_mae: 454.8800\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 503263.1875 - mae: 630.0749 - val_loss: 339134.5000 - val_mae: 446.7550\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 509158.1562 - mae: 634.0709 - val_loss: 353383.0312 - val_mae: 463.9013\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 505918.4062 - mae: 630.6571 - val_loss: 339744.0938 - val_mae: 448.8645\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 504332.6875 - mae: 630.2335 - val_loss: 360899.5000 - val_mae: 469.3738\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 509587.6875 - mae: 633.2859 - val_loss: 347719.8438 - val_mae: 454.6297\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 503868.0000 - mae: 629.8001 - val_loss: 355565.0000 - val_mae: 465.2935\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 507885.3750 - mae: 632.1465 - val_loss: 355913.5938 - val_mae: 465.5185\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 493084.6875 - mae: 625.3210 - val_loss: 355422.8750 - val_mae: 463.6025\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 506207.6250 - mae: 632.5771 - val_loss: 349397.7812 - val_mae: 455.7787\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 498750.5625 - mae: 628.0747 - val_loss: 350076.5000 - val_mae: 459.2917\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 506504.3438 - mae: 632.3098 - val_loss: 357139.0312 - val_mae: 466.3270\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 503595.7188 - mae: 629.3950 - val_loss: 350432.9062 - val_mae: 458.0891\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 508794.1562 - mae: 634.1926 - val_loss: 358240.8438 - val_mae: 468.4781\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 507421.7500 - mae: 632.7281 - val_loss: 351188.1562 - val_mae: 460.1641\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 510470.3750 - mae: 635.3107 - val_loss: 351325.0000 - val_mae: 458.6991\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 507469.6250 - mae: 633.8364 - val_loss: 352092.6562 - val_mae: 459.1737\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 505337.2500 - mae: 633.1302 - val_loss: 338228.5938 - val_mae: 444.0237\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 506123.7188 - mae: 632.6995 - val_loss: 353147.2812 - val_mae: 461.3783\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 506145.6875 - mae: 633.2967 - val_loss: 338391.2812 - val_mae: 444.1586\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501466.6562 - mae: 629.3798 - val_loss: 367037.8750 - val_mae: 473.5453\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 505211.0938 - mae: 632.3698 - val_loss: 346601.6250 - val_mae: 453.2971\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 504325.6562 - mae: 632.1328 - val_loss: 361406.5000 - val_mae: 469.0439\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 509189.5625 - mae: 635.5852 - val_loss: 355057.2812 - val_mae: 462.5677\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 500655.2188 - mae: 630.7065 - val_loss: 370157.5000 - val_mae: 479.9597\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 504718.2500 - mae: 634.0485 - val_loss: 354127.5938 - val_mae: 459.0867\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 495750.0625 - mae: 628.8959 - val_loss: 362440.1562 - val_mae: 469.7127\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 505473.2188 - mae: 633.3378 - val_loss: 356062.6250 - val_mae: 461.7414\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 506963.1875 - mae: 634.4772 - val_loss: 356430.6250 - val_mae: 461.9775\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 506857.7812 - mae: 634.3287 - val_loss: 378203.5312 - val_mae: 484.6021\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 508141.0938 - mae: 635.5422 - val_loss: 364203.7188 - val_mae: 469.3871\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 497712.1562 - mae: 630.0691 - val_loss: 350367.0000 - val_mae: 454.2545\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 507323.2188 - mae: 634.6494 - val_loss: 358227.7188 - val_mae: 464.5238\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 503356.0938 - mae: 633.0052 - val_loss: 372450.3438 - val_mae: 478.5042\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 505424.7188 - mae: 634.4653 - val_loss: 358114.4062 - val_mae: 464.4753\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 504022.4375 - mae: 632.8280 - val_loss: 365538.7188 - val_mae: 470.2777\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 503599.2812 - mae: 633.8644 - val_loss: 366723.4688 - val_mae: 472.3730\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 506177.1562 - mae: 635.7906 - val_loss: 366962.7188 - val_mae: 472.5214\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 505608.8125 - mae: 634.7824 - val_loss: 374984.3438 - val_mae: 482.9229\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 486095.6250 - mae: 625.4189 - val_loss: 375199.1562 - val_mae: 480.2176\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 505513.6250 - mae: 634.5740 - val_loss: 367999.1562 - val_mae: 473.1629\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 496423.2500 - mae: 631.2421 - val_loss: 360551.0312 - val_mae: 464.6212\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 507681.0938 - mae: 637.2547 - val_loss: 368675.3750 - val_mae: 473.5802\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 501262.9688 - mae: 631.5993 - val_loss: 362571.3438 - val_mae: 467.1602\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 505316.2812 - mae: 634.2284 - val_loss: 362223.4062 - val_mae: 465.6547\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 504154.6250 - mae: 633.6497 - val_loss: 369013.4688 - val_mae: 472.5076\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 506460.0938 - mae: 636.3724 - val_loss: 369953.1562 - val_mae: 471.7768\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 505513.2812 - mae: 635.2521 - val_loss: 362308.8438 - val_mae: 464.4514\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 504259.3125 - mae: 634.3842 - val_loss: 370084.5000 - val_mae: 473.1898\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 505067.8125 - mae: 635.6421 - val_loss: 363291.0312 - val_mae: 466.3433\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501508.9062 - mae: 633.2977 - val_loss: 371782.5938 - val_mae: 476.7411\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 504653.5312 - mae: 634.8294 - val_loss: 371986.2500 - val_mae: 476.8606\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 508020.5625 - mae: 637.4103 - val_loss: 372127.0938 - val_mae: 475.6962\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 503844.2188 - mae: 634.2614 - val_loss: 364765.9062 - val_mae: 466.0146\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 500238.9688 - mae: 631.4668 - val_loss: 379529.5312 - val_mae: 481.7095\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 498900.4062 - mae: 633.5074 - val_loss: 373142.0312 - val_mae: 476.3138\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 505597.3125 - mae: 636.2637 - val_loss: 373400.6250 - val_mae: 474.0388\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 495913.1875 - mae: 631.1322 - val_loss: 381212.9062 - val_mae: 485.4699\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 503326.5625 - mae: 635.8948 - val_loss: 358892.7812 - val_mae: 459.7638\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 507061.6562 - mae: 637.1754 - val_loss: 366727.8438 - val_mae: 469.6672\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 509209.2812 - mae: 639.6140 - val_loss: 374276.1250 - val_mae: 474.6418\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 489536.2188 - mae: 631.0623 - val_loss: 360017.9062 - val_mae: 461.6411\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 498663.1875 - mae: 633.9025 - val_loss: 359524.1250 - val_mae: 460.1865\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 502449.3438 - mae: 634.3338 - val_loss: 390937.1562 - val_mae: 494.9740\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 503946.4062 - mae: 635.2371 - val_loss: 376094.7812 - val_mae: 476.9337\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 494095.7812 - mae: 630.5736 - val_loss: 391260.5312 - val_mae: 494.0050\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 500833.0000 - mae: 633.3651 - val_loss: 376852.8438 - val_mae: 478.5513\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 503691.0938 - mae: 635.0487 - val_loss: 369307.7188 - val_mae: 470.0692\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 504478.1875 - mae: 636.0292 - val_loss: 384340.1250 - val_mae: 485.9154\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 500101.2812 - mae: 633.7670 - val_loss: 369498.7188 - val_mae: 470.2200\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 502342.9375 - mae: 635.3102 - val_loss: 361272.9688 - val_mae: 461.3490\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 504117.8438 - mae: 636.0311 - val_loss: 385892.2812 - val_mae: 487.9746\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 497487.7188 - mae: 631.7306 - val_loss: 370552.4062 - val_mae: 469.7729\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 505679.7812 - mae: 637.7063 - val_loss: 354845.0000 - val_mae: 454.6786\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 496719.2812 - mae: 632.9351 - val_loss: 378476.4062 - val_mae: 478.5028\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 502697.8438 - mae: 635.6233 - val_loss: 378744.3750 - val_mae: 478.6757\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 507042.2500 - mae: 638.5453 - val_loss: 378768.8750 - val_mae: 477.6235\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 498316.6562 - mae: 631.6882 - val_loss: 379601.5000 - val_mae: 480.2881\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 501301.3438 - mae: 633.6124 - val_loss: 357055.7500 - val_mae: 456.0351\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 506074.5625 - mae: 637.8884 - val_loss: 380237.5000 - val_mae: 480.6828\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 502759.0938 - mae: 635.4703 - val_loss: 372152.6250 - val_mae: 470.8650\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 504431.5625 - mae: 637.0093 - val_loss: 372693.0000 - val_mae: 472.2516\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 504565.8438 - mae: 637.1241 - val_loss: 388346.0312 - val_mae: 488.4459\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 502841.7812 - mae: 635.7324 - val_loss: 389501.7188 - val_mae: 491.2008\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 501427.6875 - mae: 635.4583 - val_loss: 381298.2500 - val_mae: 480.3167\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 494156.5312 - mae: 632.8571 - val_loss: 381674.9062 - val_mae: 481.5720\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 505346.5938 - mae: 638.4111 - val_loss: 367355.4062 - val_mae: 465.1573\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 504470.7500 - mae: 638.2028 - val_loss: 374203.5312 - val_mae: 473.2051\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 508031.2812 - mae: 640.7630 - val_loss: 390373.2188 - val_mae: 489.6917\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498144.2812 - mae: 636.2431 - val_loss: 374203.8438 - val_mae: 473.2311\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 501805.6875 - mae: 635.2770 - val_loss: 376028.5000 - val_mae: 475.2904\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 502129.7500 - mae: 635.8587 - val_loss: 367750.5312 - val_mae: 466.8076\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 497467.8750 - mae: 633.7039 - val_loss: 367259.0938 - val_mae: 465.1920\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 506057.5625 - mae: 638.6927 - val_loss: 376277.2188 - val_mae: 475.4544\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 506394.4375 - mae: 639.1730 - val_loss: 376334.6250 - val_mae: 474.5177\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 498838.9688 - mae: 634.3379 - val_loss: 376626.0000 - val_mae: 474.7001\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 500757.3125 - mae: 635.7108 - val_loss: 376340.0312 - val_mae: 474.5459\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 497014.0625 - mae: 633.3718 - val_loss: 377273.7188 - val_mae: 476.0549\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 498185.3438 - mae: 634.0367 - val_loss: 377104.3438 - val_mae: 475.9663\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 500940.0938 - mae: 635.5864 - val_loss: 385228.1250 - val_mae: 483.7536\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 502578.3750 - mae: 636.2342 - val_loss: 369879.7500 - val_mae: 467.7661\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 503863.3750 - mae: 637.9576 - val_loss: 370122.7188 - val_mae: 467.9151\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 502532.6250 - mae: 637.5292 - val_loss: 385448.1562 - val_mae: 483.9057\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 495284.5000 - mae: 633.0837 - val_loss: 386471.9062 - val_mae: 483.5747\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 501192.5312 - mae: 636.4141 - val_loss: 393981.7500 - val_mae: 491.9565\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 502353.5000 - mae: 636.8643 - val_loss: 378377.3438 - val_mae: 474.9112\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500869.2812 - mae: 636.2449 - val_loss: 371364.1562 - val_mae: 468.6748\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 504686.5625 - mae: 638.6595 - val_loss: 387726.2500 - val_mae: 485.2568\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 502679.4375 - mae: 637.4587 - val_loss: 394844.5000 - val_mae: 491.6013\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 504010.0000 - mae: 638.5662 - val_loss: 379467.2500 - val_mae: 476.4925\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 500077.1875 - mae: 636.2206 - val_loss: 395449.3750 - val_mae: 492.8615\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 503980.7500 - mae: 638.2872 - val_loss: 380060.5938 - val_mae: 476.8597\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 495764.9062 - mae: 632.4667 - val_loss: 396475.7812 - val_mae: 493.4714\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 501168.5000 - mae: 636.4127 - val_loss: 380139.4062 - val_mae: 477.7948\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 502331.6562 - mae: 636.3292 - val_loss: 388965.4062 - val_mae: 486.0233\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 497966.6250 - mae: 635.2819 - val_loss: 388462.4062 - val_mae: 484.8809\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 499807.1250 - mae: 636.6364 - val_loss: 381627.0000 - val_mae: 477.8039\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 495357.3750 - mae: 634.4725 - val_loss: 405358.7500 - val_mae: 503.4801\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 503568.7812 - mae: 639.0198 - val_loss: 381861.2812 - val_mae: 477.1094\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 502400.7812 - mae: 638.3837 - val_loss: 373752.7500 - val_mae: 470.1451\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 505910.6562 - mae: 640.1445 - val_loss: 397833.4062 - val_mae: 494.3239\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 502212.1875 - mae: 637.3461 - val_loss: 383244.7812 - val_mae: 478.7765\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 503299.1875 - mae: 638.6149 - val_loss: 382966.7500 - val_mae: 478.6273\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 500857.9375 - mae: 636.1689 - val_loss: 390989.4062 - val_mae: 487.5912\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 494021.5625 - mae: 634.0172 - val_loss: 391315.8750 - val_mae: 488.2496\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 502858.1250 - mae: 637.9864 - val_loss: 383413.8438 - val_mae: 478.0968\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 497438.0625 - mae: 634.9681 - val_loss: 391459.1250 - val_mae: 486.7235\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 506307.4062 - mae: 641.6278 - val_loss: 383323.0000 - val_mae: 478.0714\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 501065.9062 - mae: 637.6828 - val_loss: 399985.4062 - val_mae: 494.8267\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 502507.7188 - mae: 638.6582 - val_loss: 392035.7500 - val_mae: 487.0815\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 502801.9375 - mae: 639.5456 - val_loss: 392216.8438 - val_mae: 487.5415\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 506305.0312 - mae: 641.2893 - val_loss: 392147.7812 - val_mae: 487.9513\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 502086.9688 - mae: 637.4835 - val_loss: 383847.0000 - val_mae: 478.4355\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 501510.3750 - mae: 637.1154 - val_loss: 385023.9062 - val_mae: 480.6714\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 498288.9375 - mae: 635.5936 - val_loss: 377668.0000 - val_mae: 471.7057\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 497391.7188 - mae: 635.0404 - val_loss: 393265.6562 - val_mae: 488.6045\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 500347.7812 - mae: 637.8400 - val_loss: 401112.2812 - val_mae: 496.3193\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 500058.0625 - mae: 636.9100 - val_loss: 393651.9062 - val_mae: 488.8348\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 505648.6562 - mae: 641.2139 - val_loss: 385736.6562 - val_mae: 481.0894\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 501268.5312 - mae: 638.0474 - val_loss: 386078.4062 - val_mae: 479.7812\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 499689.0312 - mae: 637.4429 - val_loss: 371042.2500 - val_mae: 464.7354\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 498808.8438 - mae: 639.3029 - val_loss: 377451.7500 - val_mae: 472.3872\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 495098.9375 - mae: 635.9191 - val_loss: 402534.2812 - val_mae: 498.6509\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 493036.2188 - mae: 634.4799 - val_loss: 402459.3750 - val_mae: 497.8680\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 504973.0938 - mae: 640.7350 - val_loss: 410654.3750 - val_mae: 505.0555\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 500760.7500 - mae: 639.7943 - val_loss: 386747.0938 - val_mae: 480.9528\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 503414.0625 - mae: 640.1909 - val_loss: 411052.6562 - val_mae: 505.2994\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 501714.6875 - mae: 639.3965 - val_loss: 387417.5938 - val_mae: 481.3394\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 504146.0938 - mae: 640.1177 - val_loss: 411071.0938 - val_mae: 504.5940\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 500953.1250 - mae: 637.9478 - val_loss: 378936.8750 - val_mae: 472.5448\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 495400.5312 - mae: 633.8073 - val_loss: 379617.3438 - val_mae: 472.9394\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 502711.3125 - mae: 639.5768 - val_loss: 403895.0938 - val_mae: 499.0497\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 503260.2500 - mae: 639.0642 - val_loss: 388456.9062 - val_mae: 482.6684\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 501985.7188 - mae: 638.1585 - val_loss: 395778.3750 - val_mae: 489.7603\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 499113.0312 - mae: 637.1407 - val_loss: 396494.2500 - val_mae: 490.5218\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 501741.4688 - mae: 638.9433 - val_loss: 405089.5938 - val_mae: 499.3880\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 495189.9375 - mae: 636.2802 - val_loss: 404830.5000 - val_mae: 499.5914\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 503684.2812 - mae: 640.0578 - val_loss: 381560.7812 - val_mae: 474.7885\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 498998.9688 - mae: 637.0477 - val_loss: 396888.2500 - val_mae: 490.7672\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 501069.4062 - mae: 638.0416 - val_loss: 397753.3438 - val_mae: 492.2829\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 493367.6562 - mae: 633.9621 - val_loss: 397033.1250 - val_mae: 490.1828\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 504233.6250 - mae: 641.4711 - val_loss: 389499.2188 - val_mae: 482.6140\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 496880.8750 - mae: 636.8761 - val_loss: 389472.8750 - val_mae: 481.9372\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 492818.0938 - mae: 634.2843 - val_loss: 406271.1562 - val_mae: 498.7488\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 500934.3125 - mae: 639.0873 - val_loss: 405778.2500 - val_mae: 498.4710\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 504617.7188 - mae: 640.9372 - val_loss: 390948.0938 - val_mae: 482.7971\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 501799.4688 - mae: 639.2803 - val_loss: 398860.8750 - val_mae: 491.2522\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 500209.0938 - mae: 638.8724 - val_loss: 390767.7188 - val_mae: 484.0075\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 499916.4375 - mae: 637.3058 - val_loss: 382542.4062 - val_mae: 474.7373\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 501603.1875 - mae: 638.9277 - val_loss: 382721.1562 - val_mae: 475.4870\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 503713.9375 - mae: 640.2834 - val_loss: 383771.6562 - val_mae: 475.4411\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 494221.9688 - mae: 634.8468 - val_loss: 398896.7500 - val_mae: 491.9491\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500937.5000 - mae: 638.7720 - val_loss: 399460.7812 - val_mae: 492.2682\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 505020.4375 - mae: 641.3119 - val_loss: 383372.1562 - val_mae: 475.2445\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 500759.6875 - mae: 638.5925 - val_loss: 391539.4688 - val_mae: 484.4592\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 496125.6250 - mae: 638.2183 - val_loss: 391558.7188 - val_mae: 483.8486\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 500299.8750 - mae: 639.7027 - val_loss: 392018.4062 - val_mae: 483.4893\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 500677.3750 - mae: 637.7788 - val_loss: 392035.7812 - val_mae: 483.5000\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 502552.5312 - mae: 640.6913 - val_loss: 416490.2500 - val_mae: 508.6038\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 500753.2500 - mae: 638.7214 - val_loss: 400368.7500 - val_mae: 492.1906\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 501657.0312 - mae: 639.3317 - val_loss: 400095.4688 - val_mae: 492.0468\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 492289.8750 - mae: 636.3157 - val_loss: 408380.4688 - val_mae: 499.4785\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 500801.9688 - mae: 638.6534 - val_loss: 401106.4062 - val_mae: 493.8302\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 502297.0625 - mae: 639.3873 - val_loss: 377807.5312 - val_mae: 468.7982\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 502728.5312 - mae: 639.9597 - val_loss: 393592.4688 - val_mae: 485.6323\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 502893.9688 - mae: 639.9370 - val_loss: 410041.1562 - val_mae: 502.2353\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 504977.8750 - mae: 641.5636 - val_loss: 401854.5312 - val_mae: 493.6671\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501909.8750 - mae: 640.2345 - val_loss: 393473.0000 - val_mae: 484.9896\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 499240.9062 - mae: 640.0506 - val_loss: 401601.7500 - val_mae: 493.5305\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 504294.1562 - mae: 641.6393 - val_loss: 410532.9688 - val_mae: 501.9412\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 504953.6250 - mae: 642.0870 - val_loss: 385556.2812 - val_mae: 477.1622\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 498188.7500 - mae: 636.9376 - val_loss: 385626.3438 - val_mae: 477.2031\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 502836.0000 - mae: 641.2475 - val_loss: 402585.4062 - val_mae: 494.0924\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 502308.1250 - mae: 640.6528 - val_loss: 393758.5312 - val_mae: 485.7383\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 497724.3125 - mae: 636.2089 - val_loss: 394933.9688 - val_mae: 486.3993\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 496513.0312 - mae: 637.5436 - val_loss: 402767.1562 - val_mae: 493.6395\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 505664.7812 - mae: 642.9748 - val_loss: 402425.5938 - val_mae: 493.4547\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 503224.1250 - mae: 641.5325 - val_loss: 403564.7812 - val_mae: 494.6511\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 499840.7812 - mae: 638.7574 - val_loss: 394744.3438 - val_mae: 486.2962\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 500346.1562 - mae: 639.0553 - val_loss: 387446.0000 - val_mae: 478.2412\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 492782.2812 - mae: 635.4422 - val_loss: 403224.0938 - val_mae: 493.9146\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 503006.5625 - mae: 640.9640 - val_loss: 403451.5000 - val_mae: 494.5953\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 500769.2188 - mae: 639.7999 - val_loss: 387018.6250 - val_mae: 477.4585\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 495000.1250 - mae: 635.3742 - val_loss: 394776.7500 - val_mae: 486.3197\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501018.0625 - mae: 639.4806 - val_loss: 395791.9688 - val_mae: 486.3480\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 503561.7188 - mae: 642.1943 - val_loss: 404216.5000 - val_mae: 494.4904\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502022.3125 - mae: 640.2333 - val_loss: 379096.0938 - val_mae: 469.6169\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500734.2812 - mae: 638.8063 - val_loss: 396202.8438 - val_mae: 487.1222\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 497472.8750 - mae: 638.2860 - val_loss: 403862.5000 - val_mae: 493.7679\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 497552.9375 - mae: 637.2086 - val_loss: 387676.3750 - val_mae: 477.8554\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 501496.5625 - mae: 639.7917 - val_loss: 396382.5000 - val_mae: 487.2244\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502008.8125 - mae: 640.0079 - val_loss: 412860.7188 - val_mae: 503.3136\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 496206.7812 - mae: 638.5637 - val_loss: 388049.1250 - val_mae: 478.6020\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 504087.5938 - mae: 641.9776 - val_loss: 396059.3750 - val_mae: 486.5214\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 499188.6875 - mae: 638.8850 - val_loss: 396644.0312 - val_mae: 486.8510\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 504001.5000 - mae: 642.7396 - val_loss: 404852.7500 - val_mae: 495.4064\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 500607.7812 - mae: 639.1775 - val_loss: 412812.5938 - val_mae: 503.2996\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 502714.2812 - mae: 640.5168 - val_loss: 405085.0938 - val_mae: 496.0550\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502387.1875 - mae: 640.2438 - val_loss: 413565.3438 - val_mae: 503.7276\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 499028.4062 - mae: 637.9003 - val_loss: 388335.9062 - val_mae: 478.7786\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 502679.4688 - mae: 641.2030 - val_loss: 405911.2188 - val_mae: 495.5055\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503360.9375 - mae: 641.9889 - val_loss: 404950.4062 - val_mae: 495.3120\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 498656.9375 - mae: 638.2501 - val_loss: 405540.5312 - val_mae: 495.3036\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 491354.6250 - mae: 635.5950 - val_loss: 405730.7188 - val_mae: 495.9131\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 490187.0000 - mae: 635.0886 - val_loss: 413786.8438 - val_mae: 503.8714\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 498583.4375 - mae: 637.3759 - val_loss: 422566.7500 - val_mae: 512.7331\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 498344.5312 - mae: 639.2850 - val_loss: 414159.0312 - val_mae: 504.0894\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 503740.6875 - mae: 641.6421 - val_loss: 388567.4688 - val_mae: 478.4342\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 503263.9375 - mae: 641.9105 - val_loss: 397403.3438 - val_mae: 487.8115\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 497535.8750 - mae: 638.3271 - val_loss: 389844.5000 - val_mae: 479.6407\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 506331.6562 - mae: 643.6801 - val_loss: 422820.2500 - val_mae: 512.8787\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 499818.7812 - mae: 639.2495 - val_loss: 398394.1250 - val_mae: 487.8804\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 502823.7500 - mae: 641.8297 - val_loss: 389949.1562 - val_mae: 479.2209\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 500369.7500 - mae: 639.3608 - val_loss: 390092.0938 - val_mae: 479.7836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "6534d854-5c45-4d03-f0dd-2fbd1abc031e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1514655.25,\n",
              "  1499907.625,\n",
              "  1510851.625,\n",
              "  1486000.625,\n",
              "  1488147.0,\n",
              "  1492266.5,\n",
              "  1490570.75,\n",
              "  1479627.625,\n",
              "  1491138.875,\n",
              "  1486623.375,\n",
              "  1480745.75,\n",
              "  1469009.0,\n",
              "  1476101.25,\n",
              "  1460271.625,\n",
              "  1465507.625,\n",
              "  1474563.75,\n",
              "  1446695.625,\n",
              "  1474772.25,\n",
              "  1461580.5,\n",
              "  1440443.125,\n",
              "  1460667.375,\n",
              "  1441983.875,\n",
              "  1453496.0,\n",
              "  1444258.25,\n",
              "  1418206.5,\n",
              "  1441178.75,\n",
              "  1436243.125,\n",
              "  1427563.75,\n",
              "  1427555.75,\n",
              "  1441077.5,\n",
              "  1428017.75,\n",
              "  1436140.875,\n",
              "  1418636.25,\n",
              "  1410827.375,\n",
              "  1417703.625,\n",
              "  1424022.5,\n",
              "  1431338.25,\n",
              "  1405583.75,\n",
              "  1401230.25,\n",
              "  1390016.5,\n",
              "  1388073.25,\n",
              "  1396928.875,\n",
              "  1392059.0,\n",
              "  1386859.0,\n",
              "  1383058.25,\n",
              "  1384532.375,\n",
              "  1364116.625,\n",
              "  1377550.75,\n",
              "  1357483.25,\n",
              "  1385331.625,\n",
              "  1374512.75,\n",
              "  1368675.5,\n",
              "  1350416.5,\n",
              "  1355535.0,\n",
              "  1369216.375,\n",
              "  1355766.5,\n",
              "  1349434.75,\n",
              "  1361949.375,\n",
              "  1349767.875,\n",
              "  1343654.875,\n",
              "  1331952.75,\n",
              "  1342600.625,\n",
              "  1325673.75,\n",
              "  1336309.5,\n",
              "  1320611.0,\n",
              "  1330557.625,\n",
              "  1330872.125,\n",
              "  1308282.75,\n",
              "  1340319.375,\n",
              "  1342688.375,\n",
              "  1326823.625,\n",
              "  1325505.0,\n",
              "  1330264.75,\n",
              "  1308615.75,\n",
              "  1310133.75,\n",
              "  1312323.75,\n",
              "  1310443.875,\n",
              "  1297995.375,\n",
              "  1300601.5,\n",
              "  1294728.125,\n",
              "  1289156.875,\n",
              "  1302315.375,\n",
              "  1280760.375,\n",
              "  1289765.875,\n",
              "  1292866.25,\n",
              "  1270691.25,\n",
              "  1286785.75,\n",
              "  1271025.625,\n",
              "  1286222.125,\n",
              "  1274037.25,\n",
              "  1275366.125,\n",
              "  1255722.25,\n",
              "  1270242.5,\n",
              "  1271164.75,\n",
              "  1249264.375,\n",
              "  1253284.875,\n",
              "  1264214.5,\n",
              "  1237879.375,\n",
              "  1245712.125,\n",
              "  1232332.25,\n",
              "  1248612.375,\n",
              "  1257489.5,\n",
              "  1266511.125,\n",
              "  1225661.625,\n",
              "  1225169.75,\n",
              "  1213561.25,\n",
              "  1222152.125,\n",
              "  1240763.875,\n",
              "  1226583.5,\n",
              "  1201939.125,\n",
              "  1210614.5,\n",
              "  1232218.125,\n",
              "  1219079.625,\n",
              "  1216041.625,\n",
              "  1202780.0,\n",
              "  1212423.375,\n",
              "  1213785.125,\n",
              "  1201692.5,\n",
              "  1203221.25,\n",
              "  1211328.5,\n",
              "  1178257.75,\n",
              "  1186780.75,\n",
              "  1203301.625,\n",
              "  1189194.625,\n",
              "  1190406.0,\n",
              "  1172030.25,\n",
              "  1186785.875,\n",
              "  1186679.75,\n",
              "  1171631.625,\n",
              "  1180940.625,\n",
              "  1170413.125,\n",
              "  1184126.0,\n",
              "  1150577.875,\n",
              "  1179592.0,\n",
              "  1157384.375,\n",
              "  1160656.625,\n",
              "  1145320.5,\n",
              "  1159412.5,\n",
              "  1163856.375,\n",
              "  1159715.125,\n",
              "  1166522.0,\n",
              "  1148540.0,\n",
              "  1139026.375,\n",
              "  1149919.0,\n",
              "  1132555.75,\n",
              "  1125066.625,\n",
              "  1146305.0,\n",
              "  1126918.875,\n",
              "  1133882.5,\n",
              "  1128672.25,\n",
              "  1130339.25,\n",
              "  1125033.625,\n",
              "  1132929.5,\n",
              "  1117403.0,\n",
              "  1126071.375,\n",
              "  1104925.75,\n",
              "  1120973.0,\n",
              "  1134564.5,\n",
              "  1103045.125,\n",
              "  1109908.375,\n",
              "  1118774.375,\n",
              "  1117579.375,\n",
              "  1083211.625,\n",
              "  1091870.125,\n",
              "  1095639.375,\n",
              "  1106231.25,\n",
              "  1079704.875,\n",
              "  1095387.25,\n",
              "  1086258.5,\n",
              "  1090108.375,\n",
              "  1089252.375,\n",
              "  1077987.0,\n",
              "  1083713.0,\n",
              "  1095573.5,\n",
              "  1084507.5,\n",
              "  1072406.5,\n",
              "  1060932.75,\n",
              "  1082926.625,\n",
              "  1086522.625,\n",
              "  1072627.0,\n",
              "  1053656.125,\n",
              "  1053024.625,\n",
              "  1059932.75,\n",
              "  1041203.875,\n",
              "  1066782.0,\n",
              "  1061477.125,\n",
              "  1058882.0,\n",
              "  1065434.25,\n",
              "  1062396.375,\n",
              "  1019927.3125,\n",
              "  1047066.9375,\n",
              "  1052641.625,\n",
              "  1036283.9375,\n",
              "  1044402.8125,\n",
              "  1027469.3125,\n",
              "  1018624.875,\n",
              "  1038759.0625,\n",
              "  1027303.6875,\n",
              "  1024592.1875,\n",
              "  1021756.125,\n",
              "  1001686.125,\n",
              "  1012302.0625,\n",
              "  1022407.9375,\n",
              "  1038612.125,\n",
              "  999092.3125,\n",
              "  1006771.25,\n",
              "  1011673.625,\n",
              "  1020977.9375,\n",
              "  1019631.9375,\n",
              "  998463.6875,\n",
              "  997764.625,\n",
              "  1015143.6875,\n",
              "  1009578.5625,\n",
              "  1002159.0625,\n",
              "  1005177.3125,\n",
              "  993112.0,\n",
              "  987511.0,\n",
              "  1000586.6875,\n",
              "  973886.875,\n",
              "  994523.5,\n",
              "  986089.875,\n",
              "  981008.5,\n",
              "  993590.3125,\n",
              "  979482.3125,\n",
              "  980439.4375,\n",
              "  981801.8125,\n",
              "  981806.0625,\n",
              "  981230.9375,\n",
              "  976340.4375,\n",
              "  972018.5625,\n",
              "  958913.1875,\n",
              "  977196.875,\n",
              "  968982.75,\n",
              "  971453.6875,\n",
              "  968139.0,\n",
              "  961575.125,\n",
              "  954490.1875,\n",
              "  936724.8125,\n",
              "  958543.375,\n",
              "  953119.4375,\n",
              "  957971.375,\n",
              "  941734.1875,\n",
              "  949134.375,\n",
              "  942973.8125,\n",
              "  920776.9375,\n",
              "  936316.8125,\n",
              "  945600.75,\n",
              "  943939.1875,\n",
              "  930823.0625,\n",
              "  942300.6875,\n",
              "  929539.625,\n",
              "  926052.625,\n",
              "  920825.8125,\n",
              "  932403.0,\n",
              "  934282.5625,\n",
              "  927116.4375,\n",
              "  918444.375,\n",
              "  925178.3125,\n",
              "  904497.875,\n",
              "  913127.25,\n",
              "  906927.6875,\n",
              "  912427.3125,\n",
              "  906544.6875,\n",
              "  918468.0,\n",
              "  904100.8125,\n",
              "  908597.4375,\n",
              "  897676.125,\n",
              "  886003.9375,\n",
              "  896660.375,\n",
              "  902284.375,\n",
              "  884306.625,\n",
              "  901014.875,\n",
              "  884687.375,\n",
              "  874124.4375,\n",
              "  889848.5,\n",
              "  889794.5,\n",
              "  884070.1875,\n",
              "  880019.75,\n",
              "  875343.875,\n",
              "  887226.625,\n",
              "  893408.6875,\n",
              "  871875.1875,\n",
              "  873940.875,\n",
              "  866580.6875,\n",
              "  863338.1875,\n",
              "  863529.5,\n",
              "  867855.125,\n",
              "  872564.5625,\n",
              "  871265.625,\n",
              "  875151.6875,\n",
              "  856382.9375,\n",
              "  868546.4375,\n",
              "  868509.125,\n",
              "  859022.4375,\n",
              "  855629.75,\n",
              "  866676.8125,\n",
              "  858291.125,\n",
              "  858108.4375,\n",
              "  844205.25,\n",
              "  849038.4375,\n",
              "  843925.125,\n",
              "  829190.0625,\n",
              "  850625.9375,\n",
              "  848829.0,\n",
              "  849280.875,\n",
              "  842060.0625,\n",
              "  831529.8125,\n",
              "  827379.8125,\n",
              "  841811.25,\n",
              "  841580.8125,\n",
              "  830876.0,\n",
              "  821596.0625,\n",
              "  830664.6875,\n",
              "  811887.1875,\n",
              "  829495.1875,\n",
              "  832152.5625,\n",
              "  827023.875,\n",
              "  815335.75,\n",
              "  825897.8125,\n",
              "  820230.375,\n",
              "  820631.1875,\n",
              "  823988.4375,\n",
              "  816059.1875,\n",
              "  802402.0625,\n",
              "  819110.5,\n",
              "  812623.4375,\n",
              "  812662.6875,\n",
              "  803681.25,\n",
              "  804575.5625,\n",
              "  812624.6875,\n",
              "  796776.75,\n",
              "  809157.75,\n",
              "  796066.8125,\n",
              "  805230.5,\n",
              "  798150.6875,\n",
              "  789581.3125,\n",
              "  789296.0625,\n",
              "  793250.5,\n",
              "  787437.25,\n",
              "  784668.125,\n",
              "  781294.375,\n",
              "  786995.125,\n",
              "  775821.625,\n",
              "  784856.9375,\n",
              "  782851.9375,\n",
              "  758471.0,\n",
              "  782839.1875,\n",
              "  779073.25,\n",
              "  776143.125,\n",
              "  776487.4375,\n",
              "  766258.6875,\n",
              "  779613.25,\n",
              "  768427.5625,\n",
              "  774695.5,\n",
              "  769407.1875,\n",
              "  752253.375,\n",
              "  771275.125,\n",
              "  765072.125,\n",
              "  753181.5625,\n",
              "  761369.3125,\n",
              "  763800.625,\n",
              "  746301.125,\n",
              "  762743.25,\n",
              "  755556.8125,\n",
              "  751387.5625,\n",
              "  754602.0625,\n",
              "  763683.875,\n",
              "  755091.125,\n",
              "  755176.5,\n",
              "  756701.75,\n",
              "  745987.1875,\n",
              "  746717.5625,\n",
              "  735000.0,\n",
              "  739382.0,\n",
              "  745546.1875,\n",
              "  751645.5625,\n",
              "  745261.9375,\n",
              "  745282.8125,\n",
              "  744655.125,\n",
              "  736056.0625,\n",
              "  735069.0,\n",
              "  738941.9375,\n",
              "  739005.875,\n",
              "  730632.75,\n",
              "  728140.8125,\n",
              "  731159.1875,\n",
              "  726196.875,\n",
              "  708779.1875,\n",
              "  736466.375,\n",
              "  718464.6875,\n",
              "  719506.25,\n",
              "  718447.9375,\n",
              "  717792.6875,\n",
              "  707993.375,\n",
              "  717124.0625,\n",
              "  713221.9375,\n",
              "  713574.1875,\n",
              "  709650.75,\n",
              "  705317.1875,\n",
              "  711521.125,\n",
              "  714440.875,\n",
              "  710687.75,\n",
              "  716285.4375,\n",
              "  696575.1875,\n",
              "  707796.4375,\n",
              "  693291.5,\n",
              "  708342.4375,\n",
              "  695316.75,\n",
              "  699251.875,\n",
              "  691163.75,\n",
              "  687631.75,\n",
              "  699566.8125,\n",
              "  688997.375,\n",
              "  697629.1875,\n",
              "  692999.3125,\n",
              "  693981.4375,\n",
              "  687776.0,\n",
              "  690742.625,\n",
              "  686481.3125,\n",
              "  688679.4375,\n",
              "  687167.3125,\n",
              "  685978.75,\n",
              "  687872.5,\n",
              "  681476.875,\n",
              "  684874.8125,\n",
              "  678162.5625,\n",
              "  688220.875,\n",
              "  679728.5625,\n",
              "  689176.9375,\n",
              "  670738.0625,\n",
              "  668900.25,\n",
              "  671430.0625,\n",
              "  668609.0,\n",
              "  671526.8125,\n",
              "  662679.3125,\n",
              "  667479.5625,\n",
              "  668869.1875,\n",
              "  673743.625,\n",
              "  670888.0,\n",
              "  671053.5,\n",
              "  671016.25,\n",
              "  658615.9375,\n",
              "  645531.9375,\n",
              "  669766.9375,\n",
              "  667631.375,\n",
              "  655889.0625,\n",
              "  647332.125,\n",
              "  659148.0625,\n",
              "  662041.5625,\n",
              "  647745.4375,\n",
              "  658108.125,\n",
              "  647819.875,\n",
              "  651522.3125,\n",
              "  648347.5625,\n",
              "  653453.5,\n",
              "  650883.0,\n",
              "  653912.6875,\n",
              "  648107.25,\n",
              "  642559.25,\n",
              "  645455.875,\n",
              "  648420.0,\n",
              "  642561.75,\n",
              "  645708.5,\n",
              "  654763.25,\n",
              "  644237.0625,\n",
              "  650400.125,\n",
              "  652648.6875,\n",
              "  635176.4375,\n",
              "  631874.6875,\n",
              "  640428.75,\n",
              "  649637.875,\n",
              "  636221.8125,\n",
              "  632150.1875,\n",
              "  638258.0625,\n",
              "  628563.625,\n",
              "  627827.625,\n",
              "  628070.4375,\n",
              "  631643.0,\n",
              "  613347.4375,\n",
              "  626744.1875,\n",
              "  638521.5,\n",
              "  624501.5625,\n",
              "  628399.875,\n",
              "  626163.1875,\n",
              "  621637.4375,\n",
              "  624390.1875,\n",
              "  623019.875,\n",
              "  617234.75,\n",
              "  617579.4375,\n",
              "  629169.625,\n",
              "  620616.5625,\n",
              "  619038.9375,\n",
              "  621748.0,\n",
              "  605375.75,\n",
              "  608410.9375,\n",
              "  626539.0625,\n",
              "  607994.5625,\n",
              "  624165.125,\n",
              "  615287.9375,\n",
              "  616417.125,\n",
              "  623323.5,\n",
              "  609277.4375,\n",
              "  596212.3125,\n",
              "  607073.625,\n",
              "  606656.125,\n",
              "  599536.625,\n",
              "  603030.75,\n",
              "  602934.3125,\n",
              "  613877.125,\n",
              "  601974.625,\n",
              "  602911.3125,\n",
              "  601253.25,\n",
              "  602029.75,\n",
              "  601297.0625,\n",
              "  605684.3125,\n",
              "  600163.875,\n",
              "  594460.6875,\n",
              "  593097.75,\n",
              "  594704.5625,\n",
              "  597642.875,\n",
              "  601880.6875,\n",
              "  599514.75,\n",
              "  592870.375,\n",
              "  585305.375,\n",
              "  592825.1875,\n",
              "  586423.0625,\n",
              "  595959.1875,\n",
              "  587719.25,\n",
              "  587049.75,\n",
              "  591650.75,\n",
              "  585624.6875,\n",
              "  597131.875,\n",
              "  591102.8125,\n",
              "  593374.625,\n",
              "  585647.0625,\n",
              "  582345.375,\n",
              "  592987.1875,\n",
              "  581818.375,\n",
              "  586131.375,\n",
              "  576289.0,\n",
              "  570720.25,\n",
              "  584602.3125,\n",
              "  584594.1875,\n",
              "  576367.125,\n",
              "  578234.3125,\n",
              "  578692.3125,\n",
              "  571553.6875,\n",
              "  563939.5625,\n",
              "  577241.1875,\n",
              "  576307.25,\n",
              "  575275.4375,\n",
              "  580118.0,\n",
              "  575175.9375,\n",
              "  577021.8125,\n",
              "  572535.0,\n",
              "  573893.6875,\n",
              "  571359.625,\n",
              "  571876.5,\n",
              "  573960.4375,\n",
              "  564145.0625,\n",
              "  573519.625,\n",
              "  569593.8125,\n",
              "  567327.75,\n",
              "  571313.625,\n",
              "  560446.375,\n",
              "  566588.25,\n",
              "  575049.375,\n",
              "  565546.375,\n",
              "  554852.0625,\n",
              "  568093.75,\n",
              "  560110.9375,\n",
              "  562569.8125,\n",
              "  560096.75,\n",
              "  561952.3125,\n",
              "  565146.8125,\n",
              "  562088.375,\n",
              "  554693.5625,\n",
              "  557523.375,\n",
              "  559300.9375,\n",
              "  561592.25,\n",
              "  559619.75,\n",
              "  550375.5625,\n",
              "  554024.1875,\n",
              "  556324.125,\n",
              "  552661.875,\n",
              "  559897.0,\n",
              "  562454.125,\n",
              "  557995.25,\n",
              "  557147.3125,\n",
              "  556204.125,\n",
              "  543872.6875,\n",
              "  555562.0,\n",
              "  548382.25,\n",
              "  545740.5625,\n",
              "  551756.5625,\n",
              "  556106.375,\n",
              "  552231.8125,\n",
              "  553735.3125,\n",
              "  550597.5625,\n",
              "  547956.625,\n",
              "  552863.5625,\n",
              "  556524.1875,\n",
              "  537975.25,\n",
              "  547806.9375,\n",
              "  542473.375,\n",
              "  541989.3125,\n",
              "  539372.875,\n",
              "  543064.4375,\n",
              "  541613.5,\n",
              "  542672.25,\n",
              "  546279.375,\n",
              "  544288.875,\n",
              "  546840.6875,\n",
              "  544472.125,\n",
              "  542886.1875,\n",
              "  542753.8125,\n",
              "  547537.625,\n",
              "  537543.5,\n",
              "  545017.9375,\n",
              "  541138.9375,\n",
              "  540361.8125,\n",
              "  544221.1875,\n",
              "  540398.3125,\n",
              "  538717.5,\n",
              "  541389.75,\n",
              "  527928.5625,\n",
              "  537694.5,\n",
              "  541825.625,\n",
              "  536388.9375,\n",
              "  537804.625,\n",
              "  537516.9375,\n",
              "  530273.125,\n",
              "  537559.0625,\n",
              "  535080.625,\n",
              "  534379.5,\n",
              "  528668.5625,\n",
              "  533727.6875,\n",
              "  537465.0625,\n",
              "  534059.5625,\n",
              "  535571.1875,\n",
              "  527128.3125,\n",
              "  533969.1875,\n",
              "  535907.0,\n",
              "  528101.875,\n",
              "  531483.6875,\n",
              "  532904.0,\n",
              "  532256.75,\n",
              "  531118.9375,\n",
              "  535765.25,\n",
              "  531183.5,\n",
              "  527655.5625,\n",
              "  529123.25,\n",
              "  530820.125,\n",
              "  531278.0,\n",
              "  524259.09375,\n",
              "  531036.0625,\n",
              "  528284.5625,\n",
              "  530266.1875,\n",
              "  528454.9375,\n",
              "  519424.78125,\n",
              "  528938.0,\n",
              "  524457.3125,\n",
              "  519653.0625,\n",
              "  529790.6875,\n",
              "  523599.28125,\n",
              "  523073.4375,\n",
              "  525557.375,\n",
              "  525874.75,\n",
              "  525961.8125,\n",
              "  523152.21875,\n",
              "  524090.625,\n",
              "  515873.25,\n",
              "  521079.15625,\n",
              "  512479.625,\n",
              "  519865.46875,\n",
              "  523697.4375,\n",
              "  519556.5625,\n",
              "  523434.71875,\n",
              "  518707.53125,\n",
              "  520864.9375,\n",
              "  523417.6875,\n",
              "  520024.40625,\n",
              "  520603.375,\n",
              "  518706.09375,\n",
              "  523842.625,\n",
              "  517483.09375,\n",
              "  522613.28125,\n",
              "  513548.90625,\n",
              "  517436.90625,\n",
              "  522319.34375,\n",
              "  520549.9375,\n",
              "  524385.0,\n",
              "  515589.125,\n",
              "  520627.46875,\n",
              "  520521.28125,\n",
              "  513253.9375,\n",
              "  520196.5,\n",
              "  519468.3125,\n",
              "  508697.125,\n",
              "  514309.15625,\n",
              "  518134.71875,\n",
              "  516188.90625,\n",
              "  511391.65625,\n",
              "  519449.78125,\n",
              "  512727.3125,\n",
              "  514561.15625,\n",
              "  506318.75,\n",
              "  508089.28125,\n",
              "  514978.15625,\n",
              "  514735.5,\n",
              "  516809.9375,\n",
              "  507924.1875,\n",
              "  518558.40625,\n",
              "  516415.5,\n",
              "  515987.09375,\n",
              "  509766.15625,\n",
              "  511578.0625,\n",
              "  510793.125,\n",
              "  505974.09375,\n",
              "  514663.90625,\n",
              "  510925.3125,\n",
              "  505426.53125,\n",
              "  510559.9375,\n",
              "  514884.90625,\n",
              "  506363.21875,\n",
              "  512316.90625,\n",
              "  516011.65625,\n",
              "  515658.4375,\n",
              "  509837.1875,\n",
              "  514664.84375,\n",
              "  509630.90625,\n",
              "  516247.15625,\n",
              "  506951.25,\n",
              "  512671.0,\n",
              "  510553.03125,\n",
              "  510092.8125,\n",
              "  512990.25,\n",
              "  514970.5,\n",
              "  504343.96875,\n",
              "  505791.0,\n",
              "  505702.96875,\n",
              "  504417.75,\n",
              "  513349.65625,\n",
              "  513152.59375,\n",
              "  509064.625,\n",
              "  498022.3125,\n",
              "  504076.8125,\n",
              "  510347.65625,\n",
              "  507436.96875,\n",
              "  506986.40625,\n",
              "  514214.4375,\n",
              "  507613.1875,\n",
              "  511318.375,\n",
              "  502487.46875,\n",
              "  503293.15625,\n",
              "  509153.5,\n",
              "  508189.84375,\n",
              "  508138.0625,\n",
              "  514065.875,\n",
              "  506457.90625,\n",
              "  513342.5,\n",
              "  501846.75,\n",
              "  507900.09375,\n",
              "  505374.40625,\n",
              "  508563.09375,\n",
              "  508085.40625,\n",
              "  508796.6875,\n",
              "  503263.1875,\n",
              "  509158.15625,\n",
              "  505918.40625,\n",
              "  504332.6875,\n",
              "  509587.6875,\n",
              "  503868.0,\n",
              "  507885.375,\n",
              "  493084.6875,\n",
              "  506207.625,\n",
              "  498750.5625,\n",
              "  506504.34375,\n",
              "  503595.71875,\n",
              "  508794.15625,\n",
              "  507421.75,\n",
              "  510470.375,\n",
              "  507469.625,\n",
              "  505337.25,\n",
              "  506123.71875,\n",
              "  506145.6875,\n",
              "  501466.65625,\n",
              "  505211.09375,\n",
              "  504325.65625,\n",
              "  509189.5625,\n",
              "  500655.21875,\n",
              "  504718.25,\n",
              "  495750.0625,\n",
              "  505473.21875,\n",
              "  506963.1875,\n",
              "  506857.78125,\n",
              "  508141.09375,\n",
              "  497712.15625,\n",
              "  507323.21875,\n",
              "  503356.09375,\n",
              "  505424.71875,\n",
              "  504022.4375,\n",
              "  503599.28125,\n",
              "  506177.15625,\n",
              "  505608.8125,\n",
              "  486095.625,\n",
              "  505513.625,\n",
              "  496423.25,\n",
              "  507681.09375,\n",
              "  501262.96875,\n",
              "  505316.28125,\n",
              "  504154.625,\n",
              "  506460.09375,\n",
              "  505513.28125,\n",
              "  504259.3125,\n",
              "  505067.8125,\n",
              "  501508.90625,\n",
              "  504653.53125,\n",
              "  508020.5625,\n",
              "  503844.21875,\n",
              "  500238.96875,\n",
              "  498900.40625,\n",
              "  505597.3125,\n",
              "  495913.1875,\n",
              "  503326.5625,\n",
              "  507061.65625,\n",
              "  509209.28125,\n",
              "  489536.21875,\n",
              "  498663.1875,\n",
              "  502449.34375,\n",
              "  503946.40625,\n",
              "  494095.78125,\n",
              "  500833.0,\n",
              "  503691.09375,\n",
              "  504478.1875,\n",
              "  500101.28125,\n",
              "  502342.9375,\n",
              "  504117.84375,\n",
              "  497487.71875,\n",
              "  505679.78125,\n",
              "  496719.28125,\n",
              "  502697.84375,\n",
              "  507042.25,\n",
              "  498316.65625,\n",
              "  501301.34375,\n",
              "  506074.5625,\n",
              "  502759.09375,\n",
              "  504431.5625,\n",
              "  504565.84375,\n",
              "  502841.78125,\n",
              "  501427.6875,\n",
              "  494156.53125,\n",
              "  505346.59375,\n",
              "  504470.75,\n",
              "  508031.28125,\n",
              "  498144.28125,\n",
              "  501805.6875,\n",
              "  502129.75,\n",
              "  497467.875,\n",
              "  506057.5625,\n",
              "  506394.4375,\n",
              "  498838.96875,\n",
              "  500757.3125,\n",
              "  497014.0625,\n",
              "  498185.34375,\n",
              "  500940.09375,\n",
              "  502578.375,\n",
              "  503863.375,\n",
              "  502532.625,\n",
              "  495284.5,\n",
              "  501192.53125,\n",
              "  502353.5,\n",
              "  500869.28125,\n",
              "  504686.5625,\n",
              "  502679.4375,\n",
              "  504010.0,\n",
              "  500077.1875,\n",
              "  503980.75,\n",
              "  495764.90625,\n",
              "  501168.5,\n",
              "  502331.65625,\n",
              "  497966.625,\n",
              "  499807.125,\n",
              "  495357.375,\n",
              "  503568.78125,\n",
              "  502400.78125,\n",
              "  505910.65625,\n",
              "  502212.1875,\n",
              "  503299.1875,\n",
              "  500857.9375,\n",
              "  494021.5625,\n",
              "  502858.125,\n",
              "  497438.0625,\n",
              "  506307.40625,\n",
              "  501065.90625,\n",
              "  502507.71875,\n",
              "  502801.9375,\n",
              "  506305.03125,\n",
              "  502086.96875,\n",
              "  501510.375,\n",
              "  498288.9375,\n",
              "  497391.71875,\n",
              "  500347.78125,\n",
              "  500058.0625,\n",
              "  505648.65625,\n",
              "  501268.53125,\n",
              "  499689.03125,\n",
              "  498808.84375,\n",
              "  495098.9375,\n",
              "  493036.21875,\n",
              "  504973.09375,\n",
              "  500760.75,\n",
              "  503414.0625,\n",
              "  501714.6875,\n",
              "  504146.09375,\n",
              "  500953.125,\n",
              "  495400.53125,\n",
              "  502711.3125,\n",
              "  503260.25,\n",
              "  501985.71875,\n",
              "  499113.03125,\n",
              "  501741.46875,\n",
              "  495189.9375,\n",
              "  503684.28125,\n",
              "  498998.96875,\n",
              "  501069.40625,\n",
              "  493367.65625,\n",
              "  504233.625,\n",
              "  496880.875,\n",
              "  492818.09375,\n",
              "  500934.3125,\n",
              "  504617.71875,\n",
              "  501799.46875,\n",
              "  500209.09375,\n",
              "  499916.4375,\n",
              "  501603.1875,\n",
              "  503713.9375,\n",
              "  494221.96875,\n",
              "  500937.5,\n",
              "  505020.4375,\n",
              "  500759.6875,\n",
              "  496125.625,\n",
              "  500299.875,\n",
              "  500677.375,\n",
              "  502552.53125,\n",
              "  500753.25,\n",
              "  501657.03125,\n",
              "  492289.875,\n",
              "  500801.96875,\n",
              "  502297.0625,\n",
              "  502728.53125,\n",
              "  502893.96875,\n",
              "  504977.875,\n",
              "  501909.875,\n",
              "  499240.90625,\n",
              "  504294.15625,\n",
              "  504953.625,\n",
              "  498188.75,\n",
              "  502836.0,\n",
              "  502308.125,\n",
              "  497724.3125,\n",
              "  496513.03125,\n",
              "  505664.78125,\n",
              "  503224.125,\n",
              "  499840.78125,\n",
              "  500346.15625,\n",
              "  492782.28125,\n",
              "  503006.5625,\n",
              "  500769.21875,\n",
              "  495000.125,\n",
              "  501018.0625,\n",
              "  503561.71875,\n",
              "  502022.3125,\n",
              "  500734.28125,\n",
              "  497472.875,\n",
              "  497552.9375,\n",
              "  501496.5625,\n",
              "  502008.8125,\n",
              "  496206.78125,\n",
              "  504087.59375,\n",
              "  499188.6875,\n",
              "  504001.5,\n",
              "  500607.78125,\n",
              "  502714.28125,\n",
              "  502387.1875,\n",
              "  499028.40625,\n",
              "  502679.46875,\n",
              "  503360.9375,\n",
              "  498656.9375,\n",
              "  491354.625,\n",
              "  490187.0,\n",
              "  498583.4375,\n",
              "  498344.53125,\n",
              "  503740.6875,\n",
              "  503263.9375,\n",
              "  497535.875,\n",
              "  506331.65625,\n",
              "  499818.78125,\n",
              "  502823.75,\n",
              "  500369.75],\n",
              " 'mae': [1007.2506713867188,\n",
              "  999.3064575195312,\n",
              "  1005.3284912109375,\n",
              "  995.8355712890625,\n",
              "  996.5640258789062,\n",
              "  995.6510009765625,\n",
              "  994.8223876953125,\n",
              "  990.45703125,\n",
              "  995.1666870117188,\n",
              "  992.6097412109375,\n",
              "  989.0745239257812,\n",
              "  982.8165893554688,\n",
              "  986.64208984375,\n",
              "  980.6344604492188,\n",
              "  981.2481689453125,\n",
              "  985.1323852539062,\n",
              "  979.1753540039062,\n",
              "  986.547119140625,\n",
              "  984.5084838867188,\n",
              "  976.1419677734375,\n",
              "  980.5885009765625,\n",
              "  969.0305786132812,\n",
              "  976.557861328125,\n",
              "  972.319580078125,\n",
              "  962.7434692382812,\n",
              "  972.2122192382812,\n",
              "  970.1751708984375,\n",
              "  965.7700805664062,\n",
              "  965.0244140625,\n",
              "  971.8251953125,\n",
              "  965.1386108398438,\n",
              "  967.9891357421875,\n",
              "  959.95068359375,\n",
              "  958.086669921875,\n",
              "  959.875,\n",
              "  963.3915405273438,\n",
              "  966.751708984375,\n",
              "  953.79638671875,\n",
              "  953.4102172851562,\n",
              "  949.3983154296875,\n",
              "  945.6279907226562,\n",
              "  950.8741455078125,\n",
              "  949.1426391601562,\n",
              "  945.6245727539062,\n",
              "  942.1023559570312,\n",
              "  943.8873291015625,\n",
              "  935.3255004882812,\n",
              "  939.3499755859375,\n",
              "  936.5236206054688,\n",
              "  945.4033203125,\n",
              "  940.1168212890625,\n",
              "  936.8479614257812,\n",
              "  930.3828125,\n",
              "  930.9390258789062,\n",
              "  938.7453002929688,\n",
              "  932.7044677734375,\n",
              "  925.6488647460938,\n",
              "  934.2163696289062,\n",
              "  928.4719848632812,\n",
              "  927.7582397460938,\n",
              "  919.9725952148438,\n",
              "  922.7109985351562,\n",
              "  915.9199829101562,\n",
              "  920.3289184570312,\n",
              "  914.4959106445312,\n",
              "  919.5897827148438,\n",
              "  917.3621215820312,\n",
              "  908.7976684570312,\n",
              "  924.5515747070312,\n",
              "  923.7754516601562,\n",
              "  916.186767578125,\n",
              "  916.5371704101562,\n",
              "  918.777587890625,\n",
              "  909.0618286132812,\n",
              "  908.675048828125,\n",
              "  907.3314819335938,\n",
              "  908.2430419921875,\n",
              "  902.03857421875,\n",
              "  903.4335327148438,\n",
              "  900.7044067382812,\n",
              "  900.6892700195312,\n",
              "  903.7594604492188,\n",
              "  893.2342529296875,\n",
              "  900.2197875976562,\n",
              "  901.201904296875,\n",
              "  888.1146850585938,\n",
              "  896.7752075195312,\n",
              "  893.4381713867188,\n",
              "  896.7095947265625,\n",
              "  891.2881469726562,\n",
              "  890.5726318359375,\n",
              "  879.5213623046875,\n",
              "  888.107666015625,\n",
              "  889.3062744140625,\n",
              "  878.6367797851562,\n",
              "  882.4308471679688,\n",
              "  883.306884765625,\n",
              "  872.2147216796875,\n",
              "  877.0573120117188,\n",
              "  870.5748901367188,\n",
              "  878.952392578125,\n",
              "  882.4797973632812,\n",
              "  886.9874267578125,\n",
              "  866.9138793945312,\n",
              "  867.826416015625,\n",
              "  862.8765869140625,\n",
              "  866.3289794921875,\n",
              "  874.890625,\n",
              "  866.2060546875,\n",
              "  857.8160400390625,\n",
              "  860.9678955078125,\n",
              "  870.2086791992188,\n",
              "  863.65185546875,\n",
              "  861.0593872070312,\n",
              "  860.343017578125,\n",
              "  860.4766235351562,\n",
              "  862.8134765625,\n",
              "  859.9292602539062,\n",
              "  856.0336303710938,\n",
              "  860.3570556640625,\n",
              "  844.2916259765625,\n",
              "  849.572021484375,\n",
              "  859.3421020507812,\n",
              "  848.8333740234375,\n",
              "  850.1911010742188,\n",
              "  842.6257934570312,\n",
              "  847.843505859375,\n",
              "  848.1665649414062,\n",
              "  842.6994018554688,\n",
              "  847.5694580078125,\n",
              "  839.2688598632812,\n",
              "  846.0008544921875,\n",
              "  832.8087158203125,\n",
              "  846.2164916992188,\n",
              "  835.7166748046875,\n",
              "  834.5836181640625,\n",
              "  829.609375,\n",
              "  835.9727783203125,\n",
              "  835.9801635742188,\n",
              "  835.2089233398438,\n",
              "  837.754150390625,\n",
              "  832.0577392578125,\n",
              "  823.98779296875,\n",
              "  831.0489501953125,\n",
              "  823.9680786132812,\n",
              "  820.0809936523438,\n",
              "  828.9163208007812,\n",
              "  819.8778686523438,\n",
              "  822.5979614257812,\n",
              "  819.4229125976562,\n",
              "  821.2100830078125,\n",
              "  818.7703857421875,\n",
              "  821.6602783203125,\n",
              "  813.7967529296875,\n",
              "  818.48193359375,\n",
              "  807.2213134765625,\n",
              "  815.6830444335938,\n",
              "  823.0460815429688,\n",
              "  808.8255615234375,\n",
              "  812.3898315429688,\n",
              "  816.2432250976562,\n",
              "  812.9908447265625,\n",
              "  800.2680053710938,\n",
              "  801.53955078125,\n",
              "  802.6640014648438,\n",
              "  807.4629516601562,\n",
              "  793.3006591796875,\n",
              "  802.8397827148438,\n",
              "  800.7598266601562,\n",
              "  802.0667724609375,\n",
              "  798.3984375,\n",
              "  796.168212890625,\n",
              "  797.1722412109375,\n",
              "  803.2216186523438,\n",
              "  799.6583862304688,\n",
              "  795.8190307617188,\n",
              "  785.3873291015625,\n",
              "  797.2214965820312,\n",
              "  800.4246215820312,\n",
              "  791.1383056640625,\n",
              "  785.3978881835938,\n",
              "  783.771240234375,\n",
              "  784.8883056640625,\n",
              "  776.762939453125,\n",
              "  787.8627319335938,\n",
              "  788.670654296875,\n",
              "  784.44189453125,\n",
              "  787.7435302734375,\n",
              "  787.437255859375,\n",
              "  765.597412109375,\n",
              "  779.0401000976562,\n",
              "  781.4380493164062,\n",
              "  774.2846069335938,\n",
              "  777.056396484375,\n",
              "  770.42919921875,\n",
              "  766.1263427734375,\n",
              "  773.0447387695312,\n",
              "  770.4375610351562,\n",
              "  767.5978393554688,\n",
              "  767.9964599609375,\n",
              "  758.3572998046875,\n",
              "  762.7172241210938,\n",
              "  765.4542846679688,\n",
              "  775.418701171875,\n",
              "  753.3234252929688,\n",
              "  756.6569213867188,\n",
              "  759.9186401367188,\n",
              "  764.7527465820312,\n",
              "  764.800537109375,\n",
              "  753.5338134765625,\n",
              "  755.0226440429688,\n",
              "  763.4962768554688,\n",
              "  760.3602294921875,\n",
              "  757.5504760742188,\n",
              "  759.0747680664062,\n",
              "  751.531494140625,\n",
              "  747.8457641601562,\n",
              "  754.7296752929688,\n",
              "  742.318115234375,\n",
              "  751.9381103515625,\n",
              "  746.5986938476562,\n",
              "  744.8078002929688,\n",
              "  750.9733276367188,\n",
              "  743.865966796875,\n",
              "  741.9783935546875,\n",
              "  743.8963012695312,\n",
              "  745.0519409179688,\n",
              "  742.8970947265625,\n",
              "  741.5313110351562,\n",
              "  738.7102661132812,\n",
              "  730.9419555664062,\n",
              "  742.2059936523438,\n",
              "  737.5942993164062,\n",
              "  739.2447509765625,\n",
              "  737.8976440429688,\n",
              "  735.6576538085938,\n",
              "  731.0703125,\n",
              "  723.0298461914062,\n",
              "  734.4733276367188,\n",
              "  731.3010864257812,\n",
              "  734.0593872070312,\n",
              "  725.1682739257812,\n",
              "  729.8447875976562,\n",
              "  723.5762329101562,\n",
              "  715.4193725585938,\n",
              "  723.9955444335938,\n",
              "  725.8550415039062,\n",
              "  727.7343139648438,\n",
              "  721.0707397460938,\n",
              "  726.1275634765625,\n",
              "  716.2803344726562,\n",
              "  717.354248046875,\n",
              "  714.564208984375,\n",
              "  720.23291015625,\n",
              "  722.0658569335938,\n",
              "  719.4810180664062,\n",
              "  713.4979858398438,\n",
              "  717.622802734375,\n",
              "  709.3202514648438,\n",
              "  713.563232421875,\n",
              "  711.5559692382812,\n",
              "  711.782958984375,\n",
              "  706.494873046875,\n",
              "  713.9774780273438,\n",
              "  710.8637084960938,\n",
              "  708.5330810546875,\n",
              "  702.1513671875,\n",
              "  696.1396484375,\n",
              "  704.7779541015625,\n",
              "  705.7520141601562,\n",
              "  697.0035400390625,\n",
              "  706.650390625,\n",
              "  696.26416015625,\n",
              "  693.6707763671875,\n",
              "  700.9195556640625,\n",
              "  698.7511596679688,\n",
              "  695.3623046875,\n",
              "  694.0770263671875,\n",
              "  693.2532348632812,\n",
              "  699.7378540039062,\n",
              "  703.5240478515625,\n",
              "  693.8851318359375,\n",
              "  690.7931518554688,\n",
              "  690.8717041015625,\n",
              "  688.77099609375,\n",
              "  686.8521118164062,\n",
              "  690.4940185546875,\n",
              "  691.9931640625,\n",
              "  692.4493408203125,\n",
              "  694.3823852539062,\n",
              "  683.316162109375,\n",
              "  692.8031616210938,\n",
              "  692.1187744140625,\n",
              "  685.96484375,\n",
              "  686.05908203125,\n",
              "  692.6056518554688,\n",
              "  687.3938598632812,\n",
              "  687.3966064453125,\n",
              "  681.6534423828125,\n",
              "  683.1119995117188,\n",
              "  682.9479370117188,\n",
              "  671.9171752929688,\n",
              "  685.8840942382812,\n",
              "  683.7612915039062,\n",
              "  683.399658203125,\n",
              "  678.9679565429688,\n",
              "  674.6568603515625,\n",
              "  672.1134033203125,\n",
              "  680.637451171875,\n",
              "  679.48046875,\n",
              "  675.64404296875,\n",
              "  669.3595581054688,\n",
              "  674.6837158203125,\n",
              "  667.4535522460938,\n",
              "  673.3502197265625,\n",
              "  675.5902709960938,\n",
              "  674.3760986328125,\n",
              "  670.7681274414062,\n",
              "  674.5146484375,\n",
              "  668.8605346679688,\n",
              "  670.9263916015625,\n",
              "  673.4329833984375,\n",
              "  668.7118530273438,\n",
              "  663.47802734375,\n",
              "  670.9601440429688,\n",
              "  667.2643432617188,\n",
              "  668.38525390625,\n",
              "  664.5942993164062,\n",
              "  664.4228515625,\n",
              "  669.1000366210938,\n",
              "  660.107421875,\n",
              "  669.0100708007812,\n",
              "  661.1934814453125,\n",
              "  666.5355834960938,\n",
              "  661.9716796875,\n",
              "  656.139892578125,\n",
              "  658.0546264648438,\n",
              "  660.27880859375,\n",
              "  658.1170654296875,\n",
              "  654.6761474609375,\n",
              "  655.1299438476562,\n",
              "  659.7614135742188,\n",
              "  651.8082885742188,\n",
              "  658.0673217773438,\n",
              "  655.68017578125,\n",
              "  647.5806274414062,\n",
              "  658.4214477539062,\n",
              "  657.5133666992188,\n",
              "  656.7601318359375,\n",
              "  657.4926147460938,\n",
              "  651.581787109375,\n",
              "  657.3758544921875,\n",
              "  651.4291381835938,\n",
              "  658.4274291992188,\n",
              "  653.2531127929688,\n",
              "  645.165283203125,\n",
              "  658.2656860351562,\n",
              "  653.2423706054688,\n",
              "  645.725830078125,\n",
              "  650.6016235351562,\n",
              "  653.8815307617188,\n",
              "  647.7001953125,\n",
              "  656.0960693359375,\n",
              "  649.4127807617188,\n",
              "  649.9384765625,\n",
              "  649.2063598632812,\n",
              "  655.6725463867188,\n",
              "  649.0957641601562,\n",
              "  652.69189453125,\n",
              "  652.5265502929688,\n",
              "  648.8805541992188,\n",
              "  647.7265625,\n",
              "  645.07568359375,\n",
              "  645.1388549804688,\n",
              "  650.1522216796875,\n",
              "  653.9923095703125,\n",
              "  649.2366943359375,\n",
              "  651.2081298828125,\n",
              "  651.6386108398438,\n",
              "  645.2105712890625,\n",
              "  647.89892578125,\n",
              "  648.0009155273438,\n",
              "  648.5964965820312,\n",
              "  645.947021484375,\n",
              "  642.6260986328125,\n",
              "  646.7100219726562,\n",
              "  642.5838623046875,\n",
              "  633.8997802734375,\n",
              "  651.2408447265625,\n",
              "  642.2609252929688,\n",
              "  644.4952392578125,\n",
              "  641.625,\n",
              "  641.7346801757812,\n",
              "  636.9645385742188,\n",
              "  641.7764892578125,\n",
              "  640.3568115234375,\n",
              "  641.3303833007812,\n",
              "  639.8624267578125,\n",
              "  636.9136962890625,\n",
              "  641.8497924804688,\n",
              "  642.2377319335938,\n",
              "  641.5448608398438,\n",
              "  646.6040649414062,\n",
              "  636.0758056640625,\n",
              "  641.7642211914062,\n",
              "  636.036376953125,\n",
              "  643.3568725585938,\n",
              "  636.0987548828125,\n",
              "  638.4618530273438,\n",
              "  636.8715209960938,\n",
              "  634.7279052734375,\n",
              "  639.4579467773438,\n",
              "  633.8194580078125,\n",
              "  638.5516357421875,\n",
              "  636.765869140625,\n",
              "  636.77978515625,\n",
              "  636.6316528320312,\n",
              "  637.6863403320312,\n",
              "  635.4444580078125,\n",
              "  636.56689453125,\n",
              "  635.8658447265625,\n",
              "  636.0460205078125,\n",
              "  636.76708984375,\n",
              "  634.8751831054688,\n",
              "  637.9205322265625,\n",
              "  633.7589721679688,\n",
              "  639.7755737304688,\n",
              "  636.8439331054688,\n",
              "  642.2813110351562,\n",
              "  629.3011474609375,\n",
              "  632.0126953125,\n",
              "  632.1536865234375,\n",
              "  630.2311401367188,\n",
              "  634.9566040039062,\n",
              "  630.9439697265625,\n",
              "  633.9230346679688,\n",
              "  633.3261108398438,\n",
              "  636.2083740234375,\n",
              "  633.97607421875,\n",
              "  635.36279296875,\n",
              "  635.2482299804688,\n",
              "  630.6719970703125,\n",
              "  627.1317138671875,\n",
              "  636.0060424804688,\n",
              "  634.4918212890625,\n",
              "  629.2640991210938,\n",
              "  623.8207397460938,\n",
              "  632.2411499023438,\n",
              "  633.4853515625,\n",
              "  627.396240234375,\n",
              "  631.5452880859375,\n",
              "  628.0333251953125,\n",
              "  629.4782104492188,\n",
              "  629.464599609375,\n",
              "  630.5735473632812,\n",
              "  628.8839111328125,\n",
              "  631.5203247070312,\n",
              "  630.429443359375,\n",
              "  626.8976440429688,\n",
              "  628.357177734375,\n",
              "  628.4132690429688,\n",
              "  628.9466552734375,\n",
              "  628.2280883789062,\n",
              "  635.7466430664062,\n",
              "  628.6248168945312,\n",
              "  634.1317749023438,\n",
              "  636.460693359375,\n",
              "  624.9282836914062,\n",
              "  621.2634887695312,\n",
              "  628.6315307617188,\n",
              "  635.074462890625,\n",
              "  626.7451782226562,\n",
              "  623.9852294921875,\n",
              "  629.1392211914062,\n",
              "  625.780029296875,\n",
              "  621.8080444335938,\n",
              "  622.7484130859375,\n",
              "  625.2058715820312,\n",
              "  615.5441284179688,\n",
              "  624.9703979492188,\n",
              "  632.2935180664062,\n",
              "  621.7191162109375,\n",
              "  625.275634765625,\n",
              "  624.3277587890625,\n",
              "  621.6664428710938,\n",
              "  625.0379638671875,\n",
              "  623.8524780273438,\n",
              "  624.781005859375,\n",
              "  620.38818359375,\n",
              "  628.4125366210938,\n",
              "  622.4580688476562,\n",
              "  622.8821411132812,\n",
              "  624.7676391601562,\n",
              "  618.279541015625,\n",
              "  617.5321655273438,\n",
              "  629.5381469726562,\n",
              "  620.3471069335938,\n",
              "  629.9478149414062,\n",
              "  622.24072265625,\n",
              "  623.6358642578125,\n",
              "  630.3641357421875,\n",
              "  620.6036376953125,\n",
              "  612.7515869140625,\n",
              "  620.3820190429688,\n",
              "  620.2453002929688,\n",
              "  618.1368408203125,\n",
              "  618.79638671875,\n",
              "  617.7152099609375,\n",
              "  625.3391723632812,\n",
              "  618.1466674804688,\n",
              "  619.0112915039062,\n",
              "  619.5924072265625,\n",
              "  620.4989624023438,\n",
              "  618.02587890625,\n",
              "  623.3435668945312,\n",
              "  620.0067749023438,\n",
              "  615.2374267578125,\n",
              "  616.0119018554688,\n",
              "  616.3798217773438,\n",
              "  617.9868774414062,\n",
              "  621.1378784179688,\n",
              "  622.0359497070312,\n",
              "  618.4320068359375,\n",
              "  615.1176147460938,\n",
              "  618.8065795898438,\n",
              "  613.6669921875,\n",
              "  619.1444702148438,\n",
              "  615.8766479492188,\n",
              "  615.4827880859375,\n",
              "  617.6082763671875,\n",
              "  617.1930541992188,\n",
              "  625.5444946289062,\n",
              "  619.612060546875,\n",
              "  622.0812377929688,\n",
              "  616.9048461914062,\n",
              "  615.5839233398438,\n",
              "  622.5906982421875,\n",
              "  616.2980346679688,\n",
              "  619.1746215820312,\n",
              "  613.513916015625,\n",
              "  611.5104370117188,\n",
              "  618.4917602539062,\n",
              "  619.3106689453125,\n",
              "  614.8502807617188,\n",
              "  616.1473999023438,\n",
              "  616.5119018554688,\n",
              "  614.0961303710938,\n",
              "  610.9591064453125,\n",
              "  616.4813232421875,\n",
              "  617.7852783203125,\n",
              "  617.5328369140625,\n",
              "  621.572265625,\n",
              "  617.5074462890625,\n",
              "  617.8453369140625,\n",
              "  615.0848999023438,\n",
              "  619.5440063476562,\n",
              "  615.8099975585938,\n",
              "  616.2540283203125,\n",
              "  617.9677734375,\n",
              "  614.1080932617188,\n",
              "  619.7506103515625,\n",
              "  617.4873046875,\n",
              "  615.634521484375,\n",
              "  619.6854858398438,\n",
              "  613.8660888671875,\n",
              "  615.1932373046875,\n",
              "  623.0255126953125,\n",
              "  614.7904663085938,\n",
              "  612.5440673828125,\n",
              "  619.35302734375,\n",
              "  614.4759521484375,\n",
              "  615.6802978515625,\n",
              "  614.84375,\n",
              "  616.0062255859375,\n",
              "  619.11669921875,\n",
              "  616.9578247070312,\n",
              "  610.5394287109375,\n",
              "  616.2528686523438,\n",
              "  615.858154296875,\n",
              "  618.502685546875,\n",
              "  616.487548828125,\n",
              "  611.4813842773438,\n",
              "  615.143798828125,\n",
              "  615.2078247070312,\n",
              "  614.2462768554688,\n",
              "  618.6224365234375,\n",
              "  621.8090209960938,\n",
              "  619.1499633789062,\n",
              "  619.3485717773438,\n",
              "  618.1611328125,\n",
              "  612.5819091796875,\n",
              "  617.7105102539062,\n",
              "  612.6423950195312,\n",
              "  613.5330200195312,\n",
              "  616.9744262695312,\n",
              "  619.0671997070312,\n",
              "  617.1795043945312,\n",
              "  618.1593017578125,\n",
              "  618.8265380859375,\n",
              "  614.7509765625,\n",
              "  618.3828125,\n",
              "  622.47119140625,\n",
              "  610.9168701171875,\n",
              "  616.87255859375,\n",
              "  614.2958984375,\n",
              "  617.2410278320312,\n",
              "  611.6506958007812,\n",
              "  615.3599243164062,\n",
              "  614.8672485351562,\n",
              "  615.0496826171875,\n",
              "  617.59912109375,\n",
              "  617.2037353515625,\n",
              "  617.5043334960938,\n",
              "  617.0068969726562,\n",
              "  616.7261352539062,\n",
              "  615.8300170898438,\n",
              "  619.610595703125,\n",
              "  616.2467041015625,\n",
              "  619.271728515625,\n",
              "  615.8372802734375,\n",
              "  616.9508056640625,\n",
              "  620.0305786132812,\n",
              "  617.111083984375,\n",
              "  618.3917236328125,\n",
              "  618.470458984375,\n",
              "  612.255126953125,\n",
              "  617.4337768554688,\n",
              "  621.6790161132812,\n",
              "  615.9052734375,\n",
              "  617.4849853515625,\n",
              "  618.7432250976562,\n",
              "  614.752685546875,\n",
              "  619.0244750976562,\n",
              "  617.3257446289062,\n",
              "  617.6898193359375,\n",
              "  615.0927124023438,\n",
              "  617.8081665039062,\n",
              "  621.0526733398438,\n",
              "  619.3466186523438,\n",
              "  619.9307250976562,\n",
              "  614.4923095703125,\n",
              "  619.937255859375,\n",
              "  622.2405395507812,\n",
              "  616.349609375,\n",
              "  619.3070678710938,\n",
              "  620.5634155273438,\n",
              "  621.0272827148438,\n",
              "  619.96630859375,\n",
              "  623.3219604492188,\n",
              "  619.76806640625,\n",
              "  621.2747192382812,\n",
              "  620.1141357421875,\n",
              "  621.6700439453125,\n",
              "  622.9578247070312,\n",
              "  618.2122802734375,\n",
              "  621.9885864257812,\n",
              "  621.091064453125,\n",
              "  623.4833984375,\n",
              "  621.9759521484375,\n",
              "  618.3336181640625,\n",
              "  623.22314453125,\n",
              "  620.0341796875,\n",
              "  618.3283081054688,\n",
              "  626.8809204101562,\n",
              "  620.3936157226562,\n",
              "  621.1980590820312,\n",
              "  622.5200805664062,\n",
              "  623.0292358398438,\n",
              "  622.7305908203125,\n",
              "  621.4873046875,\n",
              "  623.3743896484375,\n",
              "  621.1240844726562,\n",
              "  621.7362670898438,\n",
              "  618.0446166992188,\n",
              "  620.1641845703125,\n",
              "  623.6522827148438,\n",
              "  622.345703125,\n",
              "  623.4449462890625,\n",
              "  620.5882568359375,\n",
              "  622.1002197265625,\n",
              "  624.1591796875,\n",
              "  623.0332641601562,\n",
              "  623.445556640625,\n",
              "  624.2056274414062,\n",
              "  626.6971435546875,\n",
              "  621.1755981445312,\n",
              "  626.3529663085938,\n",
              "  621.5564575195312,\n",
              "  624.6085815429688,\n",
              "  626.3780517578125,\n",
              "  625.7742309570312,\n",
              "  628.8726196289062,\n",
              "  623.2514038085938,\n",
              "  626.0735473632812,\n",
              "  626.50634765625,\n",
              "  623.0926513671875,\n",
              "  626.295654296875,\n",
              "  626.8839111328125,\n",
              "  621.9299926757812,\n",
              "  624.697509765625,\n",
              "  626.3367309570312,\n",
              "  625.803955078125,\n",
              "  622.3529052734375,\n",
              "  628.1446533203125,\n",
              "  623.5968627929688,\n",
              "  623.9585571289062,\n",
              "  621.3327026367188,\n",
              "  622.0509033203125,\n",
              "  625.2376708984375,\n",
              "  626.2305908203125,\n",
              "  627.6875,\n",
              "  624.9832153320312,\n",
              "  629.375732421875,\n",
              "  628.1386108398438,\n",
              "  628.7327270507812,\n",
              "  625.2188110351562,\n",
              "  626.5986328125,\n",
              "  626.3790893554688,\n",
              "  623.9247436523438,\n",
              "  627.2880249023438,\n",
              "  625.6082763671875,\n",
              "  624.470703125,\n",
              "  625.425537109375,\n",
              "  627.9967651367188,\n",
              "  625.5598754882812,\n",
              "  627.7020874023438,\n",
              "  630.4590454101562,\n",
              "  630.5527954101562,\n",
              "  625.8648681640625,\n",
              "  630.3536987304688,\n",
              "  628.375244140625,\n",
              "  632.461181640625,\n",
              "  627.20703125,\n",
              "  630.0203247070312,\n",
              "  628.26806640625,\n",
              "  627.7662353515625,\n",
              "  629.857177734375,\n",
              "  632.1876220703125,\n",
              "  626.6596069335938,\n",
              "  627.6776733398438,\n",
              "  626.3046875,\n",
              "  628.0631713867188,\n",
              "  632.719970703125,\n",
              "  632.8701782226562,\n",
              "  629.0738525390625,\n",
              "  624.2803955078125,\n",
              "  625.4072875976562,\n",
              "  630.5440673828125,\n",
              "  627.6273803710938,\n",
              "  629.0570068359375,\n",
              "  633.8216552734375,\n",
              "  630.2182006835938,\n",
              "  632.8021240234375,\n",
              "  626.333984375,\n",
              "  627.8370361328125,\n",
              "  630.704345703125,\n",
              "  629.8334350585938,\n",
              "  630.6406860351562,\n",
              "  634.7843017578125,\n",
              "  632.3848876953125,\n",
              "  634.918701171875,\n",
              "  627.6812744140625,\n",
              "  631.357666015625,\n",
              "  629.6744384765625,\n",
              "  631.7635498046875,\n",
              "  632.299560546875,\n",
              "  632.5533447265625,\n",
              "  630.0748901367188,\n",
              "  634.0708618164062,\n",
              "  630.6571044921875,\n",
              "  630.2335205078125,\n",
              "  633.2859497070312,\n",
              "  629.8001098632812,\n",
              "  632.146484375,\n",
              "  625.321044921875,\n",
              "  632.5771484375,\n",
              "  628.07470703125,\n",
              "  632.309814453125,\n",
              "  629.39501953125,\n",
              "  634.192626953125,\n",
              "  632.7281494140625,\n",
              "  635.3106689453125,\n",
              "  633.8363647460938,\n",
              "  633.1301879882812,\n",
              "  632.6995239257812,\n",
              "  633.2966918945312,\n",
              "  629.3797607421875,\n",
              "  632.3697509765625,\n",
              "  632.1327514648438,\n",
              "  635.585205078125,\n",
              "  630.7064819335938,\n",
              "  634.0484619140625,\n",
              "  628.8959350585938,\n",
              "  633.3378295898438,\n",
              "  634.4772338867188,\n",
              "  634.3287353515625,\n",
              "  635.5421752929688,\n",
              "  630.069091796875,\n",
              "  634.6493530273438,\n",
              "  633.0051879882812,\n",
              "  634.46533203125,\n",
              "  632.8280029296875,\n",
              "  633.8643798828125,\n",
              "  635.7905883789062,\n",
              "  634.7824096679688,\n",
              "  625.4189453125,\n",
              "  634.5740356445312,\n",
              "  631.2421264648438,\n",
              "  637.2546997070312,\n",
              "  631.5993041992188,\n",
              "  634.2283935546875,\n",
              "  633.6497192382812,\n",
              "  636.3724365234375,\n",
              "  635.2521362304688,\n",
              "  634.3842163085938,\n",
              "  635.64208984375,\n",
              "  633.2976684570312,\n",
              "  634.8294067382812,\n",
              "  637.4102783203125,\n",
              "  634.2613525390625,\n",
              "  631.466796875,\n",
              "  633.5074462890625,\n",
              "  636.2637329101562,\n",
              "  631.1322021484375,\n",
              "  635.8948364257812,\n",
              "  637.1753540039062,\n",
              "  639.6139526367188,\n",
              "  631.0623168945312,\n",
              "  633.9024658203125,\n",
              "  634.3338012695312,\n",
              "  635.237060546875,\n",
              "  630.5736083984375,\n",
              "  633.3651123046875,\n",
              "  635.0487060546875,\n",
              "  636.0292358398438,\n",
              "  633.7670288085938,\n",
              "  635.3102416992188,\n",
              "  636.0311279296875,\n",
              "  631.7305908203125,\n",
              "  637.706298828125,\n",
              "  632.9351196289062,\n",
              "  635.623291015625,\n",
              "  638.5452880859375,\n",
              "  631.6881713867188,\n",
              "  633.6123657226562,\n",
              "  637.8883666992188,\n",
              "  635.4703369140625,\n",
              "  637.0093383789062,\n",
              "  637.1241455078125,\n",
              "  635.732421875,\n",
              "  635.458251953125,\n",
              "  632.8571166992188,\n",
              "  638.4111328125,\n",
              "  638.2027587890625,\n",
              "  640.7630004882812,\n",
              "  636.2431030273438,\n",
              "  635.2769775390625,\n",
              "  635.8587036132812,\n",
              "  633.703857421875,\n",
              "  638.6926879882812,\n",
              "  639.1729736328125,\n",
              "  634.337890625,\n",
              "  635.7107543945312,\n",
              "  633.3717651367188,\n",
              "  634.0367431640625,\n",
              "  635.5863647460938,\n",
              "  636.2341918945312,\n",
              "  637.9575805664062,\n",
              "  637.5292358398438,\n",
              "  633.0836791992188,\n",
              "  636.4141235351562,\n",
              "  636.8643188476562,\n",
              "  636.2449340820312,\n",
              "  638.6595458984375,\n",
              "  637.4586791992188,\n",
              "  638.566162109375,\n",
              "  636.2205810546875,\n",
              "  638.2872314453125,\n",
              "  632.4667358398438,\n",
              "  636.4127197265625,\n",
              "  636.3291625976562,\n",
              "  635.2819213867188,\n",
              "  636.6364135742188,\n",
              "  634.4724731445312,\n",
              "  639.0198364257812,\n",
              "  638.3836669921875,\n",
              "  640.14453125,\n",
              "  637.3460693359375,\n",
              "  638.6149291992188,\n",
              "  636.1689453125,\n",
              "  634.0172119140625,\n",
              "  637.9863891601562,\n",
              "  634.9681396484375,\n",
              "  641.6278076171875,\n",
              "  637.6828002929688,\n",
              "  638.658203125,\n",
              "  639.5455932617188,\n",
              "  641.289306640625,\n",
              "  637.4834594726562,\n",
              "  637.1154174804688,\n",
              "  635.5936279296875,\n",
              "  635.0404052734375,\n",
              "  637.8399658203125,\n",
              "  636.9100341796875,\n",
              "  641.2139282226562,\n",
              "  638.04736328125,\n",
              "  637.44287109375,\n",
              "  639.3028564453125,\n",
              "  635.9190673828125,\n",
              "  634.4798583984375,\n",
              "  640.7349853515625,\n",
              "  639.7943115234375,\n",
              "  640.1908569335938,\n",
              "  639.396484375,\n",
              "  640.1177368164062,\n",
              "  637.94775390625,\n",
              "  633.8072509765625,\n",
              "  639.5768432617188,\n",
              "  639.064208984375,\n",
              "  638.1585083007812,\n",
              "  637.1407470703125,\n",
              "  638.9432983398438,\n",
              "  636.2801513671875,\n",
              "  640.0578002929688,\n",
              "  637.0476684570312,\n",
              "  638.0415649414062,\n",
              "  633.9620971679688,\n",
              "  641.4710693359375,\n",
              "  636.8760986328125,\n",
              "  634.2843017578125,\n",
              "  639.0872802734375,\n",
              "  640.9371948242188,\n",
              "  639.2802734375,\n",
              "  638.8724365234375,\n",
              "  637.3057861328125,\n",
              "  638.9276733398438,\n",
              "  640.2833862304688,\n",
              "  634.8468017578125,\n",
              "  638.7720336914062,\n",
              "  641.3118896484375,\n",
              "  638.592529296875,\n",
              "  638.2183227539062,\n",
              "  639.7026977539062,\n",
              "  637.77880859375,\n",
              "  640.6912841796875,\n",
              "  638.721435546875,\n",
              "  639.3316650390625,\n",
              "  636.315673828125,\n",
              "  638.6533813476562,\n",
              "  639.3872680664062,\n",
              "  639.9596557617188,\n",
              "  639.93701171875,\n",
              "  641.5635986328125,\n",
              "  640.2344970703125,\n",
              "  640.0505981445312,\n",
              "  641.6392822265625,\n",
              "  642.0870361328125,\n",
              "  636.9375610351562,\n",
              "  641.2474975585938,\n",
              "  640.6527709960938,\n",
              "  636.2089233398438,\n",
              "  637.5435791015625,\n",
              "  642.9747924804688,\n",
              "  641.5325317382812,\n",
              "  638.7573852539062,\n",
              "  639.0552978515625,\n",
              "  635.4421997070312,\n",
              "  640.9639892578125,\n",
              "  639.7999267578125,\n",
              "  635.3742065429688,\n",
              "  639.4805908203125,\n",
              "  642.1943359375,\n",
              "  640.2332763671875,\n",
              "  638.8062744140625,\n",
              "  638.2860107421875,\n",
              "  637.2085571289062,\n",
              "  639.791748046875,\n",
              "  640.0079345703125,\n",
              "  638.5636596679688,\n",
              "  641.9776000976562,\n",
              "  638.885009765625,\n",
              "  642.7395629882812,\n",
              "  639.177490234375,\n",
              "  640.5167846679688,\n",
              "  640.2438354492188,\n",
              "  637.9002685546875,\n",
              "  641.2030029296875,\n",
              "  641.9888916015625,\n",
              "  638.2501220703125,\n",
              "  635.5950317382812,\n",
              "  635.0885620117188,\n",
              "  637.3759155273438,\n",
              "  639.2849731445312,\n",
              "  641.64208984375,\n",
              "  641.9104614257812,\n",
              "  638.3271484375,\n",
              "  643.6800537109375,\n",
              "  639.2494506835938,\n",
              "  641.8297119140625,\n",
              "  639.36083984375],\n",
              " 'val_loss': [520760.28125,\n",
              "  510891.5,\n",
              "  517560.0,\n",
              "  529248.625,\n",
              "  514770.25,\n",
              "  551461.5,\n",
              "  519882.96875,\n",
              "  504934.65625,\n",
              "  521126.65625,\n",
              "  497677.875,\n",
              "  534643.6875,\n",
              "  520305.28125,\n",
              "  539775.4375,\n",
              "  492274.0,\n",
              "  523972.09375,\n",
              "  518177.46875,\n",
              "  520854.15625,\n",
              "  502610.0,\n",
              "  505148.71875,\n",
              "  499474.125,\n",
              "  493940.71875,\n",
              "  504744.0,\n",
              "  499078.46875,\n",
              "  477285.15625,\n",
              "  475837.25,\n",
              "  506731.15625,\n",
              "  517286.5,\n",
              "  491462.5,\n",
              "  473985.59375,\n",
              "  492603.71875,\n",
              "  475036.09375,\n",
              "  489545.65625,\n",
              "  480090.375,\n",
              "  486527.375,\n",
              "  485032.59375,\n",
              "  459980.5,\n",
              "  493211.90625,\n",
              "  492396.25,\n",
              "  479174.90625,\n",
              "  481630.15625,\n",
              "  487921.78125,\n",
              "  494188.40625,\n",
              "  477273.34375,\n",
              "  475834.96875,\n",
              "  478201.21875,\n",
              "  473004.15625,\n",
              "  475325.5,\n",
              "  447248.15625,\n",
              "  468763.96875,\n",
              "  444514.21875,\n",
              "  458358.40625,\n",
              "  456989.59375,\n",
              "  436213.21875,\n",
              "  446591.96875,\n",
              "  463908.28125,\n",
              "  440063.21875,\n",
              "  457529.0,\n",
              "  444988.78125,\n",
              "  443662.09375,\n",
              "  449608.71875,\n",
              "  448289.71875,\n",
              "  421391.0,\n",
              "  434580.40625,\n",
              "  444250.40625,\n",
              "  442916.46875,\n",
              "  434423.125,\n",
              "  458220.40625,\n",
              "  435552.375,\n",
              "  441312.09375,\n",
              "  407599.53125,\n",
              "  417121.28125,\n",
              "  437333.28125,\n",
              "  403882.25,\n",
              "  437913.53125,\n",
              "  429681.09375,\n",
              "  421468.34375,\n",
              "  409631.46875,\n",
              "  422103.46875,\n",
              "  424517.75,\n",
              "  419549.09375,\n",
              "  411489.71875,\n",
              "  417595.0,\n",
              "  426159.09375,\n",
              "  404125.78125,\n",
              "  406509.75,\n",
              "  415645.46875,\n",
              "  390813.90625,\n",
              "  409486.625,\n",
              "  405279.53125,\n",
              "  393869.90625,\n",
              "  406496.65625,\n",
              "  404590.625,\n",
              "  403370.625,\n",
              "  402156.96875,\n",
              "  401682.34375,\n",
              "  406949.21875,\n",
              "  395741.25,\n",
              "  400944.21875,\n",
              "  403337.65625,\n",
              "  388647.46875,\n",
              "  397362.46875,\n",
              "  392620.15625,\n",
              "  388714.71875,\n",
              "  393813.5,\n",
              "  386416.0,\n",
              "  401152.96875,\n",
              "  393841.96875,\n",
              "  373360.25,\n",
              "  381872.34375,\n",
              "  377311.71875,\n",
              "  395324.75,\n",
              "  394183.25,\n",
              "  373947.65625,\n",
              "  376358.5,\n",
              "  368257.84375,\n",
              "  371611.09375,\n",
              "  372949.21875,\n",
              "  368417.84375,\n",
              "  370792.90625,\n",
              "  384867.59375,\n",
              "  374435.46875,\n",
              "  360654.65625,\n",
              "  375695.59375,\n",
              "  374553.90625,\n",
              "  366700.125,\n",
              "  368957.09375,\n",
              "  364458.40625,\n",
              "  357742.34375,\n",
              "  360113.59375,\n",
              "  356830.03125,\n",
              "  352453.34375,\n",
              "  359121.46875,\n",
              "  370414.65625,\n",
              "  348144.28125,\n",
              "  344977.21875,\n",
              "  340683.28125,\n",
              "  362687.75,\n",
              "  352924.59375,\n",
              "  337716.21875,\n",
              "  342048.40625,\n",
              "  353159.71875,\n",
              "  343513.15625,\n",
              "  337196.375,\n",
              "  353440.84375,\n",
              "  335221.15625,\n",
              "  353318.21875,\n",
              "  333397.0,\n",
              "  354633.25,\n",
              "  353409.34375,\n",
              "  335685.15625,\n",
              "  338016.375,\n",
              "  342261.71875,\n",
              "  330556.09375,\n",
              "  338518.65625,\n",
              "  334157.40625,\n",
              "  324552.53125,\n",
              "  340494.25,\n",
              "  319964.09375,\n",
              "  330485.53125,\n",
              "  337646.03125,\n",
              "  338376.78125,\n",
              "  332602.90625,\n",
              "  328375.40625,\n",
              "  333949.34375,\n",
              "  320227.9375,\n",
              "  324090.21875,\n",
              "  331145.03125,\n",
              "  314442.75,\n",
              "  322904.5625,\n",
              "  337656.84375,\n",
              "  329006.15625,\n",
              "  323431.21875,\n",
              "  314826.4375,\n",
              "  304727.15625,\n",
              "  306561.96875,\n",
              "  322923.53125,\n",
              "  309901.09375,\n",
              "  309052.9375,\n",
              "  320232.71875,\n",
              "  308729.59375,\n",
              "  318499.53125,\n",
              "  318841.25,\n",
              "  318153.15625,\n",
              "  308524.375,\n",
              "  307695.28125,\n",
              "  303778.65625,\n",
              "  317733.34375,\n",
              "  294872.1875,\n",
              "  297060.3125,\n",
              "  293225.375,\n",
              "  309976.75,\n",
              "  317628.65625,\n",
              "  300709.21875,\n",
              "  293229.28125,\n",
              "  310706.59375,\n",
              "  294846.34375,\n",
              "  295049.71875,\n",
              "  301356.59375,\n",
              "  289479.03125,\n",
              "  298871.1875,\n",
              "  309833.03125,\n",
              "  291333.8125,\n",
              "  286654.15625,\n",
              "  296615.78125,\n",
              "  282231.625,\n",
              "  288156.0625,\n",
              "  289734.1875,\n",
              "  297188.875,\n",
              "  295316.34375,\n",
              "  298698.5625,\n",
              "  284761.53125,\n",
              "  284036.96875,\n",
              "  282540.5,\n",
              "  299418.90625,\n",
              "  278995.65625,\n",
              "  290582.59375,\n",
              "  286701.53125,\n",
              "  279027.78125,\n",
              "  285498.59375,\n",
              "  284901.96875,\n",
              "  267851.03125,\n",
              "  280381.84375,\n",
              "  273543.65625,\n",
              "  275645.46875,\n",
              "  277276.71875,\n",
              "  283559.46875,\n",
              "  277018.40625,\n",
              "  281504.46875,\n",
              "  274254.96875,\n",
              "  280472.15625,\n",
              "  283327.59375,\n",
              "  276276.3125,\n",
              "  281930.09375,\n",
              "  275195.4375,\n",
              "  271342.40625,\n",
              "  273874.90625,\n",
              "  279052.96875,\n",
              "  275606.5625,\n",
              "  269300.125,\n",
              "  273840.84375,\n",
              "  264577.59375,\n",
              "  269222.40625,\n",
              "  266428.625,\n",
              "  260301.078125,\n",
              "  265445.34375,\n",
              "  270541.59375,\n",
              "  266817.03125,\n",
              "  269132.03125,\n",
              "  260022.796875,\n",
              "  259010.796875,\n",
              "  264472.4375,\n",
              "  250274.375,\n",
              "  263473.65625,\n",
              "  262528.5625,\n",
              "  267674.9375,\n",
              "  258820.296875,\n",
              "  258107.953125,\n",
              "  257870.640625,\n",
              "  257183.296875,\n",
              "  259415.140625,\n",
              "  253655.734375,\n",
              "  250481.921875,\n",
              "  252547.421875,\n",
              "  254415.234375,\n",
              "  256670.75,\n",
              "  250991.328125,\n",
              "  257919.125,\n",
              "  257356.609375,\n",
              "  254393.484375,\n",
              "  249153.703125,\n",
              "  253498.5,\n",
              "  252811.671875,\n",
              "  252466.953125,\n",
              "  256388.453125,\n",
              "  253403.875,\n",
              "  248275.984375,\n",
              "  240964.546875,\n",
              "  250117.578125,\n",
              "  244547.609375,\n",
              "  244264.046875,\n",
              "  243803.546875,\n",
              "  239266.234375,\n",
              "  245174.25,\n",
              "  244707.4375,\n",
              "  251240.609375,\n",
              "  245609.3125,\n",
              "  241350.078125,\n",
              "  245090.75,\n",
              "  247504.828125,\n",
              "  246705.953125,\n",
              "  237747.375,\n",
              "  247876.0,\n",
              "  241174.25,\n",
              "  244303.359375,\n",
              "  240356.328125,\n",
              "  235848.078125,\n",
              "  239565.921875,\n",
              "  238973.984375,\n",
              "  241045.546875,\n",
              "  241814.75,\n",
              "  237232.25,\n",
              "  240778.453125,\n",
              "  228446.875,\n",
              "  238491.5625,\n",
              "  233889.671875,\n",
              "  232314.796875,\n",
              "  241302.078125,\n",
              "  234537.671875,\n",
              "  237711.328125,\n",
              "  234698.015625,\n",
              "  235810.296875,\n",
              "  235075.046875,\n",
              "  235279.078125,\n",
              "  230943.546875,\n",
              "  237934.890625,\n",
              "  227926.671875,\n",
              "  236035.421875,\n",
              "  234832.953125,\n",
              "  233886.546875,\n",
              "  230121.890625,\n",
              "  232309.375,\n",
              "  232738.671875,\n",
              "  233791.125,\n",
              "  229189.453125,\n",
              "  227552.703125,\n",
              "  229574.984375,\n",
              "  230395.421875,\n",
              "  224399.171875,\n",
              "  229988.0625,\n",
              "  221981.796875,\n",
              "  225889.1875,\n",
              "  228694.171875,\n",
              "  226666.703125,\n",
              "  224152.171875,\n",
              "  228763.4375,\n",
              "  227352.8125,\n",
              "  225229.671875,\n",
              "  223244.578125,\n",
              "  230032.484375,\n",
              "  228113.375,\n",
              "  221160.828125,\n",
              "  225747.953125,\n",
              "  223907.921875,\n",
              "  228402.828125,\n",
              "  222509.390625,\n",
              "  222292.296875,\n",
              "  219221.296875,\n",
              "  224717.953125,\n",
              "  224888.453125,\n",
              "  223876.1875,\n",
              "  221283.796875,\n",
              "  222926.640625,\n",
              "  223065.453125,\n",
              "  225326.109375,\n",
              "  220524.609375,\n",
              "  223428.390625,\n",
              "  217798.734375,\n",
              "  220218.625,\n",
              "  224667.546875,\n",
              "  224074.8125,\n",
              "  219617.5625,\n",
              "  221787.296875,\n",
              "  223575.078125,\n",
              "  223476.578125,\n",
              "  223617.125,\n",
              "  221073.3125,\n",
              "  221120.75,\n",
              "  222600.421875,\n",
              "  220089.296875,\n",
              "  222033.796875,\n",
              "  220137.078125,\n",
              "  220235.203125,\n",
              "  217895.0,\n",
              "  219931.546875,\n",
              "  219638.609375,\n",
              "  221354.296875,\n",
              "  219637.421875,\n",
              "  217557.421875,\n",
              "  217114.828125,\n",
              "  219334.890625,\n",
              "  218904.828125,\n",
              "  217003.515625,\n",
              "  216421.921875,\n",
              "  218725.734375,\n",
              "  216642.25,\n",
              "  220593.640625,\n",
              "  218709.328125,\n",
              "  220391.75,\n",
              "  215848.0625,\n",
              "  216054.671875,\n",
              "  214273.703125,\n",
              "  216522.265625,\n",
              "  217737.5,\n",
              "  219994.8125,\n",
              "  216046.125,\n",
              "  217274.234375,\n",
              "  215965.734375,\n",
              "  219580.625,\n",
              "  220006.328125,\n",
              "  218954.453125,\n",
              "  218018.1875,\n",
              "  216933.359375,\n",
              "  217209.4375,\n",
              "  217694.75,\n",
              "  217677.453125,\n",
              "  218403.0625,\n",
              "  213626.671875,\n",
              "  216707.265625,\n",
              "  215065.859375,\n",
              "  217803.046875,\n",
              "  216789.234375,\n",
              "  215854.640625,\n",
              "  217400.203125,\n",
              "  216600.890625,\n",
              "  218522.484375,\n",
              "  217974.125,\n",
              "  220441.859375,\n",
              "  215661.046875,\n",
              "  217787.453125,\n",
              "  218964.015625,\n",
              "  216043.640625,\n",
              "  218449.796875,\n",
              "  219213.0,\n",
              "  218482.078125,\n",
              "  216711.828125,\n",
              "  218798.953125,\n",
              "  217047.203125,\n",
              "  215309.671875,\n",
              "  217629.390625,\n",
              "  215403.703125,\n",
              "  215792.734375,\n",
              "  218189.140625,\n",
              "  218006.578125,\n",
              "  217371.984375,\n",
              "  220714.375,\n",
              "  215032.046875,\n",
              "  217137.0625,\n",
              "  216181.890625,\n",
              "  218516.703125,\n",
              "  219144.484375,\n",
              "  220921.515625,\n",
              "  217399.6875,\n",
              "  219477.953125,\n",
              "  216131.640625,\n",
              "  216509.046875,\n",
              "  217925.546875,\n",
              "  217988.953125,\n",
              "  216086.390625,\n",
              "  216922.953125,\n",
              "  216669.5625,\n",
              "  216623.046875,\n",
              "  218917.359375,\n",
              "  220440.3125,\n",
              "  217533.703125,\n",
              "  218637.203125,\n",
              "  219396.390625,\n",
              "  221124.4375,\n",
              "  217773.8125,\n",
              "  219942.3125,\n",
              "  220405.609375,\n",
              "  219264.171875,\n",
              "  222337.109375,\n",
              "  218197.6875,\n",
              "  219617.640625,\n",
              "  220646.375,\n",
              "  221148.625,\n",
              "  220976.765625,\n",
              "  222411.078125,\n",
              "  222130.703125,\n",
              "  219908.171875,\n",
              "  221163.5,\n",
              "  227747.890625,\n",
              "  222806.171875,\n",
              "  221726.703125,\n",
              "  222034.5625,\n",
              "  223123.5,\n",
              "  223637.25,\n",
              "  222591.625,\n",
              "  228988.453125,\n",
              "  223983.078125,\n",
              "  225254.3125,\n",
              "  222499.140625,\n",
              "  220211.921875,\n",
              "  223416.75,\n",
              "  221671.6875,\n",
              "  228337.953125,\n",
              "  225612.25,\n",
              "  222261.265625,\n",
              "  225483.578125,\n",
              "  228773.890625,\n",
              "  225311.953125,\n",
              "  227430.078125,\n",
              "  224969.4375,\n",
              "  227722.4375,\n",
              "  224495.5625,\n",
              "  224752.953125,\n",
              "  229964.1875,\n",
              "  227003.5625,\n",
              "  228665.828125,\n",
              "  227319.8125,\n",
              "  229321.328125,\n",
              "  232602.4375,\n",
              "  228608.484375,\n",
              "  230247.078125,\n",
              "  234402.3125,\n",
              "  226642.671875,\n",
              "  232441.75,\n",
              "  233062.25,\n",
              "  228851.078125,\n",
              "  233320.203125,\n",
              "  234641.734375,\n",
              "  226324.265625,\n",
              "  229128.046875,\n",
              "  230287.890625,\n",
              "  234529.453125,\n",
              "  232222.828125,\n",
              "  237132.796875,\n",
              "  235788.0,\n",
              "  234239.765625,\n",
              "  234788.015625,\n",
              "  234036.328125,\n",
              "  236129.546875,\n",
              "  238719.4375,\n",
              "  238275.546875,\n",
              "  231818.921875,\n",
              "  235263.9375,\n",
              "  233368.875,\n",
              "  238153.546875,\n",
              "  230721.875,\n",
              "  238500.453125,\n",
              "  231292.75,\n",
              "  232926.4375,\n",
              "  241194.609375,\n",
              "  239632.828125,\n",
              "  241072.765625,\n",
              "  242114.484375,\n",
              "  240163.828125,\n",
              "  248579.796875,\n",
              "  236392.953125,\n",
              "  237412.859375,\n",
              "  242199.015625,\n",
              "  241885.828125,\n",
              "  239833.234375,\n",
              "  243686.078125,\n",
              "  238695.921875,\n",
              "  238574.0625,\n",
              "  245895.078125,\n",
              "  236484.046875,\n",
              "  247210.703125,\n",
              "  246373.75,\n",
              "  246755.171875,\n",
              "  244321.359375,\n",
              "  243199.640625,\n",
              "  246484.609375,\n",
              "  252134.75,\n",
              "  247000.515625,\n",
              "  247369.453125,\n",
              "  242499.578125,\n",
              "  253482.75,\n",
              "  247070.671875,\n",
              "  254718.375,\n",
              "  252075.265625,\n",
              "  244746.3125,\n",
              "  245871.75,\n",
              "  245876.4375,\n",
              "  250565.078125,\n",
              "  251339.234375,\n",
              "  249190.4375,\n",
              "  254368.609375,\n",
              "  250759.125,\n",
              "  259231.4375,\n",
              "  254961.578125,\n",
              "  255053.453125,\n",
              "  260647.640625,\n",
              "  255048.734375,\n",
              "  254037.375,\n",
              "  257132.359375,\n",
              "  257847.3125,\n",
              "  258047.578125,\n",
              "  254291.625,\n",
              "  261469.3125,\n",
              "  259387.859375,\n",
              "  262823.59375,\n",
              "  257945.953125,\n",
              "  264415.84375,\n",
              "  253261.984375,\n",
              "  253675.234375,\n",
              "  258802.125,\n",
              "  258246.625,\n",
              "  264439.375,\n",
              "  256729.578125,\n",
              "  264371.46875,\n",
              "  269234.71875,\n",
              "  265298.21875,\n",
              "  262661.71875,\n",
              "  266284.0,\n",
              "  271547.0,\n",
              "  267850.21875,\n",
              "  258465.203125,\n",
              "  259295.078125,\n",
              "  270063.75,\n",
              "  270582.8125,\n",
              "  260657.671875,\n",
              "  265220.25,\n",
              "  271578.28125,\n",
              "  270218.65625,\n",
              "  267479.6875,\n",
              "  273049.53125,\n",
              "  279091.34375,\n",
              "  273099.5,\n",
              "  273625.4375,\n",
              "  270738.21875,\n",
              "  275468.875,\n",
              "  267650.4375,\n",
              "  284665.28125,\n",
              "  271693.03125,\n",
              "  268216.21875,\n",
              "  272183.34375,\n",
              "  283327.28125,\n",
              "  279354.40625,\n",
              "  264757.34375,\n",
              "  279428.4375,\n",
              "  280005.78125,\n",
              "  266447.75,\n",
              "  286393.28125,\n",
              "  281013.21875,\n",
              "  276135.78125,\n",
              "  268242.0,\n",
              "  286812.8125,\n",
              "  274207.21875,\n",
              "  285590.59375,\n",
              "  279425.625,\n",
              "  293662.15625,\n",
              "  274467.90625,\n",
              "  280450.34375,\n",
              "  282222.40625,\n",
              "  277496.53125,\n",
              "  287254.4375,\n",
              "  288155.65625,\n",
              "  287902.96875,\n",
              "  288792.34375,\n",
              "  274687.03125,\n",
              "  289525.75,\n",
              "  274850.15625,\n",
              "  292178.21875,\n",
              "  280756.96875,\n",
              "  296992.09375,\n",
              "  282958.03125,\n",
              "  287274.90625,\n",
              "  293108.28125,\n",
              "  289126.5625,\n",
              "  295154.40625,\n",
              "  290530.46875,\n",
              "  291028.59375,\n",
              "  302870.125,\n",
              "  296061.34375,\n",
              "  292528.34375,\n",
              "  293036.21875,\n",
              "  288278.09375,\n",
              "  298873.28125,\n",
              "  293437.75,\n",
              "  290619.90625,\n",
              "  301176.875,\n",
              "  312629.65625,\n",
              "  301118.5,\n",
              "  303165.96875,\n",
              "  298264.5,\n",
              "  303511.71875,\n",
              "  298224.46875,\n",
              "  309117.90625,\n",
              "  294204.96875,\n",
              "  311670.6875,\n",
              "  300721.21875,\n",
              "  313842.28125,\n",
              "  307233.125,\n",
              "  302096.125,\n",
              "  313632.03125,\n",
              "  309672.625,\n",
              "  314693.625,\n",
              "  309585.96875,\n",
              "  298770.78125,\n",
              "  316095.59375,\n",
              "  310917.40625,\n",
              "  300778.90625,\n",
              "  323694.625,\n",
              "  319780.09375,\n",
              "  318484.34375,\n",
              "  313113.0,\n",
              "  315280.3125,\n",
              "  308933.0,\n",
              "  309926.28125,\n",
              "  304114.8125,\n",
              "  298535.34375,\n",
              "  305974.90625,\n",
              "  311966.78125,\n",
              "  311410.34375,\n",
              "  311903.46875,\n",
              "  306422.90625,\n",
              "  319972.40625,\n",
              "  307743.5,\n",
              "  321164.375,\n",
              "  313838.65625,\n",
              "  315786.84375,\n",
              "  321392.4375,\n",
              "  329096.46875,\n",
              "  322410.78125,\n",
              "  328605.71875,\n",
              "  330169.03125,\n",
              "  318147.75,\n",
              "  311633.09375,\n",
              "  325927.5,\n",
              "  318752.4375,\n",
              "  307668.40625,\n",
              "  320715.125,\n",
              "  321124.21875,\n",
              "  315338.21875,\n",
              "  323087.0,\n",
              "  329881.96875,\n",
              "  323037.46875,\n",
              "  324487.46875,\n",
              "  317631.03125,\n",
              "  330410.09375,\n",
              "  338334.5625,\n",
              "  346210.25,\n",
              "  320507.46875,\n",
              "  333874.59375,\n",
              "  314098.21875,\n",
              "  328437.59375,\n",
              "  333929.15625,\n",
              "  335493.21875,\n",
              "  335941.28125,\n",
              "  335952.78125,\n",
              "  342964.03125,\n",
              "  337467.65625,\n",
              "  344024.6875,\n",
              "  324260.90625,\n",
              "  332318.53125,\n",
              "  332755.71875,\n",
              "  326576.65625,\n",
              "  340689.84375,\n",
              "  346939.75,\n",
              "  334515.28125,\n",
              "  334938.75,\n",
              "  334524.03125,\n",
              "  328756.84375,\n",
              "  335864.46875,\n",
              "  350041.59375,\n",
              "  350977.25,\n",
              "  344296.15625,\n",
              "  352042.53125,\n",
              "  338186.375,\n",
              "  345046.15625,\n",
              "  338648.34375,\n",
              "  346344.40625,\n",
              "  347695.65625,\n",
              "  348631.5625,\n",
              "  348133.53125,\n",
              "  347735.03125,\n",
              "  348712.15625,\n",
              "  356842.40625,\n",
              "  356874.15625,\n",
              "  357778.75,\n",
              "  336194.25,\n",
              "  343929.40625,\n",
              "  337508.34375,\n",
              "  366093.875,\n",
              "  345582.40625,\n",
              "  339134.5,\n",
              "  353383.03125,\n",
              "  339744.09375,\n",
              "  360899.5,\n",
              "  347719.84375,\n",
              "  355565.0,\n",
              "  355913.59375,\n",
              "  355422.875,\n",
              "  349397.78125,\n",
              "  350076.5,\n",
              "  357139.03125,\n",
              "  350432.90625,\n",
              "  358240.84375,\n",
              "  351188.15625,\n",
              "  351325.0,\n",
              "  352092.65625,\n",
              "  338228.59375,\n",
              "  353147.28125,\n",
              "  338391.28125,\n",
              "  367037.875,\n",
              "  346601.625,\n",
              "  361406.5,\n",
              "  355057.28125,\n",
              "  370157.5,\n",
              "  354127.59375,\n",
              "  362440.15625,\n",
              "  356062.625,\n",
              "  356430.625,\n",
              "  378203.53125,\n",
              "  364203.71875,\n",
              "  350367.0,\n",
              "  358227.71875,\n",
              "  372450.34375,\n",
              "  358114.40625,\n",
              "  365538.71875,\n",
              "  366723.46875,\n",
              "  366962.71875,\n",
              "  374984.34375,\n",
              "  375199.15625,\n",
              "  367999.15625,\n",
              "  360551.03125,\n",
              "  368675.375,\n",
              "  362571.34375,\n",
              "  362223.40625,\n",
              "  369013.46875,\n",
              "  369953.15625,\n",
              "  362308.84375,\n",
              "  370084.5,\n",
              "  363291.03125,\n",
              "  371782.59375,\n",
              "  371986.25,\n",
              "  372127.09375,\n",
              "  364765.90625,\n",
              "  379529.53125,\n",
              "  373142.03125,\n",
              "  373400.625,\n",
              "  381212.90625,\n",
              "  358892.78125,\n",
              "  366727.84375,\n",
              "  374276.125,\n",
              "  360017.90625,\n",
              "  359524.125,\n",
              "  390937.15625,\n",
              "  376094.78125,\n",
              "  391260.53125,\n",
              "  376852.84375,\n",
              "  369307.71875,\n",
              "  384340.125,\n",
              "  369498.71875,\n",
              "  361272.96875,\n",
              "  385892.28125,\n",
              "  370552.40625,\n",
              "  354845.0,\n",
              "  378476.40625,\n",
              "  378744.375,\n",
              "  378768.875,\n",
              "  379601.5,\n",
              "  357055.75,\n",
              "  380237.5,\n",
              "  372152.625,\n",
              "  372693.0,\n",
              "  388346.03125,\n",
              "  389501.71875,\n",
              "  381298.25,\n",
              "  381674.90625,\n",
              "  367355.40625,\n",
              "  374203.53125,\n",
              "  390373.21875,\n",
              "  374203.84375,\n",
              "  376028.5,\n",
              "  367750.53125,\n",
              "  367259.09375,\n",
              "  376277.21875,\n",
              "  376334.625,\n",
              "  376626.0,\n",
              "  376340.03125,\n",
              "  377273.71875,\n",
              "  377104.34375,\n",
              "  385228.125,\n",
              "  369879.75,\n",
              "  370122.71875,\n",
              "  385448.15625,\n",
              "  386471.90625,\n",
              "  393981.75,\n",
              "  378377.34375,\n",
              "  371364.15625,\n",
              "  387726.25,\n",
              "  394844.5,\n",
              "  379467.25,\n",
              "  395449.375,\n",
              "  380060.59375,\n",
              "  396475.78125,\n",
              "  380139.40625,\n",
              "  388965.40625,\n",
              "  388462.40625,\n",
              "  381627.0,\n",
              "  405358.75,\n",
              "  381861.28125,\n",
              "  373752.75,\n",
              "  397833.40625,\n",
              "  383244.78125,\n",
              "  382966.75,\n",
              "  390989.40625,\n",
              "  391315.875,\n",
              "  383413.84375,\n",
              "  391459.125,\n",
              "  383323.0,\n",
              "  399985.40625,\n",
              "  392035.75,\n",
              "  392216.84375,\n",
              "  392147.78125,\n",
              "  383847.0,\n",
              "  385023.90625,\n",
              "  377668.0,\n",
              "  393265.65625,\n",
              "  401112.28125,\n",
              "  393651.90625,\n",
              "  385736.65625,\n",
              "  386078.40625,\n",
              "  371042.25,\n",
              "  377451.75,\n",
              "  402534.28125,\n",
              "  402459.375,\n",
              "  410654.375,\n",
              "  386747.09375,\n",
              "  411052.65625,\n",
              "  387417.59375,\n",
              "  411071.09375,\n",
              "  378936.875,\n",
              "  379617.34375,\n",
              "  403895.09375,\n",
              "  388456.90625,\n",
              "  395778.375,\n",
              "  396494.25,\n",
              "  405089.59375,\n",
              "  404830.5,\n",
              "  381560.78125,\n",
              "  396888.25,\n",
              "  397753.34375,\n",
              "  397033.125,\n",
              "  389499.21875,\n",
              "  389472.875,\n",
              "  406271.15625,\n",
              "  405778.25,\n",
              "  390948.09375,\n",
              "  398860.875,\n",
              "  390767.71875,\n",
              "  382542.40625,\n",
              "  382721.15625,\n",
              "  383771.65625,\n",
              "  398896.75,\n",
              "  399460.78125,\n",
              "  383372.15625,\n",
              "  391539.46875,\n",
              "  391558.71875,\n",
              "  392018.40625,\n",
              "  392035.78125,\n",
              "  416490.25,\n",
              "  400368.75,\n",
              "  400095.46875,\n",
              "  408380.46875,\n",
              "  401106.40625,\n",
              "  377807.53125,\n",
              "  393592.46875,\n",
              "  410041.15625,\n",
              "  401854.53125,\n",
              "  393473.0,\n",
              "  401601.75,\n",
              "  410532.96875,\n",
              "  385556.28125,\n",
              "  385626.34375,\n",
              "  402585.40625,\n",
              "  393758.53125,\n",
              "  394933.96875,\n",
              "  402767.15625,\n",
              "  402425.59375,\n",
              "  403564.78125,\n",
              "  394744.34375,\n",
              "  387446.0,\n",
              "  403224.09375,\n",
              "  403451.5,\n",
              "  387018.625,\n",
              "  394776.75,\n",
              "  395791.96875,\n",
              "  404216.5,\n",
              "  379096.09375,\n",
              "  396202.84375,\n",
              "  403862.5,\n",
              "  387676.375,\n",
              "  396382.5,\n",
              "  412860.71875,\n",
              "  388049.125,\n",
              "  396059.375,\n",
              "  396644.03125,\n",
              "  404852.75,\n",
              "  412812.59375,\n",
              "  405085.09375,\n",
              "  413565.34375,\n",
              "  388335.90625,\n",
              "  405911.21875,\n",
              "  404950.40625,\n",
              "  405540.53125,\n",
              "  405730.71875,\n",
              "  413786.84375,\n",
              "  422566.75,\n",
              "  414159.03125,\n",
              "  388567.46875,\n",
              "  397403.34375,\n",
              "  389844.5,\n",
              "  422820.25,\n",
              "  398394.125,\n",
              "  389949.15625,\n",
              "  390092.09375],\n",
              " 'val_mae': [549.7438354492188,\n",
              "  544.2238159179688,\n",
              "  546.8253784179688,\n",
              "  560.800537109375,\n",
              "  548.2966918945312,\n",
              "  579.1321411132812,\n",
              "  549.9995727539062,\n",
              "  537.440673828125,\n",
              "  553.51171875,\n",
              "  532.215087890625,\n",
              "  563.6758422851562,\n",
              "  550.6226196289062,\n",
              "  568.9534301757812,\n",
              "  527.0811157226562,\n",
              "  555.01904296875,\n",
              "  547.74072265625,\n",
              "  552.4467163085938,\n",
              "  533.8230590820312,\n",
              "  534.695556640625,\n",
              "  530.6333618164062,\n",
              "  527.1763305664062,\n",
              "  536.7984008789062,\n",
              "  532.7294921875,\n",
              "  512.6652221679688,\n",
              "  511.2511291503906,\n",
              "  539.5131225585938,\n",
              "  548.8342895507812,\n",
              "  521.4905395507812,\n",
              "  507.5509338378906,\n",
              "  525.620361328125,\n",
              "  506.7037658691406,\n",
              "  522.4501953125,\n",
              "  512.2962036132812,\n",
              "  519.044677734375,\n",
              "  517.602783203125,\n",
              "  495.5006408691406,\n",
              "  525.120361328125,\n",
              "  524.3491821289062,\n",
              "  512.428955078125,\n",
              "  512.7782592773438,\n",
              "  520.0650024414062,\n",
              "  527.1109008789062,\n",
              "  508.51220703125,\n",
              "  507.095947265625,\n",
              "  512.390869140625,\n",
              "  504.8233947753906,\n",
              "  509.841064453125,\n",
              "  480.4898986816406,\n",
              "  500.3407287597656,\n",
              "  477.3710021972656,\n",
              "  488.4544372558594,\n",
              "  487.0511779785156,\n",
              "  466.330810546875,\n",
              "  480.4419860839844,\n",
              "  498.2484436035156,\n",
              "  475.853759765625,\n",
              "  488.712890625,\n",
              "  476.5358581542969,\n",
              "  475.4147033691406,\n",
              "  482.878662109375,\n",
              "  482.4253845214844,\n",
              "  453.9007568359375,\n",
              "  470.4771423339844,\n",
              "  480.0858459472656,\n",
              "  479.5013427734375,\n",
              "  470.3174743652344,\n",
              "  496.9999694824219,\n",
              "  470.8207092285156,\n",
              "  478.5868225097656,\n",
              "  446.4743347167969,\n",
              "  456.2732238769531,\n",
              "  476.4691467285156,\n",
              "  444.4343566894531,\n",
              "  481.1665344238281,\n",
              "  472.4954528808594,\n",
              "  463.1673889160156,\n",
              "  452.2274475097656,\n",
              "  468.1756896972656,\n",
              "  469.2375793457031,\n",
              "  467.7390441894531,\n",
              "  459.0520935058594,\n",
              "  463.6770935058594,\n",
              "  478.0000305175781,\n",
              "  457.5833435058594,\n",
              "  459.3333435058594,\n",
              "  469.5947265625,\n",
              "  443.212890625,\n",
              "  467.2234191894531,\n",
              "  461.3645935058594,\n",
              "  451.2205505371094,\n",
              "  463.3958435058594,\n",
              "  467.387939453125,\n",
              "  467.0779724121094,\n",
              "  467.3295593261719,\n",
              "  463.3958740234375,\n",
              "  471.3343200683594,\n",
              "  461.3645935058594,\n",
              "  469.24462890625,\n",
              "  471.2471618652344,\n",
              "  459.3333435058594,\n",
              "  468.8766174316406,\n",
              "  466.8163146972656,\n",
              "  461.3645935058594,\n",
              "  468.7892150878906,\n",
              "  461.0833435058594,\n",
              "  477.8477478027344,\n",
              "  470.7334289550781,\n",
              "  451.7432556152344,\n",
              "  461.0833435058594,\n",
              "  459.6145935058594,\n",
              "  478.121826171875,\n",
              "  478.344970703125,\n",
              "  459.3332824707031,\n",
              "  461.6458435058594,\n",
              "  457.3020935058594,\n",
              "  456.0378723144531,\n",
              "  461.0833435058594,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  477.597412109375,\n",
              "  468.2958679199219,\n",
              "  457.3020935058594,\n",
              "  470.5499267578125,\n",
              "  470.23974609375,\n",
              "  466.7110595703125,\n",
              "  468.1509704589844,\n",
              "  466.0907897949219,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  456.4436950683594,\n",
              "  454.7228698730469,\n",
              "  465.9457702636719,\n",
              "  476.8449401855469,\n",
              "  457.3020935058594,\n",
              "  452.526611328125,\n",
              "  450.80517578125,\n",
              "  474.301025390625,\n",
              "  466.0538024902344,\n",
              "  450.8927917480469,\n",
              "  457.0208435058594,\n",
              "  467.9975280761719,\n",
              "  459.6145935058594,\n",
              "  453.0391540527344,\n",
              "  469.9425354003906,\n",
              "  452.8155822753906,\n",
              "  474.0619201660156,\n",
              "  453.1540222167969,\n",
              "  476.2591552734375,\n",
              "  475.3570251464844,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  467.9629821777344,\n",
              "  454.9930114746094,\n",
              "  463.6770935058594,\n",
              "  461.0833435058594,\n",
              "  453.0477600097656,\n",
              "  469.0074768066406,\n",
              "  451.1570739746094,\n",
              "  461.3645935058594,\n",
              "  469.2012634277344,\n",
              "  473.1988830566406,\n",
              "  467.393798828125,\n",
              "  465.052490234375,\n",
              "  469.3678283691406,\n",
              "  455.7032165527344,\n",
              "  461.3645935058594,\n",
              "  469.2812805175781,\n",
              "  453.7574157714844,\n",
              "  464.88134765625,\n",
              "  480.2330017089844,\n",
              "  472.9068298339844,\n",
              "  467.1072082519531,\n",
              "  459.6145935058594,\n",
              "  448.244873046875,\n",
              "  449.9683837890625,\n",
              "  468.7425231933594,\n",
              "  455.765380859375,\n",
              "  455.794677734375,\n",
              "  468.6552429199219,\n",
              "  459.3333435058594,\n",
              "  468.598388671875,\n",
              "  471.7112121582031,\n",
              "  472.2174987792969,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  474.018798828125,\n",
              "  452.3009948730469,\n",
              "  454.0799560546875,\n",
              "  452.0770568847656,\n",
              "  468.3115234375,\n",
              "  478.9010314941406,\n",
              "  461.0,\n",
              "  454.22216796875,\n",
              "  472.9983215332031,\n",
              "  456.5917053222656,\n",
              "  459.3333435058594,\n",
              "  466.3620300292969,\n",
              "  454.3648681640625,\n",
              "  463.6770935058594,\n",
              "  477.8484191894531,\n",
              "  459.6145935058594,\n",
              "  454.7601318359375,\n",
              "  466.1897277832031,\n",
              "  452.7860107421875,\n",
              "  459.0520935058594,\n",
              "  458.9361877441406,\n",
              "  470.2265319824219,\n",
              "  467.7151184082031,\n",
              "  472.4254455566406,\n",
              "  459.6145935058594,\n",
              "  459.6145935058594,\n",
              "  457.076171875,\n",
              "  477.0216064453125,\n",
              "  457.5832824707031,\n",
              "  467.8809814453125,\n",
              "  465.2584533691406,\n",
              "  456.9366760253906,\n",
              "  465.7642517089844,\n",
              "  466.0170593261719,\n",
              "  448.8973693847656,\n",
              "  463.3670959472656,\n",
              "  457.5832824707031,\n",
              "  459.3333435058594,\n",
              "  459.1669921875,\n",
              "  467.8773498535156,\n",
              "  463.504638671875,\n",
              "  466.8945007324219,\n",
              "  458.9157409667969,\n",
              "  466.9212951660156,\n",
              "  471.5141296386719,\n",
              "  464.8327331542969,\n",
              "  471.3998718261719,\n",
              "  465.3379821777344,\n",
              "  461.3645935058594,\n",
              "  465.2807312011719,\n",
              "  470.8903503417969,\n",
              "  468.8043518066406,\n",
              "  463.4464111328125,\n",
              "  466.6368103027344,\n",
              "  457.5888671875,\n",
              "  462.75,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  462.99560546875,\n",
              "  468.6329040527344,\n",
              "  463.3958435058594,\n",
              "  466.9740295410156,\n",
              "  459.0520935058594,\n",
              "  458.6875,\n",
              "  463.3958435058594,\n",
              "  450.6534729003906,\n",
              "  463.6770935058594,\n",
              "  464.2098083496094,\n",
              "  470.1543273925781,\n",
              "  462.4034118652344,\n",
              "  462.0939636230469,\n",
              "  461.3645935058594,\n",
              "  461.0833435058594,\n",
              "  464.6038818359375,\n",
              "  458.1531677246094,\n",
              "  456.1502380371094,\n",
              "  459.3333435058594,\n",
              "  461.28125,\n",
              "  464.4630126953125,\n",
              "  459.3333740234375,\n",
              "  467.168212890625,\n",
              "  467.1109924316406,\n",
              "  464.0694580078125,\n",
              "  459.6145935058594,\n",
              "  464.2939758300781,\n",
              "  463.9837951660156,\n",
              "  464.2374267578125,\n",
              "  468.2415771484375,\n",
              "  466.4335021972656,\n",
              "  461.0,\n",
              "  454.8239440917969,\n",
              "  464.3775329589844,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  454.3750915527344,\n",
              "  462.2063903808594,\n",
              "  462.1781311035156,\n",
              "  468.4701232910156,\n",
              "  463.227294921875,\n",
              "  459.6145935058594,\n",
              "  463.535888671875,\n",
              "  466.1022033691406,\n",
              "  465.511474609375,\n",
              "  457.24560546875,\n",
              "  468.0770568847656,\n",
              "  461.3645935058594,\n",
              "  465.6546325683594,\n",
              "  461.3645935058594,\n",
              "  457.3849792480469,\n",
              "  461.3645935058594,\n",
              "  461.0833435058594,\n",
              "  463.1145935058594,\n",
              "  465.5995178222656,\n",
              "  461.42236328125,\n",
              "  465.20654296875,\n",
              "  453.5185546875,\n",
              "  463.3703918457031,\n",
              "  459.0520935058594,\n",
              "  457.6639404296875,\n",
              "  467.2399597167969,\n",
              "  461.2279968261719,\n",
              "  464.5349426269531,\n",
              "  461.3645935058594,\n",
              "  463.1742248535156,\n",
              "  462.5843505859375,\n",
              "  463.4005126953125,\n",
              "  459.3333435058594,\n",
              "  466.5119934082031,\n",
              "  457.3020935058594,\n",
              "  465.038818359375,\n",
              "  464.8768005371094,\n",
              "  463.9765930175781,\n",
              "  459.8048095703125,\n",
              "  462.8958435058594,\n",
              "  463.03125,\n",
              "  464.5904541015625,\n",
              "  460.1968688964844,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  462.7301330566406,\n",
              "  457.3020935058594,\n",
              "  462.9563903808594,\n",
              "  455.0171813964844,\n",
              "  459.3333435058594,\n",
              "  462.3108215332031,\n",
              "  460.1641540527344,\n",
              "  458.16064453125,\n",
              "  463.37255859375,\n",
              "  461.9193420410156,\n",
              "  460.4226379394531,\n",
              "  458.271240234375,\n",
              "  465.464599609375,\n",
              "  463.3958435058594,\n",
              "  457.3020935058594,\n",
              "  461.75390625,\n",
              "  460.53955078125,\n",
              "  464.9088439941406,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  456.4864501953125,\n",
              "  462.1532897949219,\n",
              "  462.6058349609375,\n",
              "  461.6458435058594,\n",
              "  459.3333435058594,\n",
              "  460.967041015625,\n",
              "  461.3645324707031,\n",
              "  463.7387390136719,\n",
              "  459.3333435058594,\n",
              "  462.5044860839844,\n",
              "  457.0419921875,\n",
              "  459.6145935058594,\n",
              "  464.0878601074219,\n",
              "  463.573974609375,\n",
              "  459.4832458496094,\n",
              "  461.7690124511719,\n",
              "  463.587158203125,\n",
              "  463.6770935058594,\n",
              "  464.0404968261719,\n",
              "  461.6589660644531,\n",
              "  461.8985290527344,\n",
              "  463.3542785644531,\n",
              "  461.0,\n",
              "  463.0177917480469,\n",
              "  461.3645935058594,\n",
              "  461.6458740234375,\n",
              "  459.4351501464844,\n",
              "  461.5699768066406,\n",
              "  461.3645935058594,\n",
              "  463.1357116699219,\n",
              "  461.6330871582031,\n",
              "  459.6822204589844,\n",
              "  459.2718200683594,\n",
              "  461.6458435058594,\n",
              "  461.2490539550781,\n",
              "  459.4764099121094,\n",
              "  458.8814392089844,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  463.4256286621094,\n",
              "  461.6431884765625,\n",
              "  463.3719787597656,\n",
              "  458.7198486328125,\n",
              "  459.0520935058594,\n",
              "  457.3020935058594,\n",
              "  459.7469787597656,\n",
              "  460.9245910644531,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  460.563232421875,\n",
              "  459.3333435058594,\n",
              "  463.1035461425781,\n",
              "  463.6770935058594,\n",
              "  462.4880065917969,\n",
              "  461.6458435058594,\n",
              "  460.4041442871094,\n",
              "  461.0276184082031,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  462.0471496582031,\n",
              "  457.0207824707031,\n",
              "  460.2431335449219,\n",
              "  458.4661560058594,\n",
              "  461.3272705078125,\n",
              "  460.5350036621094,\n",
              "  459.1885070800781,\n",
              "  461.0833435058594,\n",
              "  460.0830078125,\n",
              "  462.390625,\n",
              "  461.6458435058594,\n",
              "  464.4761047363281,\n",
              "  458.96875,\n",
              "  461.3645935058594,\n",
              "  462.5160217285156,\n",
              "  459.3333435058594,\n",
              "  461.9003601074219,\n",
              "  462.88623046875,\n",
              "  461.847900390625,\n",
              "  459.790283203125,\n",
              "  462.0761413574219,\n",
              "  460.0177917480469,\n",
              "  457.959228515625,\n",
              "  460.7347106933594,\n",
              "  457.9061279296875,\n",
              "  458.1922302246094,\n",
              "  461.0832824707031,\n",
              "  460.8384704589844,\n",
              "  459.8326721191406,\n",
              "  463.6770935058594,\n",
              "  457.0208435058594,\n",
              "  459.3333435058594,\n",
              "  457.9777526855469,\n",
              "  460.71875,\n",
              "  461.3645935058594,\n",
              "  463.3628234863281,\n",
              "  458.96875,\n",
              "  461.3845520019531,\n",
              "  457.2559509277344,\n",
              "  457.51171875,\n",
              "  459.1505432128906,\n",
              "  458.96875,\n",
              "  456.719970703125,\n",
              "  457.406982421875,\n",
              "  457.0208435058594,\n",
              "  456.8449401855469,\n",
              "  459.3600769042969,\n",
              "  460.9991760253906,\n",
              "  457.3020935058594,\n",
              "  458.6676025390625,\n",
              "  459.2558898925781,\n",
              "  461.103271484375,\n",
              "  456.9118347167969,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  458.3556213378906,\n",
              "  461.6457824707031,\n",
              "  456.5021057128906,\n",
              "  458.2005310058594,\n",
              "  458.9893493652344,\n",
              "  459.3333740234375,\n",
              "  458.9685974121094,\n",
              "  460.6921691894531,\n",
              "  459.7487487792969,\n",
              "  456.8596496582031,\n",
              "  458.3985290527344,\n",
              "  466.2005310058594,\n",
              "  459.6145935058594,\n",
              "  457.8480224609375,\n",
              "  458.4818420410156,\n",
              "  459.3333435058594,\n",
              "  459.6146240234375,\n",
              "  458.4051208496094,\n",
              "  466.0990295410156,\n",
              "  459.3333435058594,\n",
              "  460.3385009765625,\n",
              "  456.8337707519531,\n",
              "  454.4158020019531,\n",
              "  457.3020935058594,\n",
              "  455.3709411621094,\n",
              "  463.3958435058594,\n",
              "  459.3333435058594,\n",
              "  455.2162780761719,\n",
              "  458.6878356933594,\n",
              "  462.3171691894531,\n",
              "  458.0733947753906,\n",
              "  460.2766418457031,\n",
              "  456.5531921386719,\n",
              "  459.6145935058594,\n",
              "  455.6587219238281,\n",
              "  456.2738342285156,\n",
              "  461.6458435058594,\n",
              "  457.8930358886719,\n",
              "  459.8993225097656,\n",
              "  457.4768981933594,\n",
              "  459.3333435058594,\n",
              "  462.9058532714844,\n",
              "  458.0474548339844,\n",
              "  460.0551452636719,\n",
              "  464.3669128417969,\n",
              "  454.5820007324219,\n",
              "  460.7177429199219,\n",
              "  461.6458435058594,\n",
              "  455.866455078125,\n",
              "  461.3645935058594,\n",
              "  463.1145935058594,\n",
              "  452.4939880371094,\n",
              "  455.7023010253906,\n",
              "  455.7423400878906,\n",
              "  461.0833435058594,\n",
              "  457.723388671875,\n",
              "  462.9990234375,\n",
              "  461.3645935058594,\n",
              "  459.3174743652344,\n",
              "  459.0520935058594,\n",
              "  457.8834533691406,\n",
              "  459.6145935058594,\n",
              "  463.3958435058594,\n",
              "  461.1391296386719,\n",
              "  453.642578125,\n",
              "  457.4794006347656,\n",
              "  455.2926330566406,\n",
              "  459.6145935058594,\n",
              "  451.24365234375,\n",
              "  459.3333435058594,\n",
              "  451.0963439941406,\n",
              "  452.4561767578125,\n",
              "  461.6458435058594,\n",
              "  459.0520935058594,\n",
              "  460.71875,\n",
              "  461.28125,\n",
              "  459.241943359375,\n",
              "  467.9728698730469,\n",
              "  454.147705078125,\n",
              "  454.66259765625,\n",
              "  459.0520935058594,\n",
              "  459.1218566894531,\n",
              "  455.0350036621094,\n",
              "  459.3333435058594,\n",
              "  454.1448059082031,\n",
              "  452.6248474121094,\n",
              "  461.0,\n",
              "  449.8682556152344,\n",
              "  459.9913635253906,\n",
              "  459.6145935058594,\n",
              "  459.6145935058594,\n",
              "  456.8533630371094,\n",
              "  454.5160827636719,\n",
              "  457.3020935058594,\n",
              "  464.198486328125,\n",
              "  458.7895202636719,\n",
              "  458.7667236328125,\n",
              "  451.77587890625,\n",
              "  464.0106506347656,\n",
              "  456.3817443847656,\n",
              "  464.3388366699219,\n",
              "  461.3645935058594,\n",
              "  451.8218078613281,\n",
              "  453.5251159667969,\n",
              "  452.0098571777344,\n",
              "  458.2727966308594,\n",
              "  458.5304870605469,\n",
              "  454.1637268066406,\n",
              "  459.6145935058594,\n",
              "  455.865966796875,\n",
              "  464.5729675292969,\n",
              "  459.0520935058594,\n",
              "  457.5833435058594,\n",
              "  464.6438903808594,\n",
              "  458.6239929199219,\n",
              "  456.288818359375,\n",
              "  459.0520935058594,\n",
              "  458.96875,\n",
              "  459.0520935058594,\n",
              "  453.8872985839844,\n",
              "  460.4591064453125,\n",
              "  459.0520935058594,\n",
              "  460.7861633300781,\n",
              "  457.4949645996094,\n",
              "  462.5804748535156,\n",
              "  450.2005920410156,\n",
              "  450.1564636230469,\n",
              "  455.7386169433594,\n",
              "  453.68505859375,\n",
              "  461.3645935058594,\n",
              "  452.2915344238281,\n",
              "  459.3333435058594,\n",
              "  464.787353515625,\n",
              "  459.3333740234375,\n",
              "  456.1471252441406,\n",
              "  459.3333435058594,\n",
              "  464.7956237792969,\n",
              "  461.0833435058594,\n",
              "  449.3414611816406,\n",
              "  449.57861328125,\n",
              "  461.6458740234375,\n",
              "  461.6458435058594,\n",
              "  449.4441223144531,\n",
              "  453.3497009277344,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  455.3192138671875,\n",
              "  461.3645935058594,\n",
              "  469.2009582519531,\n",
              "  459.3333435058594,\n",
              "  459.3333435058594,\n",
              "  457.2427062988281,\n",
              "  461.3645935058594,\n",
              "  451.8501281738281,\n",
              "  469.4500427246094,\n",
              "  455.1285095214844,\n",
              "  451.161376953125,\n",
              "  453.3358459472656,\n",
              "  467.3843078613281,\n",
              "  461.6458435058594,\n",
              "  444.6483154296875,\n",
              "  459.6145935058594,\n",
              "  461.0833435058594,\n",
              "  444.7481384277344,\n",
              "  467.508056640625,\n",
              "  459.3333435058594,\n",
              "  452.8683166503906,\n",
              "  444.4998474121094,\n",
              "  463.5279235839844,\n",
              "  450.6228332519531,\n",
              "  463.3125,\n",
              "  454.796630859375,\n",
              "  468.1683349609375,\n",
              "  447.86572265625,\n",
              "  454.4562072753906,\n",
              "  456.7456359863281,\n",
              "  450.3392639160156,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  443.6416320800781,\n",
              "  459.0521240234375,\n",
              "  442.9519348144531,\n",
              "  461.6458435058594,\n",
              "  447.6563415527344,\n",
              "  464.1753845214844,\n",
              "  449.885009765625,\n",
              "  452.4195251464844,\n",
              "  458.6875,\n",
              "  452.9427185058594,\n",
              "  459.6145935058594,\n",
              "  454.6512145996094,\n",
              "  454.632080078125,\n",
              "  468.3963928222656,\n",
              "  457.3020935058594,\n",
              "  454.5750427246094,\n",
              "  454.5558166503906,\n",
              "  449.4603576660156,\n",
              "  459.3333435058594,\n",
              "  452.1859436035156,\n",
              "  449.9061584472656,\n",
              "  461.3645935058594,\n",
              "  473.4581298828125,\n",
              "  459.0520935058594,\n",
              "  461.6458435058594,\n",
              "  456.4179992675781,\n",
              "  459.6145935058594,\n",
              "  454.0664978027344,\n",
              "  463.9712829589844,\n",
              "  449.0057373046875,\n",
              "  468.4375915527344,\n",
              "  454.2727355957031,\n",
              "  470.7870178222656,\n",
              "  459.6145935058594,\n",
              "  453.8568420410156,\n",
              "  466.49462890625,\n",
              "  463.3958435058594,\n",
              "  466.5299377441406,\n",
              "  461.0833435058594,\n",
              "  446.902587890625,\n",
              "  464.8329162597656,\n",
              "  459.3333435058594,\n",
              "  447.0703125,\n",
              "  472.1916809082031,\n",
              "  470.7193603515625,\n",
              "  464.6434020996094,\n",
              "  457.3020935058594,\n",
              "  461.6458435058594,\n",
              "  453.6881103515625,\n",
              "  453.9502868652344,\n",
              "  447.9710388183594,\n",
              "  440.4911804199219,\n",
              "  448.464111328125,\n",
              "  453.8810119628906,\n",
              "  451.5528564453125,\n",
              "  451.5363464355469,\n",
              "  445.7377624511719,\n",
              "  461.3645935058594,\n",
              "  445.9583435058594,\n",
              "  463.1146240234375,\n",
              "  451.1761779785156,\n",
              "  453.7532043457031,\n",
              "  457.3020935058594,\n",
              "  467.2596435546875,\n",
              "  457.3020935058594,\n",
              "  462.9482727050781,\n",
              "  467.0252990722656,\n",
              "  451.6265869140625,\n",
              "  445.0719299316406,\n",
              "  459.6145935058594,\n",
              "  451.0157775878906,\n",
              "  439.5234375,\n",
              "  453.2963562011719,\n",
              "  451.5307312011719,\n",
              "  445.4472961425781,\n",
              "  455.5635070800781,\n",
              "  461.6458435058594,\n",
              "  451.4698181152344,\n",
              "  453.7666320800781,\n",
              "  445.2968444824219,\n",
              "  457.3020935058594,\n",
              "  467.5409240722656,\n",
              "  479.556396484375,\n",
              "  447.4837646484375,\n",
              "  461.6458435058594,\n",
              "  438.8707580566406,\n",
              "  455.3931579589844,\n",
              "  457.3020935058594,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  467.0460510253906,\n",
              "  459.6145935058594,\n",
              "  467.4408874511719,\n",
              "  446.3114929199219,\n",
              "  451.4768371582031,\n",
              "  451.4635314941406,\n",
              "  445.0353088378906,\n",
              "  463.3958435058594,\n",
              "  467.525390625,\n",
              "  453.1598205566406,\n",
              "  453.1470031738281,\n",
              "  452.5693054199219,\n",
              "  444.5897521972656,\n",
              "  452.8238830566406,\n",
              "  469.6332702636719,\n",
              "  469.9281311035156,\n",
              "  461.3645935058594,\n",
              "  469.9585876464844,\n",
              "  452.7541809082031,\n",
              "  457.3020935058594,\n",
              "  450.6968688964844,\n",
              "  459.3333435058594,\n",
              "  463.3958435058594,\n",
              "  463.6770935058594,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  457.5833435058594,\n",
              "  470.104736328125,\n",
              "  470.1427001953125,\n",
              "  470.7070007324219,\n",
              "  444.8145446777344,\n",
              "  453.8037414550781,\n",
              "  445.6796569824219,\n",
              "  478.0597229003906,\n",
              "  454.8800354003906,\n",
              "  446.7550048828125,\n",
              "  463.9012756347656,\n",
              "  448.8645324707031,\n",
              "  469.3738098144531,\n",
              "  454.6296691894531,\n",
              "  465.29345703125,\n",
              "  465.5184631347656,\n",
              "  463.6025085449219,\n",
              "  455.7787170410156,\n",
              "  459.2917175292969,\n",
              "  466.3269958496094,\n",
              "  458.089111328125,\n",
              "  468.4781494140625,\n",
              "  460.1640625,\n",
              "  458.6990661621094,\n",
              "  459.1737365722656,\n",
              "  444.023681640625,\n",
              "  461.3782653808594,\n",
              "  444.1585998535156,\n",
              "  473.5452575683594,\n",
              "  453.297119140625,\n",
              "  469.0439147949219,\n",
              "  462.5677185058594,\n",
              "  479.959716796875,\n",
              "  459.086669921875,\n",
              "  469.7126770019531,\n",
              "  461.7413635253906,\n",
              "  461.9775390625,\n",
              "  484.6020812988281,\n",
              "  469.3870849609375,\n",
              "  454.2545471191406,\n",
              "  464.5238342285156,\n",
              "  478.504150390625,\n",
              "  464.475341796875,\n",
              "  470.2776794433594,\n",
              "  472.3730163574219,\n",
              "  472.5213623046875,\n",
              "  482.9228820800781,\n",
              "  480.2176208496094,\n",
              "  473.1628723144531,\n",
              "  464.6212158203125,\n",
              "  473.5802307128906,\n",
              "  467.16015625,\n",
              "  465.6546630859375,\n",
              "  472.507568359375,\n",
              "  471.7767639160156,\n",
              "  464.451416015625,\n",
              "  473.1897888183594,\n",
              "  466.34326171875,\n",
              "  476.7410583496094,\n",
              "  476.860595703125,\n",
              "  475.6961669921875,\n",
              "  466.0145568847656,\n",
              "  481.70947265625,\n",
              "  476.3138122558594,\n",
              "  474.0387878417969,\n",
              "  485.4698791503906,\n",
              "  459.7637634277344,\n",
              "  469.6672058105469,\n",
              "  474.6418151855469,\n",
              "  461.6411437988281,\n",
              "  460.1865234375,\n",
              "  494.9739990234375,\n",
              "  476.9337463378906,\n",
              "  494.0050354003906,\n",
              "  478.5513000488281,\n",
              "  470.0692443847656,\n",
              "  485.9154357910156,\n",
              "  470.219970703125,\n",
              "  461.3489685058594,\n",
              "  487.9745788574219,\n",
              "  469.77294921875,\n",
              "  454.6785888671875,\n",
              "  478.5027770996094,\n",
              "  478.6756896972656,\n",
              "  477.6235046386719,\n",
              "  480.2880859375,\n",
              "  456.0351257324219,\n",
              "  480.6827697753906,\n",
              "  470.864990234375,\n",
              "  472.2515869140625,\n",
              "  488.4459228515625,\n",
              "  491.2007751464844,\n",
              "  480.3166809082031,\n",
              "  481.5719909667969,\n",
              "  465.1573181152344,\n",
              "  473.205078125,\n",
              "  489.6916809082031,\n",
              "  473.2311096191406,\n",
              "  475.2904357910156,\n",
              "  466.8076171875,\n",
              "  465.1920471191406,\n",
              "  475.4544372558594,\n",
              "  474.5177307128906,\n",
              "  474.7001037597656,\n",
              "  474.5458984375,\n",
              "  476.054931640625,\n",
              "  475.9662780761719,\n",
              "  483.7535705566406,\n",
              "  467.7660827636719,\n",
              "  467.9151306152344,\n",
              "  483.9056701660156,\n",
              "  483.5746765136719,\n",
              "  491.95654296875,\n",
              "  474.9111633300781,\n",
              "  468.6748046875,\n",
              "  485.2568054199219,\n",
              "  491.6013488769531,\n",
              "  476.4925231933594,\n",
              "  492.8614807128906,\n",
              "  476.8597106933594,\n",
              "  493.471435546875,\n",
              "  477.7947692871094,\n",
              "  486.0233459472656,\n",
              "  484.880859375,\n",
              "  477.8038635253906,\n",
              "  503.4801025390625,\n",
              "  477.109375,\n",
              "  470.1451416015625,\n",
              "  494.3238525390625,\n",
              "  478.7765197753906,\n",
              "  478.6273193359375,\n",
              "  487.5911560058594,\n",
              "  488.2496032714844,\n",
              "  478.0968322753906,\n",
              "  486.7235412597656,\n",
              "  478.0714416503906,\n",
              "  494.8266906738281,\n",
              "  487.0815124511719,\n",
              "  487.54150390625,\n",
              "  487.9512634277344,\n",
              "  478.4355163574219,\n",
              "  480.6714172363281,\n",
              "  471.7056579589844,\n",
              "  488.6045227050781,\n",
              "  496.3193359375,\n",
              "  488.8348388671875,\n",
              "  481.08935546875,\n",
              "  479.7811584472656,\n",
              "  464.7354431152344,\n",
              "  472.3871765136719,\n",
              "  498.65087890625,\n",
              "  497.8680114746094,\n",
              "  505.0555419921875,\n",
              "  480.9527587890625,\n",
              "  505.2994079589844,\n",
              "  481.3393859863281,\n",
              "  504.5940246582031,\n",
              "  472.5447692871094,\n",
              "  472.9394226074219,\n",
              "  499.0496826171875,\n",
              "  482.6683654785156,\n",
              "  489.7602844238281,\n",
              "  490.5218200683594,\n",
              "  499.3880310058594,\n",
              "  499.5914306640625,\n",
              "  474.7885437011719,\n",
              "  490.7671813964844,\n",
              "  492.2828674316406,\n",
              "  490.1828308105469,\n",
              "  482.614013671875,\n",
              "  481.9372253417969,\n",
              "  498.748779296875,\n",
              "  498.4709777832031,\n",
              "  482.7971496582031,\n",
              "  491.2522277832031,\n",
              "  484.0074768066406,\n",
              "  474.7373352050781,\n",
              "  475.4870300292969,\n",
              "  475.4411315917969,\n",
              "  491.9490661621094,\n",
              "  492.2682189941406,\n",
              "  475.2444763183594,\n",
              "  484.4591979980469,\n",
              "  483.8486022949219,\n",
              "  483.4893493652344,\n",
              "  483.5,\n",
              "  508.6037902832031,\n",
              "  492.1905822753906,\n",
              "  492.0467529296875,\n",
              "  499.4784851074219,\n",
              "  493.8302307128906,\n",
              "  468.7982482910156,\n",
              "  485.63232421875,\n",
              "  502.2353210449219,\n",
              "  493.6670837402344,\n",
              "  484.9895935058594,\n",
              "  493.5304870605469,\n",
              "  501.941162109375,\n",
              "  477.1622009277344,\n",
              "  477.2030944824219,\n",
              "  494.0924377441406,\n",
              "  485.7383117675781,\n",
              "  486.3993225097656,\n",
              "  493.6394958496094,\n",
              "  493.4547424316406,\n",
              "  494.651123046875,\n",
              "  486.2961730957031,\n",
              "  478.2412414550781,\n",
              "  493.91455078125,\n",
              "  494.5953063964844,\n",
              "  477.4585266113281,\n",
              "  486.3196716308594,\n",
              "  486.3479919433594,\n",
              "  494.4903869628906,\n",
              "  469.616943359375,\n",
              "  487.1222229003906,\n",
              "  493.7678527832031,\n",
              "  477.8554382324219,\n",
              "  487.224365234375,\n",
              "  503.3136291503906,\n",
              "  478.6020202636719,\n",
              "  486.5213928222656,\n",
              "  486.8509826660156,\n",
              "  495.4064025878906,\n",
              "  503.2995910644531,\n",
              "  496.0549621582031,\n",
              "  503.7275695800781,\n",
              "  478.778564453125,\n",
              "  495.5054931640625,\n",
              "  495.31201171875,\n",
              "  495.3035888671875,\n",
              "  495.9131164550781,\n",
              "  503.8713684082031,\n",
              "  512.7330932617188,\n",
              "  504.0894470214844,\n",
              "  478.4342041015625,\n",
              "  487.8115234375,\n",
              "  479.6407470703125,\n",
              "  512.878662109375,\n",
              "  487.88037109375,\n",
              "  479.2208557128906,\n",
              "  479.7835998535156]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "5ead4fdb-d73b-4575-ed54-543bc8170a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression_freeze_1000.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression_freeze_1000.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "999d1188-d4c3-4945-f026-2348ab5ef8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_39757a72-de61-46d8-8d39-9c1ce78ae39d\", \"2Class_regression_freeze_1000.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}