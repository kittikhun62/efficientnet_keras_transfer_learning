{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNN+C+2DgicR/ictNCJcgjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15233b89-6ef3-4d6d-9c9c-2fe6f755f399"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8971647e-c518-4125-d90c-0f07a1599ab8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a12f54a2-9149-4b22-8cc7-6f814c9fea25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a12f54a2-9149-4b22-8cc7-6f814c9fea25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a12f54a2-9149-4b22-8cc7-6f814c9fea25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a12f54a2-9149-4b22-8cc7-6f814c9fea25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "e83f441a-06a1-4d85-b79f-49fa57f80f91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhHklEQVR4nO3df7gcVZ3n8fdHfpsgSQheMSABjT+CrAh5EIRxo1kRohKYcTXIQlDcMDuwA2tYN+ozyui6C8iPGVkHnyAM0UF+iCBRUInIVRkHJGECSQiBBIMQQyIQAomKJHz3j3M6NE3f3O7bv+pWPq/nqedWn6ru+nbd09+uPnXqlCICMzMrn1f1OgAzM+sMJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ/gCkbRK0jpJI6rKPiWpv4dhmbVVrud/lLRR0npJt0jaNy+7StKf87LKdJ+kv6h6vElS1Kzzhl6/ryJygi+eHYCzeh2EWYd9OCJGAnsDa4FLq5ZdEBEjq6Z3RMQvK4+BA/N6o6rW+W2338Bw4ARfPF8FzpE0qnaBpHdLukfShvz33d0Pz6x9IuJPwA3AxF7HUkZO8MWzAOgHzqkulDQGuAX4GrAncDFwi6Q9ux2gWbtIejXwMeCuXsdSRk7wxfQF4L9L2quq7IPAwxHx7YjYHBHXAA8CH+5JhGat+b6kZ4ANwPtJv1wrzpH0TNU0tycRloATfAFFxBLgh8DsquLXA4/WrPooMK5bcZm10fERMQrYFTgT+Lmk1+VlF0bEqKppRs+iHOac4Ivri8B/5aUE/jtgv5p13gCs7mZQZu0UEVsi4kZgC3BUr+MpGyf4goqIFcB1wN/moluBN0v6uKQdJX2MdGLqh72K0axVSqYBo4FlvY6nbJzgi+1LwAiAiHgK+BAwC3gK+AzwoYh4snfhmQ3ZDyRtBJ4FvgLMiIiledlnavq4u44PkXzDDzOzcvIRvJlZSTnBm5mVlBO8mVlJOcGbmZXUjr0OAGDs2LExfvz4uss2bdrEiBEj6i4rCsfYPq3EuXDhwicjYq/B1+w91/nuGA5xdrTOR0TPp0MPPTQGcscddwy4rCgcY/u0EiewINpQH4F9gTuAB4ClwFm5fAwwH3g4/x2dy0UaI2gFcD9wyGDbcJ3vjuEQZyfrvJtozF5pMzArIiYChwNnSJpIGjri9oiYANzOS0NJHAtMyNNM4LLuh2z2Sk7wZjUiYk1E3JvnnyNdYTkOmAZUBr6aCxyf56cB38oHVXcBoyTt3d2ozV6pEG3wZkUlaTzwTuBuoC8i1uRFTwB9eX4c8FjV0x7PZWuqypA0k3SET19fH/39/XW3uXHjxgGXFcVwiBGGR5ydjLHwCX7x6g2cOvuWXoexTbMO2uwY22SwOFed98GuxSJpJPA94OyIeFbS1mUREZKaugw8IuYAcwAmTZoUkydPrrvepVffzEV3bmoq1m7uF4D+/n4Gir9IhkOcnYzRTTRmdUjaiZTcr4402iHA2krTS/67LpevJp2YrdgHj/JpBTDkBC/pLZIWVU3PSjpb0rmSVleVT21nwGadpnSofgWwLCIurlo0D6iMTT4DuLmq/JQ8MuLhwIaqphyznhlyE01ELAcOBpC0A+mI5SbgE8AlEXFhOwI064EjgZOBxZIW5bLPAecB10s6jXSzlY/mZbcCU0ndJP9A+gyY9Vy72uCnACsj4tHqdkqz4Sgi7iT1ba9nSp31Azijo0GZDUG7Evx04Jqqx2dKOoV0A+lZEbG+9gmN9ijo2y2deCsyx9g+g8VZ9B4RZkXScoKXtDNwHPDZXHQZ8GUg8t+LgE/WPq+pHgWLi93ZZ9ZBmx1jmwwW56qTJncvGLNhrh29aI4F7o2ItQARsTbSfRZfBC4HDmvDNszMrEntSPAnUtU8U3MF3wnAkjZsw8zMmtTSb3ZJI4D3A6dXFV8g6WBSE82qmmVmZtYlLSX4iNgE7FlTdnJLEZmZWVv4SlYzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykneDOzkir+8IJmNqjxQ7zfbrfv5Wrd5SN4M7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5Jq9abbq4DngC3A5oiYJGkMcB0wnnTT7Y9GxPrWwjQzs2a14wj+vRFxcERMyo9nA7dHxATg9vzYzMy6rBNNNNOAuXl+LnB8B7ZhZmaDaDXBB3CbpIWSZuayvohYk+efAPpa3IaZmQ1Bq6NJHhURqyW9Fpgv6cHqhRERkqLeE/MXwkyAvr4++vv7626gbzeYddDmFsPsLMfYPoPFOVA9MbNXainBR8Tq/HedpJuAw4C1kvaOiDWS9gbWDfDcOcAcgEmTJsXkyZPrbuPSq2/mosXFHtV41kGbHWObDBbnqpMmdy8Ys2FuyE00kkZI2r0yDxwNLAHmATPyajOAm1sN0szMmtfKIV0fcJOkyut8JyJ+LOke4HpJpwGPAh9tPUwzM2vWkBN8RDwCvKNO+VPAlFaCMjOz1hW/UdbMOmYot/obym3+urUdezkPVWBmVlJO8GZ1SLpS0jpJS6rKxkiaL+nh/Hd0Lpekr0laIel+SYf0LnKzlzjBm9V3FXBMTdlAw3AcC0zI00zgsi7FaLZNTvBmdUTEL4Cna4oHGoZjGvCtSO4CRuVrQMx6yidZzRo30DAc44DHqtZ7PJetqSorzdXb/f39bNy4samriofyftpx1XKzcfZCJ2N0gjcbgm0Nw7GN55Ti6u1VJ02mv7+fgeKv59Sh9KJpw1XLzcbZC52M0U00Zo1bW2l6qRmGYzWwb9V6++Qys54q7mGCWfFUhuE4j5cPwzEPOFPStcC7gA1VTTk2RO473zoneLM6JF0DTAbGSnoc+CIpsdcbhuNWYCqwAvgD8ImuB2xWhxO8WR0RceIAi14xDEdEBHBGZyMya57b4M3MSsoJ3syspJzgzcxKygnezKyknODNzErKCd7MrKSc4M3MSsoJ3syspJzgzcxKasgJXtK+ku6Q9ICkpZLOyuXnSlotaVGeprYvXDMza1QrQxVsBmZFxL2SdgcWSpqfl10SERe2Hp6ZmQ3VkBN8Hi1vTZ5/TtIy0k0OzMysANoy2Jik8cA7gbuBI0lDp54CLCAd5a+v85xS3N0GHGM7DRZn0e/OY1YkLSd4SSOB7wFnR8Szki4DvgxE/nsR8Mna55Xl7jaQEpJjbI/B4mzHXX7Mthct9aKRtBMpuV8dETcCRMTaiNgSES8ClwOHtR6mmZk1q5VeNAKuAJZFxMVV5dV3kz8BWDL08MzMbKha+c1+JHAysFjSolz2OeBESQeTmmhWAae3sA0zs4bV3uZv1kGbG7rhd1lv9ddKL5o7AdVZdOvQwzEzs3bxlaxmZiXlBG9mVlLF7zdnZoUyfvYtDbdtW2/5CN7MrKSc4M3MSsoJ3syspJzgzcxKygnezKyknODNzErK3STNzIagdliERnR7SAQfwZuZlZQTvJlZSTnBm5mVlBO8mVlJOcGbmZWUE7yZWUk5wZuZlZQTvJlZSTnBm5mVVMcSvKRjJC2XtELS7E5tx6woXOetaDoyVIGkHYCvA+8HHgfukTQvIh7oxPbMes113hpRb3iDwe6O1crwBp0ai+YwYEVEPAIg6VpgGuDKbmXlOj+MDWVcmeFAEdH+F5U+AhwTEZ/Kj08G3hURZ1atMxOYmR++BVg+wMuNBZ5se5Dt5Rjbp5U494uIvdoZTKNc5wtrOMTZsTrfs9EkI2IOMGew9SQtiIhJXQhpyBxj+wyXOIfCdb77hkOcnYyxUydZVwP7Vj3eJ5eZlZXrvBVOpxL8PcAESftL2hmYDszr0LbMisB13gqnI000EbFZ0pnAT4AdgCsjYukQX27Qn7QF4BjbZ7jE+TKu84U1HOLsWIwdOclqZma95ytZzcxKygnezKykCpvgi3LZt6R9Jd0h6QFJSyWdlcvPlbRa0qI8Ta16zmdz3MslfaCLsa6StDjHsyCXjZE0X9LD+e/oXC5JX8tx3i/pkC7E95aq/bVI0rOSzi7ivuyVXtZ7SVdKWidpSVVZ0/VH0oy8/sOSZrQ5xoE+j4WJU9Kukn4t6b4c49/n8v0l3Z1juS6fjEfSLvnxirx8fNVrtVb/I6JwE+kk1UrgAGBn4D5gYo9i2Rs4JM/vDjwETATOBc6ps/7EHO8uwP75fezQpVhXAWNryi4AZuf52cD5eX4q8CNAwOHA3T34Hz8B7FfEfdmjutbTeg+8BzgEWDLU+gOMAR7Jf0fn+dFtjHGgz2Nh4szbGpnndwLuztu+Hpiey78B/Lc8/zfAN/L8dOC6PN9y/S/qEfzWy74j4s9A5bLvrouINRFxb55/DlgGjNvGU6YB10bE8xHxG2AF6f30yjRgbp6fCxxfVf6tSO4CRknau4txTQFWRsSj21inaPuy03pa7yPiF8DTNcXN1p8PAPMj4umIWA/MB45pY4wDfR4LE2fe1sb8cKc8BfA+4IYBYqzEfgMwRZJoQ/0vaoIfBzxW9fhxtp1UuyL/dHon6RsZ4Mz8s+/Kyk9Ceht7ALdJWqh0WTxAX0SsyfNPAH15vtf7eDpwTdXjou3LXiji+222/nTtPdR8HgsVp6QdJC0C1pG+PFYCz0TE5jrb2xpLXr4B2LMdMRY1wReOpJHA94CzI+JZ4DLgjcDBwBrgot5Ft9VREXEIcCxwhqT3VC+M9Luv5/1ic9vjccB3c1ER96XVKEr9gbqfx62KEGdEbImIg0lXNB8GvLUXcRQ1wRfqsm9JO5Eq09URcSNARKzN/8QXgcuB90u6jRZjl7SXpAcl7dZsnBGxWtJGYCRwE6lira00veS/6/LqA8aZTxAd2Oz2m3AscG9ErM1x1+7Lys/QQtWDLiji+222/nT8PdT7PBYxToCIeAa4AziC1DxUubi0entbY8nL9wCeakeMRU3whbnsO7eFXQEsi4iLJR0l6Ve5B8jTkv4V+FvgXyPi6Bzn9HxmfH9gAvDrJjY5G7gqIv7YZJwjJO0eESOBtcDRwJIcT6WHwAzg5jw/Dzgl9zI4HNhQ9RP3QuBLzWy/SSdS1TxT0/Z/Qo67EmMr+3K4KUy9r9Js/fkJ8FFJ1+emtqNzWVvUfh5bjPNoSaPbHWc+SBuV53cj3SNgGSnRf2SAGCuxfwT4Wf4V0nr9b8dZ405MpLPfD5Harj7fwziOIv3cuz9PW4DzgX8hJaJHgH5g76rnfD7HvRw4tolt7UIaNnSfIcR5AOmM+33A0so+I7Xl3Q48DPwUGBMvnen/eo5zMTCp6rV2JZ1se10H9ucI0tHJHlVl384x3J8rdcv7crhOvaz3pC/dNcALpPbe0wapP7cBf8qfiSdJvVWOIiWu50gnBT/R5hirP4+L8jQ1x/lzYGOO5zHg41X1fBXwLPD7/PzxwCdzjG2NE/gPwL/nGJcAX8jlB5AS9ApS8+QuuXzX/HhFXn5Au+p/zyv0cJqASaQTJfWWnQrcmec/kytaZXqBdFQO6efXFfmDtBr43+SuT6RuaitqXrc/r/Or/Fo/yJX56lxh7wHGV60fwJvy/G6k9uxHSSdu7gR2y8uOI30RPJO38baa7c4HZvR6n3sq5gR8mtQM8pekL+2dgA8DXyV1e/2XHsR0DXAdqYnyqFznD8zL+kjdEY+oJPhe78NuTEVtoimqh4AtkuZKOraqt8fLRMQFETEyUnPJ20hHDdflxVcBm4E3kXoAHA18Ki87iPo3gZgOnEw6g/5G4N+Afyb14V0GfHGAeC8EDgXendf9DPCipDeTPgxnA3sBtwI/qFx4kS0D3jHQjrDtl6Q9SE14Z0TEjRGxKSJeiIgfRMT/rLP+dyU9IWmDpF9Un9+RNFXpoqXnlC52OyeXj5X0Q0nP5KbQX0oaMF9JGgH8FfB3EbExIu4k/Ro8Gbae5/kn0gHRdsMJvgmRztZXfiJeDvxe0jxJffXWz+1v3wf+MSJ+lNebSjrzvyki1gGXkBI4wCjST9ta/xwRKyNiA+ln8MqI+GmkLlXfJX1R1G77VaSfoGdFxOpIJzF/FRHPAx8DbomI+RHxAumLYDfSF0HFczkes1pHkJoVbmpw/R+R2o9fC9xL+vVZcQVwekTsDrwd+Fkun0VqJtqLdPT9ObbdM+bNwOaIeKiq7D6gk50FCq9nd3QariJiGak5BklvJbXF/wP1T9BcASyPiPPz4/1IP2XXpHNFQPqSrfR1XU+6Oq/W2qr5P9Z5PLLOc8aSPoQr6yx7PanZpvKeXpT0GC/vY7s7qfnGrNaewJPxUp/ubYqIKyvzks4F1kvaIx+wvABMlHRfpAuO1udVXyBdtbpfRKwAfjnIZkaSmiyrbaD+52m74SP4FkTEg6Qml7fXLlMaR+TNpBNVFY8Bz5OGExiVp9dEROUo4/78nHZ4knQC7I11lv2O9GVTiVWk7ljVXbDeRjoCMqv1FDC2qsvfgPIFP+dJWinpWdLJTkgHIJCaVaYCj0r6uaQjcvlXSScdb5P0iAYfl2cj8JqastdQ/xfxdsMJvgmS3ipplqR98uN9SV3+7qpZ71hS18kToqq7Y6TuWbcBF0l6jaRXSXqjpP+YV/k1qa9sy1fURepTfiVwsaTX5w/aEZJ2IY2J8UFJU3Kf4lmkL55f5fh3JbXdz281DiulfyPVl+MbWPfjpEvu/xOpg8H4XC6AiLgnIqaRmm++T6qbRMRzETErIg4gdQj4tKQp29jOQ8COkiZUlb2D1JFgu+UE35zngHcBd0vaRErsS0gJstrHSG2HyyRtzNM38rJTSANJPUD6OXoD6acokcYfuQr4L22K9xxS98N7SN0ezwdeFRHL8zYuJR3pfxj4cN4++XF/RPyuTXFYieSmlS8AX5d0vKRXS9opdzy4oGb13UlfBk8Brwb+T2WBpJ0lnZSba14gNbG8mJd9SNKb8q/LDaSujy9uI6ZNwI3Al/I1IUeSvli+XbW9XUldkQF2yY/LrdfdeDy9fCJ9MTxI7s7YoxjuBt7e633hqdgTcBKwANhEGv/lFtKJ+nPJ3SRJbeOVfvGPkg5wgtSLbGfgx6QDnUqX36Py8/4HqTlnE+lk6981EM8Y0q+ATcBvgY/XLI/aqdf7sNOTb9lnZlZSbqIxMyspd5M0s2FB0htI567qmRgRv+1mPMOBm2jMzEqqEEfwY8eOjfHjx9ddtmnTJkaMGNHdgArI+yHZ1n5YuHDhkxGxV5dDGhLX+cF5PySt1PlCJPjx48ezYMGCusv6+/uZPHlydwMqIO+HZFv7QdK2bv9XKK7zg/N+SFqp8z7JajaAfHHYv0v6YX68v9Jd71dIuq4yOFser/u6XH630q3kzHrOCd5sYGeRRtWsOB+4JCLeROq7XRmG4jRgfS6/JK9n1nNO8GZ15OEoPgh8Mz8W8D7SlccAc3npUv1p+TF5+RRVjSZn1iuFaIO3wS1evYFTZ9/S1HNWnffBDkWzXfgH0vj5ldEI9yTd7KUygmL1He7HkUcEjYjNkjbk9Z+sfkFJM4GZAH19ffT399fd8LqnN3Dp1TfXXTaQg8bt0dT6w8HGjRsH3EfD1eLVG5p+zv577DDk/eAEb1ZD0oeAdRGxUNLkdr1uRMwB5gBMmjQpBjpxdunVN3PR4uY+mqtOqv9aw1kZT7I2e5AGcNUxI4a8HwZtopH0FkmLqqZnJZ0t6dx8B5ZK+dSq53w2n3BaLukDQ4rMrHeOBI6TtAq4ltQ084+kkT4rmbf6DverScMtk5fvQRpcy6ynBk3wEbE8Ig6OiINJQ8j+gZfu5HJJZVlE3AogaSLpDkUHAscA/yRph45Eb9YBEfHZiNgnIsaT6vLPIuIk4A7SXe8BZpAG0YJ0a7gZef4jeX1fQWg91+xJ1imk28Vtq+/lNODaiHg+In5DGrT/sKEGaFYg/4s0LvkKUhv7Fbn8CmDPXP5pYLCbU5h1RbNt8NNJN2uuOFPSKaQhQ2dFuuXWOF5+A4zqk1FbNXrCqYwnWoaibzeYdVBDd0jbqoz7rdv1ISL6gf48/wh1DlYi4k/Af+5aUGYNajjB54s6jgM+m4suA75MGlf5y8BFpJs8N6TRE05lPNEyFD7xlrg+mDWumSaaY4F7I2ItQESsjYgtkW4NdzkvHdlsPeGUVZ+MMjOzLmkmwZ9IVfOMpL2rlp1AunUdpBNO0/Pl2/sDE0j3GjUzsy5q6De/pBHA+4HTq4ovkHQwqYlmVWVZRCyVdD1p3ObNwBkRsaWNMZuZWQMaSvCRbmi7Z03ZydtY/yvAV1oLzczMWuGxaMzMSsoJ3syspJzgzcxKygnezKyknODNzErKCd7MrKSc4M3MSsoJ3syspJzgzcxKygnezKyknODNzErKCd7MrKSc4M3MSsoJ3syspJzgzcxKygnezKyknODNzErKCd7MrKSc4M3MSqqhBC9plaTFkhZJWpDLxkiaL+nh/Hd0Lpekr0laIel+SYd08g2YmVl9zRzBvzciDo6ISfnxbOD2iJgA3J4fAxwLTMjTTOCydgVrZmaNa6WJZhowN8/PBY6vKv9WJHcBoyTt3cJ2zMxsCBpN8AHcJmmhpJm5rC8i1uT5J4C+PD8OeKzquY/nMjMz66IdG1zvqIhYLem1wHxJD1YvjIiQFM1sOH9RzATo6+ujv7+/7nobN24ccNn2pG83mHXQ5qaeU8b95vpg1riGEnxErM5/10m6CTgMWCtp74hYk5tg1uXVVwP7Vj19n1xW+5pzgDkAkyZNismTJ9fddn9/PwMt255cevXNXLS40e/jZNVJkzsTTA+5Ppg1btAmGkkjJO1emQeOBpYA84AZebUZwM15fh5wSu5Ncziwoaopx8zMuqSRQ8I+4CZJlfW/ExE/lnQPcL2k04BHgY/m9W8FpgIrgD8An2h71GZmNqhBE3xEPAK8o075U8CUOuUBnNGW6MzMbMh8JauZWUk5wZuZlZQTvJlZSTnBm9WQtK+kOyQ9IGmppLNyucdfsmHFCd7slTYDsyJiInA4cIakiXj8JRtmnODNakTEmoi4N88/BywjDbfh8ZdsWGnu0kiz7Yyk8cA7gbtpfvyll13g1+jwHB6WIinjsBTN/l+htf3gBG82AEkjge8BZ0fEs/liP2Bo4y81OjyHh6VIyjgsxamzb2n6OVcdM2LI+8FNNGZ1SNqJlNyvjogbc/HaStPLUMZfMus2J3izGkqH6lcAyyLi4qpFHn/JhhU30Zi90pHAycBiSYty2eeA8/D4SzaMOMGb1YiIOwENsNjjL9mw4SYaM7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyupQRP8Nm5+cK6k1ZIW5Wlq1XM+m29+sFzSBzr5BszMrL5GrmSt3PzgXkm7Awslzc/LLomIC6tXzjdGmA4cCLwe+KmkN0fElnYGbmZm2zboEfw2bn4wkGnAtRHxfET8hjQ+x2HtCNbMzBrX1Fg0NTc/OBI4U9IpwALSUf56UvK/q+pplZsf1L5WQzc/KOOg/0Phm0Akrg9mjWs4wde5+cFlwJeByH8vAj7Z6Os1evODMg76PxS+CUTi+mDWuIZ60dS7+UFErI2ILRHxInA5LzXD+OYHZmYF0Egvmro3P6i5qfAJwJI8Pw+YLmkXSfuT7jT/6/aFbGZmjWjkN/9ANz84UdLBpCaaVcDpABGxVNL1wAOkHjhnuAeNmVn3DZrgt3Hzg1u38ZyvAF9pIS4zM2uRr2Q1MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5JygjczKykneDOzknKCNzMrKSd4M7OScoI3MyupjiV4ScdIWi5phaTZndqOWVG4zlvRdCTBS9oB+DpwLDAROFHSxE5sy6wIXOetiDp1BH8YsCIiHomIPwPXAtM6tC2zInCdt8LZsUOvOw54rOrx48C7qleQNBOYmR9ulLR8gNcaCzzZ9giHn6b3g87vUCS9ta39sF83A6nR0zq/Hf6vtxvvPX/odb5TCX5QETEHmDPYepIWRMSkLoRUaN4PyXDeD67zzfF+SFrZD51qolkN7Fv1eJ9cZlZWrvNWOJ1K8PcAEyTtL2lnYDowr0PbMisC13krnI400UTEZklnAj8BdgCujIilQ3y5QX/Sbie8H5JC7gfX+Y7wfkiGvB8UEe0MxMzMCsJXspqZlZQTvJlZSRUiwUs6S9ISSUslnV1n+WRJGyQtytMXehBmR0i6UtI6SUuqysZImi/p4fx39ADPnZHXeVjSjO5F3X4t7octVXVj2JzYHGxoA0m7SLouL79b0vgehNlxDeyHUyX9vup//KlexNlp9T4DNcsl6Wt5P90v6ZBBXzQiejoBbweWAK8mnfT9KfCmmnUmAz/sdawdev/vAQ4BllSVXQDMzvOzgfPrPG8M8Ej+OzrPj+71++n2fsjLNvY6/iG83x2AlcABwM7AfcDEmnX+BvhGnp8OXNfruHu0H04F/l+vY+3CvnjFZ6Bm+VTgR4CAw4G7B3vNIhzBv40U6B8iYjPwc+AvexxT10TEL4Cna4qnAXPz/Fzg+DpP/QAwPyKejoj1wHzgmE7F2Wkt7IfhqpGhDarf/w3AFEnqYozd4CEesgE+A9WmAd+K5C5glKS9t/WaRUjwS4C/kLSnpFeTvqX2rbPeEZLuk/QjSQd2N8Su64uINXn+CaCvzjr1Lo0f1+nAuqyR/QCwq6QFku6SdHx3QmtZI/+/revkg58NwJ5dia57Gq3Hf5WbJW6QVC8/bA+a/sz3bKiCiohYJul84DZgE7AI2FKz2r3AfhGxUdJU4PvAhG7G2SsREZK2+76sg+yH/SJitaQDgJ9JWhwRK7sZn3XUD4BrIuJ5SaeTftW8r8cxDQtFOIInIq6IiEMj4j3AeuChmuXPRsTGPH8rsJOksT0ItVvWVn565b/r6qyzPVwa38h+ICJW57+PAP3AO7sVYAsa+f9tXUfSjsAewFNdia57Bt0PEfFURDyfH34TOLRLsRVN05/5QiR4Sa/Nf99Aan//Ts3y11XaHiUdRoq7bBW92jyg0itmBnBznXV+AhwtaXTuXXJ0LiuTQfdDfv+75PmxwJHAA12LcOgaGdqg+v1/BPhZ5LNtJTLofqhpZz4OWNbF+IpkHnBK7k1zOLChqgmzvl6fOc719ZekD+V9wJRc9tfAX+f5M4GlefldwLt7HXMb3/s1wBrgBVKb2mmkdtbbgYdJvYrG5HUnAd+seu4ngRV5+kSv30sv9gPwbmBxrhuLgdN6/V6aeM9TSb9WVwKfz2VfAo7L87sC383/318DB/Q65h7th/9b9fm/A3hrr2Pu0H6o9xmozoMi3VRmZa7rkwZ7TQ9VYGZWUoVoojEzs/ZzgjczKykneDOzknKCNzMrKSd4M7OScoI3MyspJ3gzs5L6/1hO5XzD9wzUAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "23b0f7a0-17b4-4ff5-93c5-ea60e85ace08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3315a0-1ad8-4100-b849-b6dd96310cef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "id": "XyxDPsEi6yYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f101565-8ce1-4f7b-cc1f-e162c087d4d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0a3641-bba9-4a13-9d23-9f70df5c46cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Total 837 (delta 0), reused 0 (delta 0), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.85 MiB | 17.93 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d13eee-111d-429e-92e1-dd14b903d04a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb74dbb-1c56-4f83-e4b7-1cd274c7f8c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea62dc2e-e702-4d50-8040-4ad5e2a85867"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603544b3-6a30-4a66-a98b-2086b912b81e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bd3f18-dcf6-4860-b8e2-cf87f230eb39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=2e-1),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c28667-6311-45ff-c660-6126f694e05b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-121c6c007c10>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 159s 4s/step - loss: 1481321.3750 - mae: 990.4717 - val_loss: 513487.3750 - val_mae: 544.1710\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1474076.6250 - mae: 985.0529 - val_loss: 490458.5312 - val_mae: 525.9037\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1457853.0000 - mae: 979.0602 - val_loss: 491442.7188 - val_mae: 524.2626\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1423413.5000 - mae: 962.9013 - val_loss: 476539.3438 - val_mae: 509.3323\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1396449.8750 - mae: 951.8234 - val_loss: 436045.7500 - val_mae: 467.9231\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1336510.0000 - mae: 923.6962 - val_loss: 456161.2500 - val_mae: 491.4367\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1316484.3750 - mae: 911.6162 - val_loss: 438866.7188 - val_mae: 477.2089\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1305982.1250 - mae: 906.4769 - val_loss: 423009.7188 - val_mae: 469.2031\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1284436.7500 - mae: 893.5519 - val_loss: 410808.6562 - val_mae: 469.4810\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1260839.6250 - mae: 882.9258 - val_loss: 405228.9688 - val_mae: 476.7440\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1234799.8750 - mae: 871.7295 - val_loss: 391149.5938 - val_mae: 470.6662\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1208802.0000 - mae: 861.4255 - val_loss: 376713.0000 - val_mae: 468.9158\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1209818.6250 - mae: 860.0438 - val_loss: 362615.0312 - val_mae: 466.0410\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1173324.5000 - mae: 840.1359 - val_loss: 345085.5312 - val_mae: 454.9355\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1143379.3750 - mae: 827.8753 - val_loss: 332213.3438 - val_mae: 452.5262\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1134426.7500 - mae: 823.5746 - val_loss: 331656.6875 - val_mae: 461.0000\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 4s 111ms/step - loss: 1121053.2500 - mae: 815.8374 - val_loss: 341024.5312 - val_mae: 480.8072\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1067697.8750 - mae: 791.4216 - val_loss: 320008.3125 - val_mae: 466.7186\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1075694.1250 - mae: 793.8209 - val_loss: 297194.4688 - val_mae: 451.9316\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1066550.7500 - mae: 789.5389 - val_loss: 300131.3438 - val_mae: 461.3646\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1032778.2500 - mae: 770.1234 - val_loss: 297132.0938 - val_mae: 466.2087\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1007561.6875 - mae: 758.7399 - val_loss: 282867.6250 - val_mae: 457.0627\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 980235.1250 - mae: 743.5114 - val_loss: 280109.0938 - val_mae: 461.0833\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 977750.0000 - mae: 741.9453 - val_loss: 283815.1562 - val_mae: 471.2826\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 946645.4375 - mae: 729.1840 - val_loss: 263228.1562 - val_mae: 457.3021\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 942354.5000 - mae: 723.8632 - val_loss: 266537.0938 - val_mae: 464.9496\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 930726.5000 - mae: 720.2177 - val_loss: 261256.0469 - val_mae: 463.3959\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 922337.2500 - mae: 716.4888 - val_loss: 253868.9844 - val_mae: 462.4236\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 907338.1875 - mae: 709.4123 - val_loss: 252287.9531 - val_mae: 463.3958\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 894174.1250 - mae: 703.4993 - val_loss: 247921.7031 - val_mae: 464.2494\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 871912.3125 - mae: 691.9656 - val_loss: 244465.2969 - val_mae: 463.3958\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 863624.6250 - mae: 689.4456 - val_loss: 240092.9219 - val_mae: 463.2126\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 833204.9375 - mae: 674.4337 - val_loss: 236182.0156 - val_mae: 462.3303\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 834372.2500 - mae: 674.6561 - val_loss: 226633.7344 - val_mae: 455.9247\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 814198.0000 - mae: 668.8657 - val_loss: 226691.6250 - val_mae: 458.1834\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 790830.0625 - mae: 656.3381 - val_loss: 227666.4531 - val_mae: 461.3645\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 794782.3750 - mae: 663.3757 - val_loss: 225018.7500 - val_mae: 460.6699\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 784753.0625 - mae: 658.7174 - val_loss: 224405.5156 - val_mae: 462.1162\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 769167.1250 - mae: 652.5489 - val_loss: 222692.7969 - val_mae: 461.8977\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 752968.8750 - mae: 647.0789 - val_loss: 218630.7969 - val_mae: 459.0521\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 762385.8750 - mae: 656.7883 - val_loss: 217865.1094 - val_mae: 459.4294\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 743576.1875 - mae: 650.2098 - val_loss: 220480.5625 - val_mae: 462.7588\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 738254.1250 - mae: 651.0517 - val_loss: 219480.1094 - val_mae: 462.3811\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 716434.9375 - mae: 642.0323 - val_loss: 217463.8594 - val_mae: 460.8197\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 704893.6875 - mae: 637.5104 - val_loss: 218350.8750 - val_mae: 461.8849\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 681096.8750 - mae: 630.9681 - val_loss: 218483.1094 - val_mae: 462.1691\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 696378.2500 - mae: 639.9222 - val_loss: 214166.2031 - val_mae: 457.3021\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 688297.0000 - mae: 637.7682 - val_loss: 218574.0469 - val_mae: 461.7520\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 11s 265ms/step - loss: 677778.8125 - mae: 635.0790 - val_loss: 218497.6719 - val_mae: 461.1731\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 668336.5625 - mae: 632.9730 - val_loss: 217702.0469 - val_mae: 459.3246\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 660818.4375 - mae: 631.8580 - val_loss: 216611.9219 - val_mae: 456.9810\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 651387.8750 - mae: 631.2267 - val_loss: 221187.7969 - val_mae: 461.2534\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 652328.3125 - mae: 631.9970 - val_loss: 222440.4531 - val_mae: 461.3519\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 646441.7500 - mae: 632.1467 - val_loss: 219688.5000 - val_mae: 456.3396\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 626989.0625 - mae: 620.8093 - val_loss: 220569.1875 - val_mae: 455.6820\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 627030.1875 - mae: 625.1140 - val_loss: 227695.3281 - val_mae: 462.2140\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 627093.7500 - mae: 628.1414 - val_loss: 225730.0625 - val_mae: 458.0273\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 598585.5625 - mae: 613.1102 - val_loss: 228974.4844 - val_mae: 459.3333\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 618008.3125 - mae: 625.7180 - val_loss: 233731.7500 - val_mae: 462.7421\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 594900.8125 - mae: 616.5011 - val_loss: 232982.3125 - val_mae: 459.3333\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 594163.2500 - mae: 616.9453 - val_loss: 229759.7656 - val_mae: 453.3143\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 585822.8750 - mae: 616.4209 - val_loss: 237626.5156 - val_mae: 459.3333\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 583946.4375 - mae: 614.3518 - val_loss: 237561.1875 - val_mae: 457.0020\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 584362.1875 - mae: 619.9611 - val_loss: 243659.1250 - val_mae: 461.3646\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 580550.6875 - mae: 618.1716 - val_loss: 245151.6719 - val_mae: 459.3333\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 578879.0000 - mae: 619.1518 - val_loss: 251902.3906 - val_mae: 464.1856\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 574638.3125 - mae: 618.8343 - val_loss: 246351.3594 - val_mae: 454.3265\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 559927.6875 - mae: 613.0449 - val_loss: 253826.6719 - val_mae: 459.6146\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 567648.8750 - mae: 618.6658 - val_loss: 252999.8906 - val_mae: 456.0437\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 559168.0000 - mae: 615.9184 - val_loss: 251204.5625 - val_mae: 450.4239\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 561060.6875 - mae: 620.1580 - val_loss: 255625.6406 - val_mae: 452.7302\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 558411.4375 - mae: 619.9478 - val_loss: 261091.6094 - val_mae: 455.3239\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 552673.9375 - mae: 618.2027 - val_loss: 272012.6562 - val_mae: 462.9220\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 545291.8125 - mae: 614.2770 - val_loss: 267737.1875 - val_mae: 455.6067\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 551109.6875 - mae: 621.6216 - val_loss: 281520.9062 - val_mae: 469.3042\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 541489.6250 - mae: 616.4149 - val_loss: 269413.8438 - val_mae: 450.6778\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 532233.1875 - mae: 614.0932 - val_loss: 275795.4062 - val_mae: 452.8827\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 531927.6875 - mae: 614.5123 - val_loss: 284709.9688 - val_mae: 461.0833\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 540010.3750 - mae: 621.8699 - val_loss: 293004.7812 - val_mae: 465.7646\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 536483.5000 - mae: 621.6884 - val_loss: 296149.1562 - val_mae: 464.1447\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 525075.4375 - mae: 616.4488 - val_loss: 289200.9688 - val_mae: 454.4051\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 533628.9375 - mae: 625.3373 - val_loss: 286604.0312 - val_mae: 447.1901\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 522300.4688 - mae: 617.4370 - val_loss: 295910.7188 - val_mae: 454.4483\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 524684.5625 - mae: 622.5403 - val_loss: 303932.5312 - val_mae: 459.3333\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 519868.8125 - mae: 621.5196 - val_loss: 307603.4062 - val_mae: 461.3646\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 522943.4375 - mae: 623.4150 - val_loss: 298777.6562 - val_mae: 446.5897\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 518939.5938 - mae: 622.2939 - val_loss: 320534.4062 - val_mae: 468.9999\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 523442.6875 - mae: 627.6559 - val_loss: 317297.7500 - val_mae: 461.3646\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 521113.1250 - mae: 627.7386 - val_loss: 320275.6250 - val_mae: 461.0833\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 516423.9688 - mae: 625.5193 - val_loss: 330125.4062 - val_mae: 467.0240\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 512976.3438 - mae: 625.1291 - val_loss: 320676.5312 - val_mae: 453.2976\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 512035.9688 - mae: 627.0557 - val_loss: 317187.7188 - val_mae: 445.3258\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 515763.5625 - mae: 628.7070 - val_loss: 332187.1562 - val_mae: 457.3021\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 515692.3125 - mae: 631.0110 - val_loss: 329505.3438 - val_mae: 455.0643\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 509383.6875 - mae: 627.4608 - val_loss: 326398.1875 - val_mae: 447.1063\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 511734.8750 - mae: 630.4680 - val_loss: 342052.2812 - val_mae: 461.3646\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 513276.1562 - mae: 632.7666 - val_loss: 350928.0938 - val_mae: 469.6587\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 510793.9062 - mae: 631.2782 - val_loss: 340646.9688 - val_mae: 454.7244\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 507266.5312 - mae: 629.7037 - val_loss: 350003.2500 - val_mae: 459.9053\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 505601.0312 - mae: 630.6501 - val_loss: 359444.3750 - val_mae: 468.3802\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 500380.8125 - mae: 627.3548 - val_loss: 348329.6562 - val_mae: 456.7049\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 512711.5312 - mae: 637.1284 - val_loss: 350133.2812 - val_mae: 457.9463\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 506560.3750 - mae: 633.3181 - val_loss: 373857.2500 - val_mae: 480.2703\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 508321.2188 - mae: 635.4565 - val_loss: 362614.9062 - val_mae: 469.8226\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 500555.4375 - mae: 629.7170 - val_loss: 365143.3438 - val_mae: 471.3745\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 494482.3750 - mae: 630.0859 - val_loss: 343789.9688 - val_mae: 447.6818\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 502967.9688 - mae: 632.6593 - val_loss: 368679.9062 - val_mae: 473.5830\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 504971.3750 - mae: 633.8706 - val_loss: 364201.7812 - val_mae: 468.1505\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 503687.3750 - mae: 634.3047 - val_loss: 371659.5938 - val_mae: 474.1888\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 504542.1250 - mae: 635.6443 - val_loss: 375555.0000 - val_mae: 476.5962\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 502679.8125 - mae: 634.3815 - val_loss: 376732.0312 - val_mae: 477.3735\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 505128.5312 - mae: 636.4858 - val_loss: 371544.2812 - val_mae: 471.4961\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 495868.4688 - mae: 632.1543 - val_loss: 373993.0938 - val_mae: 474.0560\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 503123.3125 - mae: 636.2919 - val_loss: 373908.1562 - val_mae: 473.3979\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 502063.7812 - mae: 636.4541 - val_loss: 375368.2500 - val_mae: 474.9192\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 503801.7188 - mae: 637.8354 - val_loss: 369661.3750 - val_mae: 466.6963\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 504454.2812 - mae: 638.4003 - val_loss: 378276.8438 - val_mae: 476.6701\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 501958.8125 - mae: 637.0665 - val_loss: 387793.5000 - val_mae: 485.6648\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 498513.2188 - mae: 634.4092 - val_loss: 389650.4062 - val_mae: 486.4524\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 505806.1875 - mae: 640.7081 - val_loss: 383609.7500 - val_mae: 479.0214\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 500359.7500 - mae: 635.9695 - val_loss: 400262.4062 - val_mae: 495.8038\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 495042.6250 - mae: 634.9033 - val_loss: 385606.5000 - val_mae: 481.0131\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 499683.7500 - mae: 637.4615 - val_loss: 378811.5312 - val_mae: 472.4413\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 493593.9062 - mae: 635.4440 - val_loss: 380291.0312 - val_mae: 473.3549\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 503142.0625 - mae: 640.0735 - val_loss: 388595.2500 - val_mae: 482.7566\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 503221.2188 - mae: 639.8235 - val_loss: 389732.3750 - val_mae: 483.4159\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 498994.9062 - mae: 637.0012 - val_loss: 399157.6562 - val_mae: 491.4329\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 499200.6562 - mae: 637.5007 - val_loss: 415991.9062 - val_mae: 508.3028\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 505030.5000 - mae: 641.8752 - val_loss: 400653.5312 - val_mae: 492.9664\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 501163.4375 - mae: 639.4199 - val_loss: 410031.4062 - val_mae: 502.2346\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 499797.9375 - mae: 639.5381 - val_loss: 410372.3438 - val_mae: 501.2732\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 497815.9688 - mae: 639.1317 - val_loss: 395950.1562 - val_mae: 486.4414\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498704.9375 - mae: 639.2842 - val_loss: 403975.0938 - val_mae: 494.3658\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501553.3125 - mae: 639.4846 - val_loss: 387813.8438 - val_mae: 477.9383\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 495157.3438 - mae: 637.4843 - val_loss: 404770.5000 - val_mae: 494.8428\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 499377.0000 - mae: 639.5947 - val_loss: 398166.5938 - val_mae: 487.7467\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498846.9688 - mae: 638.4224 - val_loss: 406869.6562 - val_mae: 496.0967\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 500754.5938 - mae: 642.2097 - val_loss: 390142.5312 - val_mae: 479.8220\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 499592.5625 - mae: 639.5328 - val_loss: 382970.9688 - val_mae: 471.9027\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 503753.2500 - mae: 642.6473 - val_loss: 409100.8438 - val_mae: 497.8473\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501071.3438 - mae: 640.6962 - val_loss: 408724.7500 - val_mae: 497.6322\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 496299.3125 - mae: 639.7841 - val_loss: 408450.5312 - val_mae: 497.4832\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 501613.9062 - mae: 641.2326 - val_loss: 417199.5000 - val_mae: 506.2873\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 498594.0312 - mae: 639.5382 - val_loss: 400650.5312 - val_mae: 489.6383\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 501297.2188 - mae: 641.0029 - val_loss: 409164.1250 - val_mae: 497.4595\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 499277.9062 - mae: 639.5286 - val_loss: 409962.0000 - val_mae: 497.9315\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 505351.5312 - mae: 644.4598 - val_loss: 410197.1562 - val_mae: 498.0704\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 506663.6875 - mae: 645.3506 - val_loss: 395138.9062 - val_mae: 482.6583\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 503918.0625 - mae: 642.9743 - val_loss: 410690.2812 - val_mae: 498.3615\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 501688.9062 - mae: 641.4362 - val_loss: 412695.0000 - val_mae: 499.8849\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502024.3125 - mae: 641.4542 - val_loss: 404420.5312 - val_mae: 491.3994\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 502223.4062 - mae: 641.9644 - val_loss: 403979.4062 - val_mae: 491.1448\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 503536.8750 - mae: 643.1287 - val_loss: 422782.4688 - val_mae: 509.0746\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 500775.2500 - mae: 641.5745 - val_loss: 397219.2188 - val_mae: 483.5299\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 502658.1562 - mae: 642.8855 - val_loss: 406445.4062 - val_mae: 492.8651\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 500211.9062 - mae: 641.1271 - val_loss: 406289.7500 - val_mae: 492.1518\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 502764.4375 - mae: 641.8618 - val_loss: 422861.3438 - val_mae: 508.8016\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 502601.8438 - mae: 642.6702 - val_loss: 422853.3750 - val_mae: 509.7604\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 497467.2188 - mae: 638.5433 - val_loss: 414865.7188 - val_mae: 501.4299\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 502246.5938 - mae: 642.8392 - val_loss: 399027.5000 - val_mae: 484.8654\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 496301.9688 - mae: 637.3773 - val_loss: 406296.9062 - val_mae: 492.4902\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 504773.1562 - mae: 643.8936 - val_loss: 415084.9688 - val_mae: 501.2453\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 501786.0000 - mae: 641.2656 - val_loss: 414160.9688 - val_mae: 500.7252\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 500953.7812 - mae: 641.8198 - val_loss: 408610.0000 - val_mae: 494.0588\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 496485.0625 - mae: 639.4822 - val_loss: 408142.4062 - val_mae: 493.5254\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 497150.0312 - mae: 641.1162 - val_loss: 408638.5000 - val_mae: 493.8087\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 497409.1562 - mae: 639.0332 - val_loss: 399965.5938 - val_mae: 485.3925\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 498998.0000 - mae: 640.5543 - val_loss: 424938.3438 - val_mae: 510.3177\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 495744.0625 - mae: 639.0699 - val_loss: 398595.2188 - val_mae: 484.6334\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 502839.4688 - mae: 643.6904 - val_loss: 407602.1562 - val_mae: 493.2270\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 498706.6875 - mae: 639.9623 - val_loss: 417086.4062 - val_mae: 502.1037\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 499907.6875 - mae: 640.3788 - val_loss: 401562.7812 - val_mae: 486.0479\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 499056.4375 - mae: 641.2748 - val_loss: 417220.4688 - val_mae: 502.4382\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 496089.9062 - mae: 638.6378 - val_loss: 408271.4062 - val_mae: 493.5991\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 501945.6875 - mae: 641.8950 - val_loss: 407913.0000 - val_mae: 493.6745\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 498701.3125 - mae: 639.7571 - val_loss: 426881.4062 - val_mae: 511.1767\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 501375.3438 - mae: 642.0819 - val_loss: 409299.1562 - val_mae: 494.4359\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 497359.5000 - mae: 638.9482 - val_loss: 418548.8438 - val_mae: 503.4118\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 505956.1875 - mae: 646.0136 - val_loss: 409235.1562 - val_mae: 494.1491\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 498371.0938 - mae: 640.1971 - val_loss: 417104.5000 - val_mae: 502.1142\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501558.8750 - mae: 641.8398 - val_loss: 401445.8438 - val_mae: 486.2261\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 501318.5312 - mae: 642.0040 - val_loss: 401139.5312 - val_mae: 485.8021\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 499086.9688 - mae: 640.7750 - val_loss: 409075.9062 - val_mae: 494.0583\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 501284.5000 - mae: 641.4828 - val_loss: 433928.0938 - val_mae: 519.1724\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 501098.8438 - mae: 641.2534 - val_loss: 424346.7500 - val_mae: 509.4267\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 497944.2500 - mae: 639.2327 - val_loss: 399320.9062 - val_mae: 484.7594\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501137.9375 - mae: 642.3984 - val_loss: 407748.6562 - val_mae: 493.2896\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 497823.5000 - mae: 641.0505 - val_loss: 399421.6250 - val_mae: 485.0924\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 505251.4688 - mae: 644.8154 - val_loss: 407785.6562 - val_mae: 493.3108\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 501249.9688 - mae: 641.2813 - val_loss: 406682.4062 - val_mae: 492.7002\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 501813.5938 - mae: 643.2332 - val_loss: 433132.7188 - val_mae: 518.4492\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 497472.2188 - mae: 639.7736 - val_loss: 409122.7812 - val_mae: 494.3400\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502941.6875 - mae: 642.9617 - val_loss: 417529.0312 - val_mae: 502.3455\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 501809.2500 - mae: 641.7925 - val_loss: 434719.6250 - val_mae: 519.3681\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 502633.5000 - mae: 642.8766 - val_loss: 416379.8750 - val_mae: 501.7087\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 493019.8125 - mae: 637.8820 - val_loss: 416528.7188 - val_mae: 502.3887\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 505011.4062 - mae: 645.0337 - val_loss: 409739.2812 - val_mae: 494.4363\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502571.7188 - mae: 643.0659 - val_loss: 408916.2812 - val_mae: 493.9774\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 501990.7500 - mae: 642.4326 - val_loss: 417197.8438 - val_mae: 502.4301\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 501519.0000 - mae: 642.1661 - val_loss: 409967.5312 - val_mae: 494.5564\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 490581.3750 - mae: 636.6062 - val_loss: 401314.7188 - val_mae: 485.9194\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 500271.7188 - mae: 641.5724 - val_loss: 426314.5938 - val_mae: 511.0918\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 499256.4688 - mae: 640.6020 - val_loss: 426209.3438 - val_mae: 511.0320\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 498556.6562 - mae: 640.1030 - val_loss: 399528.2812 - val_mae: 485.1523\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 495459.0312 - mae: 638.1273 - val_loss: 408305.2500 - val_mae: 493.3698\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 485420.9062 - mae: 636.0477 - val_loss: 407727.8750 - val_mae: 493.5728\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 501824.2812 - mae: 641.5229 - val_loss: 407009.4688 - val_mae: 492.8877\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 502111.2812 - mae: 641.8874 - val_loss: 417618.7812 - val_mae: 502.6565\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 498544.0625 - mae: 641.2632 - val_loss: 399667.5000 - val_mae: 485.2304\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 502786.8125 - mae: 643.4217 - val_loss: 418085.8438 - val_mae: 502.9263\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 500196.3438 - mae: 641.2133 - val_loss: 400846.2812 - val_mae: 485.6317\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 498520.6250 - mae: 640.2466 - val_loss: 411016.1562 - val_mae: 495.1712\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 501546.1562 - mae: 642.9681 - val_loss: 426259.3438 - val_mae: 510.8302\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 503423.1875 - mae: 643.6340 - val_loss: 420081.3750 - val_mae: 503.8214\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 503117.8125 - mae: 643.7585 - val_loss: 427769.7812 - val_mae: 512.4752\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 497506.0938 - mae: 638.9390 - val_loss: 418391.2500 - val_mae: 503.0924\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 495756.9062 - mae: 640.1006 - val_loss: 409951.9688 - val_mae: 494.5670\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 496679.1875 - mae: 639.4725 - val_loss: 418803.6562 - val_mae: 503.3184\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 502519.6875 - mae: 643.3882 - val_loss: 426986.6562 - val_mae: 511.4737\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 495872.7188 - mae: 639.7705 - val_loss: 409364.0938 - val_mae: 494.2226\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 502360.4062 - mae: 642.7578 - val_loss: 408490.7812 - val_mae: 493.5006\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 501231.4062 - mae: 642.1195 - val_loss: 409811.5938 - val_mae: 494.8237\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 499632.3750 - mae: 641.2224 - val_loss: 409075.2500 - val_mae: 493.8043\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 495789.9688 - mae: 639.3735 - val_loss: 400029.6562 - val_mae: 485.1886\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 502344.7500 - mae: 642.8944 - val_loss: 416253.0000 - val_mae: 501.8961\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 492857.9375 - mae: 638.4463 - val_loss: 408900.1250 - val_mae: 493.9580\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 498853.3438 - mae: 639.8736 - val_loss: 408819.7812 - val_mae: 493.9225\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 502047.3438 - mae: 642.9466 - val_loss: 409700.8438 - val_mae: 494.6571\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 501800.6250 - mae: 643.3663 - val_loss: 425847.1562 - val_mae: 510.5876\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501017.5625 - mae: 642.2360 - val_loss: 419197.7812 - val_mae: 503.5381\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 499152.8125 - mae: 640.5751 - val_loss: 401521.3750 - val_mae: 486.2683\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 500463.5625 - mae: 641.9186 - val_loss: 401944.3438 - val_mae: 486.2691\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 489246.4688 - mae: 638.1483 - val_loss: 408225.9062 - val_mae: 493.5835\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500648.8125 - mae: 641.1274 - val_loss: 408610.4062 - val_mae: 493.7825\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 502656.4375 - mae: 642.7838 - val_loss: 409106.0938 - val_mae: 493.8226\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 500505.9375 - mae: 641.6291 - val_loss: 408675.7500 - val_mae: 493.5890\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 504291.1250 - mae: 644.3409 - val_loss: 416856.8438 - val_mae: 502.4954\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 498478.4688 - mae: 639.6253 - val_loss: 417790.9062 - val_mae: 502.7528\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 496556.4688 - mae: 640.1613 - val_loss: 408789.7500 - val_mae: 493.8951\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 492110.7500 - mae: 637.0372 - val_loss: 417002.4688 - val_mae: 502.3162\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498973.1562 - mae: 639.7750 - val_loss: 424763.7500 - val_mae: 510.4905\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 500109.7500 - mae: 641.3194 - val_loss: 401299.8438 - val_mae: 486.1354\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 502040.0938 - mae: 642.8716 - val_loss: 417728.0938 - val_mae: 502.4609\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 500398.0625 - mae: 641.1221 - val_loss: 401169.0000 - val_mae: 485.8348\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 491826.6250 - mae: 636.5078 - val_loss: 425177.3750 - val_mae: 510.1726\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 503545.4062 - mae: 643.7827 - val_loss: 409589.5938 - val_mae: 494.5961\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 495862.4375 - mae: 637.3252 - val_loss: 407600.2188 - val_mae: 493.5020\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 496817.1562 - mae: 640.2006 - val_loss: 426474.7188 - val_mae: 511.1828\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 497070.1250 - mae: 638.6623 - val_loss: 409633.5938 - val_mae: 494.3761\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 498587.0625 - mae: 640.2359 - val_loss: 418302.3438 - val_mae: 502.8083\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 503472.4375 - mae: 644.2985 - val_loss: 399999.5000 - val_mae: 485.4218\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 505067.2812 - mae: 644.9453 - val_loss: 400087.2188 - val_mae: 485.4658\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500863.2188 - mae: 642.8731 - val_loss: 408068.1250 - val_mae: 493.7602\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 498974.4062 - mae: 641.5386 - val_loss: 415255.2500 - val_mae: 501.3358\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 502062.9688 - mae: 642.4921 - val_loss: 399315.8750 - val_mae: 484.7401\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 502606.0312 - mae: 642.3944 - val_loss: 416754.5938 - val_mae: 502.1774\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 499178.0938 - mae: 640.4537 - val_loss: 399782.7500 - val_mae: 485.3003\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 501877.0000 - mae: 642.6041 - val_loss: 418143.5938 - val_mae: 503.0527\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 499256.0938 - mae: 640.8958 - val_loss: 417149.0938 - val_mae: 502.1553\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 499465.0625 - mae: 641.9528 - val_loss: 426249.5312 - val_mae: 511.0548\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 501306.2812 - mae: 641.9229 - val_loss: 426199.3438 - val_mae: 511.0262\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501092.2188 - mae: 641.6857 - val_loss: 401150.3750 - val_mae: 485.8083\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 497970.8125 - mae: 639.6077 - val_loss: 417762.2500 - val_mae: 502.7456\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 499727.2188 - mae: 641.2556 - val_loss: 410427.2500 - val_mae: 495.0524\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 502864.3750 - mae: 643.5325 - val_loss: 418971.8750 - val_mae: 503.4162\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 499873.0000 - mae: 641.3462 - val_loss: 403773.7188 - val_mae: 487.3269\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 495783.7500 - mae: 640.0276 - val_loss: 402005.4062 - val_mae: 486.5426\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 504705.8125 - mae: 645.0251 - val_loss: 429338.3438 - val_mae: 512.9951\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 502749.8125 - mae: 643.5273 - val_loss: 410926.4688 - val_mae: 495.3250\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 499761.5000 - mae: 641.0132 - val_loss: 418775.0312 - val_mae: 503.3065\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 500258.3750 - mae: 641.1946 - val_loss: 402637.7500 - val_mae: 486.8908\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 497924.7812 - mae: 639.7603 - val_loss: 421508.7500 - val_mae: 504.8221\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 496355.6875 - mae: 641.8839 - val_loss: 402740.1562 - val_mae: 486.7448\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 502176.8125 - mae: 642.3767 - val_loss: 412002.9062 - val_mae: 495.7211\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503499.7812 - mae: 643.7042 - val_loss: 411098.4688 - val_mae: 495.2086\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 504159.8438 - mae: 645.2775 - val_loss: 420217.8438 - val_mae: 503.9001\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 496873.5625 - mae: 640.0939 - val_loss: 411438.1250 - val_mae: 495.4103\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 501159.2500 - mae: 642.5415 - val_loss: 420573.6562 - val_mae: 504.3069\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 497990.9375 - mae: 641.0895 - val_loss: 411397.7188 - val_mae: 495.3875\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 505388.4375 - mae: 645.2911 - val_loss: 403385.1562 - val_mae: 487.3064\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 499646.8438 - mae: 641.0135 - val_loss: 410911.4062 - val_mae: 495.1212\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 500990.5000 - mae: 641.6025 - val_loss: 394639.7812 - val_mae: 478.6302\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503913.5625 - mae: 644.4312 - val_loss: 401929.2500 - val_mae: 486.5045\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 496618.2188 - mae: 641.0112 - val_loss: 427768.5000 - val_mae: 511.6977\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 499141.4375 - mae: 640.7352 - val_loss: 419152.7812 - val_mae: 503.3144\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 499807.4375 - mae: 641.9085 - val_loss: 410397.2812 - val_mae: 494.5846\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 502862.0312 - mae: 644.0010 - val_loss: 402718.7500 - val_mae: 486.9359\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 498261.7188 - mae: 639.8246 - val_loss: 418690.9062 - val_mae: 503.0331\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500556.7500 - mae: 641.2477 - val_loss: 410513.3438 - val_mae: 494.6327\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 498005.4375 - mae: 641.0683 - val_loss: 410450.3750 - val_mae: 495.0676\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 494182.0625 - mae: 639.5837 - val_loss: 408959.1562 - val_mae: 493.7565\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 500734.2812 - mae: 641.2032 - val_loss: 410331.1562 - val_mae: 494.7826\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 499381.0625 - mae: 640.4753 - val_loss: 435119.2188 - val_mae: 519.8376\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 500692.6875 - mae: 642.3230 - val_loss: 426385.2500 - val_mae: 511.6331\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 501331.0938 - mae: 642.4011 - val_loss: 418942.3438 - val_mae: 503.1639\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 495157.5625 - mae: 641.4899 - val_loss: 409426.2500 - val_mae: 494.2581\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 498195.6250 - mae: 639.8023 - val_loss: 416786.6562 - val_mae: 501.6664\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 495832.1250 - mae: 637.8897 - val_loss: 417207.8438 - val_mae: 502.1741\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 506767.6562 - mae: 646.8712 - val_loss: 401005.0000 - val_mae: 485.9750\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 501842.4688 - mae: 643.0381 - val_loss: 417787.7500 - val_mae: 502.5104\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 501890.6875 - mae: 642.6761 - val_loss: 407840.1562 - val_mae: 493.3736\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 503868.9688 - mae: 643.5355 - val_loss: 426903.7188 - val_mae: 510.9846\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 503042.8438 - mae: 643.1851 - val_loss: 409363.2188 - val_mae: 494.2321\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 499187.6875 - mae: 640.3315 - val_loss: 402300.7812 - val_mae: 486.4756\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 503637.7188 - mae: 644.7630 - val_loss: 419837.0938 - val_mae: 503.6806\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503315.6250 - mae: 643.9750 - val_loss: 410867.7500 - val_mae: 495.0777\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 504471.4375 - mae: 644.3672 - val_loss: 402000.2812 - val_mae: 486.5398\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 495661.9688 - mae: 640.0810 - val_loss: 393813.1250 - val_mae: 478.1410\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 498139.1562 - mae: 639.0850 - val_loss: 411033.1562 - val_mae: 494.9588\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 498087.9062 - mae: 639.6511 - val_loss: 418108.2500 - val_mae: 502.7108\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 503561.7188 - mae: 643.9055 - val_loss: 409633.6250 - val_mae: 494.1753\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 505437.6250 - mae: 645.3032 - val_loss: 420508.8750 - val_mae: 504.2675\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 500708.4062 - mae: 642.1867 - val_loss: 411259.6562 - val_mae: 495.1119\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 502994.0938 - mae: 643.6476 - val_loss: 437346.5312 - val_mae: 521.0767\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 502554.5000 - mae: 642.9317 - val_loss: 420520.3750 - val_mae: 504.0745\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 502938.4375 - mae: 643.0977 - val_loss: 437488.8750 - val_mae: 521.1556\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 494776.2500 - mae: 638.8198 - val_loss: 410627.7500 - val_mae: 494.7404\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 501779.6875 - mae: 642.5844 - val_loss: 402009.3438 - val_mae: 486.5363\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 500758.3125 - mae: 641.5307 - val_loss: 411542.7500 - val_mae: 495.6661\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 498506.1250 - mae: 640.9233 - val_loss: 426294.5000 - val_mae: 510.8510\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 503720.6875 - mae: 643.5143 - val_loss: 419287.3750 - val_mae: 503.3777\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 502469.5625 - mae: 643.3542 - val_loss: 402745.2812 - val_mae: 486.7478\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 490109.1875 - mae: 636.7748 - val_loss: 419841.5000 - val_mae: 503.5209\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 504161.9375 - mae: 644.7147 - val_loss: 429072.3438 - val_mae: 512.8483\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 505721.7812 - mae: 645.7759 - val_loss: 411939.2500 - val_mae: 496.2156\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 503050.3438 - mae: 644.0171 - val_loss: 412433.8438 - val_mae: 495.9648\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 501838.4688 - mae: 643.1580 - val_loss: 437183.5938 - val_mae: 520.9863\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 502486.0938 - mae: 643.3292 - val_loss: 420294.8438 - val_mae: 504.1555\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 502866.1562 - mae: 643.4992 - val_loss: 420427.9062 - val_mae: 504.2225\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 502052.0000 - mae: 643.1537 - val_loss: 428844.0938 - val_mae: 512.5345\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 498518.5938 - mae: 641.5334 - val_loss: 418842.1562 - val_mae: 503.1351\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 486662.7188 - mae: 636.5518 - val_loss: 408990.5000 - val_mae: 494.0096\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 501341.3750 - mae: 641.5098 - val_loss: 425958.8750 - val_mae: 510.8989\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 502399.3438 - mae: 642.3034 - val_loss: 391678.0000 - val_mae: 476.9440\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503915.0625 - mae: 643.5718 - val_loss: 400329.0938 - val_mae: 485.3308\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 501679.6562 - mae: 642.4167 - val_loss: 424675.4062 - val_mae: 510.1577\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 502136.6250 - mae: 641.8452 - val_loss: 417283.0938 - val_mae: 502.2330\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 496557.4062 - mae: 640.8566 - val_loss: 408915.2188 - val_mae: 493.9667\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 500171.6562 - mae: 641.0008 - val_loss: 401807.5938 - val_mae: 486.4281\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 503655.6875 - mae: 643.1640 - val_loss: 425768.4062 - val_mae: 510.7810\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 502854.8125 - mae: 643.1836 - val_loss: 418372.0000 - val_mae: 502.8340\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 492158.3438 - mae: 636.7569 - val_loss: 434388.4062 - val_mae: 519.1766\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499832.8438 - mae: 640.8231 - val_loss: 408201.3750 - val_mae: 493.8331\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 497896.9062 - mae: 641.1875 - val_loss: 399332.3438 - val_mae: 485.0422\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 503128.3438 - mae: 643.1954 - val_loss: 417546.3750 - val_mae: 502.6205\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 494093.9062 - mae: 639.4792 - val_loss: 407769.8750 - val_mae: 493.5954\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 498299.8125 - mae: 639.8984 - val_loss: 408111.6562 - val_mae: 493.5182\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 500861.5312 - mae: 640.9547 - val_loss: 417031.8750 - val_mae: 502.3327\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 497626.7812 - mae: 639.0419 - val_loss: 417108.4688 - val_mae: 502.7132\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 501614.4688 - mae: 641.6570 - val_loss: 417455.1250 - val_mae: 502.3176\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 500192.8125 - mae: 641.5659 - val_loss: 416452.7500 - val_mae: 501.7357\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 498189.6250 - mae: 640.5118 - val_loss: 407808.4688 - val_mae: 493.6169\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 500853.2188 - mae: 641.6527 - val_loss: 417179.1562 - val_mae: 502.1727\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 499852.4375 - mae: 641.0667 - val_loss: 426266.7812 - val_mae: 511.0646\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 496474.2188 - mae: 638.0378 - val_loss: 400479.8438 - val_mae: 485.6858\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 503751.6875 - mae: 643.5422 - val_loss: 409165.6562 - val_mae: 494.3635\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 496460.9688 - mae: 640.7618 - val_loss: 416330.9688 - val_mae: 501.9448\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 504620.4375 - mae: 644.3451 - val_loss: 408369.7500 - val_mae: 493.9257\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 501011.7500 - mae: 642.1962 - val_loss: 416593.7500 - val_mae: 502.0872\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 502508.7500 - mae: 643.1116 - val_loss: 389588.2500 - val_mae: 475.7574\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 502748.6562 - mae: 642.7448 - val_loss: 417288.0312 - val_mae: 502.2057\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 497535.7812 - mae: 638.9225 - val_loss: 407162.2500 - val_mae: 493.3128\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 498819.3438 - mae: 640.4234 - val_loss: 399042.1562 - val_mae: 484.8681\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 503764.5625 - mae: 643.7281 - val_loss: 415030.6562 - val_mae: 500.9234\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 502699.0312 - mae: 642.8568 - val_loss: 415173.0000 - val_mae: 500.9905\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 503335.1562 - mae: 643.3306 - val_loss: 415566.2188 - val_mae: 501.7986\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 501096.5625 - mae: 641.9873 - val_loss: 408030.2812 - val_mae: 493.4613\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 498508.1562 - mae: 640.3712 - val_loss: 415136.5000 - val_mae: 501.2743\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 498334.1875 - mae: 640.5663 - val_loss: 425345.5938 - val_mae: 510.5401\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 501904.2812 - mae: 642.7877 - val_loss: 408375.3438 - val_mae: 493.3903\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 500838.2188 - mae: 641.7422 - val_loss: 406655.4688 - val_mae: 492.9814\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 499562.2188 - mae: 640.0552 - val_loss: 414879.4062 - val_mae: 500.4905\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 497934.2500 - mae: 640.8383 - val_loss: 415231.3438 - val_mae: 501.0246\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 504840.4688 - mae: 644.3956 - val_loss: 416840.7812 - val_mae: 501.9458\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 499223.2500 - mae: 640.4935 - val_loss: 407501.7812 - val_mae: 493.1803\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 496138.8438 - mae: 638.4152 - val_loss: 405771.1562 - val_mae: 492.1771\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 503669.3438 - mae: 643.2438 - val_loss: 399940.8438 - val_mae: 485.0885\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 499261.1562 - mae: 639.0352 - val_loss: 415246.0312 - val_mae: 501.3252\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 498299.6562 - mae: 641.8127 - val_loss: 414567.4688 - val_mae: 500.9486\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 503078.4375 - mae: 643.3206 - val_loss: 415456.2812 - val_mae: 501.7446\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 500890.8438 - mae: 640.4994 - val_loss: 414633.4688 - val_mae: 500.9857\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 501904.4375 - mae: 642.1562 - val_loss: 416257.0938 - val_mae: 502.1833\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 493503.2812 - mae: 638.0851 - val_loss: 414860.1562 - val_mae: 501.1134\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 501791.1875 - mae: 642.1063 - val_loss: 423385.3438 - val_mae: 510.0435\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 499003.3125 - mae: 640.4108 - val_loss: 416059.2812 - val_mae: 501.7874\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 503346.9688 - mae: 643.0789 - val_loss: 408290.1250 - val_mae: 493.3186\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 502393.2812 - mae: 642.8128 - val_loss: 408311.2188 - val_mae: 493.6322\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 497583.7812 - mae: 639.2916 - val_loss: 417943.0938 - val_mae: 502.3607\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 493122.8125 - mae: 638.1757 - val_loss: 417807.1250 - val_mae: 502.2792\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 500306.3750 - mae: 641.4110 - val_loss: 418373.1562 - val_mae: 503.3175\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 496194.2500 - mae: 640.2702 - val_loss: 409372.0000 - val_mae: 494.2271\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 500919.1875 - mae: 641.5703 - val_loss: 408445.1562 - val_mae: 493.4315\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 502786.2500 - mae: 642.5858 - val_loss: 417473.2812 - val_mae: 502.3282\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 496048.5000 - mae: 638.9273 - val_loss: 409747.5000 - val_mae: 494.4410\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 494260.9062 - mae: 638.3073 - val_loss: 408336.6562 - val_mae: 493.6467\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 502679.0312 - mae: 642.7708 - val_loss: 416638.7812 - val_mae: 502.1173\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 498198.2188 - mae: 640.2535 - val_loss: 425718.1562 - val_mae: 510.4917\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 504369.3125 - mae: 643.6894 - val_loss: 409441.6250 - val_mae: 494.2668\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 496951.2500 - mae: 638.3422 - val_loss: 418559.7188 - val_mae: 503.1864\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 497241.2500 - mae: 639.5294 - val_loss: 425714.9688 - val_mae: 510.5098\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 498664.3125 - mae: 640.4601 - val_loss: 400087.0938 - val_mae: 485.4658\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 500733.4688 - mae: 641.2146 - val_loss: 408811.7812 - val_mae: 494.1692\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 503555.3438 - mae: 644.0517 - val_loss: 417945.9688 - val_mae: 502.8395\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 500540.7500 - mae: 640.8409 - val_loss: 409492.6562 - val_mae: 494.5421\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 494410.8750 - mae: 638.3392 - val_loss: 407506.4688 - val_mae: 493.1722\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 501685.0000 - mae: 641.7444 - val_loss: 400750.3438 - val_mae: 485.8325\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 494304.4375 - mae: 639.7293 - val_loss: 407775.1562 - val_mae: 493.0771\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 503443.1875 - mae: 643.1489 - val_loss: 426483.9688 - val_mae: 511.1974\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 504490.8750 - mae: 644.2184 - val_loss: 417380.0000 - val_mae: 502.5320\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 500475.6562 - mae: 641.0286 - val_loss: 417224.8438 - val_mae: 502.4407\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499914.1562 - mae: 640.8350 - val_loss: 417771.4062 - val_mae: 502.4860\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 499373.6250 - mae: 640.7587 - val_loss: 408891.4688 - val_mae: 494.2123\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 494513.0000 - mae: 638.6506 - val_loss: 410201.6562 - val_mae: 494.6897\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 498556.9688 - mae: 639.6508 - val_loss: 416431.6562 - val_mae: 501.9964\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 499767.8750 - mae: 641.6392 - val_loss: 407862.5312 - val_mae: 493.6464\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 501675.6562 - mae: 642.1782 - val_loss: 409733.2812 - val_mae: 494.4329\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 493875.0312 - mae: 637.6182 - val_loss: 408174.0312 - val_mae: 493.5435\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 500423.8125 - mae: 641.7037 - val_loss: 425024.0312 - val_mae: 510.6338\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 497705.5625 - mae: 638.9527 - val_loss: 425504.4062 - val_mae: 510.3657\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 501803.5938 - mae: 642.1661 - val_loss: 417198.8438 - val_mae: 502.1690\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 499643.3750 - mae: 640.5408 - val_loss: 425341.7812 - val_mae: 510.5476\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 493818.6250 - mae: 639.1520 - val_loss: 409829.2500 - val_mae: 494.7275\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 500995.3438 - mae: 642.0925 - val_loss: 401621.1250 - val_mae: 486.0816\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 498042.7812 - mae: 639.5952 - val_loss: 402927.7500 - val_mae: 486.8383\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 498682.0625 - mae: 640.3326 - val_loss: 410943.6562 - val_mae: 495.1208\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 4s 112ms/step - loss: 503329.3750 - mae: 643.1527 - val_loss: 410368.6250 - val_mae: 494.8037\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 6s 134ms/step - loss: 501307.7812 - mae: 642.3550 - val_loss: 434930.2188 - val_mae: 519.4897\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 491940.4688 - mae: 637.3292 - val_loss: 417720.2500 - val_mae: 502.7222\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 496021.8750 - mae: 640.5510 - val_loss: 401941.7500 - val_mae: 486.2828\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 504522.5000 - mae: 644.3179 - val_loss: 426680.3438 - val_mae: 511.5432\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 496708.1250 - mae: 639.4205 - val_loss: 418023.8438 - val_mae: 503.1297\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 505554.5938 - mae: 646.4526 - val_loss: 418359.0938 - val_mae: 502.6352\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 501137.1562 - mae: 642.3898 - val_loss: 417933.3438 - val_mae: 502.6096\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 501119.4062 - mae: 641.7007 - val_loss: 427109.5312 - val_mae: 511.5434\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 500765.4688 - mae: 641.3837 - val_loss: 418088.8438 - val_mae: 502.6996\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 503688.3125 - mae: 644.0855 - val_loss: 409621.5938 - val_mae: 494.6127\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 501799.6562 - mae: 642.4989 - val_loss: 410887.3750 - val_mae: 495.3051\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 493004.3125 - mae: 637.5045 - val_loss: 436170.0000 - val_mae: 520.8647\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 501857.9688 - mae: 642.9258 - val_loss: 409908.4688 - val_mae: 494.5423\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 504066.0938 - mae: 644.0039 - val_loss: 419663.2188 - val_mae: 503.5945\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 498913.2500 - mae: 640.6476 - val_loss: 411303.3750 - val_mae: 495.5305\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 497802.5625 - mae: 640.3494 - val_loss: 428244.1250 - val_mae: 512.1954\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 500896.3438 - mae: 642.3716 - val_loss: 418955.9062 - val_mae: 503.1718\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 506193.5938 - mae: 646.2664 - val_loss: 427716.5000 - val_mae: 511.8966\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 501087.6250 - mae: 641.7783 - val_loss: 411714.7812 - val_mae: 495.3595\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 500794.8438 - mae: 642.6357 - val_loss: 419890.7188 - val_mae: 503.7115\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 499482.4375 - mae: 640.7000 - val_loss: 410394.2188 - val_mae: 495.0369\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 506684.4375 - mae: 646.1282 - val_loss: 400955.7188 - val_mae: 485.7109\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 498138.6250 - mae: 640.7899 - val_loss: 408603.7188 - val_mae: 493.5674\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503649.7500 - mae: 643.6428 - val_loss: 425838.5312 - val_mae: 510.5627\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 505036.5938 - mae: 644.4087 - val_loss: 408955.1250 - val_mae: 494.3364\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 495411.5625 - mae: 640.1598 - val_loss: 409679.6562 - val_mae: 494.4023\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 502623.8438 - mae: 643.0091 - val_loss: 426146.9062 - val_mae: 511.2498\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 507273.2500 - mae: 647.1757 - val_loss: 409356.1562 - val_mae: 494.4681\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 498410.2188 - mae: 639.9525 - val_loss: 408884.6250 - val_mae: 493.7125\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 499141.6250 - mae: 640.7459 - val_loss: 409166.0000 - val_mae: 494.3638\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 495636.3438 - mae: 640.5741 - val_loss: 408774.6250 - val_mae: 493.8865\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 498563.0312 - mae: 640.6047 - val_loss: 399950.7500 - val_mae: 485.1104\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 503048.6875 - mae: 644.4867 - val_loss: 406656.7812 - val_mae: 492.6746\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 498744.0312 - mae: 639.7297 - val_loss: 423766.7188 - val_mae: 509.6383\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 499262.3438 - mae: 641.8265 - val_loss: 407996.2188 - val_mae: 493.1444\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 503156.9375 - mae: 642.8857 - val_loss: 413892.2188 - val_mae: 500.5679\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 491837.2500 - mae: 637.0280 - val_loss: 415643.8438 - val_mae: 501.2807\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 497992.9062 - mae: 638.9681 - val_loss: 407984.3750 - val_mae: 493.1586\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 505711.8750 - mae: 645.2220 - val_loss: 407921.2500 - val_mae: 493.1424\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 498165.9375 - mae: 639.8544 - val_loss: 408539.0312 - val_mae: 493.4872\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 495640.8438 - mae: 638.8504 - val_loss: 408802.3438 - val_mae: 493.9023\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 502431.2812 - mae: 642.7365 - val_loss: 407896.4062 - val_mae: 493.1277\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 494864.7188 - mae: 639.5529 - val_loss: 425999.9062 - val_mae: 511.2592\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500592.8125 - mae: 641.5988 - val_loss: 425686.1250 - val_mae: 510.7437\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502968.9062 - mae: 642.8806 - val_loss: 401365.1250 - val_mae: 486.1810\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 499233.9688 - mae: 640.2078 - val_loss: 408242.7500 - val_mae: 493.5931\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 495553.7188 - mae: 638.7748 - val_loss: 417739.5938 - val_mae: 502.9706\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 503087.2188 - mae: 643.3840 - val_loss: 400236.9062 - val_mae: 485.5498\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 501626.7812 - mae: 642.0354 - val_loss: 424555.6562 - val_mae: 510.0994\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 490557.4688 - mae: 639.1504 - val_loss: 399236.9062 - val_mae: 484.9940\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 492471.3750 - mae: 637.8007 - val_loss: 407319.0000 - val_mae: 493.3473\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 502009.0625 - mae: 642.1323 - val_loss: 409234.8750 - val_mae: 494.4015\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 503044.5625 - mae: 643.1954 - val_loss: 416634.6562 - val_mae: 502.1102\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 499602.9062 - mae: 640.1545 - val_loss: 391419.2500 - val_mae: 476.7960\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 501165.5312 - mae: 641.8625 - val_loss: 408879.5938 - val_mae: 493.9666\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 501711.0625 - mae: 643.5523 - val_loss: 399781.6250 - val_mae: 485.2944\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 498619.8125 - mae: 639.8204 - val_loss: 399379.8438 - val_mae: 485.0689\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 497648.8125 - mae: 641.6309 - val_loss: 407304.9062 - val_mae: 492.7555\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 503380.9062 - mae: 643.6141 - val_loss: 433148.0312 - val_mae: 519.0738\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 503088.5000 - mae: 643.3718 - val_loss: 424133.4062 - val_mae: 509.8582\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503306.1562 - mae: 643.2803 - val_loss: 424567.9062 - val_mae: 510.1064\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 505961.7188 - mae: 646.6406 - val_loss: 417714.9062 - val_mae: 502.7148\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 499790.1875 - mae: 640.8821 - val_loss: 416588.9688 - val_mae: 501.8302\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 500723.7500 - mae: 641.5130 - val_loss: 416275.1562 - val_mae: 502.2513\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 504413.2500 - mae: 644.3734 - val_loss: 407610.7188 - val_mae: 493.2318\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 499139.0312 - mae: 640.2627 - val_loss: 416803.9688 - val_mae: 501.9397\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 503479.5938 - mae: 642.9836 - val_loss: 417347.4062 - val_mae: 502.7717\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 503134.2812 - mae: 643.8185 - val_loss: 399531.5000 - val_mae: 485.1541\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 492401.7188 - mae: 637.3203 - val_loss: 408029.4688 - val_mae: 493.4608\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501475.4062 - mae: 641.5983 - val_loss: 408824.5938 - val_mae: 493.9150\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 502921.6250 - mae: 643.3909 - val_loss: 417559.7500 - val_mae: 502.6280\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 500146.8125 - mae: 641.6859 - val_loss: 419094.2188 - val_mae: 503.4843\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 502591.2188 - mae: 643.1266 - val_loss: 420064.7500 - val_mae: 504.2244\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 498797.8125 - mae: 640.4639 - val_loss: 393009.3750 - val_mae: 477.7036\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 498128.7812 - mae: 639.6094 - val_loss: 428473.1250 - val_mae: 512.3162\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 499424.3438 - mae: 641.1992 - val_loss: 410455.7188 - val_mae: 495.1802\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 499440.4375 - mae: 640.4606 - val_loss: 409547.5938 - val_mae: 494.3370\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 503854.5625 - mae: 644.5165 - val_loss: 418376.8750 - val_mae: 502.8368\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 504566.8125 - mae: 644.5674 - val_loss: 407994.4688 - val_mae: 493.4618\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 493104.0625 - mae: 638.2555 - val_loss: 434313.7188 - val_mae: 519.3879\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 499646.2812 - mae: 641.0839 - val_loss: 417437.0312 - val_mae: 502.8078\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 502238.3438 - mae: 642.9076 - val_loss: 400612.8438 - val_mae: 485.7652\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 499043.2500 - mae: 640.1440 - val_loss: 426546.0938 - val_mae: 510.9990\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500795.3125 - mae: 641.8068 - val_loss: 409948.0000 - val_mae: 494.5550\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 498710.5938 - mae: 639.4480 - val_loss: 417972.8750 - val_mae: 502.6325\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 505984.1562 - mae: 645.6995 - val_loss: 418287.9062 - val_mae: 502.8147\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 501819.3750 - mae: 642.3237 - val_loss: 411233.6562 - val_mae: 495.2761\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 492860.0000 - mae: 638.9307 - val_loss: 402441.1250 - val_mae: 486.5719\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 499879.0312 - mae: 641.1099 - val_loss: 418830.7188 - val_mae: 503.3375\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 499545.2500 - mae: 640.8313 - val_loss: 418781.5938 - val_mae: 503.5434\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 502226.8438 - mae: 643.2570 - val_loss: 419629.9688 - val_mae: 503.5611\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 503140.7188 - mae: 643.4302 - val_loss: 427162.1562 - val_mae: 512.1432\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 497337.4688 - mae: 640.7223 - val_loss: 401878.0312 - val_mae: 486.4630\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 502490.2188 - mae: 643.0179 - val_loss: 417635.0000 - val_mae: 502.9143\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 499336.9062 - mae: 640.9235 - val_loss: 426035.3438 - val_mae: 510.9424\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 503151.1875 - mae: 642.9807 - val_loss: 391940.0000 - val_mae: 477.0829\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 498324.7188 - mae: 640.3160 - val_loss: 425836.3438 - val_mae: 510.8197\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 500022.6562 - mae: 641.3559 - val_loss: 426575.4688 - val_mae: 510.9967\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 501658.5000 - mae: 642.6223 - val_loss: 408572.2188 - val_mae: 494.0370\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 497309.8125 - mae: 640.6923 - val_loss: 416580.6250 - val_mae: 502.0750\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 503119.5000 - mae: 643.4185 - val_loss: 409529.5938 - val_mae: 494.3169\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499402.0000 - mae: 641.2491 - val_loss: 426565.5938 - val_mae: 511.4792\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 503280.8750 - mae: 644.0939 - val_loss: 401234.5938 - val_mae: 485.8573\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500687.5625 - mae: 641.5225 - val_loss: 409689.5312 - val_mae: 494.1877\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 502460.8125 - mae: 642.6951 - val_loss: 409868.3438 - val_mae: 494.5098\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 498028.8750 - mae: 640.0247 - val_loss: 418574.5312 - val_mae: 503.1987\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 499624.8438 - mae: 640.6078 - val_loss: 409802.3438 - val_mae: 494.2542\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 502532.7812 - mae: 643.3998 - val_loss: 419423.3438 - val_mae: 503.6676\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 501430.4375 - mae: 642.2373 - val_loss: 419828.9688 - val_mae: 503.6759\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 498622.1250 - mae: 641.0230 - val_loss: 427365.7500 - val_mae: 511.4806\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 498630.2188 - mae: 640.5931 - val_loss: 392575.6562 - val_mae: 477.4565\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 10s 232ms/step - loss: 502069.2188 - mae: 642.3372 - val_loss: 418652.4688 - val_mae: 503.2382\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 502807.5938 - mae: 644.2947 - val_loss: 419771.1250 - val_mae: 503.6426\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 500424.8125 - mae: 642.8593 - val_loss: 417340.1562 - val_mae: 502.5097\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 495796.6250 - mae: 641.3480 - val_loss: 409350.9688 - val_mae: 494.4652\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 498992.3750 - mae: 640.8105 - val_loss: 425924.9688 - val_mae: 510.6136\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 501502.6250 - mae: 642.3607 - val_loss: 408741.2812 - val_mae: 493.8674\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 499794.6250 - mae: 639.9618 - val_loss: 424349.4688 - val_mae: 509.9817\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502668.0312 - mae: 642.5672 - val_loss: 407939.9062 - val_mae: 493.4306\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 495166.7500 - mae: 639.8212 - val_loss: 399887.7812 - val_mae: 485.3489\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 501672.0625 - mae: 642.3172 - val_loss: 415292.7812 - val_mae: 501.3568\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 494783.3438 - mae: 640.3355 - val_loss: 424739.6562 - val_mae: 510.4773\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 501424.5000 - mae: 641.6249 - val_loss: 406874.0000 - val_mae: 493.1018\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 502195.1875 - mae: 641.5339 - val_loss: 414010.0938 - val_mae: 500.3113\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 505722.2812 - mae: 644.7314 - val_loss: 406213.8438 - val_mae: 492.1065\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 498679.1875 - mae: 642.5909 - val_loss: 406171.0312 - val_mae: 492.3956\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 498344.1875 - mae: 639.8210 - val_loss: 414142.3750 - val_mae: 500.3887\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 497266.7500 - mae: 640.6130 - val_loss: 414114.2188 - val_mae: 500.3885\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 501502.1875 - mae: 642.3024 - val_loss: 397808.2500 - val_mae: 483.8922\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 500972.6875 - mae: 641.2830 - val_loss: 405853.7188 - val_mae: 492.5375\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 502541.4062 - mae: 642.9285 - val_loss: 422814.2188 - val_mae: 508.7945\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 495700.5625 - mae: 639.7642 - val_loss: 407678.9688 - val_mae: 493.2603\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 496104.1875 - mae: 638.8844 - val_loss: 415550.0938 - val_mae: 501.2103\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 498774.8125 - mae: 639.4981 - val_loss: 415326.6562 - val_mae: 501.3759\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 492106.7812 - mae: 639.3183 - val_loss: 414757.2812 - val_mae: 501.3941\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 500894.6875 - mae: 641.1110 - val_loss: 398264.4062 - val_mae: 484.1591\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 492116.4062 - mae: 637.9886 - val_loss: 407927.0000 - val_mae: 493.1458\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 504034.5938 - mae: 644.2021 - val_loss: 391529.8750 - val_mae: 476.8483\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 501427.5625 - mae: 642.1207 - val_loss: 391968.2812 - val_mae: 477.1099\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 501955.0312 - mae: 642.4172 - val_loss: 415880.8750 - val_mae: 501.9742\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 501689.0312 - mae: 641.8712 - val_loss: 417068.6250 - val_mae: 502.6154\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 506627.2500 - mae: 645.8891 - val_loss: 416956.7812 - val_mae: 502.2859\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 497505.2188 - mae: 638.4149 - val_loss: 415665.6562 - val_mae: 501.2934\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 501039.8438 - mae: 641.8102 - val_loss: 423770.0938 - val_mae: 509.6505\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 502540.6562 - mae: 643.6558 - val_loss: 415777.3750 - val_mae: 501.6292\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 496370.8125 - mae: 638.7082 - val_loss: 406128.1250 - val_mae: 492.0774\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 500455.6250 - mae: 641.1680 - val_loss: 431808.2500 - val_mae: 517.6797\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 493201.3125 - mae: 638.9685 - val_loss: 407375.0938 - val_mae: 493.0971\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 498412.0000 - mae: 639.9598 - val_loss: 407265.3750 - val_mae: 493.0127\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 496841.8438 - mae: 639.8969 - val_loss: 417008.2500 - val_mae: 502.3147\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 504577.9375 - mae: 644.0073 - val_loss: 406939.2500 - val_mae: 493.1852\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 496649.2500 - mae: 640.7806 - val_loss: 400554.9062 - val_mae: 485.4622\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 501563.6562 - mae: 641.9120 - val_loss: 399212.1562 - val_mae: 484.9692\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 501939.6250 - mae: 642.3687 - val_loss: 408729.5938 - val_mae: 493.6418\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 498769.0938 - mae: 640.4885 - val_loss: 416566.3438 - val_mae: 502.0768\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 499129.3438 - mae: 639.9631 - val_loss: 408637.1562 - val_mae: 494.4115\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 495961.2812 - mae: 638.4487 - val_loss: 416858.4062 - val_mae: 501.6838\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 497566.1250 - mae: 639.6545 - val_loss: 424381.7812 - val_mae: 510.2801\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 495890.2500 - mae: 639.6019 - val_loss: 414648.5938 - val_mae: 500.9999\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 496003.5312 - mae: 638.3500 - val_loss: 430693.5000 - val_mae: 517.3567\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 501404.8125 - mae: 641.8848 - val_loss: 423758.5938 - val_mae: 509.6338\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 498252.9062 - mae: 639.1916 - val_loss: 398627.4688 - val_mae: 484.9841\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 498733.9062 - mae: 639.2192 - val_loss: 406968.5000 - val_mae: 493.1539\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 501497.8438 - mae: 642.6339 - val_loss: 415282.0312 - val_mae: 501.3508\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 499739.0938 - mae: 640.8337 - val_loss: 404665.3750 - val_mae: 491.2276\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 504631.3750 - mae: 643.2045 - val_loss: 397654.3438 - val_mae: 483.8021\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 495107.8438 - mae: 639.0812 - val_loss: 415146.8438 - val_mae: 501.5823\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 499464.0938 - mae: 639.8243 - val_loss: 406037.2188 - val_mae: 492.6391\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 502350.6875 - mae: 642.4697 - val_loss: 424239.3438 - val_mae: 510.2016\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 500679.4375 - mae: 641.8459 - val_loss: 407689.0000 - val_mae: 493.2661\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 500492.6875 - mae: 641.5193 - val_loss: 406603.8750 - val_mae: 492.9529\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 503865.3438 - mae: 643.8631 - val_loss: 409414.4062 - val_mae: 494.2513\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 503981.8125 - mae: 643.8376 - val_loss: 408132.1562 - val_mae: 493.8672\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 503941.8750 - mae: 643.7078 - val_loss: 399597.5938 - val_mae: 484.9207\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 492492.3125 - mae: 637.4592 - val_loss: 425559.3438 - val_mae: 510.1366\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 502995.5312 - mae: 643.1230 - val_loss: 409784.1562 - val_mae: 494.4521\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 503184.8125 - mae: 643.9693 - val_loss: 425418.6250 - val_mae: 510.9191\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 501737.0312 - mae: 642.0460 - val_loss: 432372.7812 - val_mae: 517.4296\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 499974.3125 - mae: 641.0980 - val_loss: 398329.6250 - val_mae: 484.1804\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 501086.5000 - mae: 641.9067 - val_loss: 409370.0938 - val_mae: 494.2161\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 500122.6562 - mae: 641.9578 - val_loss: 408450.1562 - val_mae: 493.6909\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 498155.6562 - mae: 639.9224 - val_loss: 398459.9062 - val_mae: 484.5458\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 499041.2188 - mae: 640.9670 - val_loss: 424136.0312 - val_mae: 510.1444\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 498574.3438 - mae: 639.6993 - val_loss: 407148.3750 - val_mae: 492.6624\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 496402.1875 - mae: 639.1190 - val_loss: 406724.5938 - val_mae: 492.7244\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 498437.0000 - mae: 639.2401 - val_loss: 415824.7500 - val_mae: 501.3703\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 501995.7812 - mae: 642.2854 - val_loss: 416335.9688 - val_mae: 501.9378\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 497516.6875 - mae: 639.0980 - val_loss: 423576.3750 - val_mae: 509.5398\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 503778.9688 - mae: 643.2924 - val_loss: 415698.5312 - val_mae: 501.5797\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 500369.0938 - mae: 641.5316 - val_loss: 415877.1562 - val_mae: 501.3852\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 10s 237ms/step - loss: 502656.5000 - mae: 642.4733 - val_loss: 407476.0000 - val_mae: 493.1441\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 501873.6562 - mae: 642.4395 - val_loss: 405986.3750 - val_mae: 492.6118\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 497794.9062 - mae: 639.2602 - val_loss: 416284.3750 - val_mae: 501.6222\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 498520.3438 - mae: 639.2363 - val_loss: 415481.4062 - val_mae: 501.4629\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 494631.5312 - mae: 637.1921 - val_loss: 423497.0000 - val_mae: 510.0924\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 503549.4062 - mae: 643.8600 - val_loss: 415220.7188 - val_mae: 501.0183\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499187.8438 - mae: 640.0179 - val_loss: 398736.7188 - val_mae: 484.3850\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501938.2500 - mae: 642.0140 - val_loss: 415074.8438 - val_mae: 500.6614\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 505372.5312 - mae: 644.8563 - val_loss: 399019.5000 - val_mae: 484.8719\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 504214.8125 - mae: 643.9124 - val_loss: 416614.0938 - val_mae: 501.5368\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 503040.5625 - mae: 642.7029 - val_loss: 425193.4062 - val_mae: 509.9134\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 504050.9062 - mae: 644.4099 - val_loss: 399606.5000 - val_mae: 484.9422\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 503760.5000 - mae: 644.3790 - val_loss: 416960.5312 - val_mae: 502.0307\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 502949.1562 - mae: 642.7583 - val_loss: 401012.7188 - val_mae: 485.7284\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 499877.0000 - mae: 640.9598 - val_loss: 400652.6562 - val_mae: 485.7874\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 503036.5938 - mae: 643.1742 - val_loss: 425871.6562 - val_mae: 510.5822\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 502196.1250 - mae: 642.7703 - val_loss: 400964.9062 - val_mae: 485.7163\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 496218.6250 - mae: 638.3113 - val_loss: 426597.2500 - val_mae: 511.0291\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502553.4688 - mae: 642.4777 - val_loss: 417397.0000 - val_mae: 502.0075\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 489961.5625 - mae: 635.9243 - val_loss: 419707.0000 - val_mae: 503.8217\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 499406.2500 - mae: 640.7044 - val_loss: 411034.6562 - val_mae: 495.1724\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 505758.1875 - mae: 645.5308 - val_loss: 420491.7188 - val_mae: 503.8596\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 497351.3125 - mae: 639.9974 - val_loss: 420260.1562 - val_mae: 504.1328\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 502205.8125 - mae: 643.0887 - val_loss: 402058.6562 - val_mae: 486.3506\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 502854.3125 - mae: 643.2463 - val_loss: 420557.3438 - val_mae: 504.0957\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 498611.0312 - mae: 640.6596 - val_loss: 421152.6250 - val_mae: 504.4382\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 501756.4375 - mae: 642.2779 - val_loss: 419685.7500 - val_mae: 503.8136\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 501244.7500 - mae: 641.6780 - val_loss: 402657.0000 - val_mae: 486.8935\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 501916.0938 - mae: 642.8785 - val_loss: 399859.7500 - val_mae: 485.3383\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 498409.1875 - mae: 640.1599 - val_loss: 409550.0938 - val_mae: 494.0849\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 503768.6875 - mae: 644.3480 - val_loss: 416782.7500 - val_mae: 501.9427\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 500854.0625 - mae: 641.9075 - val_loss: 407115.6250 - val_mae: 492.6861\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 503167.8438 - mae: 643.5151 - val_loss: 425855.6250 - val_mae: 510.8402\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 504302.9688 - mae: 643.8479 - val_loss: 408849.3438 - val_mae: 494.1898\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 498000.9375 - mae: 640.6326 - val_loss: 416782.0000 - val_mae: 502.1927\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 498429.5312 - mae: 639.5591 - val_loss: 408902.9688 - val_mae: 494.2186\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 496791.4688 - mae: 638.3815 - val_loss: 407896.2500 - val_mae: 493.6653\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 503774.7812 - mae: 643.6190 - val_loss: 416119.8750 - val_mae: 501.8791\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 498399.3438 - mae: 641.0499 - val_loss: 415435.2188 - val_mae: 501.4369\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 498066.3125 - mae: 639.3707 - val_loss: 406447.5312 - val_mae: 492.8666\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 505963.7188 - mae: 645.6681 - val_loss: 398006.4688 - val_mae: 484.0082\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 505556.6875 - mae: 644.9572 - val_loss: 415470.3438 - val_mae: 501.4619\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 498576.7188 - mae: 639.4584 - val_loss: 425682.7188 - val_mae: 510.7418\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 501866.0625 - mae: 642.1558 - val_loss: 417563.4062 - val_mae: 502.3804\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 499860.3438 - mae: 641.5211 - val_loss: 408934.4062 - val_mae: 493.9877\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 501949.5938 - mae: 642.0672 - val_loss: 426132.5000 - val_mae: 511.2418\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 502323.1250 - mae: 643.2985 - val_loss: 410049.4688 - val_mae: 494.6225\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 497375.0625 - mae: 639.4415 - val_loss: 401814.0312 - val_mae: 486.7686\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 503749.8438 - mae: 643.7354 - val_loss: 419104.3438 - val_mae: 503.2865\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 502117.7812 - mae: 642.9397 - val_loss: 420200.7812 - val_mae: 504.0999\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 6s 136ms/step - loss: 501520.0625 - mae: 643.0060 - val_loss: 427709.5938 - val_mae: 512.1050\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 498687.8438 - mae: 640.3690 - val_loss: 402620.3438 - val_mae: 486.8771\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 502099.9062 - mae: 642.3345 - val_loss: 410411.7812 - val_mae: 494.8283\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 504324.9375 - mae: 644.4150 - val_loss: 409652.2812 - val_mae: 494.4064\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 505829.1562 - mae: 645.9340 - val_loss: 410543.2500 - val_mae: 494.6907\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 493989.7188 - mae: 639.4837 - val_loss: 418583.4062 - val_mae: 503.5368\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 496048.3438 - mae: 640.1923 - val_loss: 419105.0312 - val_mae: 503.4864\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 502676.2188 - mae: 643.2534 - val_loss: 408590.5000 - val_mae: 493.7916\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 502300.2500 - mae: 642.0253 - val_loss: 425274.7500 - val_mae: 510.5094\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 504235.2812 - mae: 644.1529 - val_loss: 417610.7500 - val_mae: 502.6520\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 498913.7500 - mae: 640.6183 - val_loss: 424837.5312 - val_mae: 509.9922\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 502078.6875 - mae: 641.8947 - val_loss: 416219.5312 - val_mae: 501.6001\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 500925.9688 - mae: 641.4293 - val_loss: 415535.1250 - val_mae: 501.2016\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 498335.2188 - mae: 639.5022 - val_loss: 407759.2188 - val_mae: 493.6645\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 498712.2500 - mae: 640.3834 - val_loss: 391652.9062 - val_mae: 476.9187\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 500115.0938 - mae: 640.7322 - val_loss: 407094.9688 - val_mae: 493.2237\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 499110.8125 - mae: 640.2560 - val_loss: 389654.8750 - val_mae: 475.7839\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 497083.5000 - mae: 638.7003 - val_loss: 415913.5000 - val_mae: 501.4220\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 501281.6562 - mae: 641.5460 - val_loss: 400580.4688 - val_mae: 485.7373\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 502439.6875 - mae: 642.8011 - val_loss: 417555.5312 - val_mae: 502.6212\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 502285.1562 - mae: 642.5823 - val_loss: 400572.3438 - val_mae: 485.7377\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 500466.4688 - mae: 642.0803 - val_loss: 408085.6250 - val_mae: 493.2186\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 502080.2812 - mae: 642.2462 - val_loss: 433217.9688 - val_mae: 518.4987\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 503266.4688 - mae: 642.9833 - val_loss: 400519.2812 - val_mae: 485.4574\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 500828.0000 - mae: 642.0568 - val_loss: 399561.3750 - val_mae: 485.1656\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 10s 237ms/step - loss: 500517.1250 - mae: 641.3604 - val_loss: 408051.6250 - val_mae: 493.4840\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 504625.2188 - mae: 644.3105 - val_loss: 418384.7812 - val_mae: 503.0846\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 502162.3125 - mae: 642.7995 - val_loss: 418186.0938 - val_mae: 502.9779\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 504496.2188 - mae: 643.7836 - val_loss: 426242.4062 - val_mae: 511.3031\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 500345.5625 - mae: 640.8756 - val_loss: 400639.1562 - val_mae: 485.7799\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 501360.5625 - mae: 641.6431 - val_loss: 409386.6562 - val_mae: 494.2354\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 499012.7812 - mae: 640.4770 - val_loss: 409071.0938 - val_mae: 493.8019\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 502558.7812 - mae: 642.4575 - val_loss: 409090.8438 - val_mae: 494.0668\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 503128.0312 - mae: 643.0983 - val_loss: 425740.0938 - val_mae: 510.7744\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 500531.5625 - mae: 641.1689 - val_loss: 408713.0938 - val_mae: 493.8615\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 490167.4688 - mae: 635.8474 - val_loss: 407866.0000 - val_mae: 493.6488\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 501455.7812 - mae: 642.3262 - val_loss: 408406.4688 - val_mae: 493.4087\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 492750.4062 - mae: 638.2615 - val_loss: 408214.5938 - val_mae: 493.5562\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 500069.3750 - mae: 641.0108 - val_loss: 396592.5938 - val_mae: 483.5038\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 504057.0625 - mae: 643.4050 - val_loss: 415492.2188 - val_mae: 501.4690\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 500933.3750 - mae: 642.7983 - val_loss: 407733.1562 - val_mae: 493.3019\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 502597.6562 - mae: 643.0596 - val_loss: 407908.7188 - val_mae: 493.0924\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 499120.2812 - mae: 640.7157 - val_loss: 407271.2500 - val_mae: 492.7569\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 500177.5938 - mae: 641.1526 - val_loss: 406471.7188 - val_mae: 492.2819\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 499562.8750 - mae: 640.5953 - val_loss: 415507.4062 - val_mae: 501.4775\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 499541.0625 - mae: 640.0964 - val_loss: 406888.7188 - val_mae: 493.1099\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 496044.0000 - mae: 639.2822 - val_loss: 423772.1250 - val_mae: 509.3622\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 501664.7812 - mae: 642.3885 - val_loss: 432060.8438 - val_mae: 518.4277\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499653.2812 - mae: 640.2408 - val_loss: 423215.7500 - val_mae: 509.9533\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 497883.2188 - mae: 639.0580 - val_loss: 412850.8438 - val_mae: 499.6491\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 496937.4062 - mae: 638.7994 - val_loss: 413320.3438 - val_mae: 500.2449\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 495555.7812 - mae: 638.6044 - val_loss: 414610.9688 - val_mae: 500.9731\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 502697.1250 - mae: 643.4995 - val_loss: 405016.6562 - val_mae: 491.4373\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 503452.4375 - mae: 642.5270 - val_loss: 423360.2500 - val_mae: 509.1183\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499825.1562 - mae: 640.4585 - val_loss: 415326.7500 - val_mae: 501.3759\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 497296.2812 - mae: 640.4895 - val_loss: 423694.7500 - val_mae: 509.5972\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 500127.4375 - mae: 640.2559 - val_loss: 414960.7500 - val_mae: 501.4762\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 502382.4688 - mae: 642.8444 - val_loss: 423097.9062 - val_mae: 508.9628\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 504065.5938 - mae: 643.8703 - val_loss: 407749.1562 - val_mae: 493.3111\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 494807.6562 - mae: 638.8006 - val_loss: 425251.2500 - val_mae: 510.4961\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 492603.3125 - mae: 637.4942 - val_loss: 415218.8750 - val_mae: 500.7483\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 495817.8438 - mae: 638.2094 - val_loss: 415781.0000 - val_mae: 501.9203\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 500682.7188 - mae: 640.7994 - val_loss: 415628.7500 - val_mae: 501.2561\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 503190.6562 - mae: 643.1715 - val_loss: 416306.5312 - val_mae: 501.9312\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 498022.6562 - mae: 639.4381 - val_loss: 398845.5938 - val_mae: 484.4820\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 502209.9688 - mae: 642.5052 - val_loss: 425574.9688 - val_mae: 510.1461\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 499960.5312 - mae: 641.8784 - val_loss: 400431.0938 - val_mae: 485.3901\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 501716.8438 - mae: 641.4695 - val_loss: 423540.5000 - val_mae: 509.5089\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 495328.1250 - mae: 637.8082 - val_loss: 397403.5312 - val_mae: 483.6552\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 503571.9062 - mae: 643.8014 - val_loss: 407699.9062 - val_mae: 493.2829\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499949.5312 - mae: 642.5338 - val_loss: 408558.2812 - val_mae: 493.5196\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 499563.1562 - mae: 640.9484 - val_loss: 399769.0000 - val_mae: 485.2822\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 503420.5312 - mae: 643.7037 - val_loss: 399787.0000 - val_mae: 484.9987\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501987.7812 - mae: 641.9591 - val_loss: 416079.0938 - val_mae: 501.5338\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 497390.4062 - mae: 639.3843 - val_loss: 407897.4062 - val_mae: 493.6664\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 501546.3438 - mae: 641.7254 - val_loss: 416455.0938 - val_mae: 501.7216\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501623.2500 - mae: 641.6141 - val_loss: 389065.5000 - val_mae: 475.4568\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 499098.0000 - mae: 640.4570 - val_loss: 399056.5938 - val_mae: 484.6051\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 500469.2812 - mae: 641.2431 - val_loss: 398663.1562 - val_mae: 484.3754\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 499104.8750 - mae: 639.4663 - val_loss: 416404.0312 - val_mae: 501.9760\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 503427.4062 - mae: 643.5616 - val_loss: 408161.0312 - val_mae: 493.5360\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 502169.1875 - mae: 643.1126 - val_loss: 415909.0000 - val_mae: 501.7082\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 505769.6875 - mae: 644.8719 - val_loss: 415260.6562 - val_mae: 501.0575\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 494764.8750 - mae: 639.0756 - val_loss: 407707.9062 - val_mae: 493.5615\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499123.4062 - mae: 640.0596 - val_loss: 425778.4688 - val_mae: 511.0476\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 500373.4375 - mae: 641.2457 - val_loss: 400615.9062 - val_mae: 485.5135\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501117.2500 - mae: 642.0556 - val_loss: 417598.0938 - val_mae: 502.4155\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 504025.2812 - mae: 644.1406 - val_loss: 427321.0938 - val_mae: 511.6635\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 499813.2812 - mae: 642.0441 - val_loss: 417356.0312 - val_mae: 502.5095\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 502260.5625 - mae: 642.9736 - val_loss: 399810.4062 - val_mae: 485.0609\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 503083.2188 - mae: 643.6692 - val_loss: 401967.9062 - val_mae: 486.5132\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 499974.9375 - mae: 641.5859 - val_loss: 417408.8750 - val_mae: 502.5436\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 502121.9375 - mae: 642.0651 - val_loss: 408708.6562 - val_mae: 493.8590\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 500599.6875 - mae: 641.3087 - val_loss: 408868.6250 - val_mae: 494.1998\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 501882.4375 - mae: 642.1572 - val_loss: 400777.9062 - val_mae: 485.6077\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 504204.8125 - mae: 643.5494 - val_loss: 424842.9062 - val_mae: 510.2533\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 507605.8438 - mae: 646.7252 - val_loss: 407716.8750 - val_mae: 493.6404\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 500756.8438 - mae: 641.4520 - val_loss: 425796.7812 - val_mae: 510.8067\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 501411.3438 - mae: 641.9369 - val_loss: 417435.7812 - val_mae: 502.5541\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 494527.6562 - mae: 639.2789 - val_loss: 426381.4688 - val_mae: 511.3783\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 504646.8125 - mae: 644.7800 - val_loss: 415767.3438 - val_mae: 501.9073\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 499964.4375 - mae: 640.7719 - val_loss: 417820.5312 - val_mae: 503.0141\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 9s 249ms/step - loss: 501186.3750 - mae: 642.4794 - val_loss: 417912.9688 - val_mae: 502.5830\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 496245.3750 - mae: 638.0257 - val_loss: 399808.5312 - val_mae: 485.3147\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 498219.7812 - mae: 639.4018 - val_loss: 407656.5000 - val_mae: 493.5331\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 500182.3750 - mae: 641.0263 - val_loss: 408510.3750 - val_mae: 494.0040\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501267.3125 - mae: 642.1196 - val_loss: 425554.9062 - val_mae: 510.6690\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 498987.5000 - mae: 641.8726 - val_loss: 416052.4688 - val_mae: 501.2506\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 502223.1250 - mae: 642.5594 - val_loss: 424669.4062 - val_mae: 509.6239\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 497284.1875 - mae: 638.8469 - val_loss: 417389.5938 - val_mae: 502.2797\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 500720.2188 - mae: 641.5963 - val_loss: 424686.5938 - val_mae: 509.8828\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 497792.5000 - mae: 640.1442 - val_loss: 409242.7188 - val_mae: 494.1534\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 503480.5312 - mae: 643.1875 - val_loss: 409170.2500 - val_mae: 494.1122\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 492033.7500 - mae: 638.2966 - val_loss: 416702.2188 - val_mae: 502.1480\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 500061.3125 - mae: 641.4587 - val_loss: 408812.7188 - val_mae: 493.6700\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 503969.0312 - mae: 643.9594 - val_loss: 399499.1562 - val_mae: 485.1412\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 497208.2500 - mae: 639.4629 - val_loss: 424628.4062 - val_mae: 510.4156\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 501203.1250 - mae: 641.7051 - val_loss: 409545.7812 - val_mae: 494.3261\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 501511.2500 - mae: 642.4385 - val_loss: 426876.0938 - val_mae: 510.9678\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 505600.2188 - mae: 645.3323 - val_loss: 393200.5938 - val_mae: 477.8125\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 502374.5312 - mae: 642.8917 - val_loss: 428007.2812 - val_mae: 512.0613\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 497218.0312 - mae: 639.6411 - val_loss: 410455.5938 - val_mae: 494.8436\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 502243.0938 - mae: 643.5897 - val_loss: 419022.1562 - val_mae: 503.0068\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 500679.8750 - mae: 642.7778 - val_loss: 426640.0312 - val_mae: 511.0542\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 497760.9375 - mae: 639.5622 - val_loss: 402111.0938 - val_mae: 486.3505\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501984.5625 - mae: 642.5628 - val_loss: 418784.0938 - val_mae: 503.0724\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 501544.4062 - mae: 642.1035 - val_loss: 399965.5000 - val_mae: 485.1512\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 502899.6250 - mae: 642.6282 - val_loss: 426037.0000 - val_mae: 510.4276\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 489631.7188 - mae: 635.3545 - val_loss: 409032.6562 - val_mae: 494.2898\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 492018.6875 - mae: 637.1423 - val_loss: 425356.7500 - val_mae: 510.2786\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 494315.8750 - mae: 639.4232 - val_loss: 400339.0938 - val_mae: 485.6070\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 501066.1562 - mae: 641.6755 - val_loss: 409782.0938 - val_mae: 494.7027\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 500866.6875 - mae: 642.3124 - val_loss: 425282.2500 - val_mae: 510.5908\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 503913.5625 - mae: 644.5517 - val_loss: 408719.5938 - val_mae: 493.8652\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 495616.2188 - mae: 637.9386 - val_loss: 417297.6250 - val_mae: 502.2263\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 499159.4375 - mae: 639.8361 - val_loss: 399139.6562 - val_mae: 484.9340\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 504967.9688 - mae: 645.7201 - val_loss: 416375.2188 - val_mae: 501.9647\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 502431.4062 - mae: 642.7622 - val_loss: 424213.7500 - val_mae: 509.8940\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 491249.8125 - mae: 638.9684 - val_loss: 399507.4062 - val_mae: 484.8845\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 496912.3750 - mae: 638.6465 - val_loss: 400051.0312 - val_mae: 485.1849\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 503706.1250 - mae: 643.0614 - val_loss: 401357.5312 - val_mae: 486.1722\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 503025.0938 - mae: 644.4268 - val_loss: 416782.2500 - val_mae: 502.1928\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 491207.9062 - mae: 637.3196 - val_loss: 409512.2812 - val_mae: 494.5537\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 501741.5938 - mae: 642.5355 - val_loss: 415935.9062 - val_mae: 501.7182\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 497916.5938 - mae: 639.5316 - val_loss: 416849.1562 - val_mae: 502.2351\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 503225.4688 - mae: 643.6954 - val_loss: 407956.0938 - val_mae: 493.4189\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 504735.7188 - mae: 644.9500 - val_loss: 408010.4062 - val_mae: 493.4500\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 498841.1875 - mae: 640.3752 - val_loss: 408063.5312 - val_mae: 493.4803\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 493372.4375 - mae: 638.7761 - val_loss: 398914.8438 - val_mae: 484.5390\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 500715.7812 - mae: 641.5045 - val_loss: 399299.5000 - val_mae: 485.0238\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 499161.7812 - mae: 639.9939 - val_loss: 424964.5000 - val_mae: 509.8040\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 501241.1875 - mae: 642.0624 - val_loss: 400592.0000 - val_mae: 485.7438\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 10s 262ms/step - loss: 501875.3438 - mae: 642.4222 - val_loss: 419013.3438 - val_mae: 503.4353\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 501611.0938 - mae: 643.2247 - val_loss: 399841.9688 - val_mae: 485.3335\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 503968.1250 - mae: 644.1136 - val_loss: 418246.6250 - val_mae: 502.7761\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 500748.0312 - mae: 641.9025 - val_loss: 411267.7188 - val_mae: 495.0968\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 502573.9688 - mae: 642.8231 - val_loss: 418556.9062 - val_mae: 502.7285\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 501425.8750 - mae: 642.3331 - val_loss: 410984.3750 - val_mae: 495.3596\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 502277.5000 - mae: 643.3562 - val_loss: 402357.1250 - val_mae: 486.7303\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 499411.0938 - mae: 640.9950 - val_loss: 408972.7812 - val_mae: 493.7854\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 502090.6562 - mae: 642.1172 - val_loss: 401299.7188 - val_mae: 485.8951\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 500655.9375 - mae: 641.6862 - val_loss: 391582.0312 - val_mae: 476.8891\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 503991.9062 - mae: 644.5391 - val_loss: 425646.2812 - val_mae: 510.9751\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 495683.5625 - mae: 638.7419 - val_loss: 402215.6562 - val_mae: 486.4111\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 501340.8750 - mae: 642.4484 - val_loss: 417922.2188 - val_mae: 503.0688\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 495066.2812 - mae: 638.0428 - val_loss: 417303.5938 - val_mae: 502.2448\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 497183.0625 - mae: 639.5280 - val_loss: 408771.4062 - val_mae: 493.9050\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 498976.3750 - mae: 640.6631 - val_loss: 417938.2188 - val_mae: 502.6125\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 10s 237ms/step - loss: 502137.9375 - mae: 643.4280 - val_loss: 417708.3438 - val_mae: 502.4644\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 493375.4375 - mae: 639.6202 - val_loss: 416483.2812 - val_mae: 501.7688\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 500644.3438 - mae: 640.8137 - val_loss: 424568.9688 - val_mae: 510.1069\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 500276.6875 - mae: 640.6573 - val_loss: 423360.7812 - val_mae: 509.7169\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 497297.5938 - mae: 640.1795 - val_loss: 407371.1562 - val_mae: 493.0841\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 496276.0625 - mae: 638.6102 - val_loss: 415633.6562 - val_mae: 501.2433\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499842.9688 - mae: 640.0420 - val_loss: 406521.6250 - val_mae: 492.9070\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 501471.2188 - mae: 642.3353 - val_loss: 398395.3438 - val_mae: 484.5210\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 499740.0938 - mae: 640.3773 - val_loss: 416683.1562 - val_mae: 502.4076\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 501550.9062 - mae: 643.4288 - val_loss: 416826.3438 - val_mae: 501.9680\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 499680.0625 - mae: 641.2927 - val_loss: 408129.8438 - val_mae: 493.5286\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 498983.1562 - mae: 640.3488 - val_loss: 415484.6562 - val_mae: 501.1880\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 502020.4688 - mae: 641.6808 - val_loss: 424510.8750 - val_mae: 509.7992\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 10s 261ms/step - loss: 503274.9375 - mae: 643.0762 - val_loss: 416224.7188 - val_mae: 501.6030\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 503251.0312 - mae: 643.2975 - val_loss: 407324.8438 - val_mae: 493.3505\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 500994.1875 - mae: 641.7527 - val_loss: 399264.0938 - val_mae: 484.9984\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 11s 270ms/step - loss: 504719.7188 - mae: 643.3467 - val_loss: 423100.5312 - val_mae: 509.2674\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 505274.8438 - mae: 644.7901 - val_loss: 423578.1250 - val_mae: 509.2474\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 503329.6562 - mae: 643.7742 - val_loss: 415188.3438 - val_mae: 501.3035\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 493404.3125 - mae: 637.6525 - val_loss: 416106.6562 - val_mae: 501.8141\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 490732.4375 - mae: 636.2990 - val_loss: 407423.8750 - val_mae: 493.4050\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 500174.9062 - mae: 640.9958 - val_loss: 417136.0312 - val_mae: 502.3910\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 501315.9062 - mae: 643.2696 - val_loss: 417131.1250 - val_mae: 502.3882\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 503462.4375 - mae: 643.7125 - val_loss: 416921.3750 - val_mae: 501.9926\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 502836.6250 - mae: 643.2854 - val_loss: 415924.0938 - val_mae: 501.4281\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 498394.9375 - mae: 639.5571 - val_loss: 407220.5312 - val_mae: 493.2930\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 499905.8750 - mae: 641.0817 - val_loss: 415298.9688 - val_mae: 501.0798\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 501545.6250 - mae: 642.3694 - val_loss: 416031.5312 - val_mae: 502.3948\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 500312.0312 - mae: 641.7881 - val_loss: 400434.2188 - val_mae: 485.4238\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 496810.6562 - mae: 638.6940 - val_loss: 400697.3438 - val_mae: 485.5609\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 499984.8125 - mae: 640.8453 - val_loss: 425005.9062 - val_mae: 510.6233\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 500468.2188 - mae: 641.2850 - val_loss: 408723.7188 - val_mae: 493.8574\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 501432.5625 - mae: 641.4371 - val_loss: 408311.7188 - val_mae: 493.6325\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 496712.0938 - mae: 640.2623 - val_loss: 416464.0312 - val_mae: 501.7577\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 500954.7188 - mae: 641.7227 - val_loss: 417126.6562 - val_mae: 501.8451\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 497044.2188 - mae: 639.3004 - val_loss: 417437.5312 - val_mae: 502.3075\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501340.3125 - mae: 641.8449 - val_loss: 407474.9062 - val_mae: 493.4812\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 498838.8750 - mae: 639.9550 - val_loss: 407811.1562 - val_mae: 493.3253\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 503657.0312 - mae: 643.3737 - val_loss: 407616.3750 - val_mae: 493.2245\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 500021.2188 - mae: 641.2173 - val_loss: 407088.6250 - val_mae: 492.9330\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 504503.8750 - mae: 644.4501 - val_loss: 416613.4062 - val_mae: 502.0934\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 499249.0938 - mae: 639.8395 - val_loss: 408904.7188 - val_mae: 493.9607\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 498209.3125 - mae: 640.5343 - val_loss: 417389.6250 - val_mae: 502.2646\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 500542.5625 - mae: 641.7734 - val_loss: 416582.7500 - val_mae: 502.3534\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 502765.0938 - mae: 642.2794 - val_loss: 398836.5000 - val_mae: 484.7690\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 503982.4062 - mae: 643.7505 - val_loss: 416120.3750 - val_mae: 501.5424\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 497754.7188 - mae: 638.8757 - val_loss: 415184.3438 - val_mae: 501.5973\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 498104.2500 - mae: 639.0342 - val_loss: 416432.7500 - val_mae: 501.4535\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 501701.8438 - mae: 642.1127 - val_loss: 416180.3438 - val_mae: 501.5773\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501893.6562 - mae: 642.2693 - val_loss: 406777.1250 - val_mae: 492.7436\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 497339.0938 - mae: 638.6925 - val_loss: 398664.9062 - val_mae: 484.3597\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 501133.5938 - mae: 641.7474 - val_loss: 414721.2812 - val_mae: 500.7108\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 503896.6250 - mae: 644.0186 - val_loss: 406250.2500 - val_mae: 492.7903\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 499796.5000 - mae: 640.6335 - val_loss: 423837.3438 - val_mae: 509.6890\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 496103.4375 - mae: 638.8998 - val_loss: 399784.8750 - val_mae: 485.2911\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 499520.4375 - mae: 641.2925 - val_loss: 425351.1250 - val_mae: 511.1520\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 5s 99ms/step - loss: 500535.2500 - mae: 641.2364 - val_loss: 407450.3438 - val_mae: 493.1401\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 501553.3125 - mae: 642.0018 - val_loss: 408205.7188 - val_mae: 493.5720\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 499524.3438 - mae: 640.7479 - val_loss: 425919.7500 - val_mae: 510.8672\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 497771.0625 - mae: 641.5986 - val_loss: 400502.9062 - val_mae: 485.6938\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 498707.3125 - mae: 640.7046 - val_loss: 425611.9688 - val_mae: 510.6919\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 502637.1562 - mae: 642.5458 - val_loss: 417230.9062 - val_mae: 502.1725\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 504440.7500 - mae: 644.2027 - val_loss: 424461.8438 - val_mae: 509.7703\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 503897.5625 - mae: 644.0078 - val_loss: 417534.0000 - val_mae: 502.6091\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 501677.3750 - mae: 642.4420 - val_loss: 416831.8438 - val_mae: 502.4878\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 497222.5312 - mae: 639.3783 - val_loss: 407271.1250 - val_mae: 493.3209\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 500111.0000 - mae: 640.5433 - val_loss: 406571.6562 - val_mae: 492.6478\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 503211.1562 - mae: 644.3752 - val_loss: 424668.5000 - val_mae: 510.1638\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 502679.6562 - mae: 642.4968 - val_loss: 425379.6250 - val_mae: 510.3120\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 503351.1562 - mae: 642.9644 - val_loss: 417133.3438 - val_mae: 502.1461\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 493732.5000 - mae: 637.5760 - val_loss: 417002.7812 - val_mae: 502.3164\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 500383.1250 - mae: 640.7268 - val_loss: 408220.0938 - val_mae: 493.2982\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 503231.1250 - mae: 642.9971 - val_loss: 407999.0938 - val_mae: 493.4539\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 501568.0000 - mae: 641.9282 - val_loss: 424481.8750 - val_mae: 510.3350\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 501937.5938 - mae: 642.7034 - val_loss: 417189.9062 - val_mae: 502.4211\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 495558.9375 - mae: 640.7703 - val_loss: 415904.6250 - val_mae: 501.7057\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 501160.3438 - mae: 642.1072 - val_loss: 416442.2812 - val_mae: 501.7296\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 501620.6875 - mae: 642.4027 - val_loss: 408339.2500 - val_mae: 493.6275\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 503298.7500 - mae: 644.2998 - val_loss: 417258.8438 - val_mae: 502.2038\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 494496.5625 - mae: 638.0291 - val_loss: 408060.6562 - val_mae: 493.7557\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 498638.5000 - mae: 639.6827 - val_loss: 432669.8750 - val_mae: 518.1806\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 501009.8750 - mae: 640.7080 - val_loss: 416983.4062 - val_mae: 502.3008\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 496777.4688 - mae: 639.3279 - val_loss: 416616.8750 - val_mae: 501.8311\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 7s 157ms/step - loss: 501112.0312 - mae: 641.5631 - val_loss: 391135.0000 - val_mae: 476.6222\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 504269.2500 - mae: 643.4670 - val_loss: 407410.0312 - val_mae: 493.3974\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 503012.9375 - mae: 643.5072 - val_loss: 391968.1250 - val_mae: 477.0882\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 501629.3125 - mae: 641.9920 - val_loss: 407298.5938 - val_mae: 493.3360\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 502934.7188 - mae: 643.5399 - val_loss: 408263.2500 - val_mae: 493.8672\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 501964.8750 - mae: 641.8553 - val_loss: 415913.0000 - val_mae: 501.1405\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 505592.4688 - mae: 644.9406 - val_loss: 398752.5000 - val_mae: 484.7218\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 502061.4062 - mae: 641.5931 - val_loss: 425015.0000 - val_mae: 510.6288\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 502733.8438 - mae: 644.7400 - val_loss: 408414.0000 - val_mae: 493.6806\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 496434.6250 - mae: 638.5430 - val_loss: 407036.1562 - val_mae: 493.5302\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 503091.5312 - mae: 642.9307 - val_loss: 399779.9062 - val_mae: 485.0108\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 503308.6875 - mae: 644.1310 - val_loss: 424930.8750 - val_mae: 510.3134\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 5s 99ms/step - loss: 495108.9062 - mae: 638.8361 - val_loss: 407695.9688 - val_mae: 492.9875\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 498980.5625 - mae: 640.1131 - val_loss: 406809.3750 - val_mae: 492.7621\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 504056.5312 - mae: 644.8352 - val_loss: 425975.2500 - val_mae: 510.6433\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 498322.2500 - mae: 640.6484 - val_loss: 425559.7188 - val_mae: 510.9282\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 499095.0312 - mae: 640.1456 - val_loss: 408146.3750 - val_mae: 493.5276\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 495415.6562 - mae: 638.0491 - val_loss: 416516.2812 - val_mae: 502.0438\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 504884.5000 - mae: 644.7850 - val_loss: 424862.2812 - val_mae: 510.6117\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 502387.4062 - mae: 642.2144 - val_loss: 416287.7188 - val_mae: 502.1941\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 501248.5938 - mae: 641.5112 - val_loss: 424667.9688 - val_mae: 510.4373\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 501705.1250 - mae: 641.7956 - val_loss: 406698.6562 - val_mae: 492.7206\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 501736.8438 - mae: 642.2035 - val_loss: 417665.8750 - val_mae: 502.9372\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 499529.4688 - mae: 642.3739 - val_loss: 425530.9062 - val_mae: 510.6554\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 500179.2500 - mae: 641.1558 - val_loss: 401851.8438 - val_mae: 486.4441\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 499762.5312 - mae: 640.9561 - val_loss: 417714.2812 - val_mae: 502.7100\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 504515.1875 - mae: 644.2324 - val_loss: 418331.0938 - val_mae: 502.5680\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 492936.0312 - mae: 636.8583 - val_loss: 416550.7500 - val_mae: 502.0631\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 502900.0625 - mae: 643.2319 - val_loss: 417644.0938 - val_mae: 502.1814\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 10s 257ms/step - loss: 502170.2188 - mae: 642.8309 - val_loss: 409316.9688 - val_mae: 494.1957\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 506996.0000 - mae: 646.4952 - val_loss: 417123.1250 - val_mae: 502.6448\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 498943.0000 - mae: 640.4249 - val_loss: 417563.8750 - val_mae: 502.1333\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 498773.1562 - mae: 639.6534 - val_loss: 407537.9062 - val_mae: 493.2009\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 499125.3750 - mae: 641.3041 - val_loss: 418040.8438 - val_mae: 502.6570\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 498970.2812 - mae: 642.0239 - val_loss: 400275.7188 - val_mae: 485.5765\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 501653.0000 - mae: 642.4095 - val_loss: 409338.6250 - val_mae: 494.2180\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 10s 259ms/step - loss: 501856.5938 - mae: 642.3215 - val_loss: 401226.9062 - val_mae: 485.8840\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 500844.6562 - mae: 641.4750 - val_loss: 408830.6250 - val_mae: 494.1802\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 499027.1875 - mae: 641.0977 - val_loss: 418167.4688 - val_mae: 502.9633\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 501244.0312 - mae: 641.3177 - val_loss: 408438.0000 - val_mae: 493.4274\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 498350.4062 - mae: 640.1556 - val_loss: 424963.0938 - val_mae: 510.3219\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 495390.8438 - mae: 637.8085 - val_loss: 424739.4062 - val_mae: 509.9342\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 489958.3125 - mae: 636.7457 - val_loss: 399779.8438 - val_mae: 485.2882\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 503518.6875 - mae: 643.5970 - val_loss: 414298.0000 - val_mae: 500.8024\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 500539.8125 - mae: 641.2357 - val_loss: 406323.9688 - val_mae: 492.7980\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 503219.5000 - mae: 643.3781 - val_loss: 405712.8750 - val_mae: 492.1436\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 496271.8438 - mae: 640.4955 - val_loss: 388913.2812 - val_mae: 475.3571\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 501036.6875 - mae: 641.6244 - val_loss: 406281.2188 - val_mae: 492.7744\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 499713.4375 - mae: 640.8059 - val_loss: 398661.2188 - val_mae: 484.6648\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 494824.9688 - mae: 637.1696 - val_loss: 423011.5938 - val_mae: 509.2059\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 505732.0000 - mae: 645.0080 - val_loss: 407500.0312 - val_mae: 493.1579\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 495706.9375 - mae: 637.6781 - val_loss: 424345.0000 - val_mae: 510.2595\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 500603.8125 - mae: 641.1230 - val_loss: 408699.3438 - val_mae: 493.8435\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 493890.0938 - mae: 637.9818 - val_loss: 408557.2188 - val_mae: 494.0287\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 504001.0312 - mae: 644.1562 - val_loss: 426108.6250 - val_mae: 511.4870\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501365.9375 - mae: 641.4617 - val_loss: 407670.0000 - val_mae: 493.6032\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 502055.2500 - mae: 642.0649 - val_loss: 390702.8438 - val_mae: 476.3969\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 10s 260ms/step - loss: 500228.9375 - mae: 640.5164 - val_loss: 416104.3750 - val_mae: 501.8701\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 499495.0938 - mae: 640.9669 - val_loss: 390649.6562 - val_mae: 476.3665\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 499178.0625 - mae: 640.1821 - val_loss: 415049.5312 - val_mae: 500.9345\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 500426.0000 - mae: 642.2394 - val_loss: 398176.5938 - val_mae: 484.3919\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 501679.8438 - mae: 641.1873 - val_loss: 422993.9062 - val_mae: 508.9011\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 498364.3750 - mae: 639.9587 - val_loss: 424874.0000 - val_mae: 510.2711\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 503273.9062 - mae: 642.9464 - val_loss: 390712.8750 - val_mae: 476.3801\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 484263.0938 - mae: 634.2083 - val_loss: 400618.5000 - val_mae: 485.7538\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 504142.8438 - mae: 643.5580 - val_loss: 399213.3438 - val_mae: 484.6967\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 501131.3125 - mae: 641.2959 - val_loss: 434354.5000 - val_mae: 519.6668\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 500848.0000 - mae: 641.9244 - val_loss: 425185.4062 - val_mae: 510.4585\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 496741.3438 - mae: 639.0812 - val_loss: 410002.7500 - val_mae: 494.5765\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 499252.9375 - mae: 641.5353 - val_loss: 427010.0938 - val_mae: 511.4870\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 499179.9375 - mae: 640.4614 - val_loss: 417103.7500 - val_mae: 502.6344\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 501870.8438 - mae: 642.3074 - val_loss: 399015.2188 - val_mae: 484.8695\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 497533.4688 - mae: 642.0879 - val_loss: 408484.2812 - val_mae: 493.4757\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 502694.7188 - mae: 642.9866 - val_loss: 425235.7812 - val_mae: 510.8149\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 502697.5625 - mae: 643.5588 - val_loss: 408143.9062 - val_mae: 493.8015\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 495460.9375 - mae: 639.4744 - val_loss: 408608.6562 - val_mae: 494.0570\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 503103.4375 - mae: 643.3417 - val_loss: 418361.5000 - val_mae: 503.3176\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 503083.8750 - mae: 642.6773 - val_loss: 417083.5938 - val_mae: 502.3569\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 494927.5625 - mae: 639.2614 - val_loss: 409879.7188 - val_mae: 494.7551\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 502237.8125 - mae: 642.4937 - val_loss: 416342.0000 - val_mae: 501.3989\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 498276.1250 - mae: 639.7498 - val_loss: 407730.8750 - val_mae: 493.5739\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 502173.5312 - mae: 642.9800 - val_loss: 425389.9688 - val_mae: 510.2981\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 501585.5000 - mae: 642.0070 - val_loss: 416450.9688 - val_mae: 501.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f20e1d-61d9-49e7-c065-25bc88aeb6cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1481321.375,\n",
              "  1474076.625,\n",
              "  1457853.0,\n",
              "  1423413.5,\n",
              "  1396449.875,\n",
              "  1336510.0,\n",
              "  1316484.375,\n",
              "  1305982.125,\n",
              "  1284436.75,\n",
              "  1260839.625,\n",
              "  1234799.875,\n",
              "  1208802.0,\n",
              "  1209818.625,\n",
              "  1173324.5,\n",
              "  1143379.375,\n",
              "  1134426.75,\n",
              "  1121053.25,\n",
              "  1067697.875,\n",
              "  1075694.125,\n",
              "  1066550.75,\n",
              "  1032778.25,\n",
              "  1007561.6875,\n",
              "  980235.125,\n",
              "  977750.0,\n",
              "  946645.4375,\n",
              "  942354.5,\n",
              "  930726.5,\n",
              "  922337.25,\n",
              "  907338.1875,\n",
              "  894174.125,\n",
              "  871912.3125,\n",
              "  863624.625,\n",
              "  833204.9375,\n",
              "  834372.25,\n",
              "  814198.0,\n",
              "  790830.0625,\n",
              "  794782.375,\n",
              "  784753.0625,\n",
              "  769167.125,\n",
              "  752968.875,\n",
              "  762385.875,\n",
              "  743576.1875,\n",
              "  738254.125,\n",
              "  716434.9375,\n",
              "  704893.6875,\n",
              "  681096.875,\n",
              "  696378.25,\n",
              "  688297.0,\n",
              "  677778.8125,\n",
              "  668336.5625,\n",
              "  660818.4375,\n",
              "  651387.875,\n",
              "  652328.3125,\n",
              "  646441.75,\n",
              "  626989.0625,\n",
              "  627030.1875,\n",
              "  627093.75,\n",
              "  598585.5625,\n",
              "  618008.3125,\n",
              "  594900.8125,\n",
              "  594163.25,\n",
              "  585822.875,\n",
              "  583946.4375,\n",
              "  584362.1875,\n",
              "  580550.6875,\n",
              "  578879.0,\n",
              "  574638.3125,\n",
              "  559927.6875,\n",
              "  567648.875,\n",
              "  559168.0,\n",
              "  561060.6875,\n",
              "  558411.4375,\n",
              "  552673.9375,\n",
              "  545291.8125,\n",
              "  551109.6875,\n",
              "  541489.625,\n",
              "  532233.1875,\n",
              "  531927.6875,\n",
              "  540010.375,\n",
              "  536483.5,\n",
              "  525075.4375,\n",
              "  533628.9375,\n",
              "  522300.46875,\n",
              "  524684.5625,\n",
              "  519868.8125,\n",
              "  522943.4375,\n",
              "  518939.59375,\n",
              "  523442.6875,\n",
              "  521113.125,\n",
              "  516423.96875,\n",
              "  512976.34375,\n",
              "  512035.96875,\n",
              "  515763.5625,\n",
              "  515692.3125,\n",
              "  509383.6875,\n",
              "  511734.875,\n",
              "  513276.15625,\n",
              "  510793.90625,\n",
              "  507266.53125,\n",
              "  505601.03125,\n",
              "  500380.8125,\n",
              "  512711.53125,\n",
              "  506560.375,\n",
              "  508321.21875,\n",
              "  500555.4375,\n",
              "  494482.375,\n",
              "  502967.96875,\n",
              "  504971.375,\n",
              "  503687.375,\n",
              "  504542.125,\n",
              "  502679.8125,\n",
              "  505128.53125,\n",
              "  495868.46875,\n",
              "  503123.3125,\n",
              "  502063.78125,\n",
              "  503801.71875,\n",
              "  504454.28125,\n",
              "  501958.8125,\n",
              "  498513.21875,\n",
              "  505806.1875,\n",
              "  500359.75,\n",
              "  495042.625,\n",
              "  499683.75,\n",
              "  493593.90625,\n",
              "  503142.0625,\n",
              "  503221.21875,\n",
              "  498994.90625,\n",
              "  499200.65625,\n",
              "  505030.5,\n",
              "  501163.4375,\n",
              "  499797.9375,\n",
              "  497815.96875,\n",
              "  498704.9375,\n",
              "  501553.3125,\n",
              "  495157.34375,\n",
              "  499377.0,\n",
              "  498846.96875,\n",
              "  500754.59375,\n",
              "  499592.5625,\n",
              "  503753.25,\n",
              "  501071.34375,\n",
              "  496299.3125,\n",
              "  501613.90625,\n",
              "  498594.03125,\n",
              "  501297.21875,\n",
              "  499277.90625,\n",
              "  505351.53125,\n",
              "  506663.6875,\n",
              "  503918.0625,\n",
              "  501688.90625,\n",
              "  502024.3125,\n",
              "  502223.40625,\n",
              "  503536.875,\n",
              "  500775.25,\n",
              "  502658.15625,\n",
              "  500211.90625,\n",
              "  502764.4375,\n",
              "  502601.84375,\n",
              "  497467.21875,\n",
              "  502246.59375,\n",
              "  496301.96875,\n",
              "  504773.15625,\n",
              "  501786.0,\n",
              "  500953.78125,\n",
              "  496485.0625,\n",
              "  497150.03125,\n",
              "  497409.15625,\n",
              "  498998.0,\n",
              "  495744.0625,\n",
              "  502839.46875,\n",
              "  498706.6875,\n",
              "  499907.6875,\n",
              "  499056.4375,\n",
              "  496089.90625,\n",
              "  501945.6875,\n",
              "  498701.3125,\n",
              "  501375.34375,\n",
              "  497359.5,\n",
              "  505956.1875,\n",
              "  498371.09375,\n",
              "  501558.875,\n",
              "  501318.53125,\n",
              "  499086.96875,\n",
              "  501284.5,\n",
              "  501098.84375,\n",
              "  497944.25,\n",
              "  501137.9375,\n",
              "  497823.5,\n",
              "  505251.46875,\n",
              "  501249.96875,\n",
              "  501813.59375,\n",
              "  497472.21875,\n",
              "  502941.6875,\n",
              "  501809.25,\n",
              "  502633.5,\n",
              "  493019.8125,\n",
              "  505011.40625,\n",
              "  502571.71875,\n",
              "  501990.75,\n",
              "  501519.0,\n",
              "  490581.375,\n",
              "  500271.71875,\n",
              "  499256.46875,\n",
              "  498556.65625,\n",
              "  495459.03125,\n",
              "  485420.90625,\n",
              "  501824.28125,\n",
              "  502111.28125,\n",
              "  498544.0625,\n",
              "  502786.8125,\n",
              "  500196.34375,\n",
              "  498520.625,\n",
              "  501546.15625,\n",
              "  503423.1875,\n",
              "  503117.8125,\n",
              "  497506.09375,\n",
              "  495756.90625,\n",
              "  496679.1875,\n",
              "  502519.6875,\n",
              "  495872.71875,\n",
              "  502360.40625,\n",
              "  501231.40625,\n",
              "  499632.375,\n",
              "  495789.96875,\n",
              "  502344.75,\n",
              "  492857.9375,\n",
              "  498853.34375,\n",
              "  502047.34375,\n",
              "  501800.625,\n",
              "  501017.5625,\n",
              "  499152.8125,\n",
              "  500463.5625,\n",
              "  489246.46875,\n",
              "  500648.8125,\n",
              "  502656.4375,\n",
              "  500505.9375,\n",
              "  504291.125,\n",
              "  498478.46875,\n",
              "  496556.46875,\n",
              "  492110.75,\n",
              "  498973.15625,\n",
              "  500109.75,\n",
              "  502040.09375,\n",
              "  500398.0625,\n",
              "  491826.625,\n",
              "  503545.40625,\n",
              "  495862.4375,\n",
              "  496817.15625,\n",
              "  497070.125,\n",
              "  498587.0625,\n",
              "  503472.4375,\n",
              "  505067.28125,\n",
              "  500863.21875,\n",
              "  498974.40625,\n",
              "  502062.96875,\n",
              "  502606.03125,\n",
              "  499178.09375,\n",
              "  501877.0,\n",
              "  499256.09375,\n",
              "  499465.0625,\n",
              "  501306.28125,\n",
              "  501092.21875,\n",
              "  497970.8125,\n",
              "  499727.21875,\n",
              "  502864.375,\n",
              "  499873.0,\n",
              "  495783.75,\n",
              "  504705.8125,\n",
              "  502749.8125,\n",
              "  499761.5,\n",
              "  500258.375,\n",
              "  497924.78125,\n",
              "  496355.6875,\n",
              "  502176.8125,\n",
              "  503499.78125,\n",
              "  504159.84375,\n",
              "  496873.5625,\n",
              "  501159.25,\n",
              "  497990.9375,\n",
              "  505388.4375,\n",
              "  499646.84375,\n",
              "  500990.5,\n",
              "  503913.5625,\n",
              "  496618.21875,\n",
              "  499141.4375,\n",
              "  499807.4375,\n",
              "  502862.03125,\n",
              "  498261.71875,\n",
              "  500556.75,\n",
              "  498005.4375,\n",
              "  494182.0625,\n",
              "  500734.28125,\n",
              "  499381.0625,\n",
              "  500692.6875,\n",
              "  501331.09375,\n",
              "  495157.5625,\n",
              "  498195.625,\n",
              "  495832.125,\n",
              "  506767.65625,\n",
              "  501842.46875,\n",
              "  501890.6875,\n",
              "  503868.96875,\n",
              "  503042.84375,\n",
              "  499187.6875,\n",
              "  503637.71875,\n",
              "  503315.625,\n",
              "  504471.4375,\n",
              "  495661.96875,\n",
              "  498139.15625,\n",
              "  498087.90625,\n",
              "  503561.71875,\n",
              "  505437.625,\n",
              "  500708.40625,\n",
              "  502994.09375,\n",
              "  502554.5,\n",
              "  502938.4375,\n",
              "  494776.25,\n",
              "  501779.6875,\n",
              "  500758.3125,\n",
              "  498506.125,\n",
              "  503720.6875,\n",
              "  502469.5625,\n",
              "  490109.1875,\n",
              "  504161.9375,\n",
              "  505721.78125,\n",
              "  503050.34375,\n",
              "  501838.46875,\n",
              "  502486.09375,\n",
              "  502866.15625,\n",
              "  502052.0,\n",
              "  498518.59375,\n",
              "  486662.71875,\n",
              "  501341.375,\n",
              "  502399.34375,\n",
              "  503915.0625,\n",
              "  501679.65625,\n",
              "  502136.625,\n",
              "  496557.40625,\n",
              "  500171.65625,\n",
              "  503655.6875,\n",
              "  502854.8125,\n",
              "  492158.34375,\n",
              "  499832.84375,\n",
              "  497896.90625,\n",
              "  503128.34375,\n",
              "  494093.90625,\n",
              "  498299.8125,\n",
              "  500861.53125,\n",
              "  497626.78125,\n",
              "  501614.46875,\n",
              "  500192.8125,\n",
              "  498189.625,\n",
              "  500853.21875,\n",
              "  499852.4375,\n",
              "  496474.21875,\n",
              "  503751.6875,\n",
              "  496460.96875,\n",
              "  504620.4375,\n",
              "  501011.75,\n",
              "  502508.75,\n",
              "  502748.65625,\n",
              "  497535.78125,\n",
              "  498819.34375,\n",
              "  503764.5625,\n",
              "  502699.03125,\n",
              "  503335.15625,\n",
              "  501096.5625,\n",
              "  498508.15625,\n",
              "  498334.1875,\n",
              "  501904.28125,\n",
              "  500838.21875,\n",
              "  499562.21875,\n",
              "  497934.25,\n",
              "  504840.46875,\n",
              "  499223.25,\n",
              "  496138.84375,\n",
              "  503669.34375,\n",
              "  499261.15625,\n",
              "  498299.65625,\n",
              "  503078.4375,\n",
              "  500890.84375,\n",
              "  501904.4375,\n",
              "  493503.28125,\n",
              "  501791.1875,\n",
              "  499003.3125,\n",
              "  503346.96875,\n",
              "  502393.28125,\n",
              "  497583.78125,\n",
              "  493122.8125,\n",
              "  500306.375,\n",
              "  496194.25,\n",
              "  500919.1875,\n",
              "  502786.25,\n",
              "  496048.5,\n",
              "  494260.90625,\n",
              "  502679.03125,\n",
              "  498198.21875,\n",
              "  504369.3125,\n",
              "  496951.25,\n",
              "  497241.25,\n",
              "  498664.3125,\n",
              "  500733.46875,\n",
              "  503555.34375,\n",
              "  500540.75,\n",
              "  494410.875,\n",
              "  501685.0,\n",
              "  494304.4375,\n",
              "  503443.1875,\n",
              "  504490.875,\n",
              "  500475.65625,\n",
              "  499914.15625,\n",
              "  499373.625,\n",
              "  494513.0,\n",
              "  498556.96875,\n",
              "  499767.875,\n",
              "  501675.65625,\n",
              "  493875.03125,\n",
              "  500423.8125,\n",
              "  497705.5625,\n",
              "  501803.59375,\n",
              "  499643.375,\n",
              "  493818.625,\n",
              "  500995.34375,\n",
              "  498042.78125,\n",
              "  498682.0625,\n",
              "  503329.375,\n",
              "  501307.78125,\n",
              "  491940.46875,\n",
              "  496021.875,\n",
              "  504522.5,\n",
              "  496708.125,\n",
              "  505554.59375,\n",
              "  501137.15625,\n",
              "  501119.40625,\n",
              "  500765.46875,\n",
              "  503688.3125,\n",
              "  501799.65625,\n",
              "  493004.3125,\n",
              "  501857.96875,\n",
              "  504066.09375,\n",
              "  498913.25,\n",
              "  497802.5625,\n",
              "  500896.34375,\n",
              "  506193.59375,\n",
              "  501087.625,\n",
              "  500794.84375,\n",
              "  499482.4375,\n",
              "  506684.4375,\n",
              "  498138.625,\n",
              "  503649.75,\n",
              "  505036.59375,\n",
              "  495411.5625,\n",
              "  502623.84375,\n",
              "  507273.25,\n",
              "  498410.21875,\n",
              "  499141.625,\n",
              "  495636.34375,\n",
              "  498563.03125,\n",
              "  503048.6875,\n",
              "  498744.03125,\n",
              "  499262.34375,\n",
              "  503156.9375,\n",
              "  491837.25,\n",
              "  497992.90625,\n",
              "  505711.875,\n",
              "  498165.9375,\n",
              "  495640.84375,\n",
              "  502431.28125,\n",
              "  494864.71875,\n",
              "  500592.8125,\n",
              "  502968.90625,\n",
              "  499233.96875,\n",
              "  495553.71875,\n",
              "  503087.21875,\n",
              "  501626.78125,\n",
              "  490557.46875,\n",
              "  492471.375,\n",
              "  502009.0625,\n",
              "  503044.5625,\n",
              "  499602.90625,\n",
              "  501165.53125,\n",
              "  501711.0625,\n",
              "  498619.8125,\n",
              "  497648.8125,\n",
              "  503380.90625,\n",
              "  503088.5,\n",
              "  503306.15625,\n",
              "  505961.71875,\n",
              "  499790.1875,\n",
              "  500723.75,\n",
              "  504413.25,\n",
              "  499139.03125,\n",
              "  503479.59375,\n",
              "  503134.28125,\n",
              "  492401.71875,\n",
              "  501475.40625,\n",
              "  502921.625,\n",
              "  500146.8125,\n",
              "  502591.21875,\n",
              "  498797.8125,\n",
              "  498128.78125,\n",
              "  499424.34375,\n",
              "  499440.4375,\n",
              "  503854.5625,\n",
              "  504566.8125,\n",
              "  493104.0625,\n",
              "  499646.28125,\n",
              "  502238.34375,\n",
              "  499043.25,\n",
              "  500795.3125,\n",
              "  498710.59375,\n",
              "  505984.15625,\n",
              "  501819.375,\n",
              "  492860.0,\n",
              "  499879.03125,\n",
              "  499545.25,\n",
              "  502226.84375,\n",
              "  503140.71875,\n",
              "  497337.46875,\n",
              "  502490.21875,\n",
              "  499336.90625,\n",
              "  503151.1875,\n",
              "  498324.71875,\n",
              "  500022.65625,\n",
              "  501658.5,\n",
              "  497309.8125,\n",
              "  503119.5,\n",
              "  499402.0,\n",
              "  503280.875,\n",
              "  500687.5625,\n",
              "  502460.8125,\n",
              "  498028.875,\n",
              "  499624.84375,\n",
              "  502532.78125,\n",
              "  501430.4375,\n",
              "  498622.125,\n",
              "  498630.21875,\n",
              "  502069.21875,\n",
              "  502807.59375,\n",
              "  500424.8125,\n",
              "  495796.625,\n",
              "  498992.375,\n",
              "  501502.625,\n",
              "  499794.625,\n",
              "  502668.03125,\n",
              "  495166.75,\n",
              "  501672.0625,\n",
              "  494783.34375,\n",
              "  501424.5,\n",
              "  502195.1875,\n",
              "  505722.28125,\n",
              "  498679.1875,\n",
              "  498344.1875,\n",
              "  497266.75,\n",
              "  501502.1875,\n",
              "  500972.6875,\n",
              "  502541.40625,\n",
              "  495700.5625,\n",
              "  496104.1875,\n",
              "  498774.8125,\n",
              "  492106.78125,\n",
              "  500894.6875,\n",
              "  492116.40625,\n",
              "  504034.59375,\n",
              "  501427.5625,\n",
              "  501955.03125,\n",
              "  501689.03125,\n",
              "  506627.25,\n",
              "  497505.21875,\n",
              "  501039.84375,\n",
              "  502540.65625,\n",
              "  496370.8125,\n",
              "  500455.625,\n",
              "  493201.3125,\n",
              "  498412.0,\n",
              "  496841.84375,\n",
              "  504577.9375,\n",
              "  496649.25,\n",
              "  501563.65625,\n",
              "  501939.625,\n",
              "  498769.09375,\n",
              "  499129.34375,\n",
              "  495961.28125,\n",
              "  497566.125,\n",
              "  495890.25,\n",
              "  496003.53125,\n",
              "  501404.8125,\n",
              "  498252.90625,\n",
              "  498733.90625,\n",
              "  501497.84375,\n",
              "  499739.09375,\n",
              "  504631.375,\n",
              "  495107.84375,\n",
              "  499464.09375,\n",
              "  502350.6875,\n",
              "  500679.4375,\n",
              "  500492.6875,\n",
              "  503865.34375,\n",
              "  503981.8125,\n",
              "  503941.875,\n",
              "  492492.3125,\n",
              "  502995.53125,\n",
              "  503184.8125,\n",
              "  501737.03125,\n",
              "  499974.3125,\n",
              "  501086.5,\n",
              "  500122.65625,\n",
              "  498155.65625,\n",
              "  499041.21875,\n",
              "  498574.34375,\n",
              "  496402.1875,\n",
              "  498437.0,\n",
              "  501995.78125,\n",
              "  497516.6875,\n",
              "  503778.96875,\n",
              "  500369.09375,\n",
              "  502656.5,\n",
              "  501873.65625,\n",
              "  497794.90625,\n",
              "  498520.34375,\n",
              "  494631.53125,\n",
              "  503549.40625,\n",
              "  499187.84375,\n",
              "  501938.25,\n",
              "  505372.53125,\n",
              "  504214.8125,\n",
              "  503040.5625,\n",
              "  504050.90625,\n",
              "  503760.5,\n",
              "  502949.15625,\n",
              "  499877.0,\n",
              "  503036.59375,\n",
              "  502196.125,\n",
              "  496218.625,\n",
              "  502553.46875,\n",
              "  489961.5625,\n",
              "  499406.25,\n",
              "  505758.1875,\n",
              "  497351.3125,\n",
              "  502205.8125,\n",
              "  502854.3125,\n",
              "  498611.03125,\n",
              "  501756.4375,\n",
              "  501244.75,\n",
              "  501916.09375,\n",
              "  498409.1875,\n",
              "  503768.6875,\n",
              "  500854.0625,\n",
              "  503167.84375,\n",
              "  504302.96875,\n",
              "  498000.9375,\n",
              "  498429.53125,\n",
              "  496791.46875,\n",
              "  503774.78125,\n",
              "  498399.34375,\n",
              "  498066.3125,\n",
              "  505963.71875,\n",
              "  505556.6875,\n",
              "  498576.71875,\n",
              "  501866.0625,\n",
              "  499860.34375,\n",
              "  501949.59375,\n",
              "  502323.125,\n",
              "  497375.0625,\n",
              "  503749.84375,\n",
              "  502117.78125,\n",
              "  501520.0625,\n",
              "  498687.84375,\n",
              "  502099.90625,\n",
              "  504324.9375,\n",
              "  505829.15625,\n",
              "  493989.71875,\n",
              "  496048.34375,\n",
              "  502676.21875,\n",
              "  502300.25,\n",
              "  504235.28125,\n",
              "  498913.75,\n",
              "  502078.6875,\n",
              "  500925.96875,\n",
              "  498335.21875,\n",
              "  498712.25,\n",
              "  500115.09375,\n",
              "  499110.8125,\n",
              "  497083.5,\n",
              "  501281.65625,\n",
              "  502439.6875,\n",
              "  502285.15625,\n",
              "  500466.46875,\n",
              "  502080.28125,\n",
              "  503266.46875,\n",
              "  500828.0,\n",
              "  500517.125,\n",
              "  504625.21875,\n",
              "  502162.3125,\n",
              "  504496.21875,\n",
              "  500345.5625,\n",
              "  501360.5625,\n",
              "  499012.78125,\n",
              "  502558.78125,\n",
              "  503128.03125,\n",
              "  500531.5625,\n",
              "  490167.46875,\n",
              "  501455.78125,\n",
              "  492750.40625,\n",
              "  500069.375,\n",
              "  504057.0625,\n",
              "  500933.375,\n",
              "  502597.65625,\n",
              "  499120.28125,\n",
              "  500177.59375,\n",
              "  499562.875,\n",
              "  499541.0625,\n",
              "  496044.0,\n",
              "  501664.78125,\n",
              "  499653.28125,\n",
              "  497883.21875,\n",
              "  496937.40625,\n",
              "  495555.78125,\n",
              "  502697.125,\n",
              "  503452.4375,\n",
              "  499825.15625,\n",
              "  497296.28125,\n",
              "  500127.4375,\n",
              "  502382.46875,\n",
              "  504065.59375,\n",
              "  494807.65625,\n",
              "  492603.3125,\n",
              "  495817.84375,\n",
              "  500682.71875,\n",
              "  503190.65625,\n",
              "  498022.65625,\n",
              "  502209.96875,\n",
              "  499960.53125,\n",
              "  501716.84375,\n",
              "  495328.125,\n",
              "  503571.90625,\n",
              "  499949.53125,\n",
              "  499563.15625,\n",
              "  503420.53125,\n",
              "  501987.78125,\n",
              "  497390.40625,\n",
              "  501546.34375,\n",
              "  501623.25,\n",
              "  499098.0,\n",
              "  500469.28125,\n",
              "  499104.875,\n",
              "  503427.40625,\n",
              "  502169.1875,\n",
              "  505769.6875,\n",
              "  494764.875,\n",
              "  499123.40625,\n",
              "  500373.4375,\n",
              "  501117.25,\n",
              "  504025.28125,\n",
              "  499813.28125,\n",
              "  502260.5625,\n",
              "  503083.21875,\n",
              "  499974.9375,\n",
              "  502121.9375,\n",
              "  500599.6875,\n",
              "  501882.4375,\n",
              "  504204.8125,\n",
              "  507605.84375,\n",
              "  500756.84375,\n",
              "  501411.34375,\n",
              "  494527.65625,\n",
              "  504646.8125,\n",
              "  499964.4375,\n",
              "  501186.375,\n",
              "  496245.375,\n",
              "  498219.78125,\n",
              "  500182.375,\n",
              "  501267.3125,\n",
              "  498987.5,\n",
              "  502223.125,\n",
              "  497284.1875,\n",
              "  500720.21875,\n",
              "  497792.5,\n",
              "  503480.53125,\n",
              "  492033.75,\n",
              "  500061.3125,\n",
              "  503969.03125,\n",
              "  497208.25,\n",
              "  501203.125,\n",
              "  501511.25,\n",
              "  505600.21875,\n",
              "  502374.53125,\n",
              "  497218.03125,\n",
              "  502243.09375,\n",
              "  500679.875,\n",
              "  497760.9375,\n",
              "  501984.5625,\n",
              "  501544.40625,\n",
              "  502899.625,\n",
              "  489631.71875,\n",
              "  492018.6875,\n",
              "  494315.875,\n",
              "  501066.15625,\n",
              "  500866.6875,\n",
              "  503913.5625,\n",
              "  495616.21875,\n",
              "  499159.4375,\n",
              "  504967.96875,\n",
              "  502431.40625,\n",
              "  491249.8125,\n",
              "  496912.375,\n",
              "  503706.125,\n",
              "  503025.09375,\n",
              "  491207.90625,\n",
              "  501741.59375,\n",
              "  497916.59375,\n",
              "  503225.46875,\n",
              "  504735.71875,\n",
              "  498841.1875,\n",
              "  493372.4375,\n",
              "  500715.78125,\n",
              "  499161.78125,\n",
              "  501241.1875,\n",
              "  501875.34375,\n",
              "  501611.09375,\n",
              "  503968.125,\n",
              "  500748.03125,\n",
              "  502573.96875,\n",
              "  501425.875,\n",
              "  502277.5,\n",
              "  499411.09375,\n",
              "  502090.65625,\n",
              "  500655.9375,\n",
              "  503991.90625,\n",
              "  495683.5625,\n",
              "  501340.875,\n",
              "  495066.28125,\n",
              "  497183.0625,\n",
              "  498976.375,\n",
              "  502137.9375,\n",
              "  493375.4375,\n",
              "  500644.34375,\n",
              "  500276.6875,\n",
              "  497297.59375,\n",
              "  496276.0625,\n",
              "  499842.96875,\n",
              "  501471.21875,\n",
              "  499740.09375,\n",
              "  501550.90625,\n",
              "  499680.0625,\n",
              "  498983.15625,\n",
              "  502020.46875,\n",
              "  503274.9375,\n",
              "  503251.03125,\n",
              "  500994.1875,\n",
              "  504719.71875,\n",
              "  505274.84375,\n",
              "  503329.65625,\n",
              "  493404.3125,\n",
              "  490732.4375,\n",
              "  500174.90625,\n",
              "  501315.90625,\n",
              "  503462.4375,\n",
              "  502836.625,\n",
              "  498394.9375,\n",
              "  499905.875,\n",
              "  501545.625,\n",
              "  500312.03125,\n",
              "  496810.65625,\n",
              "  499984.8125,\n",
              "  500468.21875,\n",
              "  501432.5625,\n",
              "  496712.09375,\n",
              "  500954.71875,\n",
              "  497044.21875,\n",
              "  501340.3125,\n",
              "  498838.875,\n",
              "  503657.03125,\n",
              "  500021.21875,\n",
              "  504503.875,\n",
              "  499249.09375,\n",
              "  498209.3125,\n",
              "  500542.5625,\n",
              "  502765.09375,\n",
              "  503982.40625,\n",
              "  497754.71875,\n",
              "  498104.25,\n",
              "  501701.84375,\n",
              "  501893.65625,\n",
              "  497339.09375,\n",
              "  501133.59375,\n",
              "  503896.625,\n",
              "  499796.5,\n",
              "  496103.4375,\n",
              "  499520.4375,\n",
              "  500535.25,\n",
              "  501553.3125,\n",
              "  499524.34375,\n",
              "  497771.0625,\n",
              "  498707.3125,\n",
              "  502637.15625,\n",
              "  504440.75,\n",
              "  503897.5625,\n",
              "  501677.375,\n",
              "  497222.53125,\n",
              "  500111.0,\n",
              "  503211.15625,\n",
              "  502679.65625,\n",
              "  503351.15625,\n",
              "  493732.5,\n",
              "  500383.125,\n",
              "  503231.125,\n",
              "  501568.0,\n",
              "  501937.59375,\n",
              "  495558.9375,\n",
              "  501160.34375,\n",
              "  501620.6875,\n",
              "  503298.75,\n",
              "  494496.5625,\n",
              "  498638.5,\n",
              "  501009.875,\n",
              "  496777.46875,\n",
              "  501112.03125,\n",
              "  504269.25,\n",
              "  503012.9375,\n",
              "  501629.3125,\n",
              "  502934.71875,\n",
              "  501964.875,\n",
              "  505592.46875,\n",
              "  502061.40625,\n",
              "  502733.84375,\n",
              "  496434.625,\n",
              "  503091.53125,\n",
              "  503308.6875,\n",
              "  495108.90625,\n",
              "  498980.5625,\n",
              "  504056.53125,\n",
              "  498322.25,\n",
              "  499095.03125,\n",
              "  495415.65625,\n",
              "  504884.5,\n",
              "  502387.40625,\n",
              "  501248.59375,\n",
              "  501705.125,\n",
              "  501736.84375,\n",
              "  499529.46875,\n",
              "  500179.25,\n",
              "  499762.53125,\n",
              "  504515.1875,\n",
              "  492936.03125,\n",
              "  502900.0625,\n",
              "  502170.21875,\n",
              "  506996.0,\n",
              "  498943.0,\n",
              "  498773.15625,\n",
              "  499125.375,\n",
              "  498970.28125,\n",
              "  501653.0,\n",
              "  501856.59375,\n",
              "  500844.65625,\n",
              "  499027.1875,\n",
              "  501244.03125,\n",
              "  498350.40625,\n",
              "  495390.84375,\n",
              "  489958.3125,\n",
              "  503518.6875,\n",
              "  500539.8125,\n",
              "  503219.5,\n",
              "  496271.84375,\n",
              "  501036.6875,\n",
              "  499713.4375,\n",
              "  494824.96875,\n",
              "  505732.0,\n",
              "  495706.9375,\n",
              "  500603.8125,\n",
              "  493890.09375,\n",
              "  504001.03125,\n",
              "  501365.9375,\n",
              "  502055.25,\n",
              "  500228.9375,\n",
              "  499495.09375,\n",
              "  499178.0625,\n",
              "  500426.0,\n",
              "  501679.84375,\n",
              "  498364.375,\n",
              "  503273.90625,\n",
              "  484263.09375,\n",
              "  504142.84375,\n",
              "  501131.3125,\n",
              "  500848.0,\n",
              "  496741.34375,\n",
              "  499252.9375,\n",
              "  499179.9375,\n",
              "  501870.84375,\n",
              "  497533.46875,\n",
              "  502694.71875,\n",
              "  502697.5625,\n",
              "  495460.9375,\n",
              "  503103.4375,\n",
              "  503083.875,\n",
              "  494927.5625,\n",
              "  502237.8125,\n",
              "  498276.125,\n",
              "  502173.53125,\n",
              "  501585.5],\n",
              " 'mae': [990.4717407226562,\n",
              "  985.0529174804688,\n",
              "  979.0602416992188,\n",
              "  962.9013061523438,\n",
              "  951.8233642578125,\n",
              "  923.6962280273438,\n",
              "  911.6162109375,\n",
              "  906.4769287109375,\n",
              "  893.5518798828125,\n",
              "  882.9258422851562,\n",
              "  871.7294921875,\n",
              "  861.4254760742188,\n",
              "  860.0437622070312,\n",
              "  840.1358642578125,\n",
              "  827.8753051757812,\n",
              "  823.5745849609375,\n",
              "  815.83740234375,\n",
              "  791.4215698242188,\n",
              "  793.8209228515625,\n",
              "  789.5389404296875,\n",
              "  770.1234130859375,\n",
              "  758.7399291992188,\n",
              "  743.5113525390625,\n",
              "  741.9452514648438,\n",
              "  729.1840209960938,\n",
              "  723.8631591796875,\n",
              "  720.2176513671875,\n",
              "  716.48876953125,\n",
              "  709.4122924804688,\n",
              "  703.4993286132812,\n",
              "  691.9656372070312,\n",
              "  689.445556640625,\n",
              "  674.4337158203125,\n",
              "  674.6561279296875,\n",
              "  668.8656616210938,\n",
              "  656.3380737304688,\n",
              "  663.3756713867188,\n",
              "  658.7174072265625,\n",
              "  652.5488891601562,\n",
              "  647.0789184570312,\n",
              "  656.7882690429688,\n",
              "  650.2097778320312,\n",
              "  651.0516967773438,\n",
              "  642.0323486328125,\n",
              "  637.5103759765625,\n",
              "  630.9681396484375,\n",
              "  639.9222412109375,\n",
              "  637.7682495117188,\n",
              "  635.0789794921875,\n",
              "  632.9730224609375,\n",
              "  631.8580322265625,\n",
              "  631.2266845703125,\n",
              "  631.9970092773438,\n",
              "  632.1466674804688,\n",
              "  620.8092651367188,\n",
              "  625.114013671875,\n",
              "  628.1414184570312,\n",
              "  613.1102294921875,\n",
              "  625.7179565429688,\n",
              "  616.5010986328125,\n",
              "  616.9452514648438,\n",
              "  616.4208984375,\n",
              "  614.351806640625,\n",
              "  619.9611206054688,\n",
              "  618.171630859375,\n",
              "  619.1517944335938,\n",
              "  618.8342895507812,\n",
              "  613.044921875,\n",
              "  618.6658325195312,\n",
              "  615.9183959960938,\n",
              "  620.157958984375,\n",
              "  619.94775390625,\n",
              "  618.2026977539062,\n",
              "  614.2769775390625,\n",
              "  621.6216430664062,\n",
              "  616.4148559570312,\n",
              "  614.0932006835938,\n",
              "  614.5122680664062,\n",
              "  621.869873046875,\n",
              "  621.6884155273438,\n",
              "  616.4487915039062,\n",
              "  625.3372802734375,\n",
              "  617.43701171875,\n",
              "  622.540283203125,\n",
              "  621.5195922851562,\n",
              "  623.4149780273438,\n",
              "  622.2939453125,\n",
              "  627.6559448242188,\n",
              "  627.7386474609375,\n",
              "  625.519287109375,\n",
              "  625.1290893554688,\n",
              "  627.0557250976562,\n",
              "  628.70703125,\n",
              "  631.0110473632812,\n",
              "  627.4607543945312,\n",
              "  630.4679565429688,\n",
              "  632.7666015625,\n",
              "  631.2781982421875,\n",
              "  629.7036743164062,\n",
              "  630.6500854492188,\n",
              "  627.3547973632812,\n",
              "  637.1283569335938,\n",
              "  633.3180541992188,\n",
              "  635.45654296875,\n",
              "  629.717041015625,\n",
              "  630.0859375,\n",
              "  632.6593017578125,\n",
              "  633.87060546875,\n",
              "  634.3046875,\n",
              "  635.6443481445312,\n",
              "  634.3815307617188,\n",
              "  636.48583984375,\n",
              "  632.154296875,\n",
              "  636.2918701171875,\n",
              "  636.4541015625,\n",
              "  637.8353881835938,\n",
              "  638.4003295898438,\n",
              "  637.0664672851562,\n",
              "  634.4092407226562,\n",
              "  640.7080688476562,\n",
              "  635.9695434570312,\n",
              "  634.9032592773438,\n",
              "  637.4614868164062,\n",
              "  635.4439697265625,\n",
              "  640.073486328125,\n",
              "  639.823486328125,\n",
              "  637.0011596679688,\n",
              "  637.5006713867188,\n",
              "  641.875244140625,\n",
              "  639.4198608398438,\n",
              "  639.5380859375,\n",
              "  639.1317138671875,\n",
              "  639.2842407226562,\n",
              "  639.4845581054688,\n",
              "  637.4842529296875,\n",
              "  639.5947265625,\n",
              "  638.42236328125,\n",
              "  642.2096557617188,\n",
              "  639.5328369140625,\n",
              "  642.6473388671875,\n",
              "  640.6961669921875,\n",
              "  639.7840576171875,\n",
              "  641.2326049804688,\n",
              "  639.5382080078125,\n",
              "  641.0028686523438,\n",
              "  639.5286254882812,\n",
              "  644.4598388671875,\n",
              "  645.3506469726562,\n",
              "  642.9743041992188,\n",
              "  641.4362182617188,\n",
              "  641.4542236328125,\n",
              "  641.9644165039062,\n",
              "  643.1287231445312,\n",
              "  641.5745239257812,\n",
              "  642.885498046875,\n",
              "  641.1271362304688,\n",
              "  641.8617553710938,\n",
              "  642.6702270507812,\n",
              "  638.5433349609375,\n",
              "  642.8391723632812,\n",
              "  637.3772583007812,\n",
              "  643.8935546875,\n",
              "  641.2655639648438,\n",
              "  641.8197631835938,\n",
              "  639.4822387695312,\n",
              "  641.1162109375,\n",
              "  639.033203125,\n",
              "  640.5543212890625,\n",
              "  639.0698852539062,\n",
              "  643.6904296875,\n",
              "  639.9623413085938,\n",
              "  640.3787841796875,\n",
              "  641.2747802734375,\n",
              "  638.6377563476562,\n",
              "  641.89501953125,\n",
              "  639.7571411132812,\n",
              "  642.0819091796875,\n",
              "  638.9482421875,\n",
              "  646.0136108398438,\n",
              "  640.1970825195312,\n",
              "  641.8397827148438,\n",
              "  642.0039672851562,\n",
              "  640.7750244140625,\n",
              "  641.4828491210938,\n",
              "  641.2533569335938,\n",
              "  639.2327270507812,\n",
              "  642.3984375,\n",
              "  641.0504760742188,\n",
              "  644.8153686523438,\n",
              "  641.2813110351562,\n",
              "  643.2332153320312,\n",
              "  639.7735595703125,\n",
              "  642.961669921875,\n",
              "  641.7925415039062,\n",
              "  642.8765869140625,\n",
              "  637.8820190429688,\n",
              "  645.03369140625,\n",
              "  643.06591796875,\n",
              "  642.4326171875,\n",
              "  642.1661376953125,\n",
              "  636.606201171875,\n",
              "  641.5724487304688,\n",
              "  640.6019897460938,\n",
              "  640.10302734375,\n",
              "  638.1272583007812,\n",
              "  636.0476684570312,\n",
              "  641.5228881835938,\n",
              "  641.8873901367188,\n",
              "  641.26318359375,\n",
              "  643.4216918945312,\n",
              "  641.2133178710938,\n",
              "  640.24658203125,\n",
              "  642.9681396484375,\n",
              "  643.6339721679688,\n",
              "  643.7584838867188,\n",
              "  638.9390258789062,\n",
              "  640.1006469726562,\n",
              "  639.4725341796875,\n",
              "  643.38818359375,\n",
              "  639.7705078125,\n",
              "  642.7578125,\n",
              "  642.1195068359375,\n",
              "  641.2223510742188,\n",
              "  639.3734741210938,\n",
              "  642.8944091796875,\n",
              "  638.4462890625,\n",
              "  639.8735961914062,\n",
              "  642.9465942382812,\n",
              "  643.3662719726562,\n",
              "  642.2359619140625,\n",
              "  640.5750732421875,\n",
              "  641.9185791015625,\n",
              "  638.1483154296875,\n",
              "  641.12744140625,\n",
              "  642.7838134765625,\n",
              "  641.6290893554688,\n",
              "  644.3408813476562,\n",
              "  639.6253051757812,\n",
              "  640.1612548828125,\n",
              "  637.0371704101562,\n",
              "  639.7750244140625,\n",
              "  641.3193969726562,\n",
              "  642.87158203125,\n",
              "  641.1220703125,\n",
              "  636.5078125,\n",
              "  643.78271484375,\n",
              "  637.3251953125,\n",
              "  640.2006225585938,\n",
              "  638.6622924804688,\n",
              "  640.2359008789062,\n",
              "  644.2984619140625,\n",
              "  644.9452514648438,\n",
              "  642.8731079101562,\n",
              "  641.5386352539062,\n",
              "  642.4921264648438,\n",
              "  642.3944091796875,\n",
              "  640.4537353515625,\n",
              "  642.6040649414062,\n",
              "  640.8958129882812,\n",
              "  641.9528198242188,\n",
              "  641.9228515625,\n",
              "  641.6857299804688,\n",
              "  639.6077270507812,\n",
              "  641.2555541992188,\n",
              "  643.5325317382812,\n",
              "  641.34619140625,\n",
              "  640.0276489257812,\n",
              "  645.025146484375,\n",
              "  643.5272827148438,\n",
              "  641.0132446289062,\n",
              "  641.1946411132812,\n",
              "  639.76025390625,\n",
              "  641.8838500976562,\n",
              "  642.376708984375,\n",
              "  643.7042236328125,\n",
              "  645.2774658203125,\n",
              "  640.0939331054688,\n",
              "  642.54150390625,\n",
              "  641.0895385742188,\n",
              "  645.2911376953125,\n",
              "  641.0135498046875,\n",
              "  641.6024780273438,\n",
              "  644.4312133789062,\n",
              "  641.0111694335938,\n",
              "  640.7352294921875,\n",
              "  641.9085083007812,\n",
              "  644.0010375976562,\n",
              "  639.8245849609375,\n",
              "  641.2477416992188,\n",
              "  641.0682983398438,\n",
              "  639.583740234375,\n",
              "  641.2032470703125,\n",
              "  640.4752807617188,\n",
              "  642.322998046875,\n",
              "  642.401123046875,\n",
              "  641.4899291992188,\n",
              "  639.8023071289062,\n",
              "  637.8897094726562,\n",
              "  646.8712158203125,\n",
              "  643.0380859375,\n",
              "  642.6761474609375,\n",
              "  643.5354614257812,\n",
              "  643.18505859375,\n",
              "  640.3314819335938,\n",
              "  644.7630004882812,\n",
              "  643.9750366210938,\n",
              "  644.3671875,\n",
              "  640.0809936523438,\n",
              "  639.0850219726562,\n",
              "  639.651123046875,\n",
              "  643.9054565429688,\n",
              "  645.3031616210938,\n",
              "  642.1867065429688,\n",
              "  643.6476440429688,\n",
              "  642.9317016601562,\n",
              "  643.09765625,\n",
              "  638.8197631835938,\n",
              "  642.5844116210938,\n",
              "  641.5307006835938,\n",
              "  640.9232788085938,\n",
              "  643.5143432617188,\n",
              "  643.354248046875,\n",
              "  636.7748413085938,\n",
              "  644.7147216796875,\n",
              "  645.77587890625,\n",
              "  644.01708984375,\n",
              "  643.157958984375,\n",
              "  643.3292236328125,\n",
              "  643.4992065429688,\n",
              "  643.1536865234375,\n",
              "  641.5333862304688,\n",
              "  636.5518188476562,\n",
              "  641.5098266601562,\n",
              "  642.3034057617188,\n",
              "  643.57177734375,\n",
              "  642.416748046875,\n",
              "  641.8451538085938,\n",
              "  640.8566284179688,\n",
              "  641.0007934570312,\n",
              "  643.1640014648438,\n",
              "  643.18359375,\n",
              "  636.7568969726562,\n",
              "  640.8230590820312,\n",
              "  641.1875,\n",
              "  643.1954345703125,\n",
              "  639.4791870117188,\n",
              "  639.8984375,\n",
              "  640.9547119140625,\n",
              "  639.0419311523438,\n",
              "  641.656982421875,\n",
              "  641.5658569335938,\n",
              "  640.5118408203125,\n",
              "  641.6527099609375,\n",
              "  641.0667114257812,\n",
              "  638.0377807617188,\n",
              "  643.542236328125,\n",
              "  640.7618408203125,\n",
              "  644.3450927734375,\n",
              "  642.1962280273438,\n",
              "  643.1116333007812,\n",
              "  642.7448120117188,\n",
              "  638.9224853515625,\n",
              "  640.4234008789062,\n",
              "  643.7280883789062,\n",
              "  642.8567504882812,\n",
              "  643.33056640625,\n",
              "  641.9873046875,\n",
              "  640.3712158203125,\n",
              "  640.5663452148438,\n",
              "  642.7877197265625,\n",
              "  641.7421875,\n",
              "  640.0552368164062,\n",
              "  640.8382568359375,\n",
              "  644.3956298828125,\n",
              "  640.4935302734375,\n",
              "  638.4152221679688,\n",
              "  643.2438354492188,\n",
              "  639.03515625,\n",
              "  641.8126831054688,\n",
              "  643.320556640625,\n",
              "  640.4993896484375,\n",
              "  642.15625,\n",
              "  638.0850830078125,\n",
              "  642.1062622070312,\n",
              "  640.4108276367188,\n",
              "  643.0789184570312,\n",
              "  642.8128051757812,\n",
              "  639.2916259765625,\n",
              "  638.1756591796875,\n",
              "  641.4110107421875,\n",
              "  640.2702026367188,\n",
              "  641.5703125,\n",
              "  642.5858154296875,\n",
              "  638.9273071289062,\n",
              "  638.3072509765625,\n",
              "  642.7708129882812,\n",
              "  640.2535400390625,\n",
              "  643.6893920898438,\n",
              "  638.3421630859375,\n",
              "  639.5293579101562,\n",
              "  640.4600830078125,\n",
              "  641.214599609375,\n",
              "  644.0516967773438,\n",
              "  640.8408813476562,\n",
              "  638.3392333984375,\n",
              "  641.744384765625,\n",
              "  639.7293090820312,\n",
              "  643.1488647460938,\n",
              "  644.2184448242188,\n",
              "  641.028564453125,\n",
              "  640.8350219726562,\n",
              "  640.7587280273438,\n",
              "  638.650634765625,\n",
              "  639.6508178710938,\n",
              "  641.63916015625,\n",
              "  642.17822265625,\n",
              "  637.6182250976562,\n",
              "  641.7037353515625,\n",
              "  638.9526977539062,\n",
              "  642.1660766601562,\n",
              "  640.5408325195312,\n",
              "  639.1520385742188,\n",
              "  642.0924682617188,\n",
              "  639.5951538085938,\n",
              "  640.3326416015625,\n",
              "  643.1527099609375,\n",
              "  642.35498046875,\n",
              "  637.3291625976562,\n",
              "  640.5509643554688,\n",
              "  644.3179321289062,\n",
              "  639.4205322265625,\n",
              "  646.45263671875,\n",
              "  642.3897705078125,\n",
              "  641.7007446289062,\n",
              "  641.3837280273438,\n",
              "  644.0855102539062,\n",
              "  642.4989013671875,\n",
              "  637.5044555664062,\n",
              "  642.92578125,\n",
              "  644.00390625,\n",
              "  640.6475830078125,\n",
              "  640.3494262695312,\n",
              "  642.3716430664062,\n",
              "  646.266357421875,\n",
              "  641.7783203125,\n",
              "  642.6356811523438,\n",
              "  640.699951171875,\n",
              "  646.1282348632812,\n",
              "  640.7899169921875,\n",
              "  643.642822265625,\n",
              "  644.40869140625,\n",
              "  640.1597900390625,\n",
              "  643.0090942382812,\n",
              "  647.1756591796875,\n",
              "  639.9525146484375,\n",
              "  640.7459106445312,\n",
              "  640.5740966796875,\n",
              "  640.6046752929688,\n",
              "  644.4866943359375,\n",
              "  639.7296752929688,\n",
              "  641.8264770507812,\n",
              "  642.8856811523438,\n",
              "  637.0280151367188,\n",
              "  638.9680786132812,\n",
              "  645.2219848632812,\n",
              "  639.8544311523438,\n",
              "  638.8504028320312,\n",
              "  642.7365112304688,\n",
              "  639.5529174804688,\n",
              "  641.5988159179688,\n",
              "  642.880615234375,\n",
              "  640.2078247070312,\n",
              "  638.7747802734375,\n",
              "  643.3839721679688,\n",
              "  642.035400390625,\n",
              "  639.150390625,\n",
              "  637.8006591796875,\n",
              "  642.13232421875,\n",
              "  643.1954345703125,\n",
              "  640.154541015625,\n",
              "  641.8624877929688,\n",
              "  643.5523071289062,\n",
              "  639.8203735351562,\n",
              "  641.630859375,\n",
              "  643.6140747070312,\n",
              "  643.371826171875,\n",
              "  643.2803344726562,\n",
              "  646.640625,\n",
              "  640.8821411132812,\n",
              "  641.5130004882812,\n",
              "  644.3733520507812,\n",
              "  640.2626953125,\n",
              "  642.9835815429688,\n",
              "  643.8184814453125,\n",
              "  637.3203125,\n",
              "  641.5982666015625,\n",
              "  643.390869140625,\n",
              "  641.6859130859375,\n",
              "  643.1266479492188,\n",
              "  640.4638671875,\n",
              "  639.6094360351562,\n",
              "  641.19921875,\n",
              "  640.4605712890625,\n",
              "  644.5165405273438,\n",
              "  644.5673828125,\n",
              "  638.2554931640625,\n",
              "  641.0838623046875,\n",
              "  642.9075927734375,\n",
              "  640.1439819335938,\n",
              "  641.8067626953125,\n",
              "  639.447998046875,\n",
              "  645.699462890625,\n",
              "  642.3236694335938,\n",
              "  638.9307250976562,\n",
              "  641.1099243164062,\n",
              "  640.831298828125,\n",
              "  643.2570190429688,\n",
              "  643.43017578125,\n",
              "  640.7222900390625,\n",
              "  643.0179443359375,\n",
              "  640.9234619140625,\n",
              "  642.980712890625,\n",
              "  640.3160400390625,\n",
              "  641.3558959960938,\n",
              "  642.622314453125,\n",
              "  640.6923217773438,\n",
              "  643.4185180664062,\n",
              "  641.2491455078125,\n",
              "  644.0939331054688,\n",
              "  641.5225219726562,\n",
              "  642.6951293945312,\n",
              "  640.0247192382812,\n",
              "  640.6077880859375,\n",
              "  643.3998413085938,\n",
              "  642.2373046875,\n",
              "  641.0230102539062,\n",
              "  640.5930786132812,\n",
              "  642.337158203125,\n",
              "  644.2947387695312,\n",
              "  642.8593139648438,\n",
              "  641.3480224609375,\n",
              "  640.8104858398438,\n",
              "  642.3607177734375,\n",
              "  639.9617919921875,\n",
              "  642.5671997070312,\n",
              "  639.8211669921875,\n",
              "  642.3171997070312,\n",
              "  640.3355102539062,\n",
              "  641.6249389648438,\n",
              "  641.5338745117188,\n",
              "  644.7314453125,\n",
              "  642.5909423828125,\n",
              "  639.8209838867188,\n",
              "  640.6129760742188,\n",
              "  642.3024291992188,\n",
              "  641.282958984375,\n",
              "  642.928466796875,\n",
              "  639.76416015625,\n",
              "  638.8843994140625,\n",
              "  639.4981079101562,\n",
              "  639.3182983398438,\n",
              "  641.1110229492188,\n",
              "  637.9885864257812,\n",
              "  644.2020874023438,\n",
              "  642.1206665039062,\n",
              "  642.4171752929688,\n",
              "  641.8711547851562,\n",
              "  645.8890991210938,\n",
              "  638.4148559570312,\n",
              "  641.8102416992188,\n",
              "  643.65576171875,\n",
              "  638.7081909179688,\n",
              "  641.1680297851562,\n",
              "  638.968505859375,\n",
              "  639.9597778320312,\n",
              "  639.8968505859375,\n",
              "  644.00732421875,\n",
              "  640.7806396484375,\n",
              "  641.9119873046875,\n",
              "  642.36865234375,\n",
              "  640.488525390625,\n",
              "  639.9630737304688,\n",
              "  638.44873046875,\n",
              "  639.654541015625,\n",
              "  639.6019287109375,\n",
              "  638.3499755859375,\n",
              "  641.8848266601562,\n",
              "  639.1915893554688,\n",
              "  639.21923828125,\n",
              "  642.6338500976562,\n",
              "  640.8336791992188,\n",
              "  643.2044677734375,\n",
              "  639.0812377929688,\n",
              "  639.8243408203125,\n",
              "  642.4697265625,\n",
              "  641.845947265625,\n",
              "  641.5193481445312,\n",
              "  643.8630981445312,\n",
              "  643.8375854492188,\n",
              "  643.7078247070312,\n",
              "  637.459228515625,\n",
              "  643.1229858398438,\n",
              "  643.9692993164062,\n",
              "  642.0459594726562,\n",
              "  641.0979614257812,\n",
              "  641.9066772460938,\n",
              "  641.9578247070312,\n",
              "  639.92236328125,\n",
              "  640.967041015625,\n",
              "  639.6993408203125,\n",
              "  639.1190185546875,\n",
              "  639.2401123046875,\n",
              "  642.285400390625,\n",
              "  639.0979614257812,\n",
              "  643.2923583984375,\n",
              "  641.5315551757812,\n",
              "  642.4732666015625,\n",
              "  642.4395141601562,\n",
              "  639.2601928710938,\n",
              "  639.236328125,\n",
              "  637.1920776367188,\n",
              "  643.8600463867188,\n",
              "  640.0179443359375,\n",
              "  642.0140380859375,\n",
              "  644.8563232421875,\n",
              "  643.912353515625,\n",
              "  642.7029418945312,\n",
              "  644.409912109375,\n",
              "  644.3790283203125,\n",
              "  642.75830078125,\n",
              "  640.9597778320312,\n",
              "  643.1741943359375,\n",
              "  642.770263671875,\n",
              "  638.311279296875,\n",
              "  642.4777221679688,\n",
              "  635.92431640625,\n",
              "  640.7044067382812,\n",
              "  645.5308227539062,\n",
              "  639.9973754882812,\n",
              "  643.0887451171875,\n",
              "  643.2462768554688,\n",
              "  640.6596069335938,\n",
              "  642.2778930664062,\n",
              "  641.677978515625,\n",
              "  642.8785400390625,\n",
              "  640.1598510742188,\n",
              "  644.3480224609375,\n",
              "  641.9075317382812,\n",
              "  643.5150756835938,\n",
              "  643.847900390625,\n",
              "  640.6326293945312,\n",
              "  639.55908203125,\n",
              "  638.3814697265625,\n",
              "  643.6189575195312,\n",
              "  641.0499267578125,\n",
              "  639.3706665039062,\n",
              "  645.6680908203125,\n",
              "  644.9572143554688,\n",
              "  639.4583740234375,\n",
              "  642.1558227539062,\n",
              "  641.5210571289062,\n",
              "  642.0671997070312,\n",
              "  643.2985229492188,\n",
              "  639.4415283203125,\n",
              "  643.7354125976562,\n",
              "  642.939697265625,\n",
              "  643.0060424804688,\n",
              "  640.3689575195312,\n",
              "  642.3345336914062,\n",
              "  644.4149780273438,\n",
              "  645.9340209960938,\n",
              "  639.4837036132812,\n",
              "  640.1922607421875,\n",
              "  643.2533569335938,\n",
              "  642.0253295898438,\n",
              "  644.1528930664062,\n",
              "  640.6182861328125,\n",
              "  641.8946533203125,\n",
              "  641.4293212890625,\n",
              "  639.502197265625,\n",
              "  640.3834228515625,\n",
              "  640.7322387695312,\n",
              "  640.2560424804688,\n",
              "  638.7002563476562,\n",
              "  641.5460205078125,\n",
              "  642.8010864257812,\n",
              "  642.5823364257812,\n",
              "  642.0802612304688,\n",
              "  642.2461547851562,\n",
              "  642.9832763671875,\n",
              "  642.0567626953125,\n",
              "  641.3604125976562,\n",
              "  644.3104858398438,\n",
              "  642.7994995117188,\n",
              "  643.7835693359375,\n",
              "  640.8756103515625,\n",
              "  641.64306640625,\n",
              "  640.4769897460938,\n",
              "  642.45751953125,\n",
              "  643.0983276367188,\n",
              "  641.1688842773438,\n",
              "  635.8473510742188,\n",
              "  642.326171875,\n",
              "  638.2615356445312,\n",
              "  641.0108032226562,\n",
              "  643.4049682617188,\n",
              "  642.7982788085938,\n",
              "  643.0595703125,\n",
              "  640.7156982421875,\n",
              "  641.1526489257812,\n",
              "  640.5953369140625,\n",
              "  640.0963745117188,\n",
              "  639.2822265625,\n",
              "  642.3884887695312,\n",
              "  640.2408447265625,\n",
              "  639.0579833984375,\n",
              "  638.7994384765625,\n",
              "  638.6044311523438,\n",
              "  643.49951171875,\n",
              "  642.5270385742188,\n",
              "  640.45849609375,\n",
              "  640.489501953125,\n",
              "  640.2559204101562,\n",
              "  642.8444213867188,\n",
              "  643.8703002929688,\n",
              "  638.8005981445312,\n",
              "  637.4942016601562,\n",
              "  638.2093505859375,\n",
              "  640.7993774414062,\n",
              "  643.1715087890625,\n",
              "  639.4381103515625,\n",
              "  642.5052490234375,\n",
              "  641.87841796875,\n",
              "  641.469482421875,\n",
              "  637.8082275390625,\n",
              "  643.8013916015625,\n",
              "  642.5338134765625,\n",
              "  640.9484252929688,\n",
              "  643.7037353515625,\n",
              "  641.9591064453125,\n",
              "  639.3843383789062,\n",
              "  641.7254028320312,\n",
              "  641.6140747070312,\n",
              "  640.4569702148438,\n",
              "  641.2431030273438,\n",
              "  639.46630859375,\n",
              "  643.5616455078125,\n",
              "  643.1126098632812,\n",
              "  644.8718872070312,\n",
              "  639.0755615234375,\n",
              "  640.0595703125,\n",
              "  641.2456665039062,\n",
              "  642.0556030273438,\n",
              "  644.1405639648438,\n",
              "  642.0440673828125,\n",
              "  642.9735717773438,\n",
              "  643.669189453125,\n",
              "  641.5859375,\n",
              "  642.0650634765625,\n",
              "  641.3086547851562,\n",
              "  642.1572265625,\n",
              "  643.5494384765625,\n",
              "  646.7252197265625,\n",
              "  641.4519653320312,\n",
              "  641.9368896484375,\n",
              "  639.2789306640625,\n",
              "  644.780029296875,\n",
              "  640.7719116210938,\n",
              "  642.4793701171875,\n",
              "  638.0256958007812,\n",
              "  639.4017944335938,\n",
              "  641.0263061523438,\n",
              "  642.1195678710938,\n",
              "  641.8726196289062,\n",
              "  642.5594482421875,\n",
              "  638.8468627929688,\n",
              "  641.5962524414062,\n",
              "  640.1441650390625,\n",
              "  643.1875,\n",
              "  638.2965698242188,\n",
              "  641.4586791992188,\n",
              "  643.9593505859375,\n",
              "  639.462890625,\n",
              "  641.7051391601562,\n",
              "  642.4384765625,\n",
              "  645.332275390625,\n",
              "  642.8916625976562,\n",
              "  639.64111328125,\n",
              "  643.5897216796875,\n",
              "  642.7777709960938,\n",
              "  639.5621948242188,\n",
              "  642.5628051757812,\n",
              "  642.103515625,\n",
              "  642.628173828125,\n",
              "  635.3544921875,\n",
              "  637.142333984375,\n",
              "  639.4231567382812,\n",
              "  641.675537109375,\n",
              "  642.3124389648438,\n",
              "  644.5516967773438,\n",
              "  637.9385986328125,\n",
              "  639.8361206054688,\n",
              "  645.7200927734375,\n",
              "  642.76220703125,\n",
              "  638.9683837890625,\n",
              "  638.6465454101562,\n",
              "  643.0614013671875,\n",
              "  644.4267578125,\n",
              "  637.319580078125,\n",
              "  642.5355224609375,\n",
              "  639.5315551757812,\n",
              "  643.6954345703125,\n",
              "  644.949951171875,\n",
              "  640.3751831054688,\n",
              "  638.776123046875,\n",
              "  641.5044555664062,\n",
              "  639.993896484375,\n",
              "  642.0623779296875,\n",
              "  642.4222412109375,\n",
              "  643.2247314453125,\n",
              "  644.1135864257812,\n",
              "  641.9024658203125,\n",
              "  642.8230590820312,\n",
              "  642.3330688476562,\n",
              "  643.356201171875,\n",
              "  640.9949951171875,\n",
              "  642.1171875,\n",
              "  641.6862182617188,\n",
              "  644.5390625,\n",
              "  638.741943359375,\n",
              "  642.4483642578125,\n",
              "  638.0427856445312,\n",
              "  639.5279541015625,\n",
              "  640.6631469726562,\n",
              "  643.427978515625,\n",
              "  639.6201782226562,\n",
              "  640.8136596679688,\n",
              "  640.6573486328125,\n",
              "  640.1795043945312,\n",
              "  638.6102294921875,\n",
              "  640.0419921875,\n",
              "  642.3352661132812,\n",
              "  640.3773193359375,\n",
              "  643.4287719726562,\n",
              "  641.2926635742188,\n",
              "  640.3487548828125,\n",
              "  641.6807861328125,\n",
              "  643.0762329101562,\n",
              "  643.2974853515625,\n",
              "  641.752685546875,\n",
              "  643.3466796875,\n",
              "  644.7901000976562,\n",
              "  643.774169921875,\n",
              "  637.6524658203125,\n",
              "  636.2989501953125,\n",
              "  640.995849609375,\n",
              "  643.2695922851562,\n",
              "  643.7125244140625,\n",
              "  643.285400390625,\n",
              "  639.55712890625,\n",
              "  641.0816650390625,\n",
              "  642.369384765625,\n",
              "  641.7881469726562,\n",
              "  638.6940307617188,\n",
              "  640.8452758789062,\n",
              "  641.2850341796875,\n",
              "  641.4370727539062,\n",
              "  640.2622680664062,\n",
              "  641.72265625,\n",
              "  639.3004150390625,\n",
              "  641.8449096679688,\n",
              "  639.9550170898438,\n",
              "  643.3736572265625,\n",
              "  641.21728515625,\n",
              "  644.4500732421875,\n",
              "  639.8395385742188,\n",
              "  640.5343017578125,\n",
              "  641.7734375,\n",
              "  642.2793579101562,\n",
              "  643.7505493164062,\n",
              "  638.8756713867188,\n",
              "  639.0342407226562,\n",
              "  642.1127319335938,\n",
              "  642.2693481445312,\n",
              "  638.6925048828125,\n",
              "  641.7474365234375,\n",
              "  644.0185546875,\n",
              "  640.6334838867188,\n",
              "  638.8997802734375,\n",
              "  641.29248046875,\n",
              "  641.2363891601562,\n",
              "  642.0017700195312,\n",
              "  640.7478637695312,\n",
              "  641.5986328125,\n",
              "  640.70458984375,\n",
              "  642.5457763671875,\n",
              "  644.2026977539062,\n",
              "  644.0078125,\n",
              "  642.4419555664062,\n",
              "  639.3782958984375,\n",
              "  640.5432739257812,\n",
              "  644.3751831054688,\n",
              "  642.4967651367188,\n",
              "  642.96435546875,\n",
              "  637.5759887695312,\n",
              "  640.726806640625,\n",
              "  642.9970703125,\n",
              "  641.9281616210938,\n",
              "  642.703369140625,\n",
              "  640.770263671875,\n",
              "  642.107177734375,\n",
              "  642.4027099609375,\n",
              "  644.2998046875,\n",
              "  638.029052734375,\n",
              "  639.6827392578125,\n",
              "  640.7080078125,\n",
              "  639.327880859375,\n",
              "  641.5631103515625,\n",
              "  643.4669799804688,\n",
              "  643.5072021484375,\n",
              "  641.9920043945312,\n",
              "  643.5399169921875,\n",
              "  641.8553466796875,\n",
              "  644.9406127929688,\n",
              "  641.5930786132812,\n",
              "  644.739990234375,\n",
              "  638.54296875,\n",
              "  642.9307250976562,\n",
              "  644.1309814453125,\n",
              "  638.8361206054688,\n",
              "  640.1130981445312,\n",
              "  644.835205078125,\n",
              "  640.6483764648438,\n",
              "  640.1455688476562,\n",
              "  638.049072265625,\n",
              "  644.7849731445312,\n",
              "  642.2144165039062,\n",
              "  641.5111694335938,\n",
              "  641.7955932617188,\n",
              "  642.2034912109375,\n",
              "  642.3739013671875,\n",
              "  641.15576171875,\n",
              "  640.9560546875,\n",
              "  644.2323608398438,\n",
              "  636.8582763671875,\n",
              "  643.23193359375,\n",
              "  642.8308715820312,\n",
              "  646.4951782226562,\n",
              "  640.4248657226562,\n",
              "  639.6533813476562,\n",
              "  641.3040771484375,\n",
              "  642.02392578125,\n",
              "  642.4095458984375,\n",
              "  642.321533203125,\n",
              "  641.4749755859375,\n",
              "  641.0977172851562,\n",
              "  641.3177490234375,\n",
              "  640.1555786132812,\n",
              "  637.8084716796875,\n",
              "  636.7456665039062,\n",
              "  643.5970458984375,\n",
              "  641.2357177734375,\n",
              "  643.3780517578125,\n",
              "  640.4954833984375,\n",
              "  641.6243896484375,\n",
              "  640.805908203125,\n",
              "  637.1696166992188,\n",
              "  645.0079956054688,\n",
              "  637.6781005859375,\n",
              "  641.123046875,\n",
              "  637.9818115234375,\n",
              "  644.15625,\n",
              "  641.461669921875,\n",
              "  642.06494140625,\n",
              "  640.5164184570312,\n",
              "  640.9669189453125,\n",
              "  640.1820678710938,\n",
              "  642.2393798828125,\n",
              "  641.187255859375,\n",
              "  639.9586791992188,\n",
              "  642.9464111328125,\n",
              "  634.2083129882812,\n",
              "  643.5579833984375,\n",
              "  641.2958984375,\n",
              "  641.9243774414062,\n",
              "  639.0812377929688,\n",
              "  641.5352783203125,\n",
              "  640.4613647460938,\n",
              "  642.307373046875,\n",
              "  642.087890625,\n",
              "  642.986572265625,\n",
              "  643.5587768554688,\n",
              "  639.474365234375,\n",
              "  643.3417358398438,\n",
              "  642.6773071289062,\n",
              "  639.2613525390625,\n",
              "  642.49365234375,\n",
              "  639.7498168945312,\n",
              "  642.9800415039062,\n",
              "  642.0069580078125],\n",
              " 'val_loss': [513487.375,\n",
              "  490458.53125,\n",
              "  491442.71875,\n",
              "  476539.34375,\n",
              "  436045.75,\n",
              "  456161.25,\n",
              "  438866.71875,\n",
              "  423009.71875,\n",
              "  410808.65625,\n",
              "  405228.96875,\n",
              "  391149.59375,\n",
              "  376713.0,\n",
              "  362615.03125,\n",
              "  345085.53125,\n",
              "  332213.34375,\n",
              "  331656.6875,\n",
              "  341024.53125,\n",
              "  320008.3125,\n",
              "  297194.46875,\n",
              "  300131.34375,\n",
              "  297132.09375,\n",
              "  282867.625,\n",
              "  280109.09375,\n",
              "  283815.15625,\n",
              "  263228.15625,\n",
              "  266537.09375,\n",
              "  261256.046875,\n",
              "  253868.984375,\n",
              "  252287.953125,\n",
              "  247921.703125,\n",
              "  244465.296875,\n",
              "  240092.921875,\n",
              "  236182.015625,\n",
              "  226633.734375,\n",
              "  226691.625,\n",
              "  227666.453125,\n",
              "  225018.75,\n",
              "  224405.515625,\n",
              "  222692.796875,\n",
              "  218630.796875,\n",
              "  217865.109375,\n",
              "  220480.5625,\n",
              "  219480.109375,\n",
              "  217463.859375,\n",
              "  218350.875,\n",
              "  218483.109375,\n",
              "  214166.203125,\n",
              "  218574.046875,\n",
              "  218497.671875,\n",
              "  217702.046875,\n",
              "  216611.921875,\n",
              "  221187.796875,\n",
              "  222440.453125,\n",
              "  219688.5,\n",
              "  220569.1875,\n",
              "  227695.328125,\n",
              "  225730.0625,\n",
              "  228974.484375,\n",
              "  233731.75,\n",
              "  232982.3125,\n",
              "  229759.765625,\n",
              "  237626.515625,\n",
              "  237561.1875,\n",
              "  243659.125,\n",
              "  245151.671875,\n",
              "  251902.390625,\n",
              "  246351.359375,\n",
              "  253826.671875,\n",
              "  252999.890625,\n",
              "  251204.5625,\n",
              "  255625.640625,\n",
              "  261091.609375,\n",
              "  272012.65625,\n",
              "  267737.1875,\n",
              "  281520.90625,\n",
              "  269413.84375,\n",
              "  275795.40625,\n",
              "  284709.96875,\n",
              "  293004.78125,\n",
              "  296149.15625,\n",
              "  289200.96875,\n",
              "  286604.03125,\n",
              "  295910.71875,\n",
              "  303932.53125,\n",
              "  307603.40625,\n",
              "  298777.65625,\n",
              "  320534.40625,\n",
              "  317297.75,\n",
              "  320275.625,\n",
              "  330125.40625,\n",
              "  320676.53125,\n",
              "  317187.71875,\n",
              "  332187.15625,\n",
              "  329505.34375,\n",
              "  326398.1875,\n",
              "  342052.28125,\n",
              "  350928.09375,\n",
              "  340646.96875,\n",
              "  350003.25,\n",
              "  359444.375,\n",
              "  348329.65625,\n",
              "  350133.28125,\n",
              "  373857.25,\n",
              "  362614.90625,\n",
              "  365143.34375,\n",
              "  343789.96875,\n",
              "  368679.90625,\n",
              "  364201.78125,\n",
              "  371659.59375,\n",
              "  375555.0,\n",
              "  376732.03125,\n",
              "  371544.28125,\n",
              "  373993.09375,\n",
              "  373908.15625,\n",
              "  375368.25,\n",
              "  369661.375,\n",
              "  378276.84375,\n",
              "  387793.5,\n",
              "  389650.40625,\n",
              "  383609.75,\n",
              "  400262.40625,\n",
              "  385606.5,\n",
              "  378811.53125,\n",
              "  380291.03125,\n",
              "  388595.25,\n",
              "  389732.375,\n",
              "  399157.65625,\n",
              "  415991.90625,\n",
              "  400653.53125,\n",
              "  410031.40625,\n",
              "  410372.34375,\n",
              "  395950.15625,\n",
              "  403975.09375,\n",
              "  387813.84375,\n",
              "  404770.5,\n",
              "  398166.59375,\n",
              "  406869.65625,\n",
              "  390142.53125,\n",
              "  382970.96875,\n",
              "  409100.84375,\n",
              "  408724.75,\n",
              "  408450.53125,\n",
              "  417199.5,\n",
              "  400650.53125,\n",
              "  409164.125,\n",
              "  409962.0,\n",
              "  410197.15625,\n",
              "  395138.90625,\n",
              "  410690.28125,\n",
              "  412695.0,\n",
              "  404420.53125,\n",
              "  403979.40625,\n",
              "  422782.46875,\n",
              "  397219.21875,\n",
              "  406445.40625,\n",
              "  406289.75,\n",
              "  422861.34375,\n",
              "  422853.375,\n",
              "  414865.71875,\n",
              "  399027.5,\n",
              "  406296.90625,\n",
              "  415084.96875,\n",
              "  414160.96875,\n",
              "  408610.0,\n",
              "  408142.40625,\n",
              "  408638.5,\n",
              "  399965.59375,\n",
              "  424938.34375,\n",
              "  398595.21875,\n",
              "  407602.15625,\n",
              "  417086.40625,\n",
              "  401562.78125,\n",
              "  417220.46875,\n",
              "  408271.40625,\n",
              "  407913.0,\n",
              "  426881.40625,\n",
              "  409299.15625,\n",
              "  418548.84375,\n",
              "  409235.15625,\n",
              "  417104.5,\n",
              "  401445.84375,\n",
              "  401139.53125,\n",
              "  409075.90625,\n",
              "  433928.09375,\n",
              "  424346.75,\n",
              "  399320.90625,\n",
              "  407748.65625,\n",
              "  399421.625,\n",
              "  407785.65625,\n",
              "  406682.40625,\n",
              "  433132.71875,\n",
              "  409122.78125,\n",
              "  417529.03125,\n",
              "  434719.625,\n",
              "  416379.875,\n",
              "  416528.71875,\n",
              "  409739.28125,\n",
              "  408916.28125,\n",
              "  417197.84375,\n",
              "  409967.53125,\n",
              "  401314.71875,\n",
              "  426314.59375,\n",
              "  426209.34375,\n",
              "  399528.28125,\n",
              "  408305.25,\n",
              "  407727.875,\n",
              "  407009.46875,\n",
              "  417618.78125,\n",
              "  399667.5,\n",
              "  418085.84375,\n",
              "  400846.28125,\n",
              "  411016.15625,\n",
              "  426259.34375,\n",
              "  420081.375,\n",
              "  427769.78125,\n",
              "  418391.25,\n",
              "  409951.96875,\n",
              "  418803.65625,\n",
              "  426986.65625,\n",
              "  409364.09375,\n",
              "  408490.78125,\n",
              "  409811.59375,\n",
              "  409075.25,\n",
              "  400029.65625,\n",
              "  416253.0,\n",
              "  408900.125,\n",
              "  408819.78125,\n",
              "  409700.84375,\n",
              "  425847.15625,\n",
              "  419197.78125,\n",
              "  401521.375,\n",
              "  401944.34375,\n",
              "  408225.90625,\n",
              "  408610.40625,\n",
              "  409106.09375,\n",
              "  408675.75,\n",
              "  416856.84375,\n",
              "  417790.90625,\n",
              "  408789.75,\n",
              "  417002.46875,\n",
              "  424763.75,\n",
              "  401299.84375,\n",
              "  417728.09375,\n",
              "  401169.0,\n",
              "  425177.375,\n",
              "  409589.59375,\n",
              "  407600.21875,\n",
              "  426474.71875,\n",
              "  409633.59375,\n",
              "  418302.34375,\n",
              "  399999.5,\n",
              "  400087.21875,\n",
              "  408068.125,\n",
              "  415255.25,\n",
              "  399315.875,\n",
              "  416754.59375,\n",
              "  399782.75,\n",
              "  418143.59375,\n",
              "  417149.09375,\n",
              "  426249.53125,\n",
              "  426199.34375,\n",
              "  401150.375,\n",
              "  417762.25,\n",
              "  410427.25,\n",
              "  418971.875,\n",
              "  403773.71875,\n",
              "  402005.40625,\n",
              "  429338.34375,\n",
              "  410926.46875,\n",
              "  418775.03125,\n",
              "  402637.75,\n",
              "  421508.75,\n",
              "  402740.15625,\n",
              "  412002.90625,\n",
              "  411098.46875,\n",
              "  420217.84375,\n",
              "  411438.125,\n",
              "  420573.65625,\n",
              "  411397.71875,\n",
              "  403385.15625,\n",
              "  410911.40625,\n",
              "  394639.78125,\n",
              "  401929.25,\n",
              "  427768.5,\n",
              "  419152.78125,\n",
              "  410397.28125,\n",
              "  402718.75,\n",
              "  418690.90625,\n",
              "  410513.34375,\n",
              "  410450.375,\n",
              "  408959.15625,\n",
              "  410331.15625,\n",
              "  435119.21875,\n",
              "  426385.25,\n",
              "  418942.34375,\n",
              "  409426.25,\n",
              "  416786.65625,\n",
              "  417207.84375,\n",
              "  401005.0,\n",
              "  417787.75,\n",
              "  407840.15625,\n",
              "  426903.71875,\n",
              "  409363.21875,\n",
              "  402300.78125,\n",
              "  419837.09375,\n",
              "  410867.75,\n",
              "  402000.28125,\n",
              "  393813.125,\n",
              "  411033.15625,\n",
              "  418108.25,\n",
              "  409633.625,\n",
              "  420508.875,\n",
              "  411259.65625,\n",
              "  437346.53125,\n",
              "  420520.375,\n",
              "  437488.875,\n",
              "  410627.75,\n",
              "  402009.34375,\n",
              "  411542.75,\n",
              "  426294.5,\n",
              "  419287.375,\n",
              "  402745.28125,\n",
              "  419841.5,\n",
              "  429072.34375,\n",
              "  411939.25,\n",
              "  412433.84375,\n",
              "  437183.59375,\n",
              "  420294.84375,\n",
              "  420427.90625,\n",
              "  428844.09375,\n",
              "  418842.15625,\n",
              "  408990.5,\n",
              "  425958.875,\n",
              "  391678.0,\n",
              "  400329.09375,\n",
              "  424675.40625,\n",
              "  417283.09375,\n",
              "  408915.21875,\n",
              "  401807.59375,\n",
              "  425768.40625,\n",
              "  418372.0,\n",
              "  434388.40625,\n",
              "  408201.375,\n",
              "  399332.34375,\n",
              "  417546.375,\n",
              "  407769.875,\n",
              "  408111.65625,\n",
              "  417031.875,\n",
              "  417108.46875,\n",
              "  417455.125,\n",
              "  416452.75,\n",
              "  407808.46875,\n",
              "  417179.15625,\n",
              "  426266.78125,\n",
              "  400479.84375,\n",
              "  409165.65625,\n",
              "  416330.96875,\n",
              "  408369.75,\n",
              "  416593.75,\n",
              "  389588.25,\n",
              "  417288.03125,\n",
              "  407162.25,\n",
              "  399042.15625,\n",
              "  415030.65625,\n",
              "  415173.0,\n",
              "  415566.21875,\n",
              "  408030.28125,\n",
              "  415136.5,\n",
              "  425345.59375,\n",
              "  408375.34375,\n",
              "  406655.46875,\n",
              "  414879.40625,\n",
              "  415231.34375,\n",
              "  416840.78125,\n",
              "  407501.78125,\n",
              "  405771.15625,\n",
              "  399940.84375,\n",
              "  415246.03125,\n",
              "  414567.46875,\n",
              "  415456.28125,\n",
              "  414633.46875,\n",
              "  416257.09375,\n",
              "  414860.15625,\n",
              "  423385.34375,\n",
              "  416059.28125,\n",
              "  408290.125,\n",
              "  408311.21875,\n",
              "  417943.09375,\n",
              "  417807.125,\n",
              "  418373.15625,\n",
              "  409372.0,\n",
              "  408445.15625,\n",
              "  417473.28125,\n",
              "  409747.5,\n",
              "  408336.65625,\n",
              "  416638.78125,\n",
              "  425718.15625,\n",
              "  409441.625,\n",
              "  418559.71875,\n",
              "  425714.96875,\n",
              "  400087.09375,\n",
              "  408811.78125,\n",
              "  417945.96875,\n",
              "  409492.65625,\n",
              "  407506.46875,\n",
              "  400750.34375,\n",
              "  407775.15625,\n",
              "  426483.96875,\n",
              "  417380.0,\n",
              "  417224.84375,\n",
              "  417771.40625,\n",
              "  408891.46875,\n",
              "  410201.65625,\n",
              "  416431.65625,\n",
              "  407862.53125,\n",
              "  409733.28125,\n",
              "  408174.03125,\n",
              "  425024.03125,\n",
              "  425504.40625,\n",
              "  417198.84375,\n",
              "  425341.78125,\n",
              "  409829.25,\n",
              "  401621.125,\n",
              "  402927.75,\n",
              "  410943.65625,\n",
              "  410368.625,\n",
              "  434930.21875,\n",
              "  417720.25,\n",
              "  401941.75,\n",
              "  426680.34375,\n",
              "  418023.84375,\n",
              "  418359.09375,\n",
              "  417933.34375,\n",
              "  427109.53125,\n",
              "  418088.84375,\n",
              "  409621.59375,\n",
              "  410887.375,\n",
              "  436170.0,\n",
              "  409908.46875,\n",
              "  419663.21875,\n",
              "  411303.375,\n",
              "  428244.125,\n",
              "  418955.90625,\n",
              "  427716.5,\n",
              "  411714.78125,\n",
              "  419890.71875,\n",
              "  410394.21875,\n",
              "  400955.71875,\n",
              "  408603.71875,\n",
              "  425838.53125,\n",
              "  408955.125,\n",
              "  409679.65625,\n",
              "  426146.90625,\n",
              "  409356.15625,\n",
              "  408884.625,\n",
              "  409166.0,\n",
              "  408774.625,\n",
              "  399950.75,\n",
              "  406656.78125,\n",
              "  423766.71875,\n",
              "  407996.21875,\n",
              "  413892.21875,\n",
              "  415643.84375,\n",
              "  407984.375,\n",
              "  407921.25,\n",
              "  408539.03125,\n",
              "  408802.34375,\n",
              "  407896.40625,\n",
              "  425999.90625,\n",
              "  425686.125,\n",
              "  401365.125,\n",
              "  408242.75,\n",
              "  417739.59375,\n",
              "  400236.90625,\n",
              "  424555.65625,\n",
              "  399236.90625,\n",
              "  407319.0,\n",
              "  409234.875,\n",
              "  416634.65625,\n",
              "  391419.25,\n",
              "  408879.59375,\n",
              "  399781.625,\n",
              "  399379.84375,\n",
              "  407304.90625,\n",
              "  433148.03125,\n",
              "  424133.40625,\n",
              "  424567.90625,\n",
              "  417714.90625,\n",
              "  416588.96875,\n",
              "  416275.15625,\n",
              "  407610.71875,\n",
              "  416803.96875,\n",
              "  417347.40625,\n",
              "  399531.5,\n",
              "  408029.46875,\n",
              "  408824.59375,\n",
              "  417559.75,\n",
              "  419094.21875,\n",
              "  420064.75,\n",
              "  393009.375,\n",
              "  428473.125,\n",
              "  410455.71875,\n",
              "  409547.59375,\n",
              "  418376.875,\n",
              "  407994.46875,\n",
              "  434313.71875,\n",
              "  417437.03125,\n",
              "  400612.84375,\n",
              "  426546.09375,\n",
              "  409948.0,\n",
              "  417972.875,\n",
              "  418287.90625,\n",
              "  411233.65625,\n",
              "  402441.125,\n",
              "  418830.71875,\n",
              "  418781.59375,\n",
              "  419629.96875,\n",
              "  427162.15625,\n",
              "  401878.03125,\n",
              "  417635.0,\n",
              "  426035.34375,\n",
              "  391940.0,\n",
              "  425836.34375,\n",
              "  426575.46875,\n",
              "  408572.21875,\n",
              "  416580.625,\n",
              "  409529.59375,\n",
              "  426565.59375,\n",
              "  401234.59375,\n",
              "  409689.53125,\n",
              "  409868.34375,\n",
              "  418574.53125,\n",
              "  409802.34375,\n",
              "  419423.34375,\n",
              "  419828.96875,\n",
              "  427365.75,\n",
              "  392575.65625,\n",
              "  418652.46875,\n",
              "  419771.125,\n",
              "  417340.15625,\n",
              "  409350.96875,\n",
              "  425924.96875,\n",
              "  408741.28125,\n",
              "  424349.46875,\n",
              "  407939.90625,\n",
              "  399887.78125,\n",
              "  415292.78125,\n",
              "  424739.65625,\n",
              "  406874.0,\n",
              "  414010.09375,\n",
              "  406213.84375,\n",
              "  406171.03125,\n",
              "  414142.375,\n",
              "  414114.21875,\n",
              "  397808.25,\n",
              "  405853.71875,\n",
              "  422814.21875,\n",
              "  407678.96875,\n",
              "  415550.09375,\n",
              "  415326.65625,\n",
              "  414757.28125,\n",
              "  398264.40625,\n",
              "  407927.0,\n",
              "  391529.875,\n",
              "  391968.28125,\n",
              "  415880.875,\n",
              "  417068.625,\n",
              "  416956.78125,\n",
              "  415665.65625,\n",
              "  423770.09375,\n",
              "  415777.375,\n",
              "  406128.125,\n",
              "  431808.25,\n",
              "  407375.09375,\n",
              "  407265.375,\n",
              "  417008.25,\n",
              "  406939.25,\n",
              "  400554.90625,\n",
              "  399212.15625,\n",
              "  408729.59375,\n",
              "  416566.34375,\n",
              "  408637.15625,\n",
              "  416858.40625,\n",
              "  424381.78125,\n",
              "  414648.59375,\n",
              "  430693.5,\n",
              "  423758.59375,\n",
              "  398627.46875,\n",
              "  406968.5,\n",
              "  415282.03125,\n",
              "  404665.375,\n",
              "  397654.34375,\n",
              "  415146.84375,\n",
              "  406037.21875,\n",
              "  424239.34375,\n",
              "  407689.0,\n",
              "  406603.875,\n",
              "  409414.40625,\n",
              "  408132.15625,\n",
              "  399597.59375,\n",
              "  425559.34375,\n",
              "  409784.15625,\n",
              "  425418.625,\n",
              "  432372.78125,\n",
              "  398329.625,\n",
              "  409370.09375,\n",
              "  408450.15625,\n",
              "  398459.90625,\n",
              "  424136.03125,\n",
              "  407148.375,\n",
              "  406724.59375,\n",
              "  415824.75,\n",
              "  416335.96875,\n",
              "  423576.375,\n",
              "  415698.53125,\n",
              "  415877.15625,\n",
              "  407476.0,\n",
              "  405986.375,\n",
              "  416284.375,\n",
              "  415481.40625,\n",
              "  423497.0,\n",
              "  415220.71875,\n",
              "  398736.71875,\n",
              "  415074.84375,\n",
              "  399019.5,\n",
              "  416614.09375,\n",
              "  425193.40625,\n",
              "  399606.5,\n",
              "  416960.53125,\n",
              "  401012.71875,\n",
              "  400652.65625,\n",
              "  425871.65625,\n",
              "  400964.90625,\n",
              "  426597.25,\n",
              "  417397.0,\n",
              "  419707.0,\n",
              "  411034.65625,\n",
              "  420491.71875,\n",
              "  420260.15625,\n",
              "  402058.65625,\n",
              "  420557.34375,\n",
              "  421152.625,\n",
              "  419685.75,\n",
              "  402657.0,\n",
              "  399859.75,\n",
              "  409550.09375,\n",
              "  416782.75,\n",
              "  407115.625,\n",
              "  425855.625,\n",
              "  408849.34375,\n",
              "  416782.0,\n",
              "  408902.96875,\n",
              "  407896.25,\n",
              "  416119.875,\n",
              "  415435.21875,\n",
              "  406447.53125,\n",
              "  398006.46875,\n",
              "  415470.34375,\n",
              "  425682.71875,\n",
              "  417563.40625,\n",
              "  408934.40625,\n",
              "  426132.5,\n",
              "  410049.46875,\n",
              "  401814.03125,\n",
              "  419104.34375,\n",
              "  420200.78125,\n",
              "  427709.59375,\n",
              "  402620.34375,\n",
              "  410411.78125,\n",
              "  409652.28125,\n",
              "  410543.25,\n",
              "  418583.40625,\n",
              "  419105.03125,\n",
              "  408590.5,\n",
              "  425274.75,\n",
              "  417610.75,\n",
              "  424837.53125,\n",
              "  416219.53125,\n",
              "  415535.125,\n",
              "  407759.21875,\n",
              "  391652.90625,\n",
              "  407094.96875,\n",
              "  389654.875,\n",
              "  415913.5,\n",
              "  400580.46875,\n",
              "  417555.53125,\n",
              "  400572.34375,\n",
              "  408085.625,\n",
              "  433217.96875,\n",
              "  400519.28125,\n",
              "  399561.375,\n",
              "  408051.625,\n",
              "  418384.78125,\n",
              "  418186.09375,\n",
              "  426242.40625,\n",
              "  400639.15625,\n",
              "  409386.65625,\n",
              "  409071.09375,\n",
              "  409090.84375,\n",
              "  425740.09375,\n",
              "  408713.09375,\n",
              "  407866.0,\n",
              "  408406.46875,\n",
              "  408214.59375,\n",
              "  396592.59375,\n",
              "  415492.21875,\n",
              "  407733.15625,\n",
              "  407908.71875,\n",
              "  407271.25,\n",
              "  406471.71875,\n",
              "  415507.40625,\n",
              "  406888.71875,\n",
              "  423772.125,\n",
              "  432060.84375,\n",
              "  423215.75,\n",
              "  412850.84375,\n",
              "  413320.34375,\n",
              "  414610.96875,\n",
              "  405016.65625,\n",
              "  423360.25,\n",
              "  415326.75,\n",
              "  423694.75,\n",
              "  414960.75,\n",
              "  423097.90625,\n",
              "  407749.15625,\n",
              "  425251.25,\n",
              "  415218.875,\n",
              "  415781.0,\n",
              "  415628.75,\n",
              "  416306.53125,\n",
              "  398845.59375,\n",
              "  425574.96875,\n",
              "  400431.09375,\n",
              "  423540.5,\n",
              "  397403.53125,\n",
              "  407699.90625,\n",
              "  408558.28125,\n",
              "  399769.0,\n",
              "  399787.0,\n",
              "  416079.09375,\n",
              "  407897.40625,\n",
              "  416455.09375,\n",
              "  389065.5,\n",
              "  399056.59375,\n",
              "  398663.15625,\n",
              "  416404.03125,\n",
              "  408161.03125,\n",
              "  415909.0,\n",
              "  415260.65625,\n",
              "  407707.90625,\n",
              "  425778.46875,\n",
              "  400615.90625,\n",
              "  417598.09375,\n",
              "  427321.09375,\n",
              "  417356.03125,\n",
              "  399810.40625,\n",
              "  401967.90625,\n",
              "  417408.875,\n",
              "  408708.65625,\n",
              "  408868.625,\n",
              "  400777.90625,\n",
              "  424842.90625,\n",
              "  407716.875,\n",
              "  425796.78125,\n",
              "  417435.78125,\n",
              "  426381.46875,\n",
              "  415767.34375,\n",
              "  417820.53125,\n",
              "  417912.96875,\n",
              "  399808.53125,\n",
              "  407656.5,\n",
              "  408510.375,\n",
              "  425554.90625,\n",
              "  416052.46875,\n",
              "  424669.40625,\n",
              "  417389.59375,\n",
              "  424686.59375,\n",
              "  409242.71875,\n",
              "  409170.25,\n",
              "  416702.21875,\n",
              "  408812.71875,\n",
              "  399499.15625,\n",
              "  424628.40625,\n",
              "  409545.78125,\n",
              "  426876.09375,\n",
              "  393200.59375,\n",
              "  428007.28125,\n",
              "  410455.59375,\n",
              "  419022.15625,\n",
              "  426640.03125,\n",
              "  402111.09375,\n",
              "  418784.09375,\n",
              "  399965.5,\n",
              "  426037.0,\n",
              "  409032.65625,\n",
              "  425356.75,\n",
              "  400339.09375,\n",
              "  409782.09375,\n",
              "  425282.25,\n",
              "  408719.59375,\n",
              "  417297.625,\n",
              "  399139.65625,\n",
              "  416375.21875,\n",
              "  424213.75,\n",
              "  399507.40625,\n",
              "  400051.03125,\n",
              "  401357.53125,\n",
              "  416782.25,\n",
              "  409512.28125,\n",
              "  415935.90625,\n",
              "  416849.15625,\n",
              "  407956.09375,\n",
              "  408010.40625,\n",
              "  408063.53125,\n",
              "  398914.84375,\n",
              "  399299.5,\n",
              "  424964.5,\n",
              "  400592.0,\n",
              "  419013.34375,\n",
              "  399841.96875,\n",
              "  418246.625,\n",
              "  411267.71875,\n",
              "  418556.90625,\n",
              "  410984.375,\n",
              "  402357.125,\n",
              "  408972.78125,\n",
              "  401299.71875,\n",
              "  391582.03125,\n",
              "  425646.28125,\n",
              "  402215.65625,\n",
              "  417922.21875,\n",
              "  417303.59375,\n",
              "  408771.40625,\n",
              "  417938.21875,\n",
              "  417708.34375,\n",
              "  416483.28125,\n",
              "  424568.96875,\n",
              "  423360.78125,\n",
              "  407371.15625,\n",
              "  415633.65625,\n",
              "  406521.625,\n",
              "  398395.34375,\n",
              "  416683.15625,\n",
              "  416826.34375,\n",
              "  408129.84375,\n",
              "  415484.65625,\n",
              "  424510.875,\n",
              "  416224.71875,\n",
              "  407324.84375,\n",
              "  399264.09375,\n",
              "  423100.53125,\n",
              "  423578.125,\n",
              "  415188.34375,\n",
              "  416106.65625,\n",
              "  407423.875,\n",
              "  417136.03125,\n",
              "  417131.125,\n",
              "  416921.375,\n",
              "  415924.09375,\n",
              "  407220.53125,\n",
              "  415298.96875,\n",
              "  416031.53125,\n",
              "  400434.21875,\n",
              "  400697.34375,\n",
              "  425005.90625,\n",
              "  408723.71875,\n",
              "  408311.71875,\n",
              "  416464.03125,\n",
              "  417126.65625,\n",
              "  417437.53125,\n",
              "  407474.90625,\n",
              "  407811.15625,\n",
              "  407616.375,\n",
              "  407088.625,\n",
              "  416613.40625,\n",
              "  408904.71875,\n",
              "  417389.625,\n",
              "  416582.75,\n",
              "  398836.5,\n",
              "  416120.375,\n",
              "  415184.34375,\n",
              "  416432.75,\n",
              "  416180.34375,\n",
              "  406777.125,\n",
              "  398664.90625,\n",
              "  414721.28125,\n",
              "  406250.25,\n",
              "  423837.34375,\n",
              "  399784.875,\n",
              "  425351.125,\n",
              "  407450.34375,\n",
              "  408205.71875,\n",
              "  425919.75,\n",
              "  400502.90625,\n",
              "  425611.96875,\n",
              "  417230.90625,\n",
              "  424461.84375,\n",
              "  417534.0,\n",
              "  416831.84375,\n",
              "  407271.125,\n",
              "  406571.65625,\n",
              "  424668.5,\n",
              "  425379.625,\n",
              "  417133.34375,\n",
              "  417002.78125,\n",
              "  408220.09375,\n",
              "  407999.09375,\n",
              "  424481.875,\n",
              "  417189.90625,\n",
              "  415904.625,\n",
              "  416442.28125,\n",
              "  408339.25,\n",
              "  417258.84375,\n",
              "  408060.65625,\n",
              "  432669.875,\n",
              "  416983.40625,\n",
              "  416616.875,\n",
              "  391135.0,\n",
              "  407410.03125,\n",
              "  391968.125,\n",
              "  407298.59375,\n",
              "  408263.25,\n",
              "  415913.0,\n",
              "  398752.5,\n",
              "  425015.0,\n",
              "  408414.0,\n",
              "  407036.15625,\n",
              "  399779.90625,\n",
              "  424930.875,\n",
              "  407695.96875,\n",
              "  406809.375,\n",
              "  425975.25,\n",
              "  425559.71875,\n",
              "  408146.375,\n",
              "  416516.28125,\n",
              "  424862.28125,\n",
              "  416287.71875,\n",
              "  424667.96875,\n",
              "  406698.65625,\n",
              "  417665.875,\n",
              "  425530.90625,\n",
              "  401851.84375,\n",
              "  417714.28125,\n",
              "  418331.09375,\n",
              "  416550.75,\n",
              "  417644.09375,\n",
              "  409316.96875,\n",
              "  417123.125,\n",
              "  417563.875,\n",
              "  407537.90625,\n",
              "  418040.84375,\n",
              "  400275.71875,\n",
              "  409338.625,\n",
              "  401226.90625,\n",
              "  408830.625,\n",
              "  418167.46875,\n",
              "  408438.0,\n",
              "  424963.09375,\n",
              "  424739.40625,\n",
              "  399779.84375,\n",
              "  414298.0,\n",
              "  406323.96875,\n",
              "  405712.875,\n",
              "  388913.28125,\n",
              "  406281.21875,\n",
              "  398661.21875,\n",
              "  423011.59375,\n",
              "  407500.03125,\n",
              "  424345.0,\n",
              "  408699.34375,\n",
              "  408557.21875,\n",
              "  426108.625,\n",
              "  407670.0,\n",
              "  390702.84375,\n",
              "  416104.375,\n",
              "  390649.65625,\n",
              "  415049.53125,\n",
              "  398176.59375,\n",
              "  422993.90625,\n",
              "  424874.0,\n",
              "  390712.875,\n",
              "  400618.5,\n",
              "  399213.34375,\n",
              "  434354.5,\n",
              "  425185.40625,\n",
              "  410002.75,\n",
              "  427010.09375,\n",
              "  417103.75,\n",
              "  399015.21875,\n",
              "  408484.28125,\n",
              "  425235.78125,\n",
              "  408143.90625,\n",
              "  408608.65625,\n",
              "  418361.5,\n",
              "  417083.59375,\n",
              "  409879.71875,\n",
              "  416342.0,\n",
              "  407730.875,\n",
              "  425389.96875,\n",
              "  416450.96875],\n",
              " 'val_mae': [544.1709594726562,\n",
              "  525.9037475585938,\n",
              "  524.2626342773438,\n",
              "  509.332275390625,\n",
              "  467.923095703125,\n",
              "  491.4367370605469,\n",
              "  477.2088623046875,\n",
              "  469.203125,\n",
              "  469.4810485839844,\n",
              "  476.7440490722656,\n",
              "  470.6662292480469,\n",
              "  468.9158020019531,\n",
              "  466.0409851074219,\n",
              "  454.935546875,\n",
              "  452.5262145996094,\n",
              "  461.0,\n",
              "  480.8072204589844,\n",
              "  466.7185974121094,\n",
              "  451.931640625,\n",
              "  461.3645935058594,\n",
              "  466.208740234375,\n",
              "  457.0627136230469,\n",
              "  461.0833435058594,\n",
              "  471.2826232910156,\n",
              "  457.3020935058594,\n",
              "  464.9495849609375,\n",
              "  463.3958740234375,\n",
              "  462.4236145019531,\n",
              "  463.3958435058594,\n",
              "  464.2494201660156,\n",
              "  463.3958435058594,\n",
              "  463.212646484375,\n",
              "  462.330322265625,\n",
              "  455.9246826171875,\n",
              "  458.1833801269531,\n",
              "  461.3645324707031,\n",
              "  460.669921875,\n",
              "  462.1162109375,\n",
              "  461.8976745605469,\n",
              "  459.0520935058594,\n",
              "  459.4293518066406,\n",
              "  462.7587585449219,\n",
              "  462.3810729980469,\n",
              "  460.8197326660156,\n",
              "  461.8848571777344,\n",
              "  462.1690979003906,\n",
              "  457.3020935058594,\n",
              "  461.751953125,\n",
              "  461.1730651855469,\n",
              "  459.3246154785156,\n",
              "  456.98095703125,\n",
              "  461.25341796875,\n",
              "  461.3518981933594,\n",
              "  456.339599609375,\n",
              "  455.6820373535156,\n",
              "  462.2139587402344,\n",
              "  458.0273132324219,\n",
              "  459.3333435058594,\n",
              "  462.7420959472656,\n",
              "  459.3333435058594,\n",
              "  453.3143005371094,\n",
              "  459.3333435058594,\n",
              "  457.001953125,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  464.1856384277344,\n",
              "  454.3265380859375,\n",
              "  459.6145935058594,\n",
              "  456.043701171875,\n",
              "  450.4238586425781,\n",
              "  452.730224609375,\n",
              "  455.3239440917969,\n",
              "  462.9219970703125,\n",
              "  455.6066589355469,\n",
              "  469.30419921875,\n",
              "  450.6778259277344,\n",
              "  452.8826599121094,\n",
              "  461.0833435058594,\n",
              "  465.7646484375,\n",
              "  464.1446838378906,\n",
              "  454.4051208496094,\n",
              "  447.1900939941406,\n",
              "  454.4483337402344,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  446.5896911621094,\n",
              "  468.9999084472656,\n",
              "  461.3645935058594,\n",
              "  461.0833435058594,\n",
              "  467.0239562988281,\n",
              "  453.2975769042969,\n",
              "  445.3258056640625,\n",
              "  457.3020935058594,\n",
              "  455.0643005371094,\n",
              "  447.1063232421875,\n",
              "  461.3645935058594,\n",
              "  469.6586608886719,\n",
              "  454.724365234375,\n",
              "  459.9053039550781,\n",
              "  468.3802185058594,\n",
              "  456.7048645019531,\n",
              "  457.9462890625,\n",
              "  480.270263671875,\n",
              "  469.8226013183594,\n",
              "  471.37451171875,\n",
              "  447.6817932128906,\n",
              "  473.5829772949219,\n",
              "  468.1504821777344,\n",
              "  474.1888122558594,\n",
              "  476.59619140625,\n",
              "  477.37353515625,\n",
              "  471.4960632324219,\n",
              "  474.0559997558594,\n",
              "  473.39794921875,\n",
              "  474.919189453125,\n",
              "  466.6962585449219,\n",
              "  476.6701354980469,\n",
              "  485.664794921875,\n",
              "  486.4524230957031,\n",
              "  479.0213623046875,\n",
              "  495.8038330078125,\n",
              "  481.0130920410156,\n",
              "  472.4412536621094,\n",
              "  473.3548889160156,\n",
              "  482.756591796875,\n",
              "  483.4159240722656,\n",
              "  491.4328918457031,\n",
              "  508.3028259277344,\n",
              "  492.9664306640625,\n",
              "  502.234619140625,\n",
              "  501.2731628417969,\n",
              "  486.4414367675781,\n",
              "  494.3658142089844,\n",
              "  477.9382629394531,\n",
              "  494.8427734375,\n",
              "  487.7467346191406,\n",
              "  496.0967102050781,\n",
              "  479.8219909667969,\n",
              "  471.9027099609375,\n",
              "  497.8472595214844,\n",
              "  497.6322326660156,\n",
              "  497.4832458496094,\n",
              "  506.2872619628906,\n",
              "  489.6383056640625,\n",
              "  497.45947265625,\n",
              "  497.9314880371094,\n",
              "  498.0704040527344,\n",
              "  482.6583251953125,\n",
              "  498.3614501953125,\n",
              "  499.8849182128906,\n",
              "  491.3994140625,\n",
              "  491.144775390625,\n",
              "  509.0745544433594,\n",
              "  483.5298767089844,\n",
              "  492.8651428222656,\n",
              "  492.1517639160156,\n",
              "  508.8016357421875,\n",
              "  509.7604064941406,\n",
              "  501.4299011230469,\n",
              "  484.8653869628906,\n",
              "  492.490234375,\n",
              "  501.2453308105469,\n",
              "  500.7252197265625,\n",
              "  494.058837890625,\n",
              "  493.525390625,\n",
              "  493.8087463378906,\n",
              "  485.3924865722656,\n",
              "  510.3177185058594,\n",
              "  484.6333923339844,\n",
              "  493.2269592285156,\n",
              "  502.1037292480469,\n",
              "  486.0478515625,\n",
              "  502.4382019042969,\n",
              "  493.59912109375,\n",
              "  493.6744689941406,\n",
              "  511.1766662597656,\n",
              "  494.4359436035156,\n",
              "  503.4117736816406,\n",
              "  494.1490783691406,\n",
              "  502.1142272949219,\n",
              "  486.2261047363281,\n",
              "  485.8020935058594,\n",
              "  494.058349609375,\n",
              "  519.1724243164062,\n",
              "  509.4267272949219,\n",
              "  484.7593994140625,\n",
              "  493.2896423339844,\n",
              "  485.0923767089844,\n",
              "  493.310791015625,\n",
              "  492.7002258300781,\n",
              "  518.44921875,\n",
              "  494.3399963378906,\n",
              "  502.345458984375,\n",
              "  519.3681030273438,\n",
              "  501.708740234375,\n",
              "  502.388671875,\n",
              "  494.436279296875,\n",
              "  493.9774475097656,\n",
              "  502.4301452636719,\n",
              "  494.5564270019531,\n",
              "  485.9194030761719,\n",
              "  511.091796875,\n",
              "  511.0319519042969,\n",
              "  485.1522521972656,\n",
              "  493.3697814941406,\n",
              "  493.57275390625,\n",
              "  492.8876953125,\n",
              "  502.6565246582031,\n",
              "  485.2304382324219,\n",
              "  502.92626953125,\n",
              "  485.6316833496094,\n",
              "  495.1711730957031,\n",
              "  510.8302307128906,\n",
              "  503.8214416503906,\n",
              "  512.4751586914062,\n",
              "  503.0924377441406,\n",
              "  494.5670471191406,\n",
              "  503.318359375,\n",
              "  511.4736633300781,\n",
              "  494.2225646972656,\n",
              "  493.5006103515625,\n",
              "  494.82373046875,\n",
              "  493.8043212890625,\n",
              "  485.1885681152344,\n",
              "  501.8961486816406,\n",
              "  493.9580383300781,\n",
              "  493.9224548339844,\n",
              "  494.6571350097656,\n",
              "  510.587646484375,\n",
              "  503.5380859375,\n",
              "  486.268310546875,\n",
              "  486.2690734863281,\n",
              "  493.58349609375,\n",
              "  493.782470703125,\n",
              "  493.8226013183594,\n",
              "  493.5890197753906,\n",
              "  502.495361328125,\n",
              "  502.7528076171875,\n",
              "  493.8951110839844,\n",
              "  502.3161926269531,\n",
              "  510.4905090332031,\n",
              "  486.1353759765625,\n",
              "  502.4609069824219,\n",
              "  485.8348083496094,\n",
              "  510.1726379394531,\n",
              "  494.5960998535156,\n",
              "  493.5020446777344,\n",
              "  511.1828308105469,\n",
              "  494.3760681152344,\n",
              "  502.808349609375,\n",
              "  485.4217529296875,\n",
              "  485.4658203125,\n",
              "  493.7601623535156,\n",
              "  501.3357849121094,\n",
              "  484.7400817871094,\n",
              "  502.1773681640625,\n",
              "  485.30029296875,\n",
              "  503.052734375,\n",
              "  502.1552734375,\n",
              "  511.0547790527344,\n",
              "  511.0262145996094,\n",
              "  485.808349609375,\n",
              "  502.74560546875,\n",
              "  495.0523986816406,\n",
              "  503.4161682128906,\n",
              "  487.3268737792969,\n",
              "  486.5426330566406,\n",
              "  512.9951171875,\n",
              "  495.324951171875,\n",
              "  503.3065185546875,\n",
              "  486.8907775878906,\n",
              "  504.8221130371094,\n",
              "  486.7448425292969,\n",
              "  495.7210998535156,\n",
              "  495.2086181640625,\n",
              "  503.9001159667969,\n",
              "  495.4103088378906,\n",
              "  504.306884765625,\n",
              "  495.387451171875,\n",
              "  487.3064270019531,\n",
              "  495.1211853027344,\n",
              "  478.6301574707031,\n",
              "  486.5045166015625,\n",
              "  511.6977233886719,\n",
              "  503.3144226074219,\n",
              "  494.5845642089844,\n",
              "  486.9358825683594,\n",
              "  503.0331115722656,\n",
              "  494.6326904296875,\n",
              "  495.067626953125,\n",
              "  493.7565002441406,\n",
              "  494.7825622558594,\n",
              "  519.837646484375,\n",
              "  511.6330871582031,\n",
              "  503.1639099121094,\n",
              "  494.258056640625,\n",
              "  501.6663818359375,\n",
              "  502.1741027832031,\n",
              "  485.9750061035156,\n",
              "  502.5103759765625,\n",
              "  493.3736267089844,\n",
              "  510.9845886230469,\n",
              "  494.2320556640625,\n",
              "  486.4755859375,\n",
              "  503.6805725097656,\n",
              "  495.0776672363281,\n",
              "  486.5398254394531,\n",
              "  478.1410217285156,\n",
              "  494.9588317871094,\n",
              "  502.7108154296875,\n",
              "  494.17529296875,\n",
              "  504.2674865722656,\n",
              "  495.1119384765625,\n",
              "  521.0767211914062,\n",
              "  504.074462890625,\n",
              "  521.1556396484375,\n",
              "  494.7404479980469,\n",
              "  486.5362854003906,\n",
              "  495.6661071777344,\n",
              "  510.8509826660156,\n",
              "  503.377685546875,\n",
              "  486.747802734375,\n",
              "  503.5208740234375,\n",
              "  512.8482666015625,\n",
              "  496.215576171875,\n",
              "  495.9648132324219,\n",
              "  520.9862670898438,\n",
              "  504.1555480957031,\n",
              "  504.2225341796875,\n",
              "  512.5344848632812,\n",
              "  503.1351013183594,\n",
              "  494.0096130371094,\n",
              "  510.89892578125,\n",
              "  476.9439697265625,\n",
              "  485.3307800292969,\n",
              "  510.15771484375,\n",
              "  502.2330017089844,\n",
              "  493.9667053222656,\n",
              "  486.4280700683594,\n",
              "  510.7809753417969,\n",
              "  502.8340148925781,\n",
              "  519.1765747070312,\n",
              "  493.8330993652344,\n",
              "  485.042236328125,\n",
              "  502.6205139160156,\n",
              "  493.5953674316406,\n",
              "  493.5182189941406,\n",
              "  502.3326721191406,\n",
              "  502.7132263183594,\n",
              "  502.317626953125,\n",
              "  501.7356872558594,\n",
              "  493.6169128417969,\n",
              "  502.1726989746094,\n",
              "  511.0646057128906,\n",
              "  485.6858215332031,\n",
              "  494.363525390625,\n",
              "  501.94482421875,\n",
              "  493.9256896972656,\n",
              "  502.0872497558594,\n",
              "  475.7574462890625,\n",
              "  502.2056579589844,\n",
              "  493.3128356933594,\n",
              "  484.8680725097656,\n",
              "  500.9234313964844,\n",
              "  500.9905090332031,\n",
              "  501.798583984375,\n",
              "  493.4613037109375,\n",
              "  501.2743225097656,\n",
              "  510.5401306152344,\n",
              "  493.3902587890625,\n",
              "  492.9813537597656,\n",
              "  500.490478515625,\n",
              "  501.0245666503906,\n",
              "  501.94580078125,\n",
              "  493.1802673339844,\n",
              "  492.1771240234375,\n",
              "  485.0884704589844,\n",
              "  501.3252258300781,\n",
              "  500.9485778808594,\n",
              "  501.7445983886719,\n",
              "  500.9857482910156,\n",
              "  502.1832580566406,\n",
              "  501.1134338378906,\n",
              "  510.0435485839844,\n",
              "  501.7874450683594,\n",
              "  493.3186340332031,\n",
              "  493.6321716308594,\n",
              "  502.3607482910156,\n",
              "  502.2792053222656,\n",
              "  503.3174743652344,\n",
              "  494.2270812988281,\n",
              "  493.4315490722656,\n",
              "  502.3281555175781,\n",
              "  494.4410095214844,\n",
              "  493.646728515625,\n",
              "  502.1172790527344,\n",
              "  510.4917297363281,\n",
              "  494.2667541503906,\n",
              "  503.1864318847656,\n",
              "  510.509765625,\n",
              "  485.4657897949219,\n",
              "  494.1692199707031,\n",
              "  502.8395080566406,\n",
              "  494.5420837402344,\n",
              "  493.1722412109375,\n",
              "  485.8324890136719,\n",
              "  493.0771484375,\n",
              "  511.1974182128906,\n",
              "  502.531982421875,\n",
              "  502.440673828125,\n",
              "  502.4859924316406,\n",
              "  494.2122802734375,\n",
              "  494.689697265625,\n",
              "  501.9963684082031,\n",
              "  493.6463928222656,\n",
              "  494.432861328125,\n",
              "  493.54345703125,\n",
              "  510.6337585449219,\n",
              "  510.3656921386719,\n",
              "  502.1689758300781,\n",
              "  510.547607421875,\n",
              "  494.7275390625,\n",
              "  486.0816345214844,\n",
              "  486.8383483886719,\n",
              "  495.1207580566406,\n",
              "  494.8037414550781,\n",
              "  519.48974609375,\n",
              "  502.72216796875,\n",
              "  486.2828369140625,\n",
              "  511.5431823730469,\n",
              "  503.1296691894531,\n",
              "  502.6351623535156,\n",
              "  502.6096496582031,\n",
              "  511.5434265136719,\n",
              "  502.6995849609375,\n",
              "  494.6127014160156,\n",
              "  495.3050842285156,\n",
              "  520.8646850585938,\n",
              "  494.5422668457031,\n",
              "  503.5945129394531,\n",
              "  495.5305480957031,\n",
              "  512.1953735351562,\n",
              "  503.1717834472656,\n",
              "  511.8966369628906,\n",
              "  495.3594665527344,\n",
              "  503.7115478515625,\n",
              "  495.036865234375,\n",
              "  485.7109375,\n",
              "  493.5673828125,\n",
              "  510.562744140625,\n",
              "  494.33642578125,\n",
              "  494.40234375,\n",
              "  511.2497863769531,\n",
              "  494.4681091308594,\n",
              "  493.7124938964844,\n",
              "  494.36376953125,\n",
              "  493.886474609375,\n",
              "  485.1103515625,\n",
              "  492.674560546875,\n",
              "  509.6383361816406,\n",
              "  493.1443786621094,\n",
              "  500.56787109375,\n",
              "  501.2806701660156,\n",
              "  493.1585693359375,\n",
              "  493.1424255371094,\n",
              "  493.4871520996094,\n",
              "  493.9022521972656,\n",
              "  493.127685546875,\n",
              "  511.2591857910156,\n",
              "  510.7437438964844,\n",
              "  486.1810302734375,\n",
              "  493.5931396484375,\n",
              "  502.9705505371094,\n",
              "  485.5497741699219,\n",
              "  510.099365234375,\n",
              "  484.9940490722656,\n",
              "  493.3473205566406,\n",
              "  494.4015197753906,\n",
              "  502.1101989746094,\n",
              "  476.7959899902344,\n",
              "  493.9665832519531,\n",
              "  485.29443359375,\n",
              "  485.0689392089844,\n",
              "  492.7554626464844,\n",
              "  519.0737915039062,\n",
              "  509.8581848144531,\n",
              "  510.1063537597656,\n",
              "  502.7147521972656,\n",
              "  501.8301696777344,\n",
              "  502.2513427734375,\n",
              "  493.2318420410156,\n",
              "  501.9397277832031,\n",
              "  502.771728515625,\n",
              "  485.154052734375,\n",
              "  493.4608154296875,\n",
              "  493.9150085449219,\n",
              "  502.6280212402344,\n",
              "  503.4843444824219,\n",
              "  504.224365234375,\n",
              "  477.7036437988281,\n",
              "  512.3162231445312,\n",
              "  495.1802062988281,\n",
              "  494.3370056152344,\n",
              "  502.8368225097656,\n",
              "  493.4617614746094,\n",
              "  519.387939453125,\n",
              "  502.8078308105469,\n",
              "  485.7651672363281,\n",
              "  510.9990234375,\n",
              "  494.5550231933594,\n",
              "  502.6324768066406,\n",
              "  502.8147277832031,\n",
              "  495.2760925292969,\n",
              "  486.5718994140625,\n",
              "  503.3375244140625,\n",
              "  503.5433654785156,\n",
              "  503.5611267089844,\n",
              "  512.1432495117188,\n",
              "  486.4630432128906,\n",
              "  502.914306640625,\n",
              "  510.9423828125,\n",
              "  477.0829162597656,\n",
              "  510.8197021484375,\n",
              "  510.9966735839844,\n",
              "  494.0369567871094,\n",
              "  502.0750427246094,\n",
              "  494.3169250488281,\n",
              "  511.479248046875,\n",
              "  485.8572692871094,\n",
              "  494.1877136230469,\n",
              "  494.509765625,\n",
              "  503.19873046875,\n",
              "  494.2542419433594,\n",
              "  503.6676025390625,\n",
              "  503.6758728027344,\n",
              "  511.4805603027344,\n",
              "  477.4564514160156,\n",
              "  503.2381591796875,\n",
              "  503.642578125,\n",
              "  502.5097351074219,\n",
              "  494.4652404785156,\n",
              "  510.6136474609375,\n",
              "  493.867431640625,\n",
              "  509.981689453125,\n",
              "  493.4306335449219,\n",
              "  485.348876953125,\n",
              "  501.3568420410156,\n",
              "  510.477294921875,\n",
              "  493.101806640625,\n",
              "  500.3113098144531,\n",
              "  492.1065368652344,\n",
              "  492.3955993652344,\n",
              "  500.388671875,\n",
              "  500.3885498046875,\n",
              "  483.8922119140625,\n",
              "  492.5374755859375,\n",
              "  508.7945251464844,\n",
              "  493.2603454589844,\n",
              "  501.2103271484375,\n",
              "  501.3758850097656,\n",
              "  501.3941345214844,\n",
              "  484.1590881347656,\n",
              "  493.1458435058594,\n",
              "  476.8482971191406,\n",
              "  477.10986328125,\n",
              "  501.9742431640625,\n",
              "  502.6153869628906,\n",
              "  502.285888671875,\n",
              "  501.2934265136719,\n",
              "  509.6505432128906,\n",
              "  501.629150390625,\n",
              "  492.0774230957031,\n",
              "  517.6797485351562,\n",
              "  493.0970764160156,\n",
              "  493.0126953125,\n",
              "  502.314697265625,\n",
              "  493.1852111816406,\n",
              "  485.462158203125,\n",
              "  484.96923828125,\n",
              "  493.6417541503906,\n",
              "  502.0767517089844,\n",
              "  494.4114685058594,\n",
              "  501.683837890625,\n",
              "  510.2801208496094,\n",
              "  500.9998779296875,\n",
              "  517.356689453125,\n",
              "  509.6337585449219,\n",
              "  484.9841003417969,\n",
              "  493.1539306640625,\n",
              "  501.3507995605469,\n",
              "  491.2275695800781,\n",
              "  483.8021240234375,\n",
              "  501.5823059082031,\n",
              "  492.6390686035156,\n",
              "  510.2016296386719,\n",
              "  493.2660827636719,\n",
              "  492.952880859375,\n",
              "  494.2513122558594,\n",
              "  493.8671875,\n",
              "  484.9207458496094,\n",
              "  510.1365966796875,\n",
              "  494.4520568847656,\n",
              "  510.9190979003906,\n",
              "  517.4296264648438,\n",
              "  484.1803894042969,\n",
              "  494.216064453125,\n",
              "  493.69091796875,\n",
              "  484.5457763671875,\n",
              "  510.1444396972656,\n",
              "  492.6624450683594,\n",
              "  492.7243957519531,\n",
              "  501.3703308105469,\n",
              "  501.9377746582031,\n",
              "  509.5397644042969,\n",
              "  501.5797424316406,\n",
              "  501.3852233886719,\n",
              "  493.1441345214844,\n",
              "  492.61181640625,\n",
              "  501.6222229003906,\n",
              "  501.462890625,\n",
              "  510.0923767089844,\n",
              "  501.018310546875,\n",
              "  484.385009765625,\n",
              "  500.6614074707031,\n",
              "  484.8718566894531,\n",
              "  501.5367736816406,\n",
              "  509.9133605957031,\n",
              "  484.9422302246094,\n",
              "  502.0306701660156,\n",
              "  485.7283630371094,\n",
              "  485.7874450683594,\n",
              "  510.5821838378906,\n",
              "  485.71630859375,\n",
              "  511.0290832519531,\n",
              "  502.0075378417969,\n",
              "  503.8216857910156,\n",
              "  495.1723937988281,\n",
              "  503.859619140625,\n",
              "  504.1328430175781,\n",
              "  486.3505554199219,\n",
              "  504.095703125,\n",
              "  504.4382019042969,\n",
              "  503.8135986328125,\n",
              "  486.8934631347656,\n",
              "  485.3383483886719,\n",
              "  494.0848693847656,\n",
              "  501.9427185058594,\n",
              "  492.6861267089844,\n",
              "  510.8401794433594,\n",
              "  494.1898498535156,\n",
              "  502.1927490234375,\n",
              "  494.2185974121094,\n",
              "  493.665283203125,\n",
              "  501.8791198730469,\n",
              "  501.4369201660156,\n",
              "  492.8665771484375,\n",
              "  484.0082092285156,\n",
              "  501.4619445800781,\n",
              "  510.7417907714844,\n",
              "  502.38037109375,\n",
              "  493.9877014160156,\n",
              "  511.2418212890625,\n",
              "  494.6224670410156,\n",
              "  486.7686462402344,\n",
              "  503.2864685058594,\n",
              "  504.099853515625,\n",
              "  512.1050415039062,\n",
              "  486.8770751953125,\n",
              "  494.8282775878906,\n",
              "  494.4064025878906,\n",
              "  494.6907043457031,\n",
              "  503.5367736816406,\n",
              "  503.4864196777344,\n",
              "  493.7916259765625,\n",
              "  510.5094299316406,\n",
              "  502.6520080566406,\n",
              "  509.9921569824219,\n",
              "  501.60009765625,\n",
              "  501.2016296386719,\n",
              "  493.6645202636719,\n",
              "  476.918701171875,\n",
              "  493.2236633300781,\n",
              "  475.783935546875,\n",
              "  501.4219665527344,\n",
              "  485.7373046875,\n",
              "  502.6211853027344,\n",
              "  485.7376708984375,\n",
              "  493.2185974121094,\n",
              "  518.4987182617188,\n",
              "  485.4573669433594,\n",
              "  485.1655578613281,\n",
              "  493.4839782714844,\n",
              "  503.0846252441406,\n",
              "  502.9779357910156,\n",
              "  511.3030700683594,\n",
              "  485.7799072265625,\n",
              "  494.2354431152344,\n",
              "  493.8018798828125,\n",
              "  494.0668029785156,\n",
              "  510.7744140625,\n",
              "  493.8615417480469,\n",
              "  493.6488342285156,\n",
              "  493.4086608886719,\n",
              "  493.5562438964844,\n",
              "  483.5037536621094,\n",
              "  501.468994140625,\n",
              "  493.3019104003906,\n",
              "  493.0924377441406,\n",
              "  492.7569274902344,\n",
              "  492.2818603515625,\n",
              "  501.4775390625,\n",
              "  493.10986328125,\n",
              "  509.3622131347656,\n",
              "  518.427734375,\n",
              "  509.9532775878906,\n",
              "  499.6490783691406,\n",
              "  500.244873046875,\n",
              "  500.97314453125,\n",
              "  491.4372863769531,\n",
              "  509.1183166503906,\n",
              "  501.3759460449219,\n",
              "  509.59716796875,\n",
              "  501.4761657714844,\n",
              "  508.9627685546875,\n",
              "  493.3110656738281,\n",
              "  510.49609375,\n",
              "  500.748291015625,\n",
              "  501.9202880859375,\n",
              "  501.256103515625,\n",
              "  501.93115234375,\n",
              "  484.4820251464844,\n",
              "  510.1461181640625,\n",
              "  485.39013671875,\n",
              "  509.5089416503906,\n",
              "  483.6551818847656,\n",
              "  493.2829284667969,\n",
              "  493.5195617675781,\n",
              "  485.2822265625,\n",
              "  484.9986877441406,\n",
              "  501.5338439941406,\n",
              "  493.6664123535156,\n",
              "  501.7215881347656,\n",
              "  475.456787109375,\n",
              "  484.6051330566406,\n",
              "  484.3753662109375,\n",
              "  501.9759826660156,\n",
              "  493.5360412597656,\n",
              "  501.7081604003906,\n",
              "  501.0575256347656,\n",
              "  493.5615234375,\n",
              "  511.047607421875,\n",
              "  485.5135192871094,\n",
              "  502.4154968261719,\n",
              "  511.6635437011719,\n",
              "  502.509521484375,\n",
              "  485.0609436035156,\n",
              "  486.51318359375,\n",
              "  502.5436096191406,\n",
              "  493.8590393066406,\n",
              "  494.1997985839844,\n",
              "  485.6076965332031,\n",
              "  510.2533264160156,\n",
              "  493.640380859375,\n",
              "  510.8066711425781,\n",
              "  502.5541076660156,\n",
              "  511.3783264160156,\n",
              "  501.9073181152344,\n",
              "  503.0141296386719,\n",
              "  502.5829772949219,\n",
              "  485.3147277832031,\n",
              "  493.5330505371094,\n",
              "  494.0040283203125,\n",
              "  510.6690368652344,\n",
              "  501.2506103515625,\n",
              "  509.6238708496094,\n",
              "  502.2796630859375,\n",
              "  509.8828125,\n",
              "  494.1534423828125,\n",
              "  494.1121520996094,\n",
              "  502.1479797363281,\n",
              "  493.6699523925781,\n",
              "  485.1412048339844,\n",
              "  510.4155578613281,\n",
              "  494.3260803222656,\n",
              "  510.9678039550781,\n",
              "  477.8125,\n",
              "  512.0613403320312,\n",
              "  494.8435974121094,\n",
              "  503.0068054199219,\n",
              "  511.0542297363281,\n",
              "  486.3504638671875,\n",
              "  503.0723571777344,\n",
              "  485.1512145996094,\n",
              "  510.4276428222656,\n",
              "  494.2898254394531,\n",
              "  510.278564453125,\n",
              "  485.6070251464844,\n",
              "  494.7026672363281,\n",
              "  510.5908203125,\n",
              "  493.865234375,\n",
              "  502.226318359375,\n",
              "  484.9339599609375,\n",
              "  501.9647216796875,\n",
              "  509.8940124511719,\n",
              "  484.884521484375,\n",
              "  485.1849365234375,\n",
              "  486.1722106933594,\n",
              "  502.1928405761719,\n",
              "  494.5537414550781,\n",
              "  501.7181701660156,\n",
              "  502.2350769042969,\n",
              "  493.4188537597656,\n",
              "  493.449951171875,\n",
              "  493.4802551269531,\n",
              "  484.5389709472656,\n",
              "  485.0237731933594,\n",
              "  509.8039855957031,\n",
              "  485.7437744140625,\n",
              "  503.435302734375,\n",
              "  485.33349609375,\n",
              "  502.776123046875,\n",
              "  495.0968017578125,\n",
              "  502.7284851074219,\n",
              "  495.359619140625,\n",
              "  486.7303161621094,\n",
              "  493.7854309082031,\n",
              "  485.8951110839844,\n",
              "  476.8890686035156,\n",
              "  510.9750671386719,\n",
              "  486.4111022949219,\n",
              "  503.0687561035156,\n",
              "  502.2448425292969,\n",
              "  493.9049987792969,\n",
              "  502.6124572753906,\n",
              "  502.4644470214844,\n",
              "  501.768798828125,\n",
              "  510.10693359375,\n",
              "  509.7168884277344,\n",
              "  493.0840759277344,\n",
              "  501.2432556152344,\n",
              "  492.9070129394531,\n",
              "  484.5209655761719,\n",
              "  502.4075622558594,\n",
              "  501.968017578125,\n",
              "  493.5285949707031,\n",
              "  501.18798828125,\n",
              "  509.7991638183594,\n",
              "  501.60302734375,\n",
              "  493.3504943847656,\n",
              "  484.9984436035156,\n",
              "  509.2673645019531,\n",
              "  509.2474365234375,\n",
              "  501.303466796875,\n",
              "  501.8140563964844,\n",
              "  493.4049987792969,\n",
              "  502.3909912109375,\n",
              "  502.38818359375,\n",
              "  501.9925842285156,\n",
              "  501.4281005859375,\n",
              "  493.2929992675781,\n",
              "  501.079833984375,\n",
              "  502.3948059082031,\n",
              "  485.423828125,\n",
              "  485.5608825683594,\n",
              "  510.6232604980469,\n",
              "  493.857421875,\n",
              "  493.6324768066406,\n",
              "  501.7576599121094,\n",
              "  501.8450927734375,\n",
              "  502.3074645996094,\n",
              "  493.4812316894531,\n",
              "  493.3253479003906,\n",
              "  493.2245178222656,\n",
              "  492.9330139160156,\n",
              "  502.0934143066406,\n",
              "  493.960693359375,\n",
              "  502.2645568847656,\n",
              "  502.3533630371094,\n",
              "  484.76904296875,\n",
              "  501.5423583984375,\n",
              "  501.5973205566406,\n",
              "  501.4535217285156,\n",
              "  501.5772705078125,\n",
              "  492.7435607910156,\n",
              "  484.3597106933594,\n",
              "  500.7108154296875,\n",
              "  492.7903137207031,\n",
              "  509.6889953613281,\n",
              "  485.2911071777344,\n",
              "  511.1520080566406,\n",
              "  493.14013671875,\n",
              "  493.5719909667969,\n",
              "  510.8671875,\n",
              "  485.69384765625,\n",
              "  510.6918640136719,\n",
              "  502.1724548339844,\n",
              "  509.770263671875,\n",
              "  502.6091003417969,\n",
              "  502.48779296875,\n",
              "  493.3208923339844,\n",
              "  492.6477966308594,\n",
              "  510.1637878417969,\n",
              "  510.31201171875,\n",
              "  502.1460876464844,\n",
              "  502.31640625,\n",
              "  493.2982482910156,\n",
              "  493.4538879394531,\n",
              "  510.3349609375,\n",
              "  502.421142578125,\n",
              "  501.7057189941406,\n",
              "  501.7295837402344,\n",
              "  493.6275329589844,\n",
              "  502.2037658691406,\n",
              "  493.7557373046875,\n",
              "  518.1806030273438,\n",
              "  502.3008117675781,\n",
              "  501.8310546875,\n",
              "  476.6221618652344,\n",
              "  493.3974304199219,\n",
              "  477.0882263183594,\n",
              "  493.3360290527344,\n",
              "  493.8671569824219,\n",
              "  501.1405334472656,\n",
              "  484.7218017578125,\n",
              "  510.6287536621094,\n",
              "  493.6805725097656,\n",
              "  493.5301818847656,\n",
              "  485.0108337402344,\n",
              "  510.3133850097656,\n",
              "  492.987548828125,\n",
              "  492.7620849609375,\n",
              "  510.6432800292969,\n",
              "  510.92822265625,\n",
              "  493.5276184082031,\n",
              "  502.0437927246094,\n",
              "  510.6116943359375,\n",
              "  502.194091796875,\n",
              "  510.4373474121094,\n",
              "  492.7206115722656,\n",
              "  502.9371643066406,\n",
              "  510.6553649902344,\n",
              "  486.4440612792969,\n",
              "  502.7099609375,\n",
              "  502.5680236816406,\n",
              "  502.0631408691406,\n",
              "  502.181396484375,\n",
              "  494.1957092285156,\n",
              "  502.644775390625,\n",
              "  502.1332702636719,\n",
              "  493.200927734375,\n",
              "  502.6570129394531,\n",
              "  485.5765075683594,\n",
              "  494.218017578125,\n",
              "  485.884033203125,\n",
              "  494.18017578125,\n",
              "  502.9632568359375,\n",
              "  493.4273681640625,\n",
              "  510.3219299316406,\n",
              "  509.9342346191406,\n",
              "  485.2882385253906,\n",
              "  500.8023986816406,\n",
              "  492.7980041503906,\n",
              "  492.1435852050781,\n",
              "  475.3570556640625,\n",
              "  492.7743835449219,\n",
              "  484.6648254394531,\n",
              "  509.2059326171875,\n",
              "  493.1579284667969,\n",
              "  510.259521484375,\n",
              "  493.843505859375,\n",
              "  494.0286865234375,\n",
              "  511.4869689941406,\n",
              "  493.6032409667969,\n",
              "  476.3969421386719,\n",
              "  501.8701171875,\n",
              "  476.3665466308594,\n",
              "  500.9344787597656,\n",
              "  484.3919372558594,\n",
              "  508.901123046875,\n",
              "  510.2710876464844,\n",
              "  476.380126953125,\n",
              "  485.7537536621094,\n",
              "  484.6966857910156,\n",
              "  519.6668090820312,\n",
              "  510.4585266113281,\n",
              "  494.5765075683594,\n",
              "  511.4869689941406,\n",
              "  502.6343994140625,\n",
              "  484.8694763183594,\n",
              "  493.4757385253906,\n",
              "  510.81494140625,\n",
              "  493.801513671875,\n",
              "  494.0569763183594,\n",
              "  503.317626953125,\n",
              "  502.35693359375,\n",
              "  494.755126953125,\n",
              "  501.39892578125,\n",
              "  493.5738830566406,\n",
              "  510.298095703125,\n",
              "  501.75]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "32391685-6f6b-4f15-b977-d03df4093a1b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/regression.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/regression.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ce214724-01d7-4c6d-a2a1-b684f9b55181"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4c890f8c-07f6-45d3-959f-3cff1ece1f70\", \"regression.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrain"
      ],
      "metadata": {
        "id": "owI9dHky6eHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/new/regression.h5')\n",
        "\n",
        "# from efficientnet.layers import Swish, DropConnect\n",
        "# from efficientnet.model import ConvKernalInitializer\n",
        "# from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "# get_custom_objects().update({\n",
        "#     'ConvKernalInitializer': ConvKernalInitializer,\n",
        "#     'Swish': Swish,\n",
        "#     'DropConnect':DropConnect\n",
        "# })"
      ],
      "metadata": {
        "id": "bNX5VLJw54yc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load model \n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('/content/drive/My Drive/new/regression.h5')\n",
        "# height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "zF2vlFE_54wK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # set 'multiply_16' and following layers trainable (Unfreeze --> multiply_16 ) ให้เป็น layers ที่ train ชุดข้อมูลใหม่\n",
        "# model.trainable = True\n",
        "\n",
        "# set_trainable = False\n",
        "# for layer in conv_base.layers:\n",
        "#     if layer.name == 'multiply_15':\n",
        "#         set_trainable = True\n",
        "#     if set_trainable:\n",
        "#         layer.trainable = True\n",
        "#     else:\n",
        "#         layer.trainable = False\n",
        "# print('This is the number of trainable layers '\n",
        "#       'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "1OsX50Tk54tx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "pzptFLAz54rW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss='mse',\n",
        "#               optimizer=Adam(learning_rate=2e-1),\n",
        "#               metrics=['mae'])\n",
        "# history = model.fit_generator(\n",
        "#       train_generator,\n",
        "#       steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "#       epochs=epochs,\n",
        "#       validation_data=validation_generator,\n",
        "#       validation_steps= NUM_TEST //batch_size,\n",
        "#       verbose=1,\n",
        "#       use_multiprocessing=True,\n",
        "#       workers=4)"
      ],
      "metadata": {
        "id": "5os97-2Q54o2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "# mae = history.history['mae']\n",
        "# val_mae = history.history['val_mae']\n",
        "\n",
        "# epochs_x = range(len(loss))\n",
        "\n",
        "# plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "# plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "# plt.title('Training and Mean squared error')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "# plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Z-9GR-Qp6lE3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW_313T26lCd"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}