{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNNItVlhfc8FCa9uiO8R0B6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "b1094375-4450-48f8-cf8b-67ae5446d02f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "32f3ef71-13ca-4247-98ad-64e9d42c25c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e507f1b5-9649-44cf-8f94-b147b214ef17\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e507f1b5-9649-44cf-8f94-b147b214ef17')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e507f1b5-9649-44cf-8f94-b147b214ef17 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e507f1b5-9649-44cf-8f94-b147b214ef17');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "2365cda5-7604-4e46-f4c0-5a3f76d19fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "09de10ea-e8af-46b5-e738-afc8d47fec7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "b26bfbe2-5d15-4500-e80d-16a48658f3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "dfbd91bd-1065-422e-a7ad-4d81900b8baa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "eca02d0c-297f-406d-c180-9d83b4949ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fce0356-a03a-4cca-f21d-b0b60c000c10"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "262b12e5-703d-49ed-d715-38a835da7a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "f78fe990-21e5-49cc-82f6-8f6dd933de4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "9964edf5-44cb-4cc4-c153-e8854f394d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'Class_01',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "eaa4b2a3-f614-40b1-f4ff-266452e2f3ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(lr=1e-5),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30a94a9-6630-4893-c193-57137a5d27ab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "<ipython-input-43-01bcd98702c5>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 62s 1s/step - loss: 0.5774 - mse: 0.5774 - val_loss: 0.2445 - val_mse: 0.2445\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5006 - mse: 0.5006 - val_loss: 0.3626 - val_mse: 0.3626\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.4427 - mse: 0.4427 - val_loss: 0.2228 - val_mse: 0.2228\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.4097 - mse: 0.4097 - val_loss: 0.1331 - val_mse: 0.1331\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.4236 - mse: 0.4236 - val_loss: 0.2534 - val_mse: 0.2534\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.3783 - mse: 0.3783 - val_loss: 0.1253 - val_mse: 0.1253\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.3430 - mse: 0.3430 - val_loss: 0.1911 - val_mse: 0.1911\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2958 - mse: 0.2958 - val_loss: 0.1323 - val_mse: 0.1323\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.3284 - mse: 0.3284 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2849 - mse: 0.2849 - val_loss: 0.2020 - val_mse: 0.2020\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2858 - mse: 0.2858 - val_loss: 0.1182 - val_mse: 0.1182\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.2812 - mse: 0.2812 - val_loss: 0.0944 - val_mse: 0.0944\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2471 - mse: 0.2471 - val_loss: 0.1205 - val_mse: 0.1205\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2775 - mse: 0.2775 - val_loss: 0.1063 - val_mse: 0.1063\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2554 - mse: 0.2554 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.2435 - mse: 0.2435 - val_loss: 0.1049 - val_mse: 0.1049\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2135 - mse: 0.2135 - val_loss: 0.1002 - val_mse: 0.1002\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2302 - mse: 0.2302 - val_loss: 0.1177 - val_mse: 0.1177\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2384 - mse: 0.2384 - val_loss: 0.1007 - val_mse: 0.1007\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2372 - mse: 0.2372 - val_loss: 0.0909 - val_mse: 0.0909\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2145 - mse: 0.2145 - val_loss: 0.1121 - val_mse: 0.1121\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2136 - mse: 0.2136 - val_loss: 0.0897 - val_mse: 0.0897\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2049 - mse: 0.2049 - val_loss: 0.0888 - val_mse: 0.0888\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.2227 - mse: 0.2227 - val_loss: 0.1808 - val_mse: 0.1808\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2238 - mse: 0.2238 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.2214 - mse: 0.2214 - val_loss: 0.1051 - val_mse: 0.1051\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2115 - mse: 0.2115 - val_loss: 0.1188 - val_mse: 0.1188\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2001 - mse: 0.2001 - val_loss: 0.1069 - val_mse: 0.1069\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2181 - mse: 0.2181 - val_loss: 0.1688 - val_mse: 0.1688\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2146 - mse: 0.2146 - val_loss: 0.1211 - val_mse: 0.1211\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2057 - mse: 0.2057 - val_loss: 0.1055 - val_mse: 0.1055\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 4s 73ms/step - loss: 0.1995 - mse: 0.1995 - val_loss: 0.0918 - val_mse: 0.0918\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.2053 - mse: 0.2053 - val_loss: 0.0894 - val_mse: 0.0894\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2055 - mse: 0.2055 - val_loss: 0.0918 - val_mse: 0.0918\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2003 - mse: 0.2003 - val_loss: 0.0883 - val_mse: 0.0883\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2112 - mse: 0.2112 - val_loss: 0.1098 - val_mse: 0.1098\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1929 - mse: 0.1929 - val_loss: 0.0852 - val_mse: 0.0852\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1985 - mse: 0.1985 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2041 - mse: 0.2041 - val_loss: 0.0885 - val_mse: 0.0885\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2107 - mse: 0.2107 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1955 - mse: 0.1955 - val_loss: 0.0873 - val_mse: 0.0873\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2005 - mse: 0.2005 - val_loss: 0.0863 - val_mse: 0.0863\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.0943 - val_mse: 0.0943\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1839 - mse: 0.1839 - val_loss: 0.0965 - val_mse: 0.0965\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.1109 - val_mse: 0.1109\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2004 - mse: 0.2004 - val_loss: 0.1297 - val_mse: 0.1297\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2193 - mse: 0.2193 - val_loss: 0.1154 - val_mse: 0.1154\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2077 - mse: 0.2077 - val_loss: 0.1038 - val_mse: 0.1038\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 0.1945 - mse: 0.1945 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.0899 - val_mse: 0.0899\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.0964 - val_mse: 0.0964\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.1201 - val_mse: 0.1201\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1807 - mse: 0.1807 - val_loss: 0.0897 - val_mse: 0.0897\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2035 - mse: 0.2035 - val_loss: 0.0949 - val_mse: 0.0949\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.1024 - val_mse: 0.1024\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 0.0905 - val_mse: 0.0905\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1884 - mse: 0.1884 - val_loss: 0.0972 - val_mse: 0.0972\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2001 - mse: 0.2001 - val_loss: 0.1131 - val_mse: 0.1131\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.0943 - val_mse: 0.0943\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1940 - mse: 0.1940 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.1225 - val_mse: 0.1225\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1929 - mse: 0.1929 - val_loss: 0.1014 - val_mse: 0.1014\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1928 - mse: 0.1928 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1822 - mse: 0.1822 - val_loss: 0.1217 - val_mse: 0.1217\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2042 - mse: 0.2042 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.2042 - mse: 0.2042 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.1736 - mse: 0.1736 - val_loss: 0.1044 - val_mse: 0.1044\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1896 - mse: 0.1896 - val_loss: 0.0892 - val_mse: 0.0892\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.0996 - val_mse: 0.0996\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1953 - mse: 0.1953 - val_loss: 0.0837 - val_mse: 0.0837\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.1849 - mse: 0.1849 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1794 - mse: 0.1794 - val_loss: 0.0958 - val_mse: 0.0958\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1981 - mse: 0.1981 - val_loss: 0.1150 - val_mse: 0.1150\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1884 - mse: 0.1884 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1804 - mse: 0.1804 - val_loss: 0.1347 - val_mse: 0.1347\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.1162 - val_mse: 0.1162\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.1145 - val_mse: 0.1145\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.0836 - val_mse: 0.0836\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.0853 - val_mse: 0.0853\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1730 - mse: 0.1730 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1717 - mse: 0.1717 - val_loss: 0.0703 - val_mse: 0.0703\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1775 - mse: 0.1775 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.0747 - val_mse: 0.0747\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1710 - mse: 0.1710 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2024 - mse: 0.2024 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.0742 - val_mse: 0.0742\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.1150 - val_mse: 0.1150\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2017 - mse: 0.2017 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1849 - mse: 0.1849 - val_loss: 0.0756 - val_mse: 0.0756\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1930 - mse: 0.1930 - val_loss: 0.0669 - val_mse: 0.0669\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1979 - mse: 0.1979 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.0966 - val_mse: 0.0966\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.0748 - val_mse: 0.0748\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.1005 - val_mse: 0.1005\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2033 - mse: 0.2033 - val_loss: 0.0691 - val_mse: 0.0691\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1996 - mse: 0.1996 - val_loss: 0.0741 - val_mse: 0.0741\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1899 - mse: 0.1899 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1967 - mse: 0.1967 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 10s 236ms/step - loss: 0.1843 - mse: 0.1843 - val_loss: 0.0676 - val_mse: 0.0676\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1821 - mse: 0.1821 - val_loss: 0.0713 - val_mse: 0.0713\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.0712 - val_mse: 0.0712\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.1831 - mse: 0.1831 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1991 - mse: 0.1991 - val_loss: 0.0710 - val_mse: 0.0710\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1987 - mse: 0.1987 - val_loss: 0.0867 - val_mse: 0.0867\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1972 - mse: 0.1972 - val_loss: 0.0693 - val_mse: 0.0693\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1783 - mse: 0.1783 - val_loss: 0.0841 - val_mse: 0.0841\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.0878 - val_mse: 0.0878\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.0621 - val_mse: 0.0621\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1974 - mse: 0.1974 - val_loss: 0.0999 - val_mse: 0.0999\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2001 - mse: 0.2001 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1829 - mse: 0.1829 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.1292 - val_mse: 0.1292\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.1020 - val_mse: 0.1020\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1841 - mse: 0.1841 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2043 - mse: 0.2043 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.0721 - val_mse: 0.0721\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1966 - mse: 0.1966 - val_loss: 0.0997 - val_mse: 0.0997\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1783 - mse: 0.1783 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.0841 - val_mse: 0.0841\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1912 - mse: 0.1912 - val_loss: 0.1000 - val_mse: 0.1000\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1846 - mse: 0.1846 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.0836 - val_mse: 0.0836\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2023 - mse: 0.2023 - val_loss: 0.0748 - val_mse: 0.0748\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1848 - mse: 0.1848 - val_loss: 0.0895 - val_mse: 0.0895\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1985 - mse: 0.1985 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1965 - mse: 0.1965 - val_loss: 0.0983 - val_mse: 0.0983\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.1794 - mse: 0.1794 - val_loss: 0.1246 - val_mse: 0.1246\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.1222 - val_mse: 0.1222\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.0869 - val_mse: 0.0869\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.0980 - val_mse: 0.0980\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.0895 - val_mse: 0.0895\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.0962 - val_mse: 0.0962\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.1978 - mse: 0.1978 - val_loss: 0.1365 - val_mse: 0.1365\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1721 - mse: 0.1721 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1957 - mse: 0.1957 - val_loss: 0.0967 - val_mse: 0.0967\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1938 - mse: 0.1938 - val_loss: 0.0978 - val_mse: 0.0978\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.0869 - val_mse: 0.0869\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 0.1989 - mse: 0.1989 - val_loss: 0.1683 - val_mse: 0.1683\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1942 - mse: 0.1942 - val_loss: 0.1224 - val_mse: 0.1224\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1813 - mse: 0.1813 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1739 - mse: 0.1739 - val_loss: 0.0759 - val_mse: 0.0759\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.0933 - val_mse: 0.0933\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1957 - mse: 0.1957 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.0932 - val_mse: 0.0932\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.1129 - val_mse: 0.1129\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.1757 - mse: 0.1757 - val_loss: 0.0885 - val_mse: 0.0885\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.0962 - val_mse: 0.0962\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2044 - mse: 0.2044 - val_loss: 0.0854 - val_mse: 0.0854\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1717 - mse: 0.1717 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.0901 - val_mse: 0.0901\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1926 - mse: 0.1926 - val_loss: 0.1349 - val_mse: 0.1349\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.0758 - val_mse: 0.0758\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.0865 - val_mse: 0.0865\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.0867 - val_mse: 0.0867\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1977 - mse: 0.1977 - val_loss: 0.1202 - val_mse: 0.1202\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1854 - mse: 0.1854 - val_loss: 0.0879 - val_mse: 0.0879\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.1031 - val_mse: 0.1031\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1904 - mse: 0.1904 - val_loss: 0.0979 - val_mse: 0.0979\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1977 - mse: 0.1977 - val_loss: 0.1356 - val_mse: 0.1356\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.0746 - val_mse: 0.0746\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1947 - mse: 0.1947 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.0841 - val_mse: 0.0841\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1946 - mse: 0.1946 - val_loss: 0.0898 - val_mse: 0.0898\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1822 - mse: 0.1822 - val_loss: 0.0870 - val_mse: 0.0870\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1854 - mse: 0.1854 - val_loss: 0.0746 - val_mse: 0.0746\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.1733 - val_mse: 0.1733\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.1306 - val_mse: 0.1306\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.0845 - val_mse: 0.0845\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1866 - mse: 0.1866 - val_loss: 0.0993 - val_mse: 0.0993\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2015 - mse: 0.2015 - val_loss: 0.0987 - val_mse: 0.0987\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.1292 - val_mse: 0.1292\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1768 - mse: 0.1768 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.1045 - val_mse: 0.1045\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1832 - mse: 0.1832 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1933 - mse: 0.1933 - val_loss: 0.1428 - val_mse: 0.1428\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1857 - mse: 0.1857 - val_loss: 0.1127 - val_mse: 0.1127\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1877 - mse: 0.1877 - val_loss: 0.1189 - val_mse: 0.1189\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 0.1912 - mse: 0.1912 - val_loss: 0.0998 - val_mse: 0.0998\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1943 - mse: 0.1943 - val_loss: 0.0913 - val_mse: 0.0913\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1835 - mse: 0.1835 - val_loss: 0.0955 - val_mse: 0.0955\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1202 - val_mse: 0.1202\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1985 - mse: 0.1985 - val_loss: 0.1030 - val_mse: 0.1030\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.1064 - val_mse: 0.1064\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.1015 - val_mse: 0.1015\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.1927 - mse: 0.1927 - val_loss: 0.1374 - val_mse: 0.1374\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.1087 - val_mse: 0.1087\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.1812 - mse: 0.1812 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1296 - val_mse: 0.1296\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.1023 - val_mse: 0.1023\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2009 - mse: 0.2009 - val_loss: 0.1066 - val_mse: 0.1066\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 0.1006 - val_mse: 0.1006\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1192 - val_mse: 0.1192\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.1790 - mse: 0.1790 - val_loss: 0.1030 - val_mse: 0.1030\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1794 - mse: 0.1794 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.0903 - val_mse: 0.0903\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.0856 - val_mse: 0.0856\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1912 - mse: 0.1912 - val_loss: 0.0944 - val_mse: 0.0944\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1782 - mse: 0.1782 - val_loss: 0.1123 - val_mse: 0.1123\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1852 - mse: 0.1852 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1865 - mse: 0.1865 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.0914 - val_mse: 0.0914\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.1105 - val_mse: 0.1105\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1825 - mse: 0.1825 - val_loss: 0.1278 - val_mse: 0.1278\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1785 - mse: 0.1785 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1841 - mse: 0.1841 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1950 - mse: 0.1950 - val_loss: 0.0996 - val_mse: 0.0996\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.0862 - val_mse: 0.0862\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1752 - mse: 0.1752 - val_loss: 0.1088 - val_mse: 0.1088\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1923 - mse: 0.1923 - val_loss: 0.0902 - val_mse: 0.0902\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1955 - mse: 0.1955 - val_loss: 0.0935 - val_mse: 0.0935\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1777 - mse: 0.1777 - val_loss: 0.0970 - val_mse: 0.0970\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 4s 75ms/step - loss: 0.2002 - mse: 0.2002 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1797 - mse: 0.1797 - val_loss: 0.0906 - val_mse: 0.0906\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.1425 - val_mse: 0.1425\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1946 - mse: 0.1946 - val_loss: 0.1141 - val_mse: 0.1141\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1980 - mse: 0.1980 - val_loss: 0.1130 - val_mse: 0.1130\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.1036 - val_mse: 0.1036\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1763 - mse: 0.1763 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1739 - mse: 0.1739 - val_loss: 0.1229 - val_mse: 0.1229\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1954 - mse: 0.1954 - val_loss: 0.1344 - val_mse: 0.1344\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1691 - mse: 0.1691 - val_loss: 0.0895 - val_mse: 0.0895\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.1897 - mse: 0.1897 - val_loss: 0.1115 - val_mse: 0.1115\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1865 - mse: 0.1865 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.1105 - val_mse: 0.1105\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.0926 - val_mse: 0.0926\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.0909 - val_mse: 0.0909\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1949 - mse: 0.1949 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.0906 - val_mse: 0.0906\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.1732 - mse: 0.1732 - val_loss: 0.0700 - val_mse: 0.0700\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1784 - mse: 0.1784 - val_loss: 0.0735 - val_mse: 0.0735\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1842 - mse: 0.1842 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1898 - mse: 0.1898 - val_loss: 0.1216 - val_mse: 0.1216\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.0961 - val_mse: 0.0961\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.1308 - val_mse: 0.1308\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1953 - mse: 0.1953 - val_loss: 0.1046 - val_mse: 0.1046\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.0883 - val_mse: 0.0883\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2018 - mse: 0.2018 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1680 - mse: 0.1680 - val_loss: 0.0962 - val_mse: 0.0962\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1847 - mse: 0.1847 - val_loss: 0.1041 - val_mse: 0.1041\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1843 - mse: 0.1843 - val_loss: 0.0873 - val_mse: 0.0873\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1924 - mse: 0.1924 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1737 - mse: 0.1737 - val_loss: 0.0913 - val_mse: 0.0913\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.0912 - val_mse: 0.0912\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0842 - val_mse: 0.0842\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1866 - mse: 0.1866 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.0962 - val_mse: 0.0962\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.0842 - val_mse: 0.0842\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1704 - mse: 0.1704 - val_loss: 0.0910 - val_mse: 0.0910\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1800 - mse: 0.1800 - val_loss: 0.0706 - val_mse: 0.0706\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1998 - mse: 0.1998 - val_loss: 0.0665 - val_mse: 0.0665\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1897 - mse: 0.1897 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.1308 - val_mse: 0.1308\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.1332 - val_mse: 0.1332\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1860 - mse: 0.1860 - val_loss: 0.0942 - val_mse: 0.0942\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1812 - mse: 0.1812 - val_loss: 0.1363 - val_mse: 0.1363\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1855 - mse: 0.1855 - val_loss: 0.0885 - val_mse: 0.0885\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1866 - mse: 0.1866 - val_loss: 0.1389 - val_mse: 0.1389\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1815 - mse: 0.1815 - val_loss: 0.1405 - val_mse: 0.1405\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 0.1871 - mse: 0.1871 - val_loss: 0.1099 - val_mse: 0.1099\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1174 - val_mse: 0.1174\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.0985 - val_mse: 0.0985\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1952 - mse: 0.1952 - val_loss: 0.1193 - val_mse: 0.1193\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.1018 - val_mse: 0.1018\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1703 - mse: 0.1703 - val_loss: 0.0964 - val_mse: 0.0964\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1838 - mse: 0.1838 - val_loss: 0.1040 - val_mse: 0.1040\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1928 - mse: 0.1928 - val_loss: 0.1729 - val_mse: 0.1729\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1961 - mse: 0.1961 - val_loss: 0.1287 - val_mse: 0.1287\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.1110 - val_mse: 0.1110\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1714 - mse: 0.1714 - val_loss: 0.1252 - val_mse: 0.1252\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1306 - val_mse: 0.1306\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.1403 - val_mse: 0.1403\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2007 - mse: 0.2007 - val_loss: 0.1180 - val_mse: 0.1180\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1770 - mse: 0.1770 - val_loss: 0.1245 - val_mse: 0.1245\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2019 - mse: 0.2019 - val_loss: 0.1241 - val_mse: 0.1241\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1771 - mse: 0.1771 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1961 - mse: 0.1961 - val_loss: 0.1096 - val_mse: 0.1096\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1827 - mse: 0.1827 - val_loss: 0.1101 - val_mse: 0.1101\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1974 - mse: 0.1974 - val_loss: 0.1164 - val_mse: 0.1164\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1846 - mse: 0.1846 - val_loss: 0.1107 - val_mse: 0.1107\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1804 - mse: 0.1804 - val_loss: 0.1012 - val_mse: 0.1012\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 0.1830 - mse: 0.1830 - val_loss: 0.0866 - val_mse: 0.0866\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1801 - mse: 0.1801 - val_loss: 0.0852 - val_mse: 0.0852\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.0995 - val_mse: 0.0995\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1745 - mse: 0.1745 - val_loss: 0.0972 - val_mse: 0.0972\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1846 - mse: 0.1846 - val_loss: 0.0935 - val_mse: 0.0935\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 0.0997 - val_mse: 0.0997\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1839 - mse: 0.1839 - val_loss: 0.0971 - val_mse: 0.0971\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.0911 - val_mse: 0.0911\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1844 - mse: 0.1844 - val_loss: 0.0903 - val_mse: 0.0903\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1966 - mse: 0.1966 - val_loss: 0.1127 - val_mse: 0.1127\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.1103 - val_mse: 0.1103\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1904 - mse: 0.1904 - val_loss: 0.1840 - val_mse: 0.1840\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1763 - mse: 0.1763 - val_loss: 0.1171 - val_mse: 0.1171\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1816 - mse: 0.1816 - val_loss: 0.1080 - val_mse: 0.1080\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1937 - mse: 0.1937 - val_loss: 0.1719 - val_mse: 0.1719\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1820 - mse: 0.1820 - val_loss: 0.1301 - val_mse: 0.1301\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1250 - val_mse: 0.1250\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.1356 - val_mse: 0.1356\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1336 - val_mse: 0.1336\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2019 - mse: 0.2019 - val_loss: 0.1093 - val_mse: 0.1093\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.1204 - val_mse: 0.1204\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1976 - mse: 0.1976 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1810 - mse: 0.1810 - val_loss: 0.1098 - val_mse: 0.1098\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1836 - mse: 0.1836 - val_loss: 0.1257 - val_mse: 0.1257\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 0.1396 - val_mse: 0.1396\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1824 - mse: 0.1824 - val_loss: 0.1612 - val_mse: 0.1612\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.1741 - val_mse: 0.1741\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1906 - mse: 0.1906 - val_loss: 0.1435 - val_mse: 0.1435\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.1227 - val_mse: 0.1227\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.1246 - val_mse: 0.1246\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1899 - mse: 0.1899 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1279 - val_mse: 0.1279\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2003 - mse: 0.2003 - val_loss: 0.1086 - val_mse: 0.1086\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1966 - mse: 0.1966 - val_loss: 0.1251 - val_mse: 0.1251\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1844 - mse: 0.1844 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1753 - mse: 0.1753 - val_loss: 0.1024 - val_mse: 0.1024\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.0885 - val_mse: 0.0885\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1968 - mse: 0.1968 - val_loss: 0.0972 - val_mse: 0.0972\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.1976 - mse: 0.1976 - val_loss: 0.0974 - val_mse: 0.0974\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2028 - mse: 0.2028 - val_loss: 0.1677 - val_mse: 0.1677\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2046 - mse: 0.2046 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1866 - mse: 0.1866 - val_loss: 0.1121 - val_mse: 0.1121\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1717 - mse: 0.1717 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1772 - mse: 0.1772 - val_loss: 0.1162 - val_mse: 0.1162\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.1176 - val_mse: 0.1176\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.1127 - val_mse: 0.1127\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1950 - mse: 0.1950 - val_loss: 0.1152 - val_mse: 0.1152\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1844 - mse: 0.1844 - val_loss: 0.1237 - val_mse: 0.1237\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.1195 - val_mse: 0.1195\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1977 - mse: 0.1977 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.1901 - mse: 0.1901 - val_loss: 0.1114 - val_mse: 0.1114\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.1356 - val_mse: 0.1356\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1816 - mse: 0.1816 - val_loss: 0.1226 - val_mse: 0.1226\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1011 - val_mse: 0.1011\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.1709 - mse: 0.1709 - val_loss: 0.0969 - val_mse: 0.0969\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.1303 - val_mse: 0.1303\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.1135 - val_mse: 0.1135\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.1917 - mse: 0.1917 - val_loss: 0.1103 - val_mse: 0.1103\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1929 - mse: 0.1929 - val_loss: 0.1033 - val_mse: 0.1033\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.1290 - val_mse: 0.1290\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1974 - mse: 0.1974 - val_loss: 0.1126 - val_mse: 0.1126\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.0923 - val_mse: 0.0923\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.0862 - val_mse: 0.0862\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1785 - mse: 0.1785 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1859 - mse: 0.1859 - val_loss: 0.1059 - val_mse: 0.1059\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1698 - mse: 0.1698 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.1111 - val_mse: 0.1111\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.1886 - mse: 0.1886 - val_loss: 0.1415 - val_mse: 0.1415\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.1731 - mse: 0.1731 - val_loss: 0.1323 - val_mse: 0.1323\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.0939 - val_mse: 0.0939\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1916 - mse: 0.1916 - val_loss: 0.1220 - val_mse: 0.1220\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.1028 - val_mse: 0.1028\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.1001 - val_mse: 0.1001\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1864 - mse: 0.1864 - val_loss: 0.1334 - val_mse: 0.1334\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 0.1108 - val_mse: 0.1108\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1880 - mse: 0.1880 - val_loss: 0.1726 - val_mse: 0.1726\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1924 - mse: 0.1924 - val_loss: 0.1116 - val_mse: 0.1116\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1762 - mse: 0.1762 - val_loss: 0.1234 - val_mse: 0.1234\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1909 - mse: 0.1909 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1897 - mse: 0.1897 - val_loss: 0.0957 - val_mse: 0.0957\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1825 - mse: 0.1825 - val_loss: 0.0918 - val_mse: 0.0918\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1830 - mse: 0.1830 - val_loss: 0.1117 - val_mse: 0.1117\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.1256 - val_mse: 0.1256\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.1063 - val_mse: 0.1063\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2038 - mse: 0.2038 - val_loss: 0.0896 - val_mse: 0.0896\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 0.1052 - val_mse: 0.1052\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.0998 - val_mse: 0.0998\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.0948 - val_mse: 0.0948\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.0924 - val_mse: 0.0924\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1773 - mse: 0.1773 - val_loss: 0.0932 - val_mse: 0.0932\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.0908 - val_mse: 0.0908\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.0740 - val_mse: 0.0740\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1911 - mse: 0.1911 - val_loss: 0.0973 - val_mse: 0.0973\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1663 - mse: 0.1663 - val_loss: 0.0953 - val_mse: 0.0953\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1887 - mse: 0.1887 - val_loss: 0.1305 - val_mse: 0.1305\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1793 - mse: 0.1793 - val_loss: 0.0973 - val_mse: 0.0973\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.0973 - val_mse: 0.0973\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1845 - mse: 0.1845 - val_loss: 0.0945 - val_mse: 0.0945\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1827 - mse: 0.1827 - val_loss: 0.0848 - val_mse: 0.0848\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.0928 - val_mse: 0.0928\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1991 - mse: 0.1991 - val_loss: 0.0928 - val_mse: 0.0928\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.1901 - mse: 0.1901 - val_loss: 0.0874 - val_mse: 0.0874\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1708 - mse: 0.1708 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1821 - mse: 0.1821 - val_loss: 0.0856 - val_mse: 0.0856\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1071 - val_mse: 0.1071\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2072 - mse: 0.2072 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.1022 - val_mse: 0.1022\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1927 - mse: 0.1927 - val_loss: 0.1186 - val_mse: 0.1186\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.0870 - val_mse: 0.0870\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.0932 - val_mse: 0.0932\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1940 - mse: 0.1940 - val_loss: 0.0889 - val_mse: 0.0889\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1827 - mse: 0.1827 - val_loss: 0.0896 - val_mse: 0.0896\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.0938 - val_mse: 0.0938\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1742 - mse: 0.1742 - val_loss: 0.0972 - val_mse: 0.0972\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1982 - mse: 0.1982 - val_loss: 0.0997 - val_mse: 0.0997\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1382 - val_mse: 0.1382\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1881 - mse: 0.1881 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 0.1109 - val_mse: 0.1109\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1893 - mse: 0.1893 - val_loss: 0.1003 - val_mse: 0.1003\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1904 - mse: 0.1904 - val_loss: 0.1043 - val_mse: 0.1043\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1788 - mse: 0.1788 - val_loss: 0.0933 - val_mse: 0.0933\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1739 - mse: 0.1739 - val_loss: 0.0899 - val_mse: 0.0899\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1956 - mse: 0.1956 - val_loss: 0.1291 - val_mse: 0.1291\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1923 - mse: 0.1923 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2024 - mse: 0.2024 - val_loss: 0.0946 - val_mse: 0.0946\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1940 - mse: 0.1940 - val_loss: 0.1123 - val_mse: 0.1123\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1934 - mse: 0.1934 - val_loss: 0.0952 - val_mse: 0.0952\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1782 - mse: 0.1782 - val_loss: 0.0917 - val_mse: 0.0917\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.0850 - val_mse: 0.0850\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.1054 - val_mse: 0.1054\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1796 - mse: 0.1796 - val_loss: 0.0993 - val_mse: 0.0993\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.2107 - mse: 0.2107 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.0878 - val_mse: 0.0878\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1925 - mse: 0.1925 - val_loss: 0.0837 - val_mse: 0.0837\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1915 - mse: 0.1915 - val_loss: 0.0905 - val_mse: 0.0905\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.1210 - val_mse: 0.1210\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1807 - mse: 0.1807 - val_loss: 0.0880 - val_mse: 0.0880\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.0918 - val_mse: 0.0918\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2009 - mse: 0.2009 - val_loss: 0.1009 - val_mse: 0.1009\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1755 - mse: 0.1755 - val_loss: 0.0930 - val_mse: 0.0930\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.1273 - val_mse: 0.1273\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1881 - mse: 0.1881 - val_loss: 0.1048 - val_mse: 0.1048\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1188 - val_mse: 0.1188\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1128 - val_mse: 0.1128\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1946 - mse: 0.1946 - val_loss: 0.1130 - val_mse: 0.1130\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1797 - mse: 0.1797 - val_loss: 0.1136 - val_mse: 0.1136\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.1035 - val_mse: 0.1035\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2030 - mse: 0.2030 - val_loss: 0.1067 - val_mse: 0.1067\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.1039 - val_mse: 0.1039\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1950 - mse: 0.1950 - val_loss: 0.1050 - val_mse: 0.1050\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.0901 - val_mse: 0.0901\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1887 - mse: 0.1887 - val_loss: 0.0884 - val_mse: 0.0884\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.1147 - val_mse: 0.1147\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1961 - mse: 0.1961 - val_loss: 0.1144 - val_mse: 0.1144\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.1804 - mse: 0.1804 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1877 - mse: 0.1877 - val_loss: 0.1092 - val_mse: 0.1092\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2035 - mse: 0.2035 - val_loss: 0.1644 - val_mse: 0.1644\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1934 - mse: 0.1934 - val_loss: 0.1084 - val_mse: 0.1084\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1868 - mse: 0.1868 - val_loss: 0.1156 - val_mse: 0.1156\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1822 - mse: 0.1822 - val_loss: 0.1197 - val_mse: 0.1197\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1828 - mse: 0.1828 - val_loss: 0.1034 - val_mse: 0.1034\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1738 - mse: 0.1738 - val_loss: 0.0945 - val_mse: 0.0945\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1847 - mse: 0.1847 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1961 - mse: 0.1961 - val_loss: 0.1027 - val_mse: 0.1027\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.0906 - val_mse: 0.0906\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1766 - mse: 0.1766 - val_loss: 0.0959 - val_mse: 0.0959\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1690 - mse: 0.1690 - val_loss: 0.1242 - val_mse: 0.1242\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.1131 - val_mse: 0.1131\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1934 - mse: 0.1934 - val_loss: 0.1327 - val_mse: 0.1327\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1848 - mse: 0.1848 - val_loss: 0.1207 - val_mse: 0.1207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "6d65b6f9-a924-4da5-ea29-22e728abc634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.5774011015892029,\n",
              "  0.5006014108657837,\n",
              "  0.4426843225955963,\n",
              "  0.40971362590789795,\n",
              "  0.4235830307006836,\n",
              "  0.37834322452545166,\n",
              "  0.3429837226867676,\n",
              "  0.2957836985588074,\n",
              "  0.32837846875190735,\n",
              "  0.28491491079330444,\n",
              "  0.2858189344406128,\n",
              "  0.2812420725822449,\n",
              "  0.24712660908699036,\n",
              "  0.2775425612926483,\n",
              "  0.25541165471076965,\n",
              "  0.2434801161289215,\n",
              "  0.21352575719356537,\n",
              "  0.23017749190330505,\n",
              "  0.23836863040924072,\n",
              "  0.23715092241764069,\n",
              "  0.2324976921081543,\n",
              "  0.21445901691913605,\n",
              "  0.21362827718257904,\n",
              "  0.2048645168542862,\n",
              "  0.22270140051841736,\n",
              "  0.2238277643918991,\n",
              "  0.22138361632823944,\n",
              "  0.2115323841571808,\n",
              "  0.20007431507110596,\n",
              "  0.21812541782855988,\n",
              "  0.2146192044019699,\n",
              "  0.20574717223644257,\n",
              "  0.1994796246290207,\n",
              "  0.20533747971057892,\n",
              "  0.20551049709320068,\n",
              "  0.20030555129051208,\n",
              "  0.1823272854089737,\n",
              "  0.21119150519371033,\n",
              "  0.1928846836090088,\n",
              "  0.19854438304901123,\n",
              "  0.20409034192562103,\n",
              "  0.21067894995212555,\n",
              "  0.1954602748155594,\n",
              "  0.20048516988754272,\n",
              "  0.19859722256660461,\n",
              "  0.18386287987232208,\n",
              "  0.19860760867595673,\n",
              "  0.20041556656360626,\n",
              "  0.21934141218662262,\n",
              "  0.20773163437843323,\n",
              "  0.19449400901794434,\n",
              "  0.1888122260570526,\n",
              "  0.18786479532718658,\n",
              "  0.18184660375118256,\n",
              "  0.18070383369922638,\n",
              "  0.20352987945079803,\n",
              "  0.19333411753177643,\n",
              "  0.20520634949207306,\n",
              "  0.1892862617969513,\n",
              "  0.1883939951658249,\n",
              "  0.20005211234092712,\n",
              "  0.19195012748241425,\n",
              "  0.19396397471427917,\n",
              "  0.19476382434368134,\n",
              "  0.19293422996997833,\n",
              "  0.19278940558433533,\n",
              "  0.18217507004737854,\n",
              "  0.20420603454113007,\n",
              "  0.20424669981002808,\n",
              "  0.1735609769821167,\n",
              "  0.18961672484874725,\n",
              "  0.18719200789928436,\n",
              "  0.19526638090610504,\n",
              "  0.1849149465560913,\n",
              "  0.1793828308582306,\n",
              "  0.19808551669120789,\n",
              "  0.18836314976215363,\n",
              "  0.1803959608078003,\n",
              "  0.18998989462852478,\n",
              "  0.1870729774236679,\n",
              "  0.18055018782615662,\n",
              "  0.1870485246181488,\n",
              "  0.17296768724918365,\n",
              "  0.1889834702014923,\n",
              "  0.1716654747724533,\n",
              "  0.17751263082027435,\n",
              "  0.1917380541563034,\n",
              "  0.17103834450244904,\n",
              "  0.20240511000156403,\n",
              "  0.19077496230602264,\n",
              "  0.18893426656723022,\n",
              "  0.18589083850383759,\n",
              "  0.20170216262340546,\n",
              "  0.1849432736635208,\n",
              "  0.19299398362636566,\n",
              "  0.19788381457328796,\n",
              "  0.18921029567718506,\n",
              "  0.18804775178432465,\n",
              "  0.1878456473350525,\n",
              "  0.18682843446731567,\n",
              "  0.1808779537677765,\n",
              "  0.203286275267601,\n",
              "  0.19960077106952667,\n",
              "  0.18985475599765778,\n",
              "  0.1966586709022522,\n",
              "  0.18429812788963318,\n",
              "  0.18211165070533752,\n",
              "  0.1724771112203598,\n",
              "  0.18314503133296967,\n",
              "  0.19914524257183075,\n",
              "  0.19866086542606354,\n",
              "  0.19719314575195312,\n",
              "  0.18775585293769836,\n",
              "  0.1916731297969818,\n",
              "  0.17828907072544098,\n",
              "  0.18832021951675415,\n",
              "  0.18588033318519592,\n",
              "  0.19740155339241028,\n",
              "  0.2001410275697708,\n",
              "  0.18821008503437042,\n",
              "  0.17869353294372559,\n",
              "  0.18293282389640808,\n",
              "  0.19388853013515472,\n",
              "  0.18263396620750427,\n",
              "  0.18414460122585297,\n",
              "  0.20431561768054962,\n",
              "  0.19110886752605438,\n",
              "  0.19659961760044098,\n",
              "  0.17833496630191803,\n",
              "  0.18764926493167877,\n",
              "  0.1911911517381668,\n",
              "  0.1845633089542389,\n",
              "  0.1794806867837906,\n",
              "  0.1886497437953949,\n",
              "  0.20225279033184052,\n",
              "  0.17986740171909332,\n",
              "  0.18476758897304535,\n",
              "  0.19851703941822052,\n",
              "  0.19645102322101593,\n",
              "  0.17940248548984528,\n",
              "  0.18614377081394196,\n",
              "  0.19017767906188965,\n",
              "  0.18762877583503723,\n",
              "  0.19564189016819,\n",
              "  0.18761110305786133,\n",
              "  0.1856357753276825,\n",
              "  0.19778022170066833,\n",
              "  0.17211657762527466,\n",
              "  0.19573336839675903,\n",
              "  0.1860511302947998,\n",
              "  0.19377130270004272,\n",
              "  0.1858159750699997,\n",
              "  0.19697794318199158,\n",
              "  0.19891048967838287,\n",
              "  0.19417306780815125,\n",
              "  0.1787179559469223,\n",
              "  0.18128831684589386,\n",
              "  0.17386600375175476,\n",
              "  0.19221796095371246,\n",
              "  0.18901430070400238,\n",
              "  0.19570191204547882,\n",
              "  0.18829922378063202,\n",
              "  0.1791887730360031,\n",
              "  0.1832546442747116,\n",
              "  0.17572633922100067,\n",
              "  0.18833574652671814,\n",
              "  0.20443807542324066,\n",
              "  0.18699467182159424,\n",
              "  0.17174841463565826,\n",
              "  0.19476543366909027,\n",
              "  0.19258855283260345,\n",
              "  0.1805134117603302,\n",
              "  0.19247941672801971,\n",
              "  0.18527916073799133,\n",
              "  0.18045595288276672,\n",
              "  0.1976747214794159,\n",
              "  0.18535439670085907,\n",
              "  0.1809321939945221,\n",
              "  0.190414160490036,\n",
              "  0.19766244292259216,\n",
              "  0.185265451669693,\n",
              "  0.19473174214363098,\n",
              "  0.19191423058509827,\n",
              "  0.1945977658033371,\n",
              "  0.18223784863948822,\n",
              "  0.18539103865623474,\n",
              "  0.18419088423252106,\n",
              "  0.18922583758831024,\n",
              "  0.19556617736816406,\n",
              "  0.17893601953983307,\n",
              "  0.18660205602645874,\n",
              "  0.18906304240226746,\n",
              "  0.20148052275180817,\n",
              "  0.1918213963508606,\n",
              "  0.17680633068084717,\n",
              "  0.18733590841293335,\n",
              "  0.18320925533771515,\n",
              "  0.1932758092880249,\n",
              "  0.1856924295425415,\n",
              "  0.18766950070858002,\n",
              "  0.1911890059709549,\n",
              "  0.19425825774669647,\n",
              "  0.18354575335979462,\n",
              "  0.1919689178466797,\n",
              "  0.1861570179462433,\n",
              "  0.18338347971439362,\n",
              "  0.1985468566417694,\n",
              "  0.19015300273895264,\n",
              "  0.19105765223503113,\n",
              "  0.19269762933254242,\n",
              "  0.19172292947769165,\n",
              "  0.18918368220329285,\n",
              "  0.1883094608783722,\n",
              "  0.1812174916267395,\n",
              "  0.1855504959821701,\n",
              "  0.1868479996919632,\n",
              "  0.19076868891716003,\n",
              "  0.20093563199043274,\n",
              "  0.1906406581401825,\n",
              "  0.182294562458992,\n",
              "  0.17896322906017303,\n",
              "  0.1793905794620514,\n",
              "  0.1863316148519516,\n",
              "  0.18901784718036652,\n",
              "  0.19122004508972168,\n",
              "  0.1782037317752838,\n",
              "  0.18520502746105194,\n",
              "  0.1865011751651764,\n",
              "  0.1819022297859192,\n",
              "  0.19387657940387726,\n",
              "  0.18248552083969116,\n",
              "  0.18818548321723938,\n",
              "  0.17847444117069244,\n",
              "  0.18413414061069489,\n",
              "  0.1949663609266281,\n",
              "  0.18339264392852783,\n",
              "  0.19175133109092712,\n",
              "  0.1751960813999176,\n",
              "  0.1922786980867386,\n",
              "  0.19547997415065765,\n",
              "  0.17765116691589355,\n",
              "  0.20019477605819702,\n",
              "  0.17966483533382416,\n",
              "  0.17437227070331573,\n",
              "  0.19456404447555542,\n",
              "  0.1980430632829666,\n",
              "  0.18739058077335358,\n",
              "  0.17632733285427094,\n",
              "  0.17393635213375092,\n",
              "  0.1882217824459076,\n",
              "  0.19544386863708496,\n",
              "  0.16913959383964539,\n",
              "  0.1862376630306244,\n",
              "  0.18972909450531006,\n",
              "  0.1865222156047821,\n",
              "  0.1856350302696228,\n",
              "  0.18688076734542847,\n",
              "  0.1868610829114914,\n",
              "  0.17763082683086395,\n",
              "  0.18861833214759827,\n",
              "  0.1948835402727127,\n",
              "  0.1789286583662033,\n",
              "  0.17317914962768555,\n",
              "  0.17843014001846313,\n",
              "  0.18421438336372375,\n",
              "  0.18975360691547394,\n",
              "  0.18334074318408966,\n",
              "  0.1860741525888443,\n",
              "  0.19532127678394318,\n",
              "  0.1890583038330078,\n",
              "  0.20176135003566742,\n",
              "  0.1827918440103531,\n",
              "  0.16802020370960236,\n",
              "  0.18465344607830048,\n",
              "  0.18032433092594147,\n",
              "  0.184340700507164,\n",
              "  0.19239160418510437,\n",
              "  0.1736828237771988,\n",
              "  0.1888928860425949,\n",
              "  0.1799173653125763,\n",
              "  0.18656106293201447,\n",
              "  0.18700608611106873,\n",
              "  0.1792069673538208,\n",
              "  0.1704109162092209,\n",
              "  0.179960697889328,\n",
              "  0.1998341828584671,\n",
              "  0.18965986371040344,\n",
              "  0.1852789670228958,\n",
              "  0.19860491156578064,\n",
              "  0.18603339791297913,\n",
              "  0.18116137385368347,\n",
              "  0.1855003386735916,\n",
              "  0.18580450117588043,\n",
              "  0.1865566074848175,\n",
              "  0.18145833909511566,\n",
              "  0.1871384084224701,\n",
              "  0.1888592690229416,\n",
              "  0.1814420521259308,\n",
              "  0.1952333152294159,\n",
              "  0.19017188251018524,\n",
              "  0.17025083303451538,\n",
              "  0.18381792306900024,\n",
              "  0.1985936313867569,\n",
              "  0.19281262159347534,\n",
              "  0.19613134860992432,\n",
              "  0.17133866250514984,\n",
              "  0.17137394845485687,\n",
              "  0.18878790736198425,\n",
              "  0.1858024150133133,\n",
              "  0.20520463585853577,\n",
              "  0.20068345963954926,\n",
              "  0.17699873447418213,\n",
              "  0.201947420835495,\n",
              "  0.17708703875541687,\n",
              "  0.1961350440979004,\n",
              "  0.18271958827972412,\n",
              "  0.19744344055652618,\n",
              "  0.18457287549972534,\n",
              "  0.18926958739757538,\n",
              "  0.18044359982013702,\n",
              "  0.1750802844762802,\n",
              "  0.1829700917005539,\n",
              "  0.18005870282649994,\n",
              "  0.18881624937057495,\n",
              "  0.17451722919940948,\n",
              "  0.1845608651638031,\n",
              "  0.1850956678390503,\n",
              "  0.18387489020824432,\n",
              "  0.1892319917678833,\n",
              "  0.1844082921743393,\n",
              "  0.19664308428764343,\n",
              "  0.18923135101795197,\n",
              "  0.19037042558193207,\n",
              "  0.17629621922969818,\n",
              "  0.1816425323486328,\n",
              "  0.19374307990074158,\n",
              "  0.18200568854808807,\n",
              "  0.18230296671390533,\n",
              "  0.19114956259727478,\n",
              "  0.18820318579673767,\n",
              "  0.20191319286823273,\n",
              "  0.17271274328231812,\n",
              "  0.1975751370191574,\n",
              "  0.18101128935813904,\n",
              "  0.18361066281795502,\n",
              "  0.16970838606357574,\n",
              "  0.1921389400959015,\n",
              "  0.1824445128440857,\n",
              "  0.1867128163576126,\n",
              "  0.19064027070999146,\n",
              "  0.19174829125404358,\n",
              "  0.19079852104187012,\n",
              "  0.1898578554391861,\n",
              "  0.18400761485099792,\n",
              "  0.20032542943954468,\n",
              "  0.19658708572387695,\n",
              "  0.18441706895828247,\n",
              "  0.1752638965845108,\n",
              "  0.18911835551261902,\n",
              "  0.19677597284317017,\n",
              "  0.17507775127887726,\n",
              "  0.1976304054260254,\n",
              "  0.20279519259929657,\n",
              "  0.2046043872833252,\n",
              "  0.18656320869922638,\n",
              "  0.17167581617832184,\n",
              "  0.17716462910175323,\n",
              "  0.1845150589942932,\n",
              "  0.18816998600959778,\n",
              "  0.19495832920074463,\n",
              "  0.1843613237142563,\n",
              "  0.1874062567949295,\n",
              "  0.19772398471832275,\n",
              "  0.19012723863124847,\n",
              "  0.18587195873260498,\n",
              "  0.1815991997718811,\n",
              "  0.185626819729805,\n",
              "  0.17088811099529266,\n",
              "  0.18804478645324707,\n",
              "  0.18743830919265747,\n",
              "  0.19168324768543243,\n",
              "  0.18402987718582153,\n",
              "  0.192854106426239,\n",
              "  0.18932348489761353,\n",
              "  0.1973775327205658,\n",
              "  0.18778732419013977,\n",
              "  0.18804915249347687,\n",
              "  0.17993074655532837,\n",
              "  0.17852236330509186,\n",
              "  0.18589626252651215,\n",
              "  0.16981495916843414,\n",
              "  0.17868587374687195,\n",
              "  0.1886373609304428,\n",
              "  0.17305617034435272,\n",
              "  0.17985780537128448,\n",
              "  0.19155031442642212,\n",
              "  0.1818164885044098,\n",
              "  0.17950430512428284,\n",
              "  0.18639978766441345,\n",
              "  0.1851232796907425,\n",
              "  0.18802429735660553,\n",
              "  0.19238214194774628,\n",
              "  0.17615166306495667,\n",
              "  0.19086305797100067,\n",
              "  0.18969129025936127,\n",
              "  0.18246950209140778,\n",
              "  0.18303139507770538,\n",
              "  0.1818663477897644,\n",
              "  0.18062177300453186,\n",
              "  0.20378106832504272,\n",
              "  0.18507800996303558,\n",
              "  0.19176065921783447,\n",
              "  0.17252971231937408,\n",
              "  0.18909452855587006,\n",
              "  0.17725694179534912,\n",
              "  0.18327176570892334,\n",
              "  0.19214749336242676,\n",
              "  0.1910511553287506,\n",
              "  0.18749547004699707,\n",
              "  0.1663159430027008,\n",
              "  0.1886703222990036,\n",
              "  0.1793099045753479,\n",
              "  0.1819373518228531,\n",
              "  0.18448300659656525,\n",
              "  0.18273505568504333,\n",
              "  0.19000086188316345,\n",
              "  0.1990618109703064,\n",
              "  0.19010262191295624,\n",
              "  0.1707921028137207,\n",
              "  0.18214039504528046,\n",
              "  0.1888553500175476,\n",
              "  0.20724844932556152,\n",
              "  0.19692814350128174,\n",
              "  0.19265116751194,\n",
              "  0.1834079474210739,\n",
              "  0.1889752596616745,\n",
              "  0.1939520388841629,\n",
              "  0.18272973597049713,\n",
              "  0.17873282730579376,\n",
              "  0.17416569590568542,\n",
              "  0.181412011384964,\n",
              "  0.19815248250961304,\n",
              "  0.17755229771137238,\n",
              "  0.18811644613742828,\n",
              "  0.18191900849342346,\n",
              "  0.18926550447940826,\n",
              "  0.19041118025779724,\n",
              "  0.18891969323158264,\n",
              "  0.1788044571876526,\n",
              "  0.17389290034770966,\n",
              "  0.19559887051582336,\n",
              "  0.19234131276607513,\n",
              "  0.2023676335811615,\n",
              "  0.1832733154296875,\n",
              "  0.1940489113330841,\n",
              "  0.19344672560691833,\n",
              "  0.17819075286388397,\n",
              "  0.19076357781887054,\n",
              "  0.18693861365318298,\n",
              "  0.1827908605337143,\n",
              "  0.179572194814682,\n",
              "  0.2107337862253189,\n",
              "  0.17756438255310059,\n",
              "  0.1924651563167572,\n",
              "  0.19147232174873352,\n",
              "  0.18139396607875824,\n",
              "  0.18071313202381134,\n",
              "  0.1791967898607254,\n",
              "  0.20087766647338867,\n",
              "  0.17548568546772003,\n",
              "  0.19222837686538696,\n",
              "  0.18813499808311462,\n",
              "  0.18887488543987274,\n",
              "  0.17757073044776917,\n",
              "  0.1946481317281723,\n",
              "  0.1796826720237732,\n",
              "  0.17131495475769043,\n",
              "  0.2029721736907959,\n",
              "  0.1758727729320526,\n",
              "  0.19500960409641266,\n",
              "  0.18740883469581604,\n",
              "  0.18870072066783905,\n",
              "  0.19348226487636566,\n",
              "  0.19613772630691528,\n",
              "  0.18041379749774933,\n",
              "  0.18767647445201874,\n",
              "  0.2034720778465271,\n",
              "  0.1933661848306656,\n",
              "  0.18684960901737213,\n",
              "  0.18220920860767365,\n",
              "  0.18283198773860931,\n",
              "  0.1738266497850418,\n",
              "  0.18465353548526764,\n",
              "  0.19606661796569824,\n",
              "  0.1724964678287506,\n",
              "  0.1766122579574585,\n",
              "  0.16904637217521667,\n",
              "  0.18880102038383484,\n",
              "  0.1934107542037964,\n",
              "  0.18484117090702057],\n",
              " 'mse': [0.5774011015892029,\n",
              "  0.5006014108657837,\n",
              "  0.4426843225955963,\n",
              "  0.40971362590789795,\n",
              "  0.4235830307006836,\n",
              "  0.37834322452545166,\n",
              "  0.3429837226867676,\n",
              "  0.2957836985588074,\n",
              "  0.32837846875190735,\n",
              "  0.28491491079330444,\n",
              "  0.2858189344406128,\n",
              "  0.2812420725822449,\n",
              "  0.24712660908699036,\n",
              "  0.2775425612926483,\n",
              "  0.25541165471076965,\n",
              "  0.2434801161289215,\n",
              "  0.21352575719356537,\n",
              "  0.23017749190330505,\n",
              "  0.23836863040924072,\n",
              "  0.23715092241764069,\n",
              "  0.2324976921081543,\n",
              "  0.21445901691913605,\n",
              "  0.21362827718257904,\n",
              "  0.2048645168542862,\n",
              "  0.22270140051841736,\n",
              "  0.2238277643918991,\n",
              "  0.22138361632823944,\n",
              "  0.2115323841571808,\n",
              "  0.20007431507110596,\n",
              "  0.21812541782855988,\n",
              "  0.2146192044019699,\n",
              "  0.20574717223644257,\n",
              "  0.1994796246290207,\n",
              "  0.20533747971057892,\n",
              "  0.20551049709320068,\n",
              "  0.20030555129051208,\n",
              "  0.1823272854089737,\n",
              "  0.21119150519371033,\n",
              "  0.1928846836090088,\n",
              "  0.19854438304901123,\n",
              "  0.20409034192562103,\n",
              "  0.21067894995212555,\n",
              "  0.1954602748155594,\n",
              "  0.20048516988754272,\n",
              "  0.19859722256660461,\n",
              "  0.18386287987232208,\n",
              "  0.19860760867595673,\n",
              "  0.20041556656360626,\n",
              "  0.21934141218662262,\n",
              "  0.20773163437843323,\n",
              "  0.19449400901794434,\n",
              "  0.1888122260570526,\n",
              "  0.18786479532718658,\n",
              "  0.18184660375118256,\n",
              "  0.18070383369922638,\n",
              "  0.20352987945079803,\n",
              "  0.19333411753177643,\n",
              "  0.20520634949207306,\n",
              "  0.1892862617969513,\n",
              "  0.1883939951658249,\n",
              "  0.20005211234092712,\n",
              "  0.19195012748241425,\n",
              "  0.19396397471427917,\n",
              "  0.19476382434368134,\n",
              "  0.19293422996997833,\n",
              "  0.19278940558433533,\n",
              "  0.18217507004737854,\n",
              "  0.20420603454113007,\n",
              "  0.20424669981002808,\n",
              "  0.1735609769821167,\n",
              "  0.18961672484874725,\n",
              "  0.18719200789928436,\n",
              "  0.19526638090610504,\n",
              "  0.1849149465560913,\n",
              "  0.1793828308582306,\n",
              "  0.19808551669120789,\n",
              "  0.18836314976215363,\n",
              "  0.1803959608078003,\n",
              "  0.18998989462852478,\n",
              "  0.1870729774236679,\n",
              "  0.18055018782615662,\n",
              "  0.1870485246181488,\n",
              "  0.17296768724918365,\n",
              "  0.1889834702014923,\n",
              "  0.1716654747724533,\n",
              "  0.17751263082027435,\n",
              "  0.1917380541563034,\n",
              "  0.17103834450244904,\n",
              "  0.20240511000156403,\n",
              "  0.19077496230602264,\n",
              "  0.18893426656723022,\n",
              "  0.18589083850383759,\n",
              "  0.20170216262340546,\n",
              "  0.1849432736635208,\n",
              "  0.19299398362636566,\n",
              "  0.19788381457328796,\n",
              "  0.18921029567718506,\n",
              "  0.18804775178432465,\n",
              "  0.1878456473350525,\n",
              "  0.18682843446731567,\n",
              "  0.1808779537677765,\n",
              "  0.203286275267601,\n",
              "  0.19960077106952667,\n",
              "  0.18985475599765778,\n",
              "  0.1966586709022522,\n",
              "  0.18429812788963318,\n",
              "  0.18211165070533752,\n",
              "  0.1724771112203598,\n",
              "  0.18314503133296967,\n",
              "  0.19914524257183075,\n",
              "  0.19866086542606354,\n",
              "  0.19719314575195312,\n",
              "  0.18775585293769836,\n",
              "  0.1916731297969818,\n",
              "  0.17828907072544098,\n",
              "  0.18832021951675415,\n",
              "  0.18588033318519592,\n",
              "  0.19740155339241028,\n",
              "  0.2001410275697708,\n",
              "  0.18821008503437042,\n",
              "  0.17869353294372559,\n",
              "  0.18293282389640808,\n",
              "  0.19388853013515472,\n",
              "  0.18263396620750427,\n",
              "  0.18414460122585297,\n",
              "  0.20431561768054962,\n",
              "  0.19110886752605438,\n",
              "  0.19659961760044098,\n",
              "  0.17833496630191803,\n",
              "  0.18764926493167877,\n",
              "  0.1911911517381668,\n",
              "  0.1845633089542389,\n",
              "  0.1794806867837906,\n",
              "  0.1886497437953949,\n",
              "  0.20225279033184052,\n",
              "  0.17986740171909332,\n",
              "  0.18476758897304535,\n",
              "  0.19851703941822052,\n",
              "  0.19645102322101593,\n",
              "  0.17940248548984528,\n",
              "  0.18614377081394196,\n",
              "  0.19017767906188965,\n",
              "  0.18762877583503723,\n",
              "  0.19564189016819,\n",
              "  0.18761110305786133,\n",
              "  0.1856357753276825,\n",
              "  0.19778022170066833,\n",
              "  0.17211657762527466,\n",
              "  0.19573336839675903,\n",
              "  0.1860511302947998,\n",
              "  0.19377130270004272,\n",
              "  0.1858159750699997,\n",
              "  0.19697794318199158,\n",
              "  0.19891048967838287,\n",
              "  0.19417306780815125,\n",
              "  0.1787179559469223,\n",
              "  0.18128831684589386,\n",
              "  0.17386600375175476,\n",
              "  0.19221796095371246,\n",
              "  0.18901430070400238,\n",
              "  0.19570191204547882,\n",
              "  0.18829922378063202,\n",
              "  0.1791887730360031,\n",
              "  0.1832546442747116,\n",
              "  0.17572633922100067,\n",
              "  0.18833574652671814,\n",
              "  0.20443807542324066,\n",
              "  0.18699467182159424,\n",
              "  0.17174841463565826,\n",
              "  0.19476543366909027,\n",
              "  0.19258855283260345,\n",
              "  0.1805134117603302,\n",
              "  0.19247941672801971,\n",
              "  0.18527916073799133,\n",
              "  0.18045595288276672,\n",
              "  0.1976747214794159,\n",
              "  0.18535439670085907,\n",
              "  0.1809321939945221,\n",
              "  0.190414160490036,\n",
              "  0.19766244292259216,\n",
              "  0.185265451669693,\n",
              "  0.19473174214363098,\n",
              "  0.19191423058509827,\n",
              "  0.1945977658033371,\n",
              "  0.18223784863948822,\n",
              "  0.18539103865623474,\n",
              "  0.18419088423252106,\n",
              "  0.18922583758831024,\n",
              "  0.19556617736816406,\n",
              "  0.17893601953983307,\n",
              "  0.18660205602645874,\n",
              "  0.18906304240226746,\n",
              "  0.20148052275180817,\n",
              "  0.1918213963508606,\n",
              "  0.17680633068084717,\n",
              "  0.18733590841293335,\n",
              "  0.18320925533771515,\n",
              "  0.1932758092880249,\n",
              "  0.1856924295425415,\n",
              "  0.18766950070858002,\n",
              "  0.1911890059709549,\n",
              "  0.19425825774669647,\n",
              "  0.18354575335979462,\n",
              "  0.1919689178466797,\n",
              "  0.1861570179462433,\n",
              "  0.18338347971439362,\n",
              "  0.1985468566417694,\n",
              "  0.19015300273895264,\n",
              "  0.19105765223503113,\n",
              "  0.19269762933254242,\n",
              "  0.19172292947769165,\n",
              "  0.18918368220329285,\n",
              "  0.1883094608783722,\n",
              "  0.1812174916267395,\n",
              "  0.1855504959821701,\n",
              "  0.1868479996919632,\n",
              "  0.19076868891716003,\n",
              "  0.20093563199043274,\n",
              "  0.1906406581401825,\n",
              "  0.182294562458992,\n",
              "  0.17896322906017303,\n",
              "  0.1793905794620514,\n",
              "  0.1863316148519516,\n",
              "  0.18901784718036652,\n",
              "  0.19122004508972168,\n",
              "  0.1782037317752838,\n",
              "  0.18520502746105194,\n",
              "  0.1865011751651764,\n",
              "  0.1819022297859192,\n",
              "  0.19387657940387726,\n",
              "  0.18248552083969116,\n",
              "  0.18818548321723938,\n",
              "  0.17847444117069244,\n",
              "  0.18413414061069489,\n",
              "  0.1949663609266281,\n",
              "  0.18339264392852783,\n",
              "  0.19175133109092712,\n",
              "  0.1751960813999176,\n",
              "  0.1922786980867386,\n",
              "  0.19547997415065765,\n",
              "  0.17765116691589355,\n",
              "  0.20019477605819702,\n",
              "  0.17966483533382416,\n",
              "  0.17437227070331573,\n",
              "  0.19456404447555542,\n",
              "  0.1980430632829666,\n",
              "  0.18739058077335358,\n",
              "  0.17632733285427094,\n",
              "  0.17393635213375092,\n",
              "  0.1882217824459076,\n",
              "  0.19544386863708496,\n",
              "  0.16913959383964539,\n",
              "  0.1862376630306244,\n",
              "  0.18972909450531006,\n",
              "  0.1865222156047821,\n",
              "  0.1856350302696228,\n",
              "  0.18688076734542847,\n",
              "  0.1868610829114914,\n",
              "  0.17763082683086395,\n",
              "  0.18861833214759827,\n",
              "  0.1948835402727127,\n",
              "  0.1789286583662033,\n",
              "  0.17317914962768555,\n",
              "  0.17843014001846313,\n",
              "  0.18421438336372375,\n",
              "  0.18975360691547394,\n",
              "  0.18334074318408966,\n",
              "  0.1860741525888443,\n",
              "  0.19532127678394318,\n",
              "  0.1890583038330078,\n",
              "  0.20176135003566742,\n",
              "  0.1827918440103531,\n",
              "  0.16802020370960236,\n",
              "  0.18465344607830048,\n",
              "  0.18032433092594147,\n",
              "  0.184340700507164,\n",
              "  0.19239160418510437,\n",
              "  0.1736828237771988,\n",
              "  0.1888928860425949,\n",
              "  0.1799173653125763,\n",
              "  0.18656106293201447,\n",
              "  0.18700608611106873,\n",
              "  0.1792069673538208,\n",
              "  0.1704109162092209,\n",
              "  0.179960697889328,\n",
              "  0.1998341828584671,\n",
              "  0.18965986371040344,\n",
              "  0.1852789670228958,\n",
              "  0.19860491156578064,\n",
              "  0.18603339791297913,\n",
              "  0.18116137385368347,\n",
              "  0.1855003386735916,\n",
              "  0.18580450117588043,\n",
              "  0.1865566074848175,\n",
              "  0.18145833909511566,\n",
              "  0.1871384084224701,\n",
              "  0.1888592690229416,\n",
              "  0.1814420521259308,\n",
              "  0.1952333152294159,\n",
              "  0.19017188251018524,\n",
              "  0.17025083303451538,\n",
              "  0.18381792306900024,\n",
              "  0.1985936313867569,\n",
              "  0.19281262159347534,\n",
              "  0.19613134860992432,\n",
              "  0.17133866250514984,\n",
              "  0.17137394845485687,\n",
              "  0.18878790736198425,\n",
              "  0.1858024150133133,\n",
              "  0.20520463585853577,\n",
              "  0.20068345963954926,\n",
              "  0.17699873447418213,\n",
              "  0.201947420835495,\n",
              "  0.17708703875541687,\n",
              "  0.1961350440979004,\n",
              "  0.18271958827972412,\n",
              "  0.19744344055652618,\n",
              "  0.18457287549972534,\n",
              "  0.18926958739757538,\n",
              "  0.18044359982013702,\n",
              "  0.1750802844762802,\n",
              "  0.1829700917005539,\n",
              "  0.18005870282649994,\n",
              "  0.18881624937057495,\n",
              "  0.17451722919940948,\n",
              "  0.1845608651638031,\n",
              "  0.1850956678390503,\n",
              "  0.18387489020824432,\n",
              "  0.1892319917678833,\n",
              "  0.1844082921743393,\n",
              "  0.19664308428764343,\n",
              "  0.18923135101795197,\n",
              "  0.19037042558193207,\n",
              "  0.17629621922969818,\n",
              "  0.1816425323486328,\n",
              "  0.19374307990074158,\n",
              "  0.18200568854808807,\n",
              "  0.18230296671390533,\n",
              "  0.19114956259727478,\n",
              "  0.18820318579673767,\n",
              "  0.20191319286823273,\n",
              "  0.17271274328231812,\n",
              "  0.1975751370191574,\n",
              "  0.18101128935813904,\n",
              "  0.18361066281795502,\n",
              "  0.16970838606357574,\n",
              "  0.1921389400959015,\n",
              "  0.1824445128440857,\n",
              "  0.1867128163576126,\n",
              "  0.19064027070999146,\n",
              "  0.19174829125404358,\n",
              "  0.19079852104187012,\n",
              "  0.1898578554391861,\n",
              "  0.18400761485099792,\n",
              "  0.20032542943954468,\n",
              "  0.19658708572387695,\n",
              "  0.18441706895828247,\n",
              "  0.1752638965845108,\n",
              "  0.18911835551261902,\n",
              "  0.19677597284317017,\n",
              "  0.17507775127887726,\n",
              "  0.1976304054260254,\n",
              "  0.20279519259929657,\n",
              "  0.2046043872833252,\n",
              "  0.18656320869922638,\n",
              "  0.17167581617832184,\n",
              "  0.17716462910175323,\n",
              "  0.1845150589942932,\n",
              "  0.18816998600959778,\n",
              "  0.19495832920074463,\n",
              "  0.1843613237142563,\n",
              "  0.1874062567949295,\n",
              "  0.19772398471832275,\n",
              "  0.19012723863124847,\n",
              "  0.18587195873260498,\n",
              "  0.1815991997718811,\n",
              "  0.185626819729805,\n",
              "  0.17088811099529266,\n",
              "  0.18804478645324707,\n",
              "  0.18743830919265747,\n",
              "  0.19168324768543243,\n",
              "  0.18402987718582153,\n",
              "  0.192854106426239,\n",
              "  0.18932348489761353,\n",
              "  0.1973775327205658,\n",
              "  0.18778732419013977,\n",
              "  0.18804915249347687,\n",
              "  0.17993074655532837,\n",
              "  0.17852236330509186,\n",
              "  0.18589626252651215,\n",
              "  0.16981495916843414,\n",
              "  0.17868587374687195,\n",
              "  0.1886373609304428,\n",
              "  0.17305617034435272,\n",
              "  0.17985780537128448,\n",
              "  0.19155031442642212,\n",
              "  0.1818164885044098,\n",
              "  0.17950430512428284,\n",
              "  0.18639978766441345,\n",
              "  0.1851232796907425,\n",
              "  0.18802429735660553,\n",
              "  0.19238214194774628,\n",
              "  0.17615166306495667,\n",
              "  0.19086305797100067,\n",
              "  0.18969129025936127,\n",
              "  0.18246950209140778,\n",
              "  0.18303139507770538,\n",
              "  0.1818663477897644,\n",
              "  0.18062177300453186,\n",
              "  0.20378106832504272,\n",
              "  0.18507800996303558,\n",
              "  0.19176065921783447,\n",
              "  0.17252971231937408,\n",
              "  0.18909452855587006,\n",
              "  0.17725694179534912,\n",
              "  0.18327176570892334,\n",
              "  0.19214749336242676,\n",
              "  0.1910511553287506,\n",
              "  0.18749547004699707,\n",
              "  0.1663159430027008,\n",
              "  0.1886703222990036,\n",
              "  0.1793099045753479,\n",
              "  0.1819373518228531,\n",
              "  0.18448300659656525,\n",
              "  0.18273505568504333,\n",
              "  0.19000086188316345,\n",
              "  0.1990618109703064,\n",
              "  0.19010262191295624,\n",
              "  0.1707921028137207,\n",
              "  0.18214039504528046,\n",
              "  0.1888553500175476,\n",
              "  0.20724844932556152,\n",
              "  0.19692814350128174,\n",
              "  0.19265116751194,\n",
              "  0.1834079474210739,\n",
              "  0.1889752596616745,\n",
              "  0.1939520388841629,\n",
              "  0.18272973597049713,\n",
              "  0.17873282730579376,\n",
              "  0.17416569590568542,\n",
              "  0.181412011384964,\n",
              "  0.19815248250961304,\n",
              "  0.17755229771137238,\n",
              "  0.18811644613742828,\n",
              "  0.18191900849342346,\n",
              "  0.18926550447940826,\n",
              "  0.19041118025779724,\n",
              "  0.18891969323158264,\n",
              "  0.1788044571876526,\n",
              "  0.17389290034770966,\n",
              "  0.19559887051582336,\n",
              "  0.19234131276607513,\n",
              "  0.2023676335811615,\n",
              "  0.1832733154296875,\n",
              "  0.1940489113330841,\n",
              "  0.19344672560691833,\n",
              "  0.17819075286388397,\n",
              "  0.19076357781887054,\n",
              "  0.18693861365318298,\n",
              "  0.1827908605337143,\n",
              "  0.179572194814682,\n",
              "  0.2107337862253189,\n",
              "  0.17756438255310059,\n",
              "  0.1924651563167572,\n",
              "  0.19147232174873352,\n",
              "  0.18139396607875824,\n",
              "  0.18071313202381134,\n",
              "  0.1791967898607254,\n",
              "  0.20087766647338867,\n",
              "  0.17548568546772003,\n",
              "  0.19222837686538696,\n",
              "  0.18813499808311462,\n",
              "  0.18887488543987274,\n",
              "  0.17757073044776917,\n",
              "  0.1946481317281723,\n",
              "  0.1796826720237732,\n",
              "  0.17131495475769043,\n",
              "  0.2029721736907959,\n",
              "  0.1758727729320526,\n",
              "  0.19500960409641266,\n",
              "  0.18740883469581604,\n",
              "  0.18870072066783905,\n",
              "  0.19348226487636566,\n",
              "  0.19613772630691528,\n",
              "  0.18041379749774933,\n",
              "  0.18767647445201874,\n",
              "  0.2034720778465271,\n",
              "  0.1933661848306656,\n",
              "  0.18684960901737213,\n",
              "  0.18220920860767365,\n",
              "  0.18283198773860931,\n",
              "  0.1738266497850418,\n",
              "  0.18465353548526764,\n",
              "  0.19606661796569824,\n",
              "  0.1724964678287506,\n",
              "  0.1766122579574585,\n",
              "  0.16904637217521667,\n",
              "  0.18880102038383484,\n",
              "  0.1934107542037964,\n",
              "  0.18484117090702057],\n",
              " 'val_loss': [0.24453122913837433,\n",
              "  0.3625939190387726,\n",
              "  0.22276520729064941,\n",
              "  0.1330775022506714,\n",
              "  0.2533901035785675,\n",
              "  0.12532489001750946,\n",
              "  0.19112592935562134,\n",
              "  0.13225938379764557,\n",
              "  0.13822627067565918,\n",
              "  0.20199783146381378,\n",
              "  0.1182318925857544,\n",
              "  0.0943826362490654,\n",
              "  0.12053074687719345,\n",
              "  0.10626127570867538,\n",
              "  0.10381834954023361,\n",
              "  0.10493311285972595,\n",
              "  0.10020598769187927,\n",
              "  0.11766324192285538,\n",
              "  0.10072840005159378,\n",
              "  0.09090147167444229,\n",
              "  0.09229854494333267,\n",
              "  0.11214325577020645,\n",
              "  0.08968210220336914,\n",
              "  0.08881786465644836,\n",
              "  0.18076109886169434,\n",
              "  0.15452329814434052,\n",
              "  0.10513091832399368,\n",
              "  0.11880695074796677,\n",
              "  0.1068728044629097,\n",
              "  0.16878707706928253,\n",
              "  0.12111866474151611,\n",
              "  0.1055048331618309,\n",
              "  0.09176719188690186,\n",
              "  0.08940082788467407,\n",
              "  0.09179648011922836,\n",
              "  0.08828314393758774,\n",
              "  0.10189341753721237,\n",
              "  0.10976698249578476,\n",
              "  0.08522900938987732,\n",
              "  0.07760170102119446,\n",
              "  0.08847855776548386,\n",
              "  0.07317376881837845,\n",
              "  0.08734303712844849,\n",
              "  0.08627919107675552,\n",
              "  0.09432271867990494,\n",
              "  0.09648676961660385,\n",
              "  0.1108788326382637,\n",
              "  0.12966421246528625,\n",
              "  0.11544410139322281,\n",
              "  0.10377469658851624,\n",
              "  0.0770956501364708,\n",
              "  0.08991318196058273,\n",
              "  0.0963718593120575,\n",
              "  0.1201409175992012,\n",
              "  0.0896613672375679,\n",
              "  0.09490856528282166,\n",
              "  0.10244938731193542,\n",
              "  0.09053799510002136,\n",
              "  0.14249861240386963,\n",
              "  0.09717321395874023,\n",
              "  0.11314976215362549,\n",
              "  0.094325490295887,\n",
              "  0.12338230758905411,\n",
              "  0.12245479971170425,\n",
              "  0.10135721415281296,\n",
              "  0.09232603758573532,\n",
              "  0.12165370583534241,\n",
              "  0.11558067798614502,\n",
              "  0.07238012552261353,\n",
              "  0.10436848551034927,\n",
              "  0.08922114223241806,\n",
              "  0.09961052983999252,\n",
              "  0.08368923515081406,\n",
              "  0.07666075974702835,\n",
              "  0.09581301361322403,\n",
              "  0.11503976583480835,\n",
              "  0.14898599684238434,\n",
              "  0.13474419713020325,\n",
              "  0.11620572209358215,\n",
              "  0.11445081233978271,\n",
              "  0.08362064510583878,\n",
              "  0.08528152853250504,\n",
              "  0.07907255738973618,\n",
              "  0.08071838319301605,\n",
              "  0.07028871774673462,\n",
              "  0.07851210981607437,\n",
              "  0.07472717761993408,\n",
              "  0.07913204282522202,\n",
              "  0.07839148491621017,\n",
              "  0.07731159776449203,\n",
              "  0.07417465001344681,\n",
              "  0.1149817481637001,\n",
              "  0.10544886440038681,\n",
              "  0.07557391375303268,\n",
              "  0.06687156111001968,\n",
              "  0.07610437273979187,\n",
              "  0.09657981991767883,\n",
              "  0.08268306404352188,\n",
              "  0.07770383358001709,\n",
              "  0.07477455586194992,\n",
              "  0.10047460347414017,\n",
              "  0.06910724937915802,\n",
              "  0.07413996011018753,\n",
              "  0.07213687896728516,\n",
              "  0.09762784838676453,\n",
              "  0.06755653768777847,\n",
              "  0.07128765434026718,\n",
              "  0.0711669996380806,\n",
              "  0.08753854036331177,\n",
              "  0.07097195833921432,\n",
              "  0.08671156316995621,\n",
              "  0.06926126033067703,\n",
              "  0.08226605504751205,\n",
              "  0.07938704639673233,\n",
              "  0.08410382270812988,\n",
              "  0.08782309293746948,\n",
              "  0.062113624066114426,\n",
              "  0.09991473704576492,\n",
              "  0.07856696844100952,\n",
              "  0.07933714985847473,\n",
              "  0.10663618892431259,\n",
              "  0.08176476508378983,\n",
              "  0.12921930849552155,\n",
              "  0.10196498036384583,\n",
              "  0.0720759704709053,\n",
              "  0.0784989595413208,\n",
              "  0.07208587974309921,\n",
              "  0.09965909272432327,\n",
              "  0.14532114565372467,\n",
              "  0.08405119180679321,\n",
              "  0.09996485710144043,\n",
              "  0.13006596267223358,\n",
              "  0.0824475809931755,\n",
              "  0.08362007886171341,\n",
              "  0.07480645924806595,\n",
              "  0.0791376605629921,\n",
              "  0.0894799456000328,\n",
              "  0.10357251763343811,\n",
              "  0.0982990637421608,\n",
              "  0.12463721632957458,\n",
              "  0.12221849709749222,\n",
              "  0.08686817437410355,\n",
              "  0.09797845035791397,\n",
              "  0.08950402587652206,\n",
              "  0.0961880311369896,\n",
              "  0.07719685137271881,\n",
              "  0.1364607810974121,\n",
              "  0.09946902841329575,\n",
              "  0.0966540202498436,\n",
              "  0.09949704259634018,\n",
              "  0.09779445081949234,\n",
              "  0.08108904957771301,\n",
              "  0.08694443106651306,\n",
              "  0.16834096610546112,\n",
              "  0.1224348247051239,\n",
              "  0.08150917291641235,\n",
              "  0.07627318054437637,\n",
              "  0.07586590200662613,\n",
              "  0.07731109112501144,\n",
              "  0.09333715587854385,\n",
              "  0.0915093645453453,\n",
              "  0.09322283416986465,\n",
              "  0.08137182891368866,\n",
              "  0.11291906982660294,\n",
              "  0.08853482455015182,\n",
              "  0.09620111435651779,\n",
              "  0.08537733554840088,\n",
              "  0.0787176713347435,\n",
              "  0.08010593056678772,\n",
              "  0.0900559350848198,\n",
              "  0.13490130007266998,\n",
              "  0.07580950856208801,\n",
              "  0.08649512380361557,\n",
              "  0.09109419584274292,\n",
              "  0.08667179942131042,\n",
              "  0.1202426552772522,\n",
              "  0.08789718151092529,\n",
              "  0.10311434417963028,\n",
              "  0.09788823127746582,\n",
              "  0.1356297880411148,\n",
              "  0.07464029639959335,\n",
              "  0.07719340175390244,\n",
              "  0.08412506431341171,\n",
              "  0.08979848027229309,\n",
              "  0.08695799112319946,\n",
              "  0.07460270076990128,\n",
              "  0.08108935505151749,\n",
              "  0.17331697046756744,\n",
              "  0.13059084117412567,\n",
              "  0.08445430546998978,\n",
              "  0.09932377189397812,\n",
              "  0.07833395153284073,\n",
              "  0.09865603595972061,\n",
              "  0.12918300926685333,\n",
              "  0.10460198670625687,\n",
              "  0.10452020168304443,\n",
              "  0.16140520572662354,\n",
              "  0.14283354580402374,\n",
              "  0.11267122626304626,\n",
              "  0.11894180625677109,\n",
              "  0.09977709501981735,\n",
              "  0.09125721454620361,\n",
              "  0.09549485892057419,\n",
              "  0.1472208946943283,\n",
              "  0.12021145969629288,\n",
              "  0.14642135798931122,\n",
              "  0.10304649919271469,\n",
              "  0.10639137029647827,\n",
              "  0.10151191800832748,\n",
              "  0.13742505013942719,\n",
              "  0.10960463434457779,\n",
              "  0.10874307155609131,\n",
              "  0.14997123181819916,\n",
              "  0.13010865449905396,\n",
              "  0.15189512073993683,\n",
              "  0.12960763275623322,\n",
              "  0.10231325775384903,\n",
              "  0.10664713382720947,\n",
              "  0.10061316937208176,\n",
              "  0.11916971206665039,\n",
              "  0.10299541801214218,\n",
              "  0.10122624039649963,\n",
              "  0.09028994292020798,\n",
              "  0.08563414961099625,\n",
              "  0.0943867638707161,\n",
              "  0.11226153373718262,\n",
              "  0.1491088718175888,\n",
              "  0.15104611217975616,\n",
              "  0.09142681211233139,\n",
              "  0.11052606254816055,\n",
              "  0.12780460715293884,\n",
              "  0.10371148586273193,\n",
              "  0.07979222387075424,\n",
              "  0.08047813177108765,\n",
              "  0.09959107637405396,\n",
              "  0.08620000630617142,\n",
              "  0.08250955492258072,\n",
              "  0.1088065505027771,\n",
              "  0.09022017568349838,\n",
              "  0.09353730827569962,\n",
              "  0.09697654098272324,\n",
              "  0.10603895038366318,\n",
              "  0.09056359529495239,\n",
              "  0.14253729581832886,\n",
              "  0.11412546038627625,\n",
              "  0.11303896456956863,\n",
              "  0.10360055416822433,\n",
              "  0.09950347989797592,\n",
              "  0.12294230610132217,\n",
              "  0.15192288160324097,\n",
              "  0.13435013592243195,\n",
              "  0.0895083025097847,\n",
              "  0.14983807504177094,\n",
              "  0.11145520210266113,\n",
              "  0.11012204736471176,\n",
              "  0.1486005336046219,\n",
              "  0.11046243458986282,\n",
              "  0.0925869420170784,\n",
              "  0.09087660163640976,\n",
              "  0.15097706019878387,\n",
              "  0.14609025418758392,\n",
              "  0.09064333885908127,\n",
              "  0.06998401135206223,\n",
              "  0.07349810004234314,\n",
              "  0.07972986996173859,\n",
              "  0.12163447588682175,\n",
              "  0.09607461839914322,\n",
              "  0.1307615488767624,\n",
              "  0.10455041378736496,\n",
              "  0.08827602118253708,\n",
              "  0.1083226278424263,\n",
              "  0.08068322390317917,\n",
              "  0.09622181206941605,\n",
              "  0.10410293191671371,\n",
              "  0.09229419380426407,\n",
              "  0.08728361874818802,\n",
              "  0.08181634545326233,\n",
              "  0.09132007509469986,\n",
              "  0.09116831421852112,\n",
              "  0.08423560857772827,\n",
              "  0.09152107685804367,\n",
              "  0.09621620923280716,\n",
              "  0.08417942374944687,\n",
              "  0.0910024419426918,\n",
              "  0.07060717791318893,\n",
              "  0.06654419004917145,\n",
              "  0.10855281352996826,\n",
              "  0.13075025379657745,\n",
              "  0.1332293003797531,\n",
              "  0.09418216347694397,\n",
              "  0.13627780973911285,\n",
              "  0.08853300660848618,\n",
              "  0.13270635902881622,\n",
              "  0.13892200589179993,\n",
              "  0.14049799740314484,\n",
              "  0.10993573069572449,\n",
              "  0.11743447929620743,\n",
              "  0.09854810684919357,\n",
              "  0.11934661865234375,\n",
              "  0.1017656922340393,\n",
              "  0.09639019519090652,\n",
              "  0.1040254756808281,\n",
              "  0.11945909261703491,\n",
              "  0.17285941541194916,\n",
              "  0.1287008672952652,\n",
              "  0.11103478819131851,\n",
              "  0.12517745792865753,\n",
              "  0.1306057572364807,\n",
              "  0.14032600820064545,\n",
              "  0.15156467258930206,\n",
              "  0.1179603636264801,\n",
              "  0.12454185634851456,\n",
              "  0.12409510463476181,\n",
              "  0.1558866947889328,\n",
              "  0.10956304520368576,\n",
              "  0.11013389378786087,\n",
              "  0.1163514256477356,\n",
              "  0.11071572452783585,\n",
              "  0.10194651037454605,\n",
              "  0.10118934512138367,\n",
              "  0.08069229871034622,\n",
              "  0.08658456802368164,\n",
              "  0.08520627021789551,\n",
              "  0.09954040497541428,\n",
              "  0.0971842035651207,\n",
              "  0.09351006150245667,\n",
              "  0.09972607344388962,\n",
              "  0.0971112921833992,\n",
              "  0.09106940031051636,\n",
              "  0.09027532488107681,\n",
              "  0.11270099878311157,\n",
              "  0.11031394451856613,\n",
              "  0.1839994192123413,\n",
              "  0.11713489145040512,\n",
              "  0.10803791135549545,\n",
              "  0.17191481590270996,\n",
              "  0.1301443725824356,\n",
              "  0.1250130832195282,\n",
              "  0.13556748628616333,\n",
              "  0.13361704349517822,\n",
              "  0.10928821563720703,\n",
              "  0.12035489827394485,\n",
              "  0.12337996810674667,\n",
              "  0.10979665070772171,\n",
              "  0.1256893426179886,\n",
              "  0.13955192267894745,\n",
              "  0.14548160135746002,\n",
              "  0.16118277609348297,\n",
              "  0.174142524600029,\n",
              "  0.14353413879871368,\n",
              "  0.12266632169485092,\n",
              "  0.12455233931541443,\n",
              "  0.10332456976175308,\n",
              "  0.1278742402791977,\n",
              "  0.10855758935213089,\n",
              "  0.12507455050945282,\n",
              "  0.1304982453584671,\n",
              "  0.10241543501615524,\n",
              "  0.08847778290510178,\n",
              "  0.09717220067977905,\n",
              "  0.09738745540380478,\n",
              "  0.09735389798879623,\n",
              "  0.16771627962589264,\n",
              "  0.15614594519138336,\n",
              "  0.11209938675165176,\n",
              "  0.10371432453393936,\n",
              "  0.11620620638132095,\n",
              "  0.11764323711395264,\n",
              "  0.11270693689584732,\n",
              "  0.11521685123443604,\n",
              "  0.12366431206464767,\n",
              "  0.11949724704027176,\n",
              "  0.11896499246358871,\n",
              "  0.11136743426322937,\n",
              "  0.1355777084827423,\n",
              "  0.12262434512376785,\n",
              "  0.10106799751520157,\n",
              "  0.09692806750535965,\n",
              "  0.1302824169397354,\n",
              "  0.11353424936532974,\n",
              "  0.11027180403470993,\n",
              "  0.10478463023900986,\n",
              "  0.10326620191335678,\n",
              "  0.1290374994277954,\n",
              "  0.1125928983092308,\n",
              "  0.09232112765312195,\n",
              "  0.08621826767921448,\n",
              "  0.08159304410219193,\n",
              "  0.12341848760843277,\n",
              "  0.10585270076990128,\n",
              "  0.10518189519643784,\n",
              "  0.11108133941888809,\n",
              "  0.14147920906543732,\n",
              "  0.13229399919509888,\n",
              "  0.09391849488019943,\n",
              "  0.12200865894556046,\n",
              "  0.10282810777425766,\n",
              "  0.10011477023363113,\n",
              "  0.1333913654088974,\n",
              "  0.11082452535629272,\n",
              "  0.17259638011455536,\n",
              "  0.11161934584379196,\n",
              "  0.1233537420630455,\n",
              "  0.09150023013353348,\n",
              "  0.09570201486349106,\n",
              "  0.09176614135503769,\n",
              "  0.11173560470342636,\n",
              "  0.125614196062088,\n",
              "  0.10629557818174362,\n",
              "  0.08959981054067612,\n",
              "  0.10523421317338943,\n",
              "  0.09982899576425552,\n",
              "  0.09478592872619629,\n",
              "  0.09243907779455185,\n",
              "  0.09323537349700928,\n",
              "  0.09081775695085526,\n",
              "  0.07401164621114731,\n",
              "  0.09725984930992126,\n",
              "  0.10365531593561172,\n",
              "  0.09526688605546951,\n",
              "  0.1304568499326706,\n",
              "  0.097322978079319,\n",
              "  0.09728582948446274,\n",
              "  0.09445822983980179,\n",
              "  0.08480945974588394,\n",
              "  0.09277846664190292,\n",
              "  0.09283813089132309,\n",
              "  0.08737169951200485,\n",
              "  0.10335316509008408,\n",
              "  0.0856335237622261,\n",
              "  0.10706207901239395,\n",
              "  0.0824560597538948,\n",
              "  0.10223367065191269,\n",
              "  0.11860060691833496,\n",
              "  0.08703792095184326,\n",
              "  0.09323006123304367,\n",
              "  0.0888952910900116,\n",
              "  0.08961278945207596,\n",
              "  0.09376076608896255,\n",
              "  0.09715017676353455,\n",
              "  0.07774441689252853,\n",
              "  0.09973910450935364,\n",
              "  0.13823942840099335,\n",
              "  0.07979921251535416,\n",
              "  0.11085327714681625,\n",
              "  0.10034968703985214,\n",
              "  0.10426214337348938,\n",
              "  0.07986868172883987,\n",
              "  0.09331619739532471,\n",
              "  0.08994865417480469,\n",
              "  0.12910474836826324,\n",
              "  0.09760802984237671,\n",
              "  0.09460935741662979,\n",
              "  0.09761565178632736,\n",
              "  0.11227693408727646,\n",
              "  0.09523609280586243,\n",
              "  0.09166637808084488,\n",
              "  0.08271606266498566,\n",
              "  0.08498113602399826,\n",
              "  0.10544242709875107,\n",
              "  0.09933701157569885,\n",
              "  0.08601749688386917,\n",
              "  0.08779042959213257,\n",
              "  0.08366068452596664,\n",
              "  0.09049659967422485,\n",
              "  0.12100280076265335,\n",
              "  0.08801668882369995,\n",
              "  0.09182348102331161,\n",
              "  0.10088389366865158,\n",
              "  0.09296166151762009,\n",
              "  0.12725131213665009,\n",
              "  0.10480622202157974,\n",
              "  0.11875095218420029,\n",
              "  0.11281422525644302,\n",
              "  0.11300472170114517,\n",
              "  0.11364126205444336,\n",
              "  0.10354643315076828,\n",
              "  0.10674241185188293,\n",
              "  0.1038709506392479,\n",
              "  0.10501187294721603,\n",
              "  0.09012503176927567,\n",
              "  0.0884382352232933,\n",
              "  0.11473303288221359,\n",
              "  0.11437404900789261,\n",
              "  0.1326606571674347,\n",
              "  0.1091536283493042,\n",
              "  0.16440154612064362,\n",
              "  0.10841048508882523,\n",
              "  0.11562070995569229,\n",
              "  0.11967772245407104,\n",
              "  0.10336188226938248,\n",
              "  0.09448874741792679,\n",
              "  0.09152040630578995,\n",
              "  0.10271745920181274,\n",
              "  0.09055181592702866,\n",
              "  0.09588131308555603,\n",
              "  0.12416095286607742,\n",
              "  0.11309940367937088,\n",
              "  0.13273178040981293,\n",
              "  0.12073803693056107],\n",
              " 'val_mse': [0.24453122913837433,\n",
              "  0.3625939190387726,\n",
              "  0.22276520729064941,\n",
              "  0.1330775022506714,\n",
              "  0.2533901035785675,\n",
              "  0.12532489001750946,\n",
              "  0.19112592935562134,\n",
              "  0.13225938379764557,\n",
              "  0.13822627067565918,\n",
              "  0.20199783146381378,\n",
              "  0.1182318925857544,\n",
              "  0.0943826362490654,\n",
              "  0.12053074687719345,\n",
              "  0.10626127570867538,\n",
              "  0.10381834954023361,\n",
              "  0.10493311285972595,\n",
              "  0.10020598769187927,\n",
              "  0.11766324192285538,\n",
              "  0.10072840005159378,\n",
              "  0.09090147167444229,\n",
              "  0.09229854494333267,\n",
              "  0.11214325577020645,\n",
              "  0.08968210220336914,\n",
              "  0.08881786465644836,\n",
              "  0.18076109886169434,\n",
              "  0.15452329814434052,\n",
              "  0.10513091832399368,\n",
              "  0.11880695074796677,\n",
              "  0.1068728044629097,\n",
              "  0.16878707706928253,\n",
              "  0.12111866474151611,\n",
              "  0.1055048331618309,\n",
              "  0.09176719188690186,\n",
              "  0.08940082788467407,\n",
              "  0.09179648011922836,\n",
              "  0.08828314393758774,\n",
              "  0.10189341753721237,\n",
              "  0.10976698249578476,\n",
              "  0.08522900938987732,\n",
              "  0.07760170102119446,\n",
              "  0.08847855776548386,\n",
              "  0.07317376881837845,\n",
              "  0.08734303712844849,\n",
              "  0.08627919107675552,\n",
              "  0.09432271867990494,\n",
              "  0.09648676961660385,\n",
              "  0.1108788326382637,\n",
              "  0.12966421246528625,\n",
              "  0.11544410139322281,\n",
              "  0.10377469658851624,\n",
              "  0.0770956501364708,\n",
              "  0.08991318196058273,\n",
              "  0.0963718593120575,\n",
              "  0.1201409175992012,\n",
              "  0.0896613672375679,\n",
              "  0.09490856528282166,\n",
              "  0.10244938731193542,\n",
              "  0.09053799510002136,\n",
              "  0.14249861240386963,\n",
              "  0.09717321395874023,\n",
              "  0.11314976215362549,\n",
              "  0.094325490295887,\n",
              "  0.12338230758905411,\n",
              "  0.12245479971170425,\n",
              "  0.10135721415281296,\n",
              "  0.09232603758573532,\n",
              "  0.12165370583534241,\n",
              "  0.11558067798614502,\n",
              "  0.07238012552261353,\n",
              "  0.10436848551034927,\n",
              "  0.08922114223241806,\n",
              "  0.09961052983999252,\n",
              "  0.08368923515081406,\n",
              "  0.07666075974702835,\n",
              "  0.09581301361322403,\n",
              "  0.11503976583480835,\n",
              "  0.14898599684238434,\n",
              "  0.13474419713020325,\n",
              "  0.11620572209358215,\n",
              "  0.11445081233978271,\n",
              "  0.08362064510583878,\n",
              "  0.08528152853250504,\n",
              "  0.07907255738973618,\n",
              "  0.08071838319301605,\n",
              "  0.07028871774673462,\n",
              "  0.07851210981607437,\n",
              "  0.07472717761993408,\n",
              "  0.07913204282522202,\n",
              "  0.07839148491621017,\n",
              "  0.07731159776449203,\n",
              "  0.07417465001344681,\n",
              "  0.1149817481637001,\n",
              "  0.10544886440038681,\n",
              "  0.07557391375303268,\n",
              "  0.06687156111001968,\n",
              "  0.07610437273979187,\n",
              "  0.09657981991767883,\n",
              "  0.08268306404352188,\n",
              "  0.07770383358001709,\n",
              "  0.07477455586194992,\n",
              "  0.10047460347414017,\n",
              "  0.06910724937915802,\n",
              "  0.07413996011018753,\n",
              "  0.07213687896728516,\n",
              "  0.09762784838676453,\n",
              "  0.06755653768777847,\n",
              "  0.07128765434026718,\n",
              "  0.0711669996380806,\n",
              "  0.08753854036331177,\n",
              "  0.07097195833921432,\n",
              "  0.08671156316995621,\n",
              "  0.06926126033067703,\n",
              "  0.08226605504751205,\n",
              "  0.07938704639673233,\n",
              "  0.08410382270812988,\n",
              "  0.08782309293746948,\n",
              "  0.062113624066114426,\n",
              "  0.09991473704576492,\n",
              "  0.07856696844100952,\n",
              "  0.07933714985847473,\n",
              "  0.10663618892431259,\n",
              "  0.08176476508378983,\n",
              "  0.12921930849552155,\n",
              "  0.10196498036384583,\n",
              "  0.0720759704709053,\n",
              "  0.0784989595413208,\n",
              "  0.07208587974309921,\n",
              "  0.09965909272432327,\n",
              "  0.14532114565372467,\n",
              "  0.08405119180679321,\n",
              "  0.09996485710144043,\n",
              "  0.13006596267223358,\n",
              "  0.0824475809931755,\n",
              "  0.08362007886171341,\n",
              "  0.07480645924806595,\n",
              "  0.0791376605629921,\n",
              "  0.0894799456000328,\n",
              "  0.10357251763343811,\n",
              "  0.0982990637421608,\n",
              "  0.12463721632957458,\n",
              "  0.12221849709749222,\n",
              "  0.08686817437410355,\n",
              "  0.09797845035791397,\n",
              "  0.08950402587652206,\n",
              "  0.0961880311369896,\n",
              "  0.07719685137271881,\n",
              "  0.1364607810974121,\n",
              "  0.09946902841329575,\n",
              "  0.0966540202498436,\n",
              "  0.09949704259634018,\n",
              "  0.09779445081949234,\n",
              "  0.08108904957771301,\n",
              "  0.08694443106651306,\n",
              "  0.16834096610546112,\n",
              "  0.1224348247051239,\n",
              "  0.08150917291641235,\n",
              "  0.07627318054437637,\n",
              "  0.07586590200662613,\n",
              "  0.07731109112501144,\n",
              "  0.09333715587854385,\n",
              "  0.0915093645453453,\n",
              "  0.09322283416986465,\n",
              "  0.08137182891368866,\n",
              "  0.11291906982660294,\n",
              "  0.08853482455015182,\n",
              "  0.09620111435651779,\n",
              "  0.08537733554840088,\n",
              "  0.0787176713347435,\n",
              "  0.08010593056678772,\n",
              "  0.0900559350848198,\n",
              "  0.13490130007266998,\n",
              "  0.07580950856208801,\n",
              "  0.08649512380361557,\n",
              "  0.09109419584274292,\n",
              "  0.08667179942131042,\n",
              "  0.1202426552772522,\n",
              "  0.08789718151092529,\n",
              "  0.10311434417963028,\n",
              "  0.09788823127746582,\n",
              "  0.1356297880411148,\n",
              "  0.07464029639959335,\n",
              "  0.07719340175390244,\n",
              "  0.08412506431341171,\n",
              "  0.08979848027229309,\n",
              "  0.08695799112319946,\n",
              "  0.07460270076990128,\n",
              "  0.08108935505151749,\n",
              "  0.17331697046756744,\n",
              "  0.13059084117412567,\n",
              "  0.08445430546998978,\n",
              "  0.09932377189397812,\n",
              "  0.07833395153284073,\n",
              "  0.09865603595972061,\n",
              "  0.12918300926685333,\n",
              "  0.10460198670625687,\n",
              "  0.10452020168304443,\n",
              "  0.16140520572662354,\n",
              "  0.14283354580402374,\n",
              "  0.11267122626304626,\n",
              "  0.11894180625677109,\n",
              "  0.09977709501981735,\n",
              "  0.09125721454620361,\n",
              "  0.09549485892057419,\n",
              "  0.1472208946943283,\n",
              "  0.12021145969629288,\n",
              "  0.14642135798931122,\n",
              "  0.10304649919271469,\n",
              "  0.10639137029647827,\n",
              "  0.10151191800832748,\n",
              "  0.13742505013942719,\n",
              "  0.10960463434457779,\n",
              "  0.10874307155609131,\n",
              "  0.14997123181819916,\n",
              "  0.13010865449905396,\n",
              "  0.15189512073993683,\n",
              "  0.12960763275623322,\n",
              "  0.10231325775384903,\n",
              "  0.10664713382720947,\n",
              "  0.10061316937208176,\n",
              "  0.11916971206665039,\n",
              "  0.10299541801214218,\n",
              "  0.10122624039649963,\n",
              "  0.09028994292020798,\n",
              "  0.08563414961099625,\n",
              "  0.0943867638707161,\n",
              "  0.11226153373718262,\n",
              "  0.1491088718175888,\n",
              "  0.15104611217975616,\n",
              "  0.09142681211233139,\n",
              "  0.11052606254816055,\n",
              "  0.12780460715293884,\n",
              "  0.10371148586273193,\n",
              "  0.07979222387075424,\n",
              "  0.08047813177108765,\n",
              "  0.09959107637405396,\n",
              "  0.08620000630617142,\n",
              "  0.08250955492258072,\n",
              "  0.1088065505027771,\n",
              "  0.09022017568349838,\n",
              "  0.09353730827569962,\n",
              "  0.09697654098272324,\n",
              "  0.10603895038366318,\n",
              "  0.09056359529495239,\n",
              "  0.14253729581832886,\n",
              "  0.11412546038627625,\n",
              "  0.11303896456956863,\n",
              "  0.10360055416822433,\n",
              "  0.09950347989797592,\n",
              "  0.12294230610132217,\n",
              "  0.15192288160324097,\n",
              "  0.13435013592243195,\n",
              "  0.0895083025097847,\n",
              "  0.14983807504177094,\n",
              "  0.11145520210266113,\n",
              "  0.11012204736471176,\n",
              "  0.1486005336046219,\n",
              "  0.11046243458986282,\n",
              "  0.0925869420170784,\n",
              "  0.09087660163640976,\n",
              "  0.15097706019878387,\n",
              "  0.14609025418758392,\n",
              "  0.09064333885908127,\n",
              "  0.06998401135206223,\n",
              "  0.07349810004234314,\n",
              "  0.07972986996173859,\n",
              "  0.12163447588682175,\n",
              "  0.09607461839914322,\n",
              "  0.1307615488767624,\n",
              "  0.10455041378736496,\n",
              "  0.08827602118253708,\n",
              "  0.1083226278424263,\n",
              "  0.08068322390317917,\n",
              "  0.09622181206941605,\n",
              "  0.10410293191671371,\n",
              "  0.09229419380426407,\n",
              "  0.08728361874818802,\n",
              "  0.08181634545326233,\n",
              "  0.09132007509469986,\n",
              "  0.09116831421852112,\n",
              "  0.08423560857772827,\n",
              "  0.09152107685804367,\n",
              "  0.09621620923280716,\n",
              "  0.08417942374944687,\n",
              "  0.0910024419426918,\n",
              "  0.07060717791318893,\n",
              "  0.06654419004917145,\n",
              "  0.10855281352996826,\n",
              "  0.13075025379657745,\n",
              "  0.1332293003797531,\n",
              "  0.09418216347694397,\n",
              "  0.13627780973911285,\n",
              "  0.08853300660848618,\n",
              "  0.13270635902881622,\n",
              "  0.13892200589179993,\n",
              "  0.14049799740314484,\n",
              "  0.10993573069572449,\n",
              "  0.11743447929620743,\n",
              "  0.09854810684919357,\n",
              "  0.11934661865234375,\n",
              "  0.1017656922340393,\n",
              "  0.09639019519090652,\n",
              "  0.1040254756808281,\n",
              "  0.11945909261703491,\n",
              "  0.17285941541194916,\n",
              "  0.1287008672952652,\n",
              "  0.11103478819131851,\n",
              "  0.12517745792865753,\n",
              "  0.1306057572364807,\n",
              "  0.14032600820064545,\n",
              "  0.15156467258930206,\n",
              "  0.1179603636264801,\n",
              "  0.12454185634851456,\n",
              "  0.12409510463476181,\n",
              "  0.1558866947889328,\n",
              "  0.10956304520368576,\n",
              "  0.11013389378786087,\n",
              "  0.1163514256477356,\n",
              "  0.11071572452783585,\n",
              "  0.10194651037454605,\n",
              "  0.10118934512138367,\n",
              "  0.08069229871034622,\n",
              "  0.08658456802368164,\n",
              "  0.08520627021789551,\n",
              "  0.09954040497541428,\n",
              "  0.0971842035651207,\n",
              "  0.09351006150245667,\n",
              "  0.09972607344388962,\n",
              "  0.0971112921833992,\n",
              "  0.09106940031051636,\n",
              "  0.09027532488107681,\n",
              "  0.11270099878311157,\n",
              "  0.11031394451856613,\n",
              "  0.1839994192123413,\n",
              "  0.11713489145040512,\n",
              "  0.10803791135549545,\n",
              "  0.17191481590270996,\n",
              "  0.1301443725824356,\n",
              "  0.1250130832195282,\n",
              "  0.13556748628616333,\n",
              "  0.13361704349517822,\n",
              "  0.10928821563720703,\n",
              "  0.12035489827394485,\n",
              "  0.12337996810674667,\n",
              "  0.10979665070772171,\n",
              "  0.1256893426179886,\n",
              "  0.13955192267894745,\n",
              "  0.14548160135746002,\n",
              "  0.16118277609348297,\n",
              "  0.174142524600029,\n",
              "  0.14353413879871368,\n",
              "  0.12266632169485092,\n",
              "  0.12455233931541443,\n",
              "  0.10332456976175308,\n",
              "  0.1278742402791977,\n",
              "  0.10855758935213089,\n",
              "  0.12507455050945282,\n",
              "  0.1304982453584671,\n",
              "  0.10241543501615524,\n",
              "  0.08847778290510178,\n",
              "  0.09717220067977905,\n",
              "  0.09738745540380478,\n",
              "  0.09735389798879623,\n",
              "  0.16771627962589264,\n",
              "  0.15614594519138336,\n",
              "  0.11209938675165176,\n",
              "  0.10371432453393936,\n",
              "  0.11620620638132095,\n",
              "  0.11764323711395264,\n",
              "  0.11270693689584732,\n",
              "  0.11521685123443604,\n",
              "  0.12366431206464767,\n",
              "  0.11949724704027176,\n",
              "  0.11896499246358871,\n",
              "  0.11136743426322937,\n",
              "  0.1355777084827423,\n",
              "  0.12262434512376785,\n",
              "  0.10106799751520157,\n",
              "  0.09692806750535965,\n",
              "  0.1302824169397354,\n",
              "  0.11353424936532974,\n",
              "  0.11027180403470993,\n",
              "  0.10478463023900986,\n",
              "  0.10326620191335678,\n",
              "  0.1290374994277954,\n",
              "  0.1125928983092308,\n",
              "  0.09232112765312195,\n",
              "  0.08621826767921448,\n",
              "  0.08159304410219193,\n",
              "  0.12341848760843277,\n",
              "  0.10585270076990128,\n",
              "  0.10518189519643784,\n",
              "  0.11108133941888809,\n",
              "  0.14147920906543732,\n",
              "  0.13229399919509888,\n",
              "  0.09391849488019943,\n",
              "  0.12200865894556046,\n",
              "  0.10282810777425766,\n",
              "  0.10011477023363113,\n",
              "  0.1333913654088974,\n",
              "  0.11082452535629272,\n",
              "  0.17259638011455536,\n",
              "  0.11161934584379196,\n",
              "  0.1233537420630455,\n",
              "  0.09150023013353348,\n",
              "  0.09570201486349106,\n",
              "  0.09176614135503769,\n",
              "  0.11173560470342636,\n",
              "  0.125614196062088,\n",
              "  0.10629557818174362,\n",
              "  0.08959981054067612,\n",
              "  0.10523421317338943,\n",
              "  0.09982899576425552,\n",
              "  0.09478592872619629,\n",
              "  0.09243907779455185,\n",
              "  0.09323537349700928,\n",
              "  0.09081775695085526,\n",
              "  0.07401164621114731,\n",
              "  0.09725984930992126,\n",
              "  0.10365531593561172,\n",
              "  0.09526688605546951,\n",
              "  0.1304568499326706,\n",
              "  0.097322978079319,\n",
              "  0.09728582948446274,\n",
              "  0.09445822983980179,\n",
              "  0.08480945974588394,\n",
              "  0.09277846664190292,\n",
              "  0.09283813089132309,\n",
              "  0.08737169951200485,\n",
              "  0.10335316509008408,\n",
              "  0.0856335237622261,\n",
              "  0.10706207901239395,\n",
              "  0.0824560597538948,\n",
              "  0.10223367065191269,\n",
              "  0.11860060691833496,\n",
              "  0.08703792095184326,\n",
              "  0.09323006123304367,\n",
              "  0.0888952910900116,\n",
              "  0.08961278945207596,\n",
              "  0.09376076608896255,\n",
              "  0.09715017676353455,\n",
              "  0.07774441689252853,\n",
              "  0.09973910450935364,\n",
              "  0.13823942840099335,\n",
              "  0.07979921251535416,\n",
              "  0.11085327714681625,\n",
              "  0.10034968703985214,\n",
              "  0.10426214337348938,\n",
              "  0.07986868172883987,\n",
              "  0.09331619739532471,\n",
              "  0.08994865417480469,\n",
              "  0.12910474836826324,\n",
              "  0.09760802984237671,\n",
              "  0.09460935741662979,\n",
              "  0.09761565178632736,\n",
              "  0.11227693408727646,\n",
              "  0.09523609280586243,\n",
              "  0.09166637808084488,\n",
              "  0.08271606266498566,\n",
              "  0.08498113602399826,\n",
              "  0.10544242709875107,\n",
              "  0.09933701157569885,\n",
              "  0.08601749688386917,\n",
              "  0.08779042959213257,\n",
              "  0.08366068452596664,\n",
              "  0.09049659967422485,\n",
              "  0.12100280076265335,\n",
              "  0.08801668882369995,\n",
              "  0.09182348102331161,\n",
              "  0.10088389366865158,\n",
              "  0.09296166151762009,\n",
              "  0.12725131213665009,\n",
              "  0.10480622202157974,\n",
              "  0.11875095218420029,\n",
              "  0.11281422525644302,\n",
              "  0.11300472170114517,\n",
              "  0.11364126205444336,\n",
              "  0.10354643315076828,\n",
              "  0.10674241185188293,\n",
              "  0.1038709506392479,\n",
              "  0.10501187294721603,\n",
              "  0.09012503176927567,\n",
              "  0.0884382352232933,\n",
              "  0.11473303288221359,\n",
              "  0.11437404900789261,\n",
              "  0.1326606571674347,\n",
              "  0.1091536283493042,\n",
              "  0.16440154612064362,\n",
              "  0.10841048508882523,\n",
              "  0.11562070995569229,\n",
              "  0.11967772245407104,\n",
              "  0.10336188226938248,\n",
              "  0.09448874741792679,\n",
              "  0.09152040630578995,\n",
              "  0.10271745920181274,\n",
              "  0.09055181592702866,\n",
              "  0.09588131308555603,\n",
              "  0.12416095286607742,\n",
              "  0.11309940367937088,\n",
              "  0.13273178040981293,\n",
              "  0.12073803693056107]}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "b3d23258-c8e0-4827-ed85-29b6815eff99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/2Class_regression_freeze_500.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/2Class_regression_freeze_500.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "af6f98b0-fa22-42ff-ca30-f3d949e0d973"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0042d833-68c8-44bb-a1de-7ce054065781\", \"2Class_regression_freeze_500.h5\", 16623464)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}